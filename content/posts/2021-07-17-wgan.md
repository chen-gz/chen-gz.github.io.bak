---
title: Wasserstein GAN 1
author: Guangzong Chen
date: 2021-07-17 10:36:00 -0400
categories: [ML]
tags: [GAN]
math: true
mermaid: true
---

There are two paper to introduce Wasserstein GAN.
[Towards Principled Methods for Training Generative Adversarial Networks](https://arxiv.org/abs/1701.04862) and [Wasserstein GAN](https://arxiv.org/abs/1701.07875).

First one present the issue in the GAN. It prove several theorem to get understanding why GAN is unstable and why it is hard to train.
There are a lot resources in the internet. This post will take ever theorem and give intuitive. Intuition means, it is not rigorous prove like the paper did.

The general idea is cross entropy cost function will not give enough gradient to train the network.

#### Theorem 2.1 (smooth optimal separate function for two distribution)
If two distributions $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ have support contained on two disjoint compact subset $\mathcal{M}$ and $\mathcal{P}$ respectively, then there is a smooth optimal decimator $D^{*}: \mathcal{X} \rightarrow[0,1]$ that has accuracy 1 and $\nabla_{x} D^{*}(x)=0$ for all $x \in \mathcal{M} \cup \mathcal{P} .$

This theorem is very obvious. If there are two disjoin distributions, then there is some interval between these two distributions. We can map this distribution to another space, to make the interval larger. In mapped space, we can easily find a smooth optimal discriminator. Then we can map this optimal discriminator back. The prove in the paper, is divide interval between two distributions tow 3 parts. Then can find a smooth function in the middle part. That means there is a smooth optimal discriminator to separate these distributions.

#### Definition 2.1 (transversally)
We first need to recall the definition of transversally. Let $\mathcal{M}$ and $\mathcal{P}$ be two boundary free regular submanifolds of $\mathcal{F}$, which in our cases will simply be $\mathcal{F}=\mathbb{R}^{d}$. Let $x \in \mathcal{M} \cap \mathcal{P}$ be an intersection point of the two manifolds. We say that $\mathcal{M}$ and $\mathcal{P}$ intersect transversally in $x$ if $T_{x} \mathcal{M}+T_{x} \mathcal{P}=T_{x} \mathcal{F}$, where $T_{x} \mathcal{M}$ means the tangent space of $\mathcal{M}$ around $x$.

This is the definition of transversally. This is onsite of tangent space. This almost means they have different tangent space at certain point.

#### Definition 2.2 (perfectly align)
We say that two manifolds without boundary $\mathcal{M}$ and $\mathcal{P}$ perfectly align if there is an $x \in \mathcal{M} \cap \mathcal{P}$ such that $\mathcal{M}$ and $\mathcal{P}$ don't intersect transversally in $x$. We shall note the boundary and interior of a manifold $\mathcal{M}$ by $\partial M$ and Int $M$ respectively. We say that two manifolds $\mathcal{M}$ and $\mathcal{P}$ (with or without boundary) perfectly align if any of the boundary free manifold pairs $(\operatorname{Int} \mathcal{M}, \operatorname{Int} \mathcal{P}),(\operatorname{Int} \mathcal{M}, \partial \mathcal{P}),(\partial \mathcal{M}$, Int $\mathcal{P})$ or $(\partial \mathcal{M}, \partial \mathcal{P})$ perfectly align.

We can simply regard $\mathcal{M}$ and $\mathcal{P}$ are same.

#### Lemma 2 (not perfectly align)

Lemma 2. Let $\mathcal{M}$ and $\mathcal{P}$ be two regular submanifolds of $\mathbb{R}^{d}$ that don't have full dimension. Let $\eta, \eta^{\prime}$ be arbitrary independent continuous random variables. We therefore define the perturbed manifolds as $\tilde{\mathcal{M}}=\mathcal{M}+\eta$ and $\tilde{\mathcal{P}}=\mathcal{P}+\eta^{\prime} .$ Then

$\mathbb{P}_{\eta, \eta^{\prime}}(\tilde{\mathcal{M}}$ does not perfectly align with $\tilde{\mathcal{P}})=1$

Lemma 2 shows that as long as there exist noise, two submanifold never perfect align.

This will be use later to prove cross entropy function does not work well for GAN.

#### Lemma 3 (intersection is lower dimension and measure is 0)

Lemma 3. Let $\mathcal{M}$ and $\mathcal{P}$ be two regular submanifolds of $\mathbb{R}^{d}$ that don't perfectly align and don't have full dimension. Let $\mathcal{L}=\mathcal{M} \cap \mathcal{P}$. If $\mathcal{M}$ and $\mathcal{P}$ don't have boundary, then $\mathcal{L}$ is also a manifold, and has strictly lower dimension than both the one of $\mathcal{M}$ and the one of $\mathcal{P}$. If they have boundary, $\mathcal{L}$ is a union of at most 4 strictly lower dimensional manifolds. In both cases, $\mathcal{L}$ has measure 0 in both $\mathcal{M}$ and $\mathcal{P}$.

This lemma means $M$ and $P$ almost disjoin.  Even they have intersection.

#### Theorem 2.2
Theorem 2.2. Let $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ be two distributions that have support contained in two closed manifolds $\mathcal{M}$ and $\mathcal{P}$ that don't perfectly align and don't have full dimension. We further assume that $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ are continuous in their respective manifolds, meaning that if there is a set $A$ with measure 0 in $\mathcal{M}$, then $\mathbb{P}_{r}(A)=0$ (and analogously for $\mathbb{P}_{g}$ ). Then, there exists an optimal discriminator $D^{*}: \mathcal{X} \rightarrow[0,1]$ that has accuracy 1 and for almost any $x$ in $\mathcal{M}$ or $\mathcal{P}, D^{*}$ is smooth in a neighborhood of $x$ and $\nabla_{x} D^{*}(x)=0 .$

This theorem shows that if two distribution is in two manifolds. These manifolds does not perfect align, then optimal discriminator exist and accuracy is 1. 

#### Theorem 2.3 
Theorem 2.3. Let $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ be two distributions whose support lies in two manifolds $\mathcal{M}$ and $\mathcal{P}$ that don't have full dimension and don't perfectly align. We further assume that $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ are continuous in their respective manifolds. Then,
$$
\begin{aligned}
J S D\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right) &=\log 2 \\
K L\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right) &=+\infty \\
K L\left(\mathbb{P}_{g} \| \mathbb{P}_{r}\right) &=+\infty
\end{aligned}
$$

This Theorem tell us JSD and KL divergence will not give correct gradient for generator when we have a good discriminator, because they value does not depend on two distribution.

#### Theorem 2.4 Vanishing gradients on the generator

Theorem $2.4$ (Vanishing gradients on the generator). Let $g_{\theta}: \mathcal{Z} \rightarrow \mathcal{X}$ be a differentiable function that induces a distribution $\mathbb{P}_{g}$. Let $\mathbb{P}_{r}$ be the real data distribution. Let $D$ be a differentiable discriminator. If the conditions of Theorems $2.1$ or $2.2$ are satisfied, $\left\|D-D^{*}\right\|<\epsilon$, and $\mathbb{E}_{z \sim p(z)}\left[\left\|J_{\theta} g_{\theta}(z)\right\|_{2}^{2}\right] \leq M^{2}, 2$ then
$$
\left\|\nabla_{\theta} \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D\left(g_{\theta}(z)\right)\right)\right]\right\|_{2}<M \frac{\epsilon}{1-\epsilon}
$$

This theorem give us upbound of generator gradient base on optimal discriminator. This gradient is really small and it make us unable to train GAN.

#### Corollary 2.1

Corollary 2.1. Under the same assumptions of Theorem $2.4$
$$
\lim _{\left\|D-D^{*}\right\| \rightarrow 0} \nabla_{\theta} \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D\left(g_{\theta}(z)\right)\right)\right]=0
$$

This corollary shows that gradient almost 0 when we have optimal discriminator. 

## The $-logD$ Alternative

The original GAN paper post use $-logD(g_\theta(z))$ as cost function to train the GAN at early stage. But this step will cost GAN instability. Following theorem give the prove for this statement. 

#### Theorem 2.5

Let $\mathbb{P}_{r}$ and $\mathbb{P}_{g_{\theta}}$ be two continuous distributions, with densities $P_{r}$ and $P_{g_{\theta}}$ respectively. Let $D^{*}=\frac{P_{r}}{P_{g_{\theta}}+P_{r}}$ be the optimal discriminator, fixed for a value $\theta_{0}$. Therefore,
$$
\mathbb{E}_{z \sim p(z)}\left[-\left.\nabla_{\theta} \log D^{*}\left(g_{\theta}(z)\right)\right|_{\theta=\theta_{0}}\right]=\left.\nabla_{\theta}\left[K L\left(\mathbb{P}_{g_{\theta}} \| \mathbb{P}_{r}\right)-2 J S D\left(\mathbb{P}_{g_{\theta}} \| \mathbb{P}_{r}\right)\right]\right|_{\theta=\theta_{0}}
$$
This theorem shows that JSD seems like a fault update when using $-log D$ alternative. Because for generator we wan $\mathbb{P}_\theta$ close to $\mathbb{P}_r$. But $JSD$ in opposite sign, it will push them away from each other.

#### Theorem 2.6 (Instability of generator gradient updates)

Let $g_{\theta}: \mathcal{Z} \rightarrow \mathcal{X}$ be a differentiable function that induces a distribution $\mathbb{P}_{g}$. Let $\mathbb{P}_{r}$ be the real data distribution, with either conditions of Theorems 2.1 or $2.2$ satisfied. Let $D$ be a discriminator such that $D^{*}-D=\epsilon$ is a centered Gaussian process indexed by $x$ and independent for every $x$ (popularly known as white noise) and $\nabla_{x} D^{*}-\nabla_{x} D=r$ another independent centered Gaussian process indexed by $x$ and independent for every $x .$ Then, each coordinate of
$$
\mathbb{E}_{z \sim p(z)}\left[-\nabla_{\theta} \log D\left(g_{\theta}(z)\right)\right]
$$
is a centered Cauchy distribution with infinite expectation and variance.



Because Cauchy distribution is unstable. The gradient follows the Cauchy distribution, so the training state will be unstable. 

## Towards Softer Metrics and Distributions 

The theorem in this section is aim to fix the instability and vanishing gradient issue.

#### Theorem 3.1

If $X$ has distribution $\mathbb{P}_{X}$ with support on $\mathcal{M}$ and $\epsilon$ is an absolutely continuous random variable with density $P_{\epsilon}$, then $\mathbb{P}_{X+\epsilon}$ is absolutely continuous with density
$$
\begin{aligned}
P_{X+\epsilon}(x) &=\mathbb{E}_{y \sim \mathbb{P}_{X}}\left[P_{\epsilon}(x-y)\right] \\
&=\int_{\mathcal{M}} P_{\epsilon}(x-y) \mathrm{d} \mathbb{P}_{X}(y)
\end{aligned}
$$

#### Corollary 3.1

- If $\epsilon \sim \mathcal{N}\left(0, \sigma^{2} I\right)$ then
$$
P_{X+c}(x)=\frac{1}{Z} \int_{\mathcal{M}} e^{-\frac{\|y-x\|^{2}}{2 \sigma^{2}}} \mathrm{~d} \mathbb{P}_{X}(y)
$$
- If $c \sim \mathcal{N}(0, \Sigma)$ then
$$
P_{X+\epsilon}(x)=\frac{1}{Z} \mathbb{E}_{y \sim \mathbf{P}_{X}}\left[e^{-\frac{1}{2}\|y-x\|_{\Sigma-1}^{2}}\right]
$$
- If $P_{\epsilon}(x) \propto \frac{1}{\|x\|^{d+1}}$ then
$$
P_{X+\epsilon}(x)=\frac{1}{Z} \mathbb{E}_{y \sim \mathbb{P}_{X}}\left[\frac{1}{\|x-y\|^{d+1}}\right]
$$

#### Theorem 3.2 

Theorem 3.2. Let $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ be two distributions with support on $\mathcal{M}$ and $\mathcal{P}$ respectively, with $\epsilon \sim \mathcal{N}\left(0, \sigma^{2} I\right) .$ Then, the gradient passed to the generator has the form
$$
\begin{aligned}
\mathbb{E}_{z \sim p(z)} &\left[\nabla_{\theta} \log \left(1-D^{*}\left(g_{0}(z)\right)\right)\right] \\
&=\mathbb{E}_{z \sim p(z)}\left[a(z) \int_{\mathcal{M}} P_{\epsilon}\left(g_{\theta}(z)-y\right) \nabla_{\theta}\left\|g_{\theta}(z)-y\right\|^{2} \mathrm{~d} \mathbb{P}_{r}(y)\right.\\
&\left.-b(z) \int_{\mathcal{P}} P_{\epsilon}\left(g_{\theta}(z)-y\right) \nabla_{\theta}\left\|g_{\theta}(z)-y\right\|^{2} \mathrm{~d} \mathbb{P}_{g}(y)\right]
\end{aligned}
$$
where $a(z)$ and $b(z)$ are positive functions. Furthermore, $b>a$ if and only if $P_{r+\epsilon}>P_{g+\epsilon}$, and $b<a$ if and only if $P_{r+\epsilon}<P_{g+\epsilon}$.



#### Corollary 3.2

Let $c, \epsilon^{\prime} \sim \mathcal{N}\left(0, \sigma^{2} I\right)$ and $\tilde{g}_{\theta}(z)=g_{\theta}(z)+c^{\prime}$, then
$$
\begin{aligned}
\mathbb{E}_{z \sim p(z), \epsilon^{\prime}} &\left[\nabla_{\theta} \log \left(1-D^{*}\left(\tilde{g}_{0}(z)\right)\right)\right] \\
&=\mathbb{E}_{z \sim p(z), \epsilon^{\prime}}\left[a(z) \int_{\mathcal{M}} P_{\epsilon}\left(\tilde{g}_{0}(z)-y\right) \nabla_{\theta}\left\|\tilde{g}_{0}(z)-y\right\|^{2} \mathrm{~d} \mathbb{P}_{r}(y)\right.\\
&\left.-b(z) \int_{\mathcal{P}} P_{\epsilon}\left(\tilde{g}_{\theta}(z)-y\right) \nabla_{\theta}\left\|\tilde{g}_{\theta}(z)-y\right\|^{2} \mathrm{~d} \mathbb{P}_{g}(y)\right] \\
&=2 \nabla_{\theta} J S D\left(\mathbb{P}_{r+\epsilon} \| \mathbb{P}_{g+\epsilon}\right)
\end{aligned}
$$

#### Definition 3.1 

Definition 3.1. We recall the definition of the Wasserstein metric $W(P, Q)$ for $P$ and $Q$ two distributions over $\mathcal{X}$. Namely,
$$
W(P, Q)=\inf _{\gamma \in \Gamma} \int_{\mathcal{X} \times \mathcal{X}}\|x-y\|_{2} d \gamma(x, y)
$$
where $\Gamma$ is the set of all possible joints on $\mathcal{X} \times \mathcal{X}$ that have marginals $P$ and $Q$.

#### Lemma 4.

If $\epsilon$ is a random vector with mean 0, then we have
$$
W\left(\mathbb{P}_{X}, \mathbb{P}_{X+\epsilon}\right) \leq V^{\frac{1}{2}}
$$
where $V=\mathbb{E}\left[\|c\|_{2}^{2}\right]$ is the variance of $\epsilon$

#### Theorem 3.3. 

Let $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ be any two distributions, and $\epsilon$ be a random vector with mean 0 and variance $V$. If $\mathbb{P}_{r+c}$ and $\mathbb{P}_{g+c}$ have support contained on a ball of diameter $C$, then
$$
W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right) \leq 2 V^{\frac{1}{2}}+2 C \sqrt{J S D\left(\mathbb{P}_{r+\epsilon} \| \mathbb{P}_{g+\epsilon}\right)}
$$