[{"categories":null,"contents":"7.22 Definitions Suppose \\(V\\) is an open set in \\(R^k, T\\) maps \\(V\\) into \\(R^k\\), and \\(x \\in V\\). If there exists a linear operator \\(A\\) on \\(R^k\\) (i.e., a linear mapping of \\(R^k\\) into \\(R^k\\), as in Definition 2.1) such that \\[ \\lim _{h \\rightarrow 0} \\frac{|T(x+h)-T(x)-A h|}{|h|}=0 \\] (where, of course, \\(h \\in R^k\\) ), then we say that \\(T\\) is differentiable at \\(x\\), and define \\[ T^{\\prime}(x)=A \\text {. } \\] The linear operator \\(T^{\\prime}(x)\\) is called the derivative of \\(T\\) at \\(x\\). (One shows easily that there is at most one linear \\(A\\) that satisfies the preceding requirements; thus it is legitimate to talk about the derivative of \\(T\\).) The term differential is also often used for \\(T^{\\prime}(x)\\).\nThe point of (1) is of course that the difference \\(T(x+h)-T(x)\\) is approximated by \\(T^{\\prime}(x) h\\), a linear function of \\(h\\).\nSince every real number \\(\\alpha\\) gives rise to a linear operator on \\(R^1\\) (mapping \\(h\\) to \\(\\alpha h\\) ), our definition of \\(T^{\\prime}(x)\\) coincides with the usual one when \\(k=1\\).\nWhen \\(A: R^k \\rightarrow R^k\\) is linear, Theorem \\(2.20(e)\\) shows that there is a number \\(\\Delta(A)\\) such that \\[ m(A(E))=\\Delta(A) m(E) \\] for all measurable sets \\(E \\subset R^k\\). Since \\[ A^{\\prime}(x)=A \\quad\\left(x \\in R^k\\right) \\] and since every differentiable transformation \\(T\\) can be locally approximated by a constant plus a linear transformation, one may conjecture that \\[ \\frac{m(T(E))}{m(E)} \\sim \\Delta\\left(T^{\\prime}(x)\\right) \\] for suitable sets \\(E\\) that are close to \\(x\\). This will be proved in Theorem 7.24, and furnishes the motivation for Theorem 7.26.\nRecall that \\(\\Delta(A)=|\\operatorname{det} A|\\) was proved in Sec. 2.23. When \\(T\\) is differentiable at \\(x\\), the determinant of \\(T^{\\prime}(x)\\) is called the Jacobian of \\(T\\) at \\(x\\), and is denoted by \\(J_T(x)\\). Thus \\[ \\Delta\\left(T^{\\prime}(x)\\right)=\\left|J_T(x)\\right| . \\] The following lemma seems geometrically obvious. Its proof depends on the Brouwer fixed point theorem. One can avoid the use of this theorem by imposing\nstronger hypotheses on \\(F\\), for example, by assuming that \\(F\\) is an open mapping. But this would lead to unnecessarily strong assumptions in Theorem 7.26.\n7.23 Lemma Let \\(S=\\{x:|x|=1\\}\\) be the sphere in \\(R^k\\) that is the boundary of the open unit ball \\(B=B(0,1)\\). If \\(F: \\bar{B} \\rightarrow R^k\\) is continuous, \\(0\u0026lt;\\epsilon\u0026lt;1\\), and \\[ |F(x)-x|\u0026lt;\\epsilon \\] for all \\(x \\in S\\), then \\(F(B) \\supset B(0,1-\\epsilon)\\).\nProof Assume, to reach a contradiction, that some point \\(a \\in B(0,1-\\epsilon)\\) is not in \\(F(B)\\). By (1), \\(|F(x)|\u0026gt;1-\\epsilon\\) if \\(x \\in S\\). Thus \\(a\\) is not in \\(F(S)\\), and therefore \\(a \\neq F(x)\\), for every \\(x \\in \\bar{B}\\). This enables us to define a continuous map \\(G: \\bar{B} \\rightarrow \\bar{B}\\) by \\[ G(x)=\\frac{a-F(x)}{|a-F(x)|} \\] If \\(x \\in S\\), then \\(x \\cdot x=|x|^2=1\\), so that \\[ x \\cdot(a-F(x))=x \\cdot a+x \\cdot(x-F(x))-1\u0026lt;|a|+\\epsilon-1\u0026lt;0 \\] This shows that \\(x \\cdot G(x)\u0026lt;0\\), hence \\(x \\neq G(x)\\).\nIf \\(x \\in B\\), then obviously \\(x \\neq G(x)\\), simply because \\(G(x) \\in S\\).\nThus \\(G\\) fixes no point of \\(\\bar{B}\\), contrary to Brouwer’s theorem which states that every continuous map of \\(\\bar{B}\\) into \\(\\bar{B}\\) has at least one fixed point.\n7.24 Theorem If (a) \\(V\\) is open in \\(R^k\\) (b) \\(T: V \\rightarrow R^k\\) is continuous, and (c) \\(T\\) is differentiable at some point \\(x \\in V\\), then \\[ \\lim _{r \\rightarrow 0} \\frac{m(T(B(x, r)))}{m(B(x, r))}=\\Delta\\left(T^{\\prime}(x)\\right) . \\] Note that \\(T(B(x, r))\\) is Lebesgue measurable; in fact, it is \\(\\sigma\\)-compact, because \\(B(x, r)\\) is \\(\\sigma\\)-compact and \\(T\\) is continuous.\nProof Assume, without loss of generality, that \\(x=0\\) and \\(T(x)=0\\). Put \\(A=T^{\\prime}(0)\\)\nThe following elementary fact about linear operators on finitedimensional vector spaces will be used: A linear operator \\(A\\) on \\(R^k\\) is one-to-one if and only if the range of \\(A\\) is all of \\(R^k\\). In that case, the inverse \\(A^{-1}\\) of \\(A\\) is also linear.\nAccordingly, we split the proof into two cases.\nCASE I \\(A\\) is one-to-one. Define \\[ F(x)=A^{-1} T(x) \\quad(x \\in V) . \\] Then \\(F^{\\prime}(0)=A^{-1} T^{\\prime}(0)=A^{-1} A=I\\), the identity operator. We shall prove that \\[ \\lim _{r \\rightarrow 0} \\frac{m(F(B(0, r)))}{m(B(0, r))}=1 . \\] Since \\(T(x)=A F(x)\\), we have \\[ m(T(B))=m(A(F(B)))=\\Delta(A) m(F(B)) \\] for every ball \\(B\\), by \\(7.22(3)\\). Hence (3) will give the desired result. Choose \\(\\epsilon\u0026gt;0\\). Since \\(F(0)=0\\) and \\(F^{\\prime}(0)=I\\), there exists a \\(\\delta\u0026gt;0\\) such that \\(0\u0026lt;|x|\u0026lt;\\delta\\) implies \\[ |F(x)-x|\u0026lt;\\epsilon|x| . \\] We claim that the inclusions \\[ B(0,(1-\\epsilon) r) \\subset F(B(0, r)) \\subset B(0,(1+\\epsilon) r) \\] hold if \\(0\u0026lt;r\u0026lt;\\delta\\). The first of these follows from Lemma 7.23, applied to \\(B(0, r)\\) in place of \\(B(0,1)\\), because \\(|F(x)-x|\u0026lt;\\epsilon r\\) for all \\(x\\) with \\(|x|=r\\). The second follows directly from (5), since \\(|F(x)|\u0026lt;(1+\\epsilon)|x|\\). It is clear that (6) implies \\[ (1-\\epsilon)^k \\leq \\frac{m(F(B(0, r)))}{m(B(0, r))} \\leq(1+\\epsilon)^k \\] and this proves (3).\nCASE II \\(A\\) is not one-to-one. In this case, \\(A\\) maps \\(R^k\\) into a subspace of lower dimension, i.e., into a set of measure 0. Given \\(\\epsilon\u0026gt;0\\), there is therefore an \\(\\eta\u0026gt;0\\) such that \\(m\\left(E_\\eta\\right)\u0026lt;\\epsilon\\) if \\(E_\\eta\\) is the set of all points in \\(R^k\\) whose distance from \\(A(B(0,1))\\) is less than \\(\\eta\\). Since \\(A=T^{\\prime}(0)\\), there is a \\(\\delta\u0026gt;0\\) such that \\(|x|\u0026lt;\\delta\\) implies \\[ |T(x)-A x| \\leq \\eta|x| . \\] If \\(r\u0026lt;\\delta\\), then \\(T(B(0, r))\\) lies therefore in the set \\(E\\) that consists of the points whose distance from \\(A(B(0, r))\\) is less than \\(\\eta r\\). Our choice of \\(\\eta\\) shows that \\(m(E)\u0026lt;\\epsilon r^k\\). Hence \\[ m(T(B(0, r)))\u0026lt;\\epsilon r^k \\quad(0\u0026lt;r\u0026lt;\\delta) . \\] Since \\(r^k=m(B(0, r)) / m(B(0,1)),(9)\\) implies that \\[ \\lim _{r \\rightarrow 0} \\frac{m(T(B(0, r)))}{m(B(0, r))}=0 . \\] This proves (1), since \\(\\Delta\\left(T^{\\prime}(0)\\right)=\\Delta(A)=0\\).\n7.25 Lemma Suppose \\(E \\subset R^k, m(E)=0, T\\) maps \\(E\\) into \\(R^k\\), and \\[ \\lim \\sup \\frac{|T(y)-T(x)|}{|y-x|}\u0026lt;\\infty \\] for every \\(x \\in E\\), as \\(y\\) tends to \\(x\\) within \\(E\\).\nThen \\(m(T(E))=0\\).\nProof Fix positive integers \\(n\\) and \\(p\\), let \\(F=F_{n, p}\\) be the set of all \\(x \\in E\\) such that \\[ |T(y)-T(x)| \\leq n|y-x| \\] for all \\(y \\in B(x, 1 / p) \\cap E\\), and choose \\(\\epsilon\u0026gt;0\\). Since \\(m(F)=0, F\\) can be covered by balls \\(B_i=B\\left(x_i, r_i\\right)\\), where \\(x_i \\in F, r_i\u0026lt;1 / p\\), in such a way that \\(\\sum m\\left(B_i\\right)\u0026lt;\\epsilon\\). (To do this, cover \\(F\\) by an open set \\(W\\) of small measure, decompose \\(W\\) into disjoint boxes of small diameter, as in Sec. 2.19, and cover each of those that intersect \\(F\\) by a ball whose center lies in the box and in \\(F\\).)\nIf \\(x \\in F \\cap B_i\\) then \\(\\left|x_i-x\\right|\u0026lt;r_i\u0026lt;1 / p\\) and \\(x_i \\in F\\). Hence \\[ \\left|T\\left(x_i\\right)-T(x)\\right| \\leq n\\left|x_i-x\\right|\u0026lt;n r_i \\] so that \\(T\\left(F \\cap B_i\\right) \\subset B\\left(T\\left(x_i\\right), n r_i\\right)\\). Therefore \\[ T(F) \\subset \\bigcup_i B\\left(T\\left(x_i\\right), n r_i\\right) . \\] The measure of this union is at most \\[ \\sum_i m\\left(B\\left(T\\left(x_i\\right), n r_i\\right)=n^k \\sum_i m\\left(B_i\\right)\u0026lt;n^k \\epsilon\\right. \\] Since Lebesgue measure is complete and \\(\\epsilon\\) was arbitrary, it follows that \\(T(F)\\) is measurable and \\(m(T(F))=0\\).\nTo complete the proof, note that \\(E\\) is the union of the countable collec\\(\\operatorname{tion}\\left\\{F_{n, p}\\right\\}\\)\nHere is a special case of the lemma:\nIf \\(V\\) is open in \\(R^k\\) and \\(T: V \\rightarrow R^k\\) is differentiable at every point of \\(V\\), then \\(T\\) maps sets of measure 0 to sets of measure 0 .\nWe now come to the change-of-variables theorem.\n7.26 Theorem Suppose that\n\\(X \\subset V \\subset R^k, V\\) is open, \\(T: V \\rightarrow R^k\\) is continuous; \\(X\\) is Lebesgue measurable, \\(T\\) is one-to-one on \\(X\\), and \\(T\\) is differentiable at every point of \\(X\\); \\(m(T(V-X))=0\\). Then, setting \\(Y=T(X)\\), \\[ \\int_Y f d m=\\int_X(f \\circ T)\\left|J_T\\right| d m \\] for every measurable f: \\(R^k \\rightarrow[0, \\infty]\\). The case \\(X=V\\) is perhaps the most interesting one. As regards condition (iii), it holds, for instance, when \\(m(V-X)=0\\) and \\(T\\) satisfies the hypotheses of Lemma \\(7.25\\) on \\(V-X\\).\nThe proof has some elements in common with that of the implication \\((b) \\rightarrow(c)\\) in Theorem \\(7.18\\).\nIt will be important in this proof to distinguish between Borel sets and Lebesgue measurable sets. The \\(\\sigma\\)-algebra consisting of the Lebesgue measurable subsets of \\(R^k\\) will be denoted by \\(\\mathfrak{M}\\).\nProof We break the proof into the following three steps: (I) If \\(E \\in \\mathfrak{M}\\) and \\(E \\subset V\\), then \\(T(E) \\in \\mathfrak{M}\\). (II) For every \\(E \\in \\mathfrak{M}\\), \\[ m(T(E \\cap X))=\\int_X \\chi_E\\left|J_T\\right| d m . \\] (III) For every \\(A \\in \\mathfrak{M}\\), \\[ \\int_Y \\chi_A d m=\\int_X\\left(\\chi_A \\circ T\\right)\\left|J_T\\right| d m . \\] If \\(E_0 \\in \\mathfrak{M}, E_0 \\subset V\\), and \\(m\\left(E_0\\right)=0\\), then \\(m\\left(T\\left(E_0-X\\right)\\right)=0\\) by (iii), and \\(m\\left(T\\left(E_0 \\cap X\\right)\\right)=0\\) by Lemma \\(7.25\\). Thus \\(m\\left(T\\left(E_0\\right)\\right)=0\\).\nIf \\(E_1 \\subset V\\) is an \\(F_\\sigma\\), then \\(E_1\\) is \\(\\sigma\\)-compact, hence \\(T\\left(E_1\\right)\\) is \\(\\sigma\\)-compact, because \\(T\\) is continuous. Thus \\(T\\left(E_1\\right) \\in \\mathfrak{M}\\).\nSince every \\(E \\in \\mathfrak{M}\\) is the union of an \\(F_\\sigma\\) and a set of measure 0 (Theorem 2.20), (I) is proved. To prove (II), let \\(n\\) be a positive integer, and put \\[ V_n=\\{x \\in V:|T(x)|\u0026lt;n\\}, \\quad X_n=X \\cap V_n . \\] Because of (I), we can define \\[ \\mu_n(E)=m\\left(T\\left(E \\cap X_n\\right)\\right) \\quad(E \\in \\mathfrak{M}) . \\] Since \\(T\\) is one-to-one on \\(X_n\\), the countable additivity of \\(m\\) shows that \\(\\mu_n\\) is a measure on \\(\\mathfrak{M}\\). Also, \\(\\mu_n\\) is bounded (this was the reason for replacing \\(X\\) temporarily by \\(X_n\\) ), and \\(\\mu_n \\ll m\\), by another application of Lemma 7.25.\nTheorem \\(7.8\\) tells us therefore that \\(\\left(D \\mu_n\\right)(x)\\) exists a.e. \\([m]\\), that \\(D \\mu_n \\in L^1(m)\\), and that \\[ \\mu_n(E)=\\int_E\\left(D \\mu_n\\right) d m \\quad(E \\in \\mathfrak{M}) . \\] We claim next that \\[ \\left(D \\mu_n\\right)(x)=\\left|J_T(x)\\right| \\quad\\left(x \\in X_n\\right) \\] To see this, fix \\(x \\in X_n\\), and note that \\(B(x, r) \\subset V_n\\) for all sufficiently small \\(r\u0026gt;0\\), because \\(V_n\\) is open. Since \\(V_n-X_n \\subset V-X\\), hypothesis (iii) enables us to replace \\(X_n\\) by \\(V_n\\) in (3) without changing \\(\\mu_n(E)\\). Hence, for small \\(r\u0026gt;0\\), \\[ \\mu_n(B(x, r))=m(T(B(x, r))) . \\] If we divide both sides of \\((6)\\) by \\(m(B(x, r))\\) and refer to Theorem \\(7.24\\) and formula 7.22(6), we obtain (5).\nSince (3) implies that \\(\\mu_n(E)=\\mu_n\\left(E \\cap X_n\\right)\\), it follows from (3), (4), and (5) that \\[ m\\left(T\\left(E \\cap X_n\\right)\\right)=\\int_{X_n} \\chi_E\\left|J_T\\right| d m \\quad(E \\in \\mathfrak{M}) . \\] If we apply the monotone convergence theorem to (7), letting \\(n \\rightarrow \\infty\\), we obtain (II).\nWe begin the proof of (III) by letting \\(A\\) be a Borel set in \\(R^k\\). Put \\[ E=T^{-1}(A)=\\{x \\in V: T(x) \\in A\\} \\] Then \\(\\chi_E=\\chi_A \\circ T\\). Since \\(\\chi_A\\) is a Borel function and \\(T\\) is continuous, \\(\\chi_E\\) is a Borel function (Theorem 1.12), hence \\(E \\in \\mathfrak{M}\\). Also \\[ T(E \\cap X)=A \\cap Y \\] which implies, by (II), that \\[ \\int_Y \\chi_A d m=m(T(E \\cap X))=\\int_X\\left(\\chi_A \\circ T\\right)\\left|J_T\\right| d m . \\] Finally, if \\(N \\in \\mathfrak{M}\\) and \\(m(N)=0\\), there is a Borel set \\(A \\supset N\\) with \\(m(A)=0\\). For this \\(A,(10)\\) shows that \\(\\left(\\chi_A \\circ T\\right)\\left|J_T\\right|=0\\) a.e. \\([m]\\). Since \\(0 \\leq\\) \\(\\chi_N \\leq \\chi_A\\), it follows that both integrals in (10) are 0 if \\(A\\) is replaced by \\(N\\). Since every Lebesgue measurable set is the disjoint union of a Borel set and a set of measure 0 , (10) holds for every \\(A \\in \\mathfrak{M}\\). This proves (III).\nOnce we have (III), it is clear that (1) holds for every nonnegative Lebesgue measurable simple function \\(f\\). Another application of the monotone convergence theorem completes the proof.\nNote that we did not prove that \\(f \\circ T\\) is Lebesgue measurable for all Lebesgue measurable \\(f\\). It need not be; see Exercise 8. What the proof does establish is the Lebesgue measurability of the product \\((f \\circ T)\\left|J_T\\right|\\). Here is a special case of the theorem:\nSuppose \\(\\varphi:[a, b] \\rightarrow[\\alpha, \\beta]\\) is \\(\\mathrm{AC}\\), monotonic, \\(\\varphi(a)=\\alpha, \\varphi(b)=\\beta\\), and \\(f \\geq 0\\) is Lebesgue measurable. Then \\[ \\int_\\alpha^\\beta f(t) d t=\\int_a^b f(\\varphi(x)) \\varphi^{\\prime}(x) d x . \\] To derive this from Theorem 7.26, put \\(V=(a, b), T=\\varphi\\), let \\(\\Omega\\) be the union of the maximal segments on which \\(\\varphi\\) is constant (if there are any) and let \\(X\\) be the set of all \\(x \\in V-\\Omega\\) where \\(\\varphi^{\\prime}(x)\\) exists (and is finite).\n","date":"2023-01-13T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-7/3-differentiable-transformations/","section":"papa rudin","tags":null,"title":"Differentiable Transformations"},{"categories":null,"contents":"7.16 This theorem concerns functions defined on some compact interval \\([a, b]\\) in \\(R^1\\). It has two parts. The first asserts, roughly speaking, that the derivative of the indefinite integral of a function is that same function. We dealt with this in Theorem 7.11. The second part goes the other way: one returns to the original function by integrating its derivative. More precisely \\[ f(x)-f(a)=\\int_a^x f^{\\prime}(t) d t \\quad(a \\leq x \\leq b) . \\] In the elementary version of this theorem, one assumes that \\(f\\) is differentiable at every point of \\([a, b]\\) and that \\(f^{\\prime}\\) is a continuous function. The proof of \\((1)\\) is then easy.\nIn trying to extend (1) to the setting of the Lebesgue integral, questions such as the following come up naturally: Is it enough to assume that \\(f^{\\prime} \\in L^1\\), rather than that \\(f^{\\prime}\\) is continuous? If \\(f\\) is continuous and differentiable at almost all points of \\([a, b]\\), must (1) then hold?\nBefore proving any positive results, here are two examples that show how (1) can fail.\nPut \\(f(x)=x^2 \\sin \\left(x^{-2}\\right)\\) if \\(x \\neq 0, f(0)=0\\). Then \\(f\\) is differentiable at every point, but \\[ \\int_0^1\\left|f^{\\prime}(t)\\right| d t=\\infty, \\] so \\(f^{\\prime} \\notin L^1\\). If we interpret the integral in (1) (with \\([0,1]\\) in place of \\([a, b]\\) ) as the limit, as \\(\\epsilon \\rightarrow 0\\), of the integrals over \\([\\epsilon, 1]\\), then (1) still holds for this \\(f\\). More complicated situations can arise where this kind of passage to the limit is of no use. There are integration processes, due to Denjoy and Perron (see [18], [28]), which are so designed that (1) holds whenever \\(f\\) is differentiable at every point. These fail to have the property that the integrability of \\(f\\) implies that of \\(|f|\\), and therefore do not play such an important role in analysis.\nSuppose \\(f\\) is continuous on \\([a, b], f\\) is differentiable at almost every point of \\([a, b]\\), and \\(f^{\\prime} \\in L^1\\) on \\([a, b]\\). Do these assumptions imply that (1) holds? Answer: No. Choose \\(\\left\\{\\delta_n\\right\\}\\) so that \\(1=\\delta_0\u0026gt;\\delta_1\u0026gt;\\delta_2\u0026gt;\\cdots, \\delta_n \\rightarrow 0\\). Put \\(E_0=[0,1]\\).\nSuppose \\(n \\geq 0\\) and \\(E_n\\) is constructed so that \\(E_n\\) is the union of \\(2^n\\) disjoint closed intervals, each of length \\(2^{-n} \\delta_n\\). Delete a segment in the center of each of these \\(2^n\\) intervals, so that each of the remaining \\(2^{n+1}\\) intervals has length\n\\(2^{-n-1} \\delta_{n+1}\\) (this is possible, since \\(\\delta_{n+1}\u0026lt;\\delta_n\\) ), and let \\(E_{n+1}\\) be the union of these \\(2^{n+1}\\) intervals. Then \\(E_1 \\supset E_2 \\supset \\cdots, m\\left(E_n\\right)=\\delta_n\\), and if \\[ E=\\bigcap_{n=1}^{\\infty} E_n, \\] then \\(E\\) is compact and \\(m(E)=0\\). (In fact, \\(E\\) is perfect.) Put \\[ g_n=\\delta_n^{-1} \\chi_{E_n} \\text { and } f_n(x)=\\int_0^x g_n(t) d t \\quad(n=0,1,2, \\ldots) . \\] Then \\(f_n(0)=0, f_n(1)=1\\), and each \\(f_n\\) is a monotonic function which is constant on each segment in the complement of \\(E_n\\). If \\(I\\) is one of the \\(2^n\\) intervals whose union is \\(E_n\\), then \\[ \\int_I g_n(t) d t=\\int_I g_{n+1}(t) d t=2^{-n} \\] It follows from (5) that \\[ f_{n+1}(x)=f_n(x) \\quad\\left(x \\notin E_n\\right) \\] and that \\[ \\left|f_n(x)-f_{n+1}(x)\\right| \\leq \\int_I\\left|g_n-g_{n+1}\\right|\u0026lt;2^{-n+1} \\quad\\left(x \\in E_n\\right) \\] Hence \\(\\left\\{f_n\\right\\}\\) converges uniformly to a continuous monotonic function \\(f\\), with \\(f(0)=0, f(1)=1\\), and \\(f^{\\prime}(x)=0\\) for all \\(x \\notin E\\). Since \\(m(E)=0\\), we have \\(f^{\\prime}=0\\) a.e.\nThus (1) fails.\nIf \\(\\delta_n=(2 / 3)^n\\), the set \\(E\\) is Cantor’s ” middle thirds” set.\nHaving seen what can go wrong, assume now that \\(f^{\\prime} \\in L^1\\) and that (1) does hold. There is then a measure \\(\\mu\\), defined by \\(d \\mu=f^{\\prime} d m\\). Since \\(\\mu \\ll m\\), Theorem \\(6.11\\) shows that there corresponds to each \\(\\epsilon\u0026gt;0\\) a \\(\\delta\u0026gt;0\\) so that \\(|\\mu|(E)\u0026lt;\\epsilon\\) whenever \\(E\\) is a union of disjoint segments whose total length is less than \\(\\delta\\). Since \\(f(y)-f(x)=\\mu((x, y))\\) if \\(a \\leq x\u0026lt;y \\leq b\\), it follows that the absolute continuity of \\(f\\), as defined below, is necessary for (1). Theorem \\(7.20\\) will show that this necessary condition is also sufficient.\n7.17 Definition A complex function \\(f\\), defined on an interval \\(I=[a, b]\\), is said to be absolutely continuous on \\(I\\) (briefly, \\(f\\) is AC on \\(I\\) ) if there corresponds to every \\(\\epsilon\u0026gt;0\\) a \\(\\delta\u0026gt;0\\) so that \\[ \\sum_{i=1}^n\\left|f\\left(\\beta_i\\right)-f\\left(\\alpha_i\\right)\\right|\u0026lt;\\epsilon \\] for any \\(n\\) and any disjoint collection of segments \\(\\left(\\alpha_1, \\beta_1\\right), \\ldots,\\left(\\alpha_n, \\beta_n\\right)\\) in \\(I\\) whose lengths satisfy \\[ \\sum_{i=1}^n\\left(\\beta_i-\\alpha_i\\right)\u0026lt;\\delta . \\] Such an \\(f\\) is obviously continuous: simply take \\(n=1\\).\nIn the following theorem, the implication \\((b) \\rightarrow(c)\\) is probably the most interesting. That \\((a) \\rightarrow(c)\\) without assuming monotonicity of \\(f\\) is the content of Theorem 7.20.\n7.18 Theorem Let \\(I=[a, b]\\), let \\(f: I \\rightarrow R^1\\) be continuous and nondecreasing. Each of the following three statements about \\(f\\) implies the other two:\n\\(f\\) is \\(\\mathrm{AC}\\) on I. \\(f\\) maps sets of measure 0 to sets of measure 0. \\(f\\) is differentiable a.e. on \\(I, f^{\\prime} \\in L^1\\), and \\[ f(x)-f(a)=\\int_a^x f^{\\prime}(t) d t \\quad(\\alpha \\leq x \\leq b) . \\] Note that the functions constructed in Example \\(7.16(b)\\) map certain compact sets of measure 0 onto the whole unit interval! Exercise 12 complements this theorem.\nProof We will show that \\((a) \\rightarrow(b) \\rightarrow(c) \\rightarrow(a)\\).\nLet \\(\\mathfrak{M}\\) denote the \\(\\sigma\\)-algebra of all Lebesgue measurable subsets of \\(R^1\\).\nAssume \\(f\\) is AC on \\(I\\), pick \\(E \\subset I\\) so that \\(E \\in \\mathfrak{M}\\) and \\(m(E)=0\\). We have to show that \\(f(E) \\in \\mathfrak{M}\\) and \\(m(f(E))=0\\). Without loss of generality, assume that neither \\(a\\) nor \\(b\\) lie in \\(E\\).\nChoose \\(\\epsilon\u0026gt;0\\). Associate \\(\\delta\u0026gt;0\\) to \\(f\\) and \\(\\epsilon\\), as in Definition 7.17. There is then an open set \\(V\\) with \\(m(V)\u0026lt;\\delta\\), so that \\(E \\subset V \\subset I\\). Let \\(\\left(\\alpha_i, \\beta_i\\right)\\) be the disjoint segments whose union is \\(V\\). Then \\(\\sum\\left(\\beta_i-\\alpha_i\\right)\u0026lt;\\delta\\), and our choice of \\(\\delta\\) shows that therefore \\[ \\sum_i\\left(f\\left(\\beta_i\\right)-f\\left(\\alpha_i\\right)\\right) \\leq \\epsilon . \\] [Definition \\(7.17\\) was stated in terms of finite sums; thus (2) holds for every partial sum of the (possibly) infinite series, hence (2) holds also for the sum of the whole series, as stated.]\nSince \\(E \\subset V, f(E) \\subset \\bigcup\\left[f\\left(\\alpha_i\\right), f\\left(\\beta_i\\right)\\right]\\). The Lebesgue measure of this union is the left side of (2). This says that \\(f(E)\\) is a subset of Borel sets of arbitrarily small measure. Since Lebesgue measure is complete, it follows that \\(f(E) \\in \\mathfrak{M}\\) and \\(m(f(E))=0\\).\nWe have now proved that \\((a)\\) implies \\((b)\\).\nAssume next that \\((b)\\) holds. Define \\[ g(x)=x+f(x) \\quad(a \\leq x \\leq b) . \\] If the \\(f\\)-image of some segment of length \\(\\eta\\) has length \\(\\eta^{\\prime}\\), then the \\(g\\)-image of that same segment has length \\(\\eta+\\eta^{\\prime}\\). From this it follows easily that \\(g\\) satisfies (b), since \\(f\\) does.\nNow suppose \\(E \\subset I, E \\in \\mathfrak{M}\\). Then \\(E=E_1 \\cup E_0\\) where \\(m\\left(E_0\\right)=0\\) and \\(E_1\\) is an \\(F_\\sigma\\) (Theorem 2.20). Thus \\(E_1\\) is a countable union of compact sets, and so is \\(g\\left(E_1\\right)\\), because \\(g\\) is continuous. Since \\(g\\) satisfies \\((b), m\\left(g\\left(E_0\\right)\\right)=0\\). Since \\(g(E)=g\\left(E_1\\right) \\cup g\\left(E_0\\right)\\), we conclude: \\(g(E) \\in \\mathfrak{M}\\). Therefore we can define \\[ \\mu(E)=m(g(E)) \\quad(E \\subset I, E \\in \\mathfrak{M}) . \\] Since \\(g\\) is one-to-one (this is our reason for working with \\(g\\) rather than \\(f\\) ), disjoint sets in \\(I\\) have disjoint \\(g\\)-images. The countable additivity of \\(m\\) shows therefore that \\(\\mu\\) is a (positive, bounded) measure on \\(\\mathfrak{M}\\). Also, \\(\\mu \\ll m\\), because \\(g\\) satisfies \\((b)\\). Thus \\[ d \\mu=h d m \\] for some \\(h \\in L^1(m)\\), by the Radon-Nikodym theorem.\nIf \\(E=[a, x]\\), then \\(g(E)=[g(a), g(x)]\\), and (5) gives \\[ g(x)-g(a)=m(g(E))=\\mu(E)=\\int_E h d m=\\int_a^x h(t) d t . \\] If we now use (3), we conclude that \\[ f(x)-f(a)=\\int_a^x[h(t)-1] d t \\quad(\\alpha \\leq x \\leq b) . \\] Thus \\(f^{\\prime}(x)=h(x)-1\\) a.e. \\([m]\\), by Theorem 7.11.\nWe have now proved that \\((b)\\) implies \\((c)\\).\nThe discussion that preceded Definition \\(7.17\\) showed that \\((c)\\) implies \\((a)\\).\n7.19 Theorem Suppose \\(f: I \\rightarrow R^1\\) is \\(\\mathrm{AC}, I=[a, b]\\). Define \\[ F(x)=\\sup \\sum_{i=1}^N\\left|f\\left(t_i\\right)-f\\left(t_{i-1}\\right)\\right| \\quad(a \\leq x \\leq b) \\] where the supremum is taken over all \\(N\\) and over all choices of \\(\\left\\{t_i\\right\\}\\) such that \\[ a=t_0\u0026lt;t_1\u0026lt;\\cdots\u0026lt;t_N=x . \\] The functions \\(F, F+f, F-f\\) are then nondecreasing and \\(\\mathrm{AC}\\) on I.\n[ \\(F\\) is called the total variation function of \\(f\\). If \\(f\\) is any (complex) function on \\(I\\), AC or not, and \\(F(b)\u0026lt;\\infty\\), then \\(f\\) is said to have bounded variation on \\(I\\), and \\(F(b)\\) is the total variation of \\(f\\) on \\(I\\). Exercise 13 is relevant to this.]\nProof If (2) holds and \\(x\u0026lt;y \\leq b\\), then \\[ F(y) \\geq|f(y)-f(x)|+\\sum_{i=1}^N\\left|f\\left(t_i\\right)-f\\left(t_{i-1}\\right)\\right| . \\] Hence \\(F(y) \\geq|f(y)-f(x)|+F(x)\\). In particular \\[ F(y) \\geq f(y)-f(x)+F(x) \\text { and } F(y) \\geq f(x)-f(y)+F(x) . \\] This proves that \\(F, F+f, F-f\\) are nondecreasing.\nSince sums of two \\(\\mathrm{AC}\\) functions are obviously \\(\\mathrm{AC}\\), it only remains to be proved that \\(F\\) is AC on \\(I\\). If \\((a, \\beta) \\subset I\\) then \\[ F(\\beta)-F(\\alpha)=\\sup \\sum_1^n\\left|f\\left(t_i\\right)-f\\left(t_{i-1}\\right)\\right| \\text {, } \\] the supremum being taken over all \\(\\left\\{t_i\\right\\}\\) that satisfy \\(\\alpha=t_0\u0026lt;\\cdots\u0026lt;t_n=\\beta\\). Note that \\(\\sum\\left(t_i-t_{i-1}\\right)=\\beta-\\alpha\\).\nNow pick \\(\\epsilon\u0026gt;0\\), associate \\(\\delta\u0026gt;0\\) to \\(f\\) and \\(\\epsilon\\) as in Definition 7.17, choose disjoint segments \\(\\left(\\alpha_j, \\beta_j\\right) \\subset I\\) with \\(\\sum\\left(\\beta_j-\\alpha_j\\right)\u0026lt;\\delta\\), and apply (5) to each \\(\\left(\\alpha_j, \\beta_j\\right)\\). It follows that \\[ \\sum_j\\left(F\\left(\\beta_j\\right)-F\\left(\\alpha_j\\right)\\right) \\leq \\epsilon, \\] by our choice of \\(\\delta\\). Thus \\(F\\) is \\(\\mathrm{AC}\\) on \\(I\\).\nWe have now reached our main objective:\n7.20 Theorem If \\(f\\) is a complex function that is \\(\\mathrm{AC}\\) on \\(I=[a, b]\\), then \\(f\\) is differentiable at almost all points of \\(I, f^{\\prime} \\in L^1(m)\\), and \\[ f(x)-f(a)=\\int_a^x f^{\\prime}(t) d t \\quad(a \\leq x \\leq b) . \\] Proof It is of course enough to prove this for real \\(f\\). Let \\(F\\) be its total variation function, as in Theorem 7.19, define \\[ f_1=\\frac{1}{2}(F+f), \\quad f_2=\\frac{1}{2}(F-f), \\] and apply the implication \\((a) \\rightarrow(c)\\) of Theorem \\(7.18\\) to \\(f_1\\) and \\(f_2\\). Since \\[ f=f_1-f_2 \\] this yields (1).\nThe next theorem derives (1) from a different set of hypotheses, by an entirely different method of proof.\n7.21 Theorem If \\(f:[a, b] \\rightarrow R^1\\) is differentiable at every point of \\([a, b]\\) and \\(f^{\\prime} \\in L^1\\) on \\([a, b]\\), then \\[ f(x)-f(a)=\\int_a^x f^{\\prime}(t) d t \\quad(a \\leq x \\leq b) . \\] Note that differentiability is assumed to hold at every point of \\([a, b]\\).\nProof It is clear that it is enough to prove this for \\(x=b\\). Fix \\(\\epsilon\u0026gt;0\\). Theorem \\(2.25\\) ensures the existence of a lower semicontinuous function \\(g\\) on \\([a, b]\\) such that \\(g\u0026gt;f^{\\prime}\\) and \\[ \\int_a^b g(t) d t\u0026lt;\\int_a^b f^{\\prime}(t) d t+\\epsilon . \\] Actually, Theorem \\(2.25\\) only gives \\(g \\geq f^{\\prime}\\), but since \\(m([a, b])\u0026lt;\\infty\\), we can add a small constant to \\(g\\) without affecting (2). For any \\(\\eta\u0026gt;0\\), define \\[ F_\\eta(x)=\\int_a^x g(t) d t-f(x)+f(a)+\\eta(x-a) \\quad(a \\leq x \\leq b) \\] Keep \\(\\eta\\) fixed for the moment. To each \\(x \\in[a, b)\\) there corresponds a \\(\\delta_x\u0026gt;0\\) such that \\[ g(t)\u0026gt;f^{\\prime}(x) \\text { and } \\frac{f(t)-f(x)}{t-x}\u0026lt;f^{\\prime}(x)+\\eta \\] for all \\(t \\in\\left(x, x+\\delta_x\\right)\\), since \\(g\\) is lower semicontinuous and \\(g(x)\u0026gt;f^{\\prime}(x)\\). For any such \\(t\\) we therefore have \\[ \\begin{aligned} F_\\eta(t)-F_\\eta(x) \u0026amp; =\\int_x^t g(s) d s-[f(t)-f(x)]+\\eta(t-x) \\\\ \u0026amp; \u0026gt;(t-x) f^{\\prime}(x)-(t-x)\\left[f^{\\prime}(x)+\\eta\\right]+\\eta(t-x)=0 . \\end{aligned} \\] Since \\(F_\\eta(a)=0\\) and \\(F_\\eta\\) is continuous, there is a last point \\(x \\in[a, b]\\) at which \\(F_\\eta(x)=0\\). If \\(x\u0026lt;b\\), the preceding computation implies that \\(F_\\eta(t)\u0026gt;0\\) for \\(t \\in(x, b]\\). In any case, \\(F_\\eta(b) \\geq 0\\). Since this holds for every \\(\\eta\u0026gt;0\\), (2) and (3) now give \\[ f(b)-f(a) \\leq \\int_a^b g(t) d t\u0026lt;\\int_a^b f^{\\prime}(t) d t+\\epsilon, \\] and since \\(\\epsilon\\) was arbitrary, we conclude that \\[ f(b)-f(a) \\leq \\int_a^b f^{\\prime}(t) d t . \\] If \\(f\\) satisfies the hypotheses of the theorem, so does \\(-f\\); therefore (6) holds with \\(-f\\) in place of \\(f\\), and these two inequalities together give (1). ////\n","date":"2023-01-13T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-7/2-the-fundamental-theorem-of-calculus/","section":"papa rudin","tags":null,"title":"The Fundamental Theorem of Calculus"},{"categories":null,"contents":"We begin with a simple, theorem whose main purpose is to motivate the definitions that follow.\n7.1 Theorem Suppose \\(\\mu\\) is a complex Borel measure on \\(R^1\\) and\n\\[ f(x)=\\mu((-\\infty, x)) \\quad\\left(x \\in R^1\\right) . \\]\nIf \\(x \\in R^1\\) and \\(A\\) is a complex number, each of the following two statements implies the other:\n\\(f\\) is differentiable at \\(x\\) and \\(f^{\\prime}(x)=A\\). To every \\(\\epsilon\u0026gt;0\\) corresponds a \\(\\delta\u0026gt;0\\) such that \\[ \\left|\\frac{\\mu(I)}{m(I)}-A\\right|\u0026lt;\\epsilon \\]\nfor every open segment I that contains \\(x\\) and whose length is less than \\(\\delta\\). Here \\(m\\) denotes Lebesgue measure on \\(R^1\\).\n7.2 Definitions Theorem \\(7.1\\) suggests that one might define the derivative of \\(\\mu\\) at \\(x\\) to be the limit of the quotients \\(\\mu(I) / m(I)\\), as the segments \\(I\\) shrink to \\(x\\), and that an analogous definition might be appropriate in several variables, i.e., in \\(R^k\\) rather than in \\(R^1\\).\nAccordingly, let us fix a dimension \\(k\\), denote the open ball with center \\(x \\in R^k\\) and radius \\(r\u0026gt;0\\) by\n\\[ B(x, r)=\\left\\{y \\in R^k:|y-x|\u0026lt;r\\right\\} \\]\n(the absolute value indicates the euclidean metric, as in Sec. 2.19), associate to any complex Borel measure \\(\\mu\\) on \\(R^k\\) the quotients\n\\[ \\left(Q_r \\mu\\right)(x)=\\frac{\\mu(B(x, r))}{m(B(x, r))}, \\]\nwhere \\(m=m_k\\) is Lebesgue measure on \\(R^k\\), and define the symmetric derivative of \\(\\mu\\) at \\(x\\) to be\n\\[ (D \\mu)(x)=\\lim _{r \\rightarrow 0}\\left(Q_r \\mu\\right)(x) \\]\nat those points \\(x \\in R^k\\) at which this limit exists.\nWe shall study \\(D \\mu\\) by means of the maximal function \\(M \\mu\\). For \\(\\mu \\geq 0\\), this is defined by\n\\[ (M \\mu)(x)=\\sup _{0\u0026lt;r\u0026lt;\\infty}\\left(Q_r \\mu\\right)(x), \\]\nand the maximal function of a complex Borel measure \\(\\mu\\) is, by definition, that of its total variation \\(|\\mu|\\).\nThe functions \\(M \\mu: R^k \\rightarrow[0, \\infty]\\) are lower semicontinuous, hence measurable.\nTo see this, assume \\(\\mu \\geq 0\\), pick \\(\\lambda\u0026gt;0\\), let \\(E=\\{M \\mu\u0026gt;\\lambda\\}\\), and fix \\(x \\in E\\). Then there is an \\(r\u0026gt;0\\) such that\n\\[ \\mu(B(x, r))=\\operatorname{tm}(B(x, r)) \\]\nfor some \\(t\u0026gt;\\lambda\\), and there is a \\(\\delta\u0026gt;0\\) that satisfies\n\\[ (r+\\delta)^k\u0026lt;r^k t / \\lambda \\]\nIf \\(|y-x|\u0026lt;\\delta\\), then \\(B(y, r+\\delta) \\supset B(x, r)\\), and therefore\n\\[ \\mu(B(y, r+\\delta)) \\geq \\operatorname{tm}(B(x, r))=t[r /(r+\\delta)]^k m(B(y, r+\\delta))\u0026gt;\\lambda m(B(y, r+\\delta)) \\text {. } \\]\nThus \\(B(x, \\delta) \\subset E\\). This proves that \\(E\\) is open.\nOur first objective is the “maximal theorem” 7.4. The following covering lemma will be used in its proof.\n7.3 Lemma If \\(W\\) is the union of a finite collection of balls \\(B\\left(x_i, r_i\\right), 1 \\leq i \\leq N\\), then there is a set \\(S \\subset\\{1, \\ldots, N\\}\\) so that\nthe balls \\(B\\left(x_i, r_i\\right)\\) with \\(i \\in S\\) are disjoint, \\(W \\subset \\bigcup_{i \\in S} B\\left(x_i, 3 r_i\\right)\\), and \\(m(W) \\leq 3^k \\sum_{i \\in S} m\\left(B\\left(x_i, r_i\\right)\\right)\\) Proof Order the balls \\(B_i=B\\left(x_i, r_i\\right)\\) so that \\(r_1 \\geq r_2 \\geq \\cdots \\geq r_N\\). Put \\(i_1=1\\). Discard all \\(B_j\\) that intersect \\(B_{i_1}\\). Let \\(B_{i_2}\\) be the first of the remaining \\(B_j\\), if there are any. Discard all \\(B_j\\) with \\(j\u0026gt;i_2\\) that intersect \\(B_{i_2}\\), let \\(B_{i_3}\\) be the first of the remaining ones, and so on, as long as possible. This process stops after a finite number of steps and gives \\(S=\\left\\{i_1, i_2, \\ldots\\right\\}\\).\nIt is clear that \\((a)\\) holds. Every discarded \\(B_j\\) is a subset of \\(B\\left(x_i, 3 r_i\\right)\\) for some \\(i \\in S\\), for if \\(r^{\\prime} \\leq r\\) and \\(B\\left(x^{\\prime}, r^{\\prime}\\right)\\) intersects \\(B(x, r)\\), then \\(B\\left(x^{\\prime}, r^{\\prime}\\right) \\subset B(x, 3 r)\\). This proves \\((b)\\), and \\((c)\\) follows from \\((b)\\) because in \\(R^k\\).\n\\[ m(B(x, 3 r))=3^k m(B(x, r)) \\]\nThe following theorem says, roughly speaking, that the maximal function of a measure cannot be large on a large set.\n7.4 Theorem If \\(\\mu\\) is a complex Borel measure on \\(R^k\\) and \\(\\lambda\\) is a positive number, then\n\\[ m\\{M \\mu\u0026gt;\\lambda\\} \\leq 3^k \\lambda^{-1}\\|\\mu\\| . \\]\nHere \\(\\|\\mu\\|=|\\mu|\\left(R^k\\right)\\), and the left side of (1) is an abbreviation for the more cumbersome expression\n\\[ m\\left(\\left\\{x \\in R^k:(M \\mu)(x)\u0026gt;\\lambda\\right\\}\\right) . \\]\nWe shall often simplify notation in this way.\nProof Fix \\(\\mu\\) and \\(\\lambda\\). Let \\(K\\) be a compact subset of the open set \\(\\{M \\mu\u0026gt;\\lambda\\}\\). Each \\(x \\in K\\) is the center of an open ball \\(B\\) for which\n\\[ |\\mu|(B)\u0026gt;\\lambda m(B) . \\]\nSome finite collection of these \\(B\\) ’s covers \\(K\\), and Lemma \\(7.3\\) gives us a disjoint subcollection, say \\(\\left\\{B_1, \\ldots, B_n\\right\\}\\), that satisfies\n\\[ m(K) \\leq 3^k \\sum_1^n m\\left(B_i\\right) \\leq 3^k \\lambda^{-1} \\sum_1^n|\\mu|\\left(B_i\\right) \\leq 3^k \\lambda^{-1}\\|\\mu\\| . \\]\nThe disjointness of \\(\\left\\{B_1, \\ldots, B_n\\right\\}\\) was used in the last inequality. Now (1) follows by taking the supremum over all compact \\(K \\subset\\{M \\mu\u0026gt;\\lambda\\}\\).\n7.5 Weak \\(L^1\\) If \\(f \\in L^1\\left(R^k\\right)\\) and \\(\\lambda\u0026gt;0\\), then\n\\[ m\\{|f|\u0026gt;\\lambda\\} \\leq \\lambda^{-1}\\|f\\|_1 \\]\nbecause, putting \\(E=\\{|f|\u0026gt;\\lambda\\}\\), we have\n\\[ \\lambda m(E) \\leq \\int_E|f| d m \\leq \\int_{R^k}|f| d m=\\|f\\|_1 . \\]\nAccordingly, any measurable function \\(f\\) for which\n\\[ \\lambda \\cdot m\\{|f|\u0026gt;\\lambda\\} \\]\nis a bounded function of \\(\\lambda\\) on \\((0, \\infty)\\) is said to belong to weak \\(L^1\\). Thus weak \\(L^1\\) contains \\(L^1\\). That it is actually larger is shown most simply by the function \\(1 / x\\) on \\((0,1)\\).\nWe associate to each \\(f \\in L^1\\left(R^k\\right)\\) its maximal function \\(M f: R^k \\rightarrow[0, \\infty]\\), by setting\n\\[ (M f)(x)=\\sup _{0\u0026lt;r\u0026lt;\\infty} \\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|f| d m . \\]\n[We wrote \\(B_r\\) in place of \\(B(x, r)\\) because \\(m(B(x, r))\\) depends only on the radius \\(r\\).] If we identify \\(f\\) with the measure \\(\\mu\\) given by \\(d \\mu=f d m\\), we see that (4) agrees with the previously defined \\(M \\mu\\). Theorem \\(7.4\\) states therefore that the “maximal operator” \\(M\\) sends \\(L^1\\) to weak \\(L^1\\), with a bound (namely \\(3^k\\) ) that depends only on the space \\(R^k\\) :\nFor every \\(f \\in L^1\\left(R^k\\right)\\) and every \\(\\lambda\u0026gt;0\\)\n\\[ m\\{M f\u0026gt;\\lambda\\} \\leq 3^k \\lambda^{-1}\\|f\\|_1 \\]\n7.6 Lebesgue points If \\(f \\in L^1\\left(R^k\\right)\\), any \\(x \\in R^k\\) for which it is true that\n\\[ \\lim _{r \\rightarrow 0} \\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|f(y)-f(x)| d m(y)=0 \\]\nis called a Lebesgue point of \\(f\\).\nFor example, (1) holds if \\(f\\) is continuous at the point \\(x\\). In general, (1) means that the averages of \\(|f-f(x)|\\) are small on small balls centered at \\(x\\). The Lebesgue points of \\(f\\) are thus points where \\(f\\) does not oscillate too much, in an average sense.\nIt is probably far from obvious that every \\(f \\in L^1\\) has Lebesgue points. But the following remarkable theorem shows that they always exist. (See also Exercise 23.)\n7.7 Theorem If \\(f \\in L^1\\left(R^k\\right)\\), then almost every \\(x \\in R^k\\) is a Lebesgue point of \\(f\\).\nProof Define \\[ \\left(T_r f\\right)(x)=\\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|f-f(x)| d m \\] for \\(x \\in R^k, r\u0026gt;0\\), and put \\[ (T f)(x)=\\underset{r \\rightarrow 0}{\\lim \\sup }\\left(T_r f\\right)(x) . \\] We have to prove that \\(T f=0\\) a.e. \\([\\mathrm{m}]\\).\nPick \\(y\u0026gt;0\\). Let \\(n\\) be a positive integer. By Theorem 3.14, there exists \\(g \\in C\\left(R^k\\right)\\) so that \\(\\|f-g\\|_1\u0026lt;1 / n\\) : Put \\(h=f-g\\).\nSince \\(g\\) is continuous, \\(T g=0\\). Since \\[ \\left(T_r h\\right)(x) \\leq \\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|h| d m+|h(x)| \\] we have \\[ T h \\leq M h+|h| . \\] Since \\(T_r f \\leq T_r g+T_r h\\), it follows that \\[ T f \\leq M h+|h| . \\] Proof Define \\[ \\left(T_r f\\right)(x)=\\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|f-f(x)| d m \\] for \\(x \\in R^k, r\u0026gt;0\\), and put \\[ (T f)(x)=\\underset{r \\rightarrow 0}{\\lim \\sup }\\left(T_r f\\right)(x) . \\] We have to prove that \\(T f=0\\) a.e. \\([\\mathrm{m}]\\).\nPick \\(y\u0026gt;0\\). Let \\(n\\) be a positive integer. By Theorem 3.14, there exists \\(g \\in C\\left(R^k\\right)\\) so that \\(\\|f-g\\|_1\u0026lt;1 / n\\) : Put \\(h=f-g\\).\nSince \\(g\\) is continuous, \\(T g=0\\). Since \\[ \\left(T_r h\\right)(x) \\leq \\frac{1}{m\\left(B_r\\right)} \\int_{B(x, r)}|h| d m+|h(x)| \\] we have \\[ T h \\leq M h+|h| . \\] Since \\(T_r f \\leq T_r g+T_r h\\), it follows that \\[ T f \\leq M h+|h| . \\] Therefore \\[ \\{T f\u0026gt;2 y\\} \\subset\\{M h\u0026gt;y\\} \\cup\\{|h|\u0026gt;y\\} . \\] Denote the union on the right of (6) by \\(E(y, n)\\). Since \\(\\|h\\|_1\u0026lt;1 / n\\), Theorem \\(7.4\\) and the inequality \\(7.5(1)\\) show that \\[ m(E(y, n)) \\leq\\left(3^k+1\\right) /(y n) . \\] The left side of (6) is independent of \\(n\\). Hence \\[ \\{T f\u0026gt;2 y\\} \\subset \\bigcap_{n=1}^{\\infty} E(y, n) . \\] This intersection has measure 0 , by (7), so that \\(\\{T f\u0026gt;2 y\\}\\) is a subset of a set of measure 0. Since Lebesgue measure is complete, \\(\\{T f\u0026gt;2 y\\}\\) is Lebesgue measurable, and has measure 0 . This holds for every positive \\(y\\). Hence \\(T f=0\\) a.e. \\([m]\\).\nTheorem \\(7.7\\) yields interesting information, with very little effort, about topics such as (a) differentiation of absolutely continuous measures, (b) differentiation using sets other than balls, (c) differentiation of indefinite integrals in \\(R^1\\), (d) metric density of measurable sets.\nWe shall now discuss these topics. ### 7.8 Theorem\nSuppose \\(\\mu\\) is a complex Borel measure on \\(R^k\\), and \\(\\mu \\ll m\\). Let \\(f\\) be the Radon-Nikodym derivative of \\(\\mu\\) with respect to \\(m\\). Then \\(D \\mu=f\\) a.e. \\([\\mathrm{m}]\\), and \\[ \\mu(E)=\\int_E(D \\mu) d m \\] for all Borel sets \\(E \\subset R^k\\).\nIn other words, the Radon-Nikodym derivative can also be obtained as a limit of the quotients \\(Q_r \\mu\\).\n7.9 Nicely shrinking sets Suppose \\(x \\in R^k\\). A sequence \\(\\left\\{E_i\\right\\}\\) of Borel sets in \\(R^k\\) is said to shrink to \\(x\\) nicely if there is a number \\(\\alpha\u0026gt;0\\) with the following property: There is a sequence of balls \\(B\\left(x, r_i\\right)\\), with \\(\\lim r_i=0\\), such that \\(E_i \\subset B\\left(x, r_i\\right)\\) and \\[ m\\left(E_i\\right) \\geq \\alpha \\cdot m\\left(B\\left(x, r_i\\right)\\right) \\] for \\(i=1,2,3, \\ldots\\)\nNote that it is not required that \\(x \\in E_i\\), nor even that \\(x\\) be in the closure of \\(E_i\\). Condition (1) is a quantitative version of the requirement that each \\(E_i\\) must occupy a substantial portion of some spherical neighborhood of \\(x\\). For example, a nested sequence of \\(k\\)-cells whose longest edge is at most 1,000 times as long as its shortest edge and whose diameter tends to 0 shrinks nicely. A nested sequence of rectangles (in \\(R^2\\) ) whose edges have lengths \\(1 / i\\) and \\((1 / i)^2\\) does not shrink nicely.\n7.10 Theorem Associate to each \\(x \\in R^k\\) a sequence \\(\\left\\{E_i(x)\\right\\}\\) that shrinks to \\(x\\) nicely, and let \\(f \\in L^1\\left(R^k\\right)\\). Then \\[ f(x)=\\lim _{i \\rightarrow \\infty} \\frac{1}{m\\left(E_i(x)\\right)} \\int_{E_i(x)} f d m \\] at every Lebesgue point of \\(f\\), hence a.e. \\([\\mathrm{m}]\\).\nProof Let \\(x\\) be a Lebesgue point of \\(f\\) and let \\(\\alpha(x)\\) and \\(B\\left(x, r_i\\right)\\) be the positive number and the balls that are associated to the sequence \\(\\left\\{E_i(x)\\right\\}\\). Then, because \\(E_i(x) \\subset B\\left(x, r_i\\right)\\), \\[ \\frac{\\alpha(x)}{m\\left(E_i(x)\\right)} \\int_{E_i(x)}|f-f(x)| d m \\leq \\frac{1}{m\\left(B\\left(x, r_i\\right)\\right)} \\int_{B\\left(x, r_i\\right)}|f-f(x)| d m . \\] The right side converges to 0 as \\(i \\rightarrow \\infty\\), because \\(r_i \\rightarrow 0\\) and \\(x\\) is a Lebesgue point of \\(f\\). Hence the left side converges to 0 , and (1) follows.\nNote that no relation of any sort was assumed to exist between \\(\\left\\{E_i(x)\\right\\}\\) and \\(\\left\\{E_i(y)\\right\\}\\), for different points \\(x\\) and \\(y\\).\nNote also that Theorem \\(7.10\\) leads to a correspondingly stronger form of Theorem 7.8. We omit the details.\n7.11 Theorem If \\(f \\in L^1\\left(R^1\\right)\\) and \\[ F(x)=\\int_{-\\infty}^x f d m \\quad(-\\infty\u0026lt;x\u0026lt;\\infty), \\] then \\(F^{\\prime}(x)=f(x)\\) at every Lebesgue point of \\(f\\), hence a.e. \\([m]\\). (This is the easy half of the fundamental theorem of Calculus, extended to Lebesgue integrals.)\nProof Let \\(\\left\\{\\delta_i\\right\\}\\) be a sequence of positive numbers that converges to 0 . Theorem 7.10, with \\(E_i(x)=\\left[x, x+\\delta_i\\right]\\), shows then that the right-hand derivative of \\(F\\) exists at all Lebesgue points of \\(x\\) of \\(f\\) and that it is equal to \\(f(x)\\) at these points. If we let \\(E_i(x)\\) be \\(\\left[x-\\delta_i, x\\right]\\) instead, we obtain the same result for the left-hand derivative of \\(F\\) at \\(x\\).\n7.12 Metric density Let \\(E\\) be a Lebesgue measurable subset of \\(R^k\\). The metric density of \\(E\\) at a point \\(x \\in R^k\\) is defined to be \\[ \\lim _{r \\rightarrow 0} \\frac{m(E \\cap B(x, r))}{m(B(x, r))} \\] provided, of course, that this limit exists.\nIf we let \\(f\\) be the characteristic function of \\(E\\) and apply Theorem \\(7.8\\) or Theorem 7.10, we see that the metric density of \\(E\\) is 1 at almost every point of \\(E\\), and that it is 0 at almost every point of the complement of \\(E\\).\nHere is a rather striking consequence of this, which should be compared with Exercise 8 in Chap. 2 : If \\(\\epsilon\u0026gt;0\\), there is no set \\(E \\subset R^1\\) that satisfies \\[ \\epsilon\u0026lt;\\frac{m(E \\cap I)}{m(I)}\u0026lt;1-\\epsilon \\] for every segment \\(I\\).\nHaving dealt with differentiation of absolutely continuous measures, we now turn to those that are singular with respect to \\(m\\).\n7.13 Theorem Associate to each \\(x \\in R^k\\) a sequence \\(\\left\\{E_i(x)\\right\\}\\) that shrinks to \\(x\\) nicely. If \\(\\mu\\) is a complex Borel measure and \\(\\mu \\perp m\\), then \\[ \\lim _{i \\rightarrow \\infty} \\frac{\\mu\\left(E_i(x)\\right)}{m\\left(E_i(x)\\right)}=0 \\quad \\text { a.e. }[m] \\] Proof The Jordan decomposition theorem shows that it suffices to prove (1) under the additional assumption that \\(\\mu \\geq 0\\). In that case, arguing as in the proof of Theorem \\(7.10\\), we have \\[ \\frac{\\alpha(x) \\mu\\left(E_i(x)\\right)}{m\\left(E_i(x)\\right)} \\leq \\frac{\\mu\\left(E_i(x)\\right)}{m\\left(B\\left(x, r_i\\right)\\right)} \\leq \\frac{\\mu\\left(B\\left(x, r_i\\right)\\right)}{m\\left(B\\left(x, r_i\\right)\\right)} . \\] Hence (1) is a consequence of the special case \\[ (D \\mu)(x)=0 \\quad \\text { a.e. }[m], \\] which will now be proved.\nThe upper derivative \\(\\bar{D} \\mu\\), defined by \\[ (\\bar{D} \\mu)(x)=\\lim _{n \\rightarrow \\infty}\\left[\\sup _{0\u0026lt;r\u0026lt;1 / n}\\left(Q_r \\mu\\right)(x)\\right] \\quad\\left(x \\in R^k\\right) \\] is a Borel function, because the quantity in brackets decreases as \\(n\\) increases and is, for each \\(n\\), a lower semicontinuous function of \\(x\\); the reasoning used in Sec. \\(7.2\\) proves this.\nChoose \\(\\lambda\u0026gt;0, \\epsilon\u0026gt;0\\). Since \\(\\mu \\perp m, \\mu\\) is concentrated on a set of Lebesgue measure 0 . The regularity of \\(\\mu\\) (Theorem 2.18) shows therefore that there is a compact set \\(K\\), with \\(m(K)=0, \\mu(K)\u0026gt;\\|\\mu\\|-\\epsilon\\).\nDefine \\(\\mu_1(E)=\\mu(K \\cap E)\\), for any Borel set \\(E \\subset R^k\\), and put \\(\\mu_2=\\mu-\\mu_1\\). Then \\(\\left\\|\\mu_2\\right\\|\u0026lt;\\epsilon\\), and, for every \\(x\\) outside \\(K\\), \\[ (\\bar{D} \\mu)(x)=\\left(\\bar{D} \\mu_2\\right)(x) \\leq\\left(M \\mu_2\\right)(x) . \\] Hence \\[ \\{\\bar{D} \\mu\u0026gt;\\lambda\\} \\subset K \\cup\\left\\{M \\mu_2\u0026gt;\\lambda\\right\\}, \\] and Theorem \\(7.4\\) shows that \\[ m\\{\\bar{D} \\mu\u0026gt;\\lambda\\} \\leq 3^k \\lambda^{-1}\\left\\|\\mu_2\\right\\|\u0026lt;3^k \\lambda^{-1} \\epsilon . \\] Since (6) holds for every \\(\\epsilon\u0026gt;0\\) and for every \\(\\lambda\u0026gt;0\\), we conclude that \\(\\bar{D} \\mu=0\\) a.e. \\([m]\\), i.e., that (2) holds. \\(\\quad / / / /\\) Theorems \\(7.10\\) and \\(7.13\\) can be combined in the following way:\n7.14 Theorem Suppose that to each \\(x \\in R^k\\) is associated some sequence \\(\\left\\{E_i(x)\\right\\}\\) that shrinks to \\(x\\) nicely, and that \\(\\mu\\) is a complex Borel measure on \\(R^k\\).\nLet \\(d \\mu=f d m+d \\mu_s\\) be the Lebesgue decomposition of \\(\\mu\\) with respect to \\(m\\). Then \\[ \\lim _{i \\rightarrow \\infty} \\frac{\\mu\\left(E_i(x)\\right)}{m\\left(E_i(x)\\right)}=f(x) \\quad \\text { a.e. }[m] . \\] In particular, \\(\\mu \\perp m\\) if and only if \\((D \\mu)(x)=0\\) a.e. \\([m]\\).\nThe following result contrasts strongly with Theorem 7.13:\n7.15 Theorem If \\(\\mu\\) is a positive Borel measure on \\(R^k\\) and \\(\\mu \\perp m\\), then \\[ (D \\mu)(x)=\\infty \\quad \\text { a.e. }[\\mu] . \\] Proof There is a Borel set \\(S \\subset R^k\\) with \\(m(S)=0\\) and \\(\\mu\\left(R^k-S\\right)=0\\), and there are open sets \\(V_j \\supset S\\) with \\(m\\left(V_j\\right)\u0026lt;1 / j\\), for \\(j=1,2,3, \\ldots\\).\nFor \\(N=1,2,3, \\ldots\\), let \\(E_N\\) be the set of all \\(x \\in S\\) to which correspond radii \\(r_i=r_i(x)\\), with \\(\\lim r_i=0\\), such that \\[ \\mu\\left(B\\left(x, r_i\\right)\\right)\u0026lt;N m\\left(B\\left(x, r_i\\right)\\right) . \\] Then (1) holds for every \\(x \\in S-\\bigcup_N E_N\\).\nFix \\(N\\) and \\(j\\), for the moment. Every \\(x \\in E_N\\) is then the center of a ball \\(B_x \\subset V_j\\) that satisfies (2). Let \\(\\beta_x\\) be the open ball with center \\(x\\) whose radius is \\(1 / 3\\) of that of \\(B_x\\). The union of these balls \\(\\beta_x\\) is an open set \\(W_{j, N}\\) that contains \\(E_N\\) and lies in \\(V_j\\). We claim that \\[ \\mu\\left(W_{j, N}\\right)\u0026lt;3^k N / j . \\] To prove (3), let \\(K \\subset W_{j, N}\\) be compact. Finitely many \\(\\beta_x\\) cover \\(K\\). Lemma \\(7.3\\) shows therefore that there is a finite set \\(F \\subset E_N\\) with the following properties:\n\\(\\left\\{\\beta_x: x \\in F\\right\\}\\) is a disjoint collection, and \\(K \\subset \\bigcup_{x \\in F} B_x\\). Thus \\[ \\begin{aligned} \\mu(K) \u0026amp; \\leq \\sum_{x \\in F} \\mu\\left(B_x\\right)\u0026lt;N \\sum_{x \\in F} m\\left(B_x\\right) \\\\ \u0026amp; =3^k N \\sum_{x \\in F} m\\left(\\beta_x\\right) \\leq 3^k N m\\left(V_j\\right)\u0026lt;3^k N / j . \\end{aligned} \\] This proves (3).\nNow put \\(\\Omega_N=\\bigcap_j W_{j, N}\\). Then \\(E_N \\subset \\Omega_N, \\Omega_N\\) is a \\(G_\\delta, \\mu\\left(\\Omega_N\\right)=0\\), and \\((D \\mu)(x)=\\infty\\) at every point of \\(S-\\bigcup_N \\Omega_N\\).\n","date":"2023-01-01T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-7/1-derivatives-of-measures/","section":"papa rudin","tags":null,"title":"Derivatives of Measures"},{"categories":null,"contents":"6.15 Let \\(\\mu\\) be a positive measure, suppose \\(1 \\leq p \\leq \\infty\\), and let \\(q\\) be the exponent conjugate to \\(p\\). The Hölder inequality (Theorem 3.8) shows that if \\(g \\in L^q(\\mu)\\) and if \\(\\Phi_g\\) is defined by\n\\[ \\Phi_g(f)=\\int_X f g d \\mu, \\]\nthen \\(\\Phi_g\\) is a bounded linear functional on \\(L^p(\\mu)\\), of norm at most \\(\\|g\\|_q\\). The question naturally arises whether all bounded linear functionals on \\(L^p(\\mu)\\) have this form, and whether the representation is unique.\nFor \\(p=\\infty\\), Exercise 13 shows that the answer is negative: \\(L^1(m)\\) does not furnish all bounded linear functionals on \\(L^{\\infty}(m)\\). For \\(1\u0026lt;p\u0026lt;\\infty\\), the answer is affirmative. It is also affirmative for \\(p=1\\), provided certain measure-theoretic pathologies are excluded. For \\(\\sigma\\)-finite measure spaces, no difficulties arise, and we shall confine ourselves to this case.\nthen \\(\\Phi_g\\) is a bounded linear functional on \\(L^p(\\mu)\\), of norm at most \\(\\|g\\|_q\\). The question naturally arises whether all bounded linear functionals on \\(L^p(\\mu)\\) have this form, and whether the representation is unique.\nFor \\(p=\\infty\\), Exercise 13 shows that the answer is negative: \\(L^1(m)\\) does not furnish all bounded linear functionals on \\(L^{\\infty}(m)\\). For \\(1\u0026lt;p\u0026lt;\\infty\\), the answer is affirmative. It is also affirmative for \\(p=1\\), provided certain measure-theoretic pathologies are excluded. For \\(\\sigma\\)-finite measure spaces, no difficulties arise, and we shall confine ourselves to this case.\n6.16 Theorem Suppose \\(1 \\leq p\u0026lt;\\infty, \\mu\\) is a \\(\\sigma\\)-finite positive measure on \\(X\\), and \\(\\Phi\\) is a bounded linear functional on \\(L^p(\\mu)\\). Then there is a unique \\(g \\in L^q(\\mu)\\), where \\(q\\) is the exponent conjugate to \\(p\\), such that\n\\[ \\Phi(f)=\\int_X f g d \\mu \\quad\\left(f \\in L^p(\\mu)\\right) . \\]\nMoreover, if \\(\\Phi\\) and \\(g\\) are related as in (1), we have\n\\[ \\|\\boldsymbol{\\Phi}\\|=\\|g\\|_{\\boldsymbol{q}} . \\]\nIn other words, \\(L^q(\\mu)\\) is isometrically isomorphic to the dual space of \\(L^p(\\mu)\\), under the stated conditions.\nProof The uniqueness of \\(g\\) is clear, for if \\(g\\) and \\(g^{\\prime}\\) satisfy (1), then the integral of \\(g-g^{\\prime}\\) over any measurable set \\(E\\) of finite measure is 0 (as we see by taking \\(\\chi_E\\) for \\(f\\) ), and the \\(\\sigma\\)-finiteness of \\(\\mu\\) implies therefore that \\(g-g^{\\prime}=0\\) a.e. Next, if (1) holds, Hölder’s inequality implies\n\\[ \\|\\Phi\\| \\leq\\|g\\|_q . \\]\nSo it remains to prove that \\(g\\) exists and that equality holds in (3). If \\(\\|\\Phi\\|=0\\), (1) and (2) hold with \\(g=0\\). So assume \\(\\|\\Phi\\|\u0026gt;0\\). We first consider the case \\(\\mu(X)\u0026lt;\\infty\\). For any measurable set \\(E \\subset X\\), define\n\\[ \\lambda(E)=\\Phi\\left(\\chi_E\\right) \\]\nSince \\(\\Phi\\) is linear, and since \\(\\chi_{A \\cup B}=\\chi_A+\\chi_B\\) if \\(A\\) and \\(B\\) are disjoint, we see that \\(\\lambda\\) is additive. To prove countable additivity, suppose \\(E\\) is the union of countably many disjoint measurable sets \\(E_i\\), put \\(A_k=E_1 \\cup \\cdots \\cup E_k\\), and note that\n\\[ \\left\\|\\chi_E-\\chi_{A_k}\\right\\|_p=\\left[\\mu\\left(E-A_k\\right)\\right]^{1 / p} \\rightarrow 0 \\quad(k \\rightarrow \\infty) ; \\]\nthe continuity of \\(\\Phi\\) shows now that \\(\\lambda\\left(A_k\\right) \\rightarrow \\lambda(E)\\). So \\(\\lambda\\) is a complex measure. [In (4) the assumption \\(p\u0026lt;\\infty\\) was used.] It is clear that \\(\\lambda(E)=0\\) if \\(\\mu(E)=0\\),\nsince then \\(\\left\\|\\chi_E\\right\\|_p=0\\). Thus \\(\\lambda \\ll \\mu\\), and the Radon-Nikodym theorem ensures the existence of a function \\(g \\in L^1(\\mu)\\) such that, for every measurable \\(E \\subset X\\),\n\\[ \\Phi\\left(\\chi_E\\right)=\\int_E g d \\mu=\\int_X \\chi_E g d \\mu . \\]\nBy linearity it follows that\n\\[ \\Phi(f)=\\int_{\\boldsymbol{X}} f g d \\mu \\]\nholds for every simple measurable \\(f\\), and so also for every \\(f \\in L^{\\infty}(\\mu)\\), since every \\(f \\in L^{\\infty}(\\mu)\\) is a uniform limit of simple functions \\(f_i\\). Note that the uniform convergence of \\(f_i\\) to \\(f\\) implies \\(\\left\\|f_i-f\\right\\|_p \\rightarrow 0\\), hence \\(\\Phi\\left(f_i\\right) \\rightarrow \\Phi(f)\\), as \\(i \\rightarrow \\infty\\).\nWe want to conclude that \\(g \\in L^q(\\mu)\\) and that (2) holds; it is best to split the argument into two cases.\nCASE \\(1 p=1\\). Here (5) shows that\n\\[ \\left|\\int_E g d \\mu\\right| \\leq\\|\\Phi\\| \\cdot\\left\\|\\chi_E\\right\\|_1=\\|\\Phi\\| \\cdot \\mu(E) \\]\nfor every \\(E \\in \\mathfrak{M}\\). By Theorem \\(1.40,|g(x)| \\leq\\|\\Phi\\|\\) a.e., so that \\(\\|g\\|_{\\infty} \\leq\\|\\Phi\\|\\).\nCASE \\(21\u0026lt;p\u0026lt;\\infty\\). There is a measurable function \\(\\alpha,|\\alpha|=1\\), such that \\(\\alpha g=|g|\\) [Proposition \\(1.9(e)\\) ]. Let \\(E_n=\\{x:|g(x)| \\leq n\\}\\), and define \\(f=\\) \\(\\chi_{E_n}|g|^{q-1} \\alpha\\). Then \\(|f|^p=|g|^q\\) on \\(E_n, f \\in L^{\\infty}(\\mu)\\), and (6) gives\n\\[ \\int_{E_n}|g|^q d \\mu=\\int_X f g d \\mu=\\Phi(f) \\leq\\|\\Phi\\|\\left\\{\\int_{E_n}|g|^q\\right\\}^{1 / p}, \\]\nso that\n\\[ \\int_X \\chi_{E_n}|g|^q d \\mu \\leq\\|\\Phi\\|^q \\quad(n=1,2,3, \\ldots) . \\]\nIf we apply the monotone convergence theorem to (7), we obtain \\(\\|g\\|_q \\leq\\|\\Phi\\|\\).\nThus (2) holds and \\(g \\in L^q(\\mu)\\). It follows that both sides of (6) are continuous functions on \\(L^p(\\mu)\\). They coincide on the dense subset \\(L^{\\infty}(\\mu)\\) of \\(L^p(\\mu)\\); hence they coincide on all of \\(L^p(\\mu)\\), and this completes the proof if \\(\\mu(X)\u0026lt;\\infty\\).\nIf \\(\\mu(X)=\\infty\\) but \\(\\mu\\) is \\(\\sigma\\)-finite, choose \\(w \\in L^1(\\mu)\\) as in Lemma 6.9. Then \\(d \\tilde{\\mu}=w d \\mu\\) defines a finite measure on \\(\\mathfrak{M}\\), and\n\\[ F \\rightarrow w^{1 / p} F \\]\nis a linear isometry of \\(L^p(\\tilde{\\mu})\\) onto \\(L^p(\\mu)\\), because \\(w(x)\u0026gt;0\\) for every \\(x \\in X\\). Hence\n\\[ \\Psi(F)=\\Phi\\left(w^{1 / p} F\\right) \\]\ndefines a bounded linear functional \\(\\Psi\\) on \\(L^p(\\tilde{\\mu})\\), with \\(\\|\\Psi\\|=\\|\\Phi\\|\\).\nThe first part of the proof shows now that there exists \\(G \\in L^q(\\tilde{\\mu})\\) such that\n\\[ \\Psi(F)=\\int_X F G d \\tilde{\\mu} \\quad\\left(F \\in L^p(\\tilde{\\mu})\\right) . \\]\nPut \\(g=w^{1 / q} G\\). (If \\(p=1, g=G\\).) Then\n\\[ \\int_X|g|^q d \\mu=\\int_X|G|^q d \\tilde{\\mu}=\\|\\Psi\\|^q=\\|\\Phi\\|^q \\]\nif \\(p\u0026gt;1\\), whereas \\(\\|g\\|_{\\infty}=\\|G\\|_{\\infty}=\\|\\Psi\\|=\\|\\Phi\\|\\) if \\(p=1\\). Thus (2) holds, and since \\(G d \\tilde{\\mu}=w^{1 / p} g d \\mu\\), we finally get\n\\[ \\Phi(f)=\\Psi\\left(w^{-1 / p} f\\right)=\\int_X w^{-1 / p} f G d \\tilde{\\mu}=\\int_X f g d \\mu \\]\nfor every \\(f \\in L^p(\\mu)\\).\n6.17 Remark We have already encountered the special case \\(p=q=2\\) of Theorem 6.16. In fact, the proof of the general case was based on this special case, for we used the knowledge of the bounded linear functionals on \\(L^2(\\mu)\\) in the proof of the Radon-Nikodym theorem, and the latter was the key to the proof of Theorem 6.16. The special case \\(p=2\\), in turn, depended on the completeness of \\(L^2(\\mu)\\), on the fact that \\(L^2(\\mu)\\) is therefore a Hilbert space, and on the fact that the bounded linear functionals on a Hilbert space are given by inner products. We now turn to the complex version of Theorem 2.14.\n","date":"2022-12-20T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-6/4-bounded-linear-functionals-on-lp/","section":"papa rudin","tags":null,"title":"Bounded linear functional on Lp"},{"categories":null,"contents":"6.12 Theorem Let \\(\\mu\\) be a complex measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in \\(X\\). Then there is a measurable function \\(h\\) such that \\(|h(x)|=1\\) for all \\(x \\in X\\) and such that \\[ d \\mu=h d|\\mu| . \\]\nBy analogy with the representation of a complex number as the product of ts absolute value and a number of absolute value 1, Eq. (1) is sometimes referred o as the polar representation (or polar decomposition) of \\(\\mu\\).\nProof It is trivial that \\(\\mu \\ll|\\mu|\\), and therefore the Radon-Nikodym theorem guarantees the existence of some \\(h \\in L^1(|\\mu|)\\) which satisfies (1).\nLet \\(A_r=\\{x:|h(x)|\u0026lt;r\\}\\), where \\(r\\) is some positive number, and let \\(\\left\\{E_j\\right\\}\\) be a partition of \\(A_r\\). Then\n\\[ \\sum_j\\left|\\mu\\left(E_j\\right)\\right|=\\sum_j\\left|\\int_{E_j} h d\\right| \\mu|| \\leq \\sum_j r|\\mu|\\left(E_j\\right)=r|\\mu|\\left(A_r\\right), \\]\nso that \\(|\\mu|\\left(A_r\\right) \\leq r|\\mu|\\left(A_r\\right)\\). If \\(r\u0026lt;1\\), this forces \\(|\\mu|\\left(A_r\\right)=0\\). Thus \\(|h| \\geq 1\\) a.e. On the other hand, if \\(|\\mu|(E)\u0026gt;0\\), (1) shows that\n\\[ \\left|\\frac{1}{|\\mu|(E)} \\int_E h d\\right| \\mu||=\\frac{|\\mu(E)|}{|\\mu|(E)} \\leq 1 \\]\nWe now apply Theorem \\(1.40\\) (with the closed unit disc in place of \\(S\\) ) and conclude that \\(|h| \\leq 1\\) a.e.\nLet \\(B=\\{x \\in X:|h(x)| \\neq 1\\}\\). We have shown that \\(|\\mu|(B)=0\\), and if we redefine \\(h\\) on \\(B\\) so that \\(h(x)=1\\) on \\(B\\), we obtain a function with the desired properties.\n6.13 Theorem Suppose \\(\\mu\\) is a positive measure on \\(\\mathfrak{M}, g \\in L^1(\\mu)\\), and\n\\[ \\lambda(E)=\\int_E g d \\mu \\quad(E \\in \\mathfrak{M}) . \\]\nThen\n\\[ |\\lambda|(E)=\\int_E|g| d \\mu \\quad(E \\in \\mathfrak{M}) \\]\nProof By Theorem 6.12, there is a function \\(h\\), of absolute value 1, such that \\(d \\lambda=h d|\\lambda|\\). By hypothesis, \\(d \\lambda=g d \\mu\\). Hence\n\\[ h d|\\lambda|=g d \\mu . \\]\nThis gives \\(d|\\lambda|=\\bar{h} g d \\mu\\). (Compare with Theorem 1.29.) Since \\(|\\lambda| \\geq 0\\) and \\(\\mu \\geq 0\\), it follows that \\(\\bar{h} g \\geq 0\\) a.e. \\([\\mu]\\), so that \\(\\bar{h} g=|g|\\) a.e. \\([\\mu]\\).\n6.14 The Hahn Decomposition Theorem Let \\(\\mu\\) be a real measure on a \\(\\sigma\\) algebra \\(\\mathfrak{M}\\) in a set \\(X\\).\nThen there exist sets \\(A\\) and \\(B \\in \\mathfrak{M}\\) such that\n\\(A \\cup B=X, A \\cap B=\\varnothing\\), and such that the positive and negative variations \\(\\mu^{+}\\)and \\(\\mu^{-}\\)of \\(\\mu\\) satisfy\n\\[ \\mu^{+}(E)=\\mu(A \\cap E), \\quad \\mu^{-}(E)=-\\mu(B \\cap E) \\quad(E \\in \\mathfrak{M}) . \\]\nIn other words, \\(X\\) is the union of two disjoint measurable sets \\(A\\) and \\(B\\), such that ” \\(A\\) carries all the positive mass of \\(\\mu\\) ” [since (1) implies that \\(\\mu(E) \\geq 0\\) if \\(E \\subset A]\\) and ” \\(B\\) carries all the negative mass of \\(\\mu\\) ” [since \\(\\mu(E) \\leq 0\\) if \\(E \\subset B]\\). The pair \\((A, B)\\) is called a Hahn decomposition of \\(X\\), induced by \\(\\mu\\).\nProof By Theorem 6.12, \\(d \\mu=h d|\\mu|\\), where \\(|h|=1\\). Since \\(\\mu\\) is real, it follows that \\(h\\) is real (a.e., and therefore everywhere, by redefining on a set of measure 0 ), hence \\(h=\\pm 1\\). Put\n\\[ A=\\{x: h(x)=1\\}, \\quad B=\\{x: h(x)=-1\\} . \\]\nSince \\(\\mu^{+}=\\frac{1}{2}(|\\mu|+\\mu)\\), and since\n\\[ \\frac{1}{2}(1+h)= \\begin{cases}h \u0026amp; \\text { on } A, \\\\ 0 \u0026amp; \\text { on } B\\end{cases} \\]\nwe have, for any \\(E \\in \\mathfrak{M}\\),\n\\[ \\mu^{+}(E)=\\frac{1}{2} \\int_E(1+h) d|\\mu|=\\int_{E \\cap A} h d|\\mu|=\\mu(E \\cap A) . \\]\nSince \\(\\mu(E)=\\mu(E \\cap A)+\\mu(E \\cap B)\\) and since \\(\\mu=\\mu^{+}-\\mu^{-}\\), the second half of (1) follows from the first.\nCorollary If \\(\\mu=\\lambda_1-\\lambda_2\\), where \\(\\lambda_1\\) and \\(\\lambda_2\\) are positive measures, then \\(\\lambda_1 \\geq \\mu^{+}\\) and \\(\\lambda_2 \\geq \\mu^{-}\\).\nThis is the minimum property of the Jordan decomposition which was mentioned in Sec. 6.6.\nProof Since \\(\\mu \\leq \\lambda_1\\), we have \\[ \\mu^{+}(E)=\\mu(E \\cap A) \\leq \\lambda_1(E \\cap A) \\leq \\lambda_1(E) . \\]\n","date":"2022-12-20T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-6/3-consequences-of-the-radon-nikodym-theorem/","section":"papa rudin","tags":null,"title":"Consequences of the Radon-Nikodym Theorem"},{"categories":null,"contents":"6.7 Definitions Let \\(\\mu\\) be a positive measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\), and let \\(\\lambda\\) be an arbitrary measure on \\(\\mathfrak{M} ; \\lambda\\) may be positive or complex. (Recall that a complex measure has its range in the complex plane, but that our usage of the term “positive measure” includes \\(\\infty\\) as an admissible value. Thus the positive measures do not form a subclass of the complex ones.)\nWe say that \\(\\lambda\\) is absolutely continuous with respect to \\(\\mu\\), and write\n\\[ \\lambda \\ll \\mu \\]\nif \\(\\lambda(E)=0\\) for every \\(E \\in \\mathfrak{M}\\) for which \\(\\mu(E)=0\\). If there is a set \\(A \\in \\mathfrak{M}\\) such that \\(\\lambda(E)=\\lambda(A \\cap E)\\) for every \\(E \\in \\mathfrak{M}\\), we say that \\(\\lambda\\) is concentrated on \\(A\\). This is equivalent to the hypothesis that \\(\\lambda(E)=0\\) whenever \\(E \\cap A=\\varnothing\\).\nSuppose \\(\\lambda_1\\) and \\(\\lambda_2\\) are measures on \\(\\mathfrak{M}\\), and suppose there exists a pair of disjoint sets \\(A\\) and \\(B\\) such that \\(\\lambda_1\\) is concentrated on \\(A\\) and \\(\\lambda_2\\) is concentrated on \\(B\\). Then we say that \\(\\lambda_1\\) and \\(\\lambda_2\\) are mutually singular, and write\n\\[ \\lambda_1 \\perp \\lambda_2 . \\]\nHere are some elementary properties of these concepts.\n6.8 Proposition Suppose, \\(\\mu, \\lambda, \\lambda_1\\), and \\(\\lambda_2\\) are measures on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\), and \\(\\mu\\) is positive.\nIf \\(\\lambda\\) is concentrated on \\(A\\), so is \\(|\\lambda|\\). If \\(\\lambda_1 \\perp \\lambda_2\\), then \\(\\left|\\lambda_1\\right| \\perp\\left|\\lambda_2\\right|\\). If \\(\\lambda_1 \\perp \\mu\\) and \\(\\lambda_2 \\perp \\mu\\), then \\(\\lambda_1+\\lambda_2 \\perp \\mu\\). If \\(\\lambda_1 \\ll \\mu\\) and \\(\\lambda_2 \\ll \\mu\\), then \\(\\lambda_1+\\lambda_2 \\ll \\mu\\). If \\(\\lambda \\ll \\mu\\), then \\(|\\lambda| \\ll \\mu\\). If \\(\\lambda_1 \\ll \\mu\\) and \\(\\lambda_2 \\perp \\mu\\), then \\(\\lambda_1 \\perp \\lambda_2\\). If \\(\\lambda \\ll \\mu\\) and \\(\\lambda \\perp \\mu\\), then \\(\\lambda=0\\). PROOF (a) If \\(E \\cap A=\\varnothing\\) and \\(\\left\\{E_j\\right\\}\\) is any partition of \\(E\\), then \\(\\lambda\\left(E_j\\right)=0\\) for all \\(j\\). Hence \\(|\\lambda|(E)=0\\). (b) This follows immediately from \\((a)\\). (c) There are disjoint sets \\(A_1\\) and \\(B_1\\) such that \\(\\lambda_1\\) is concentrated on \\(A_1\\) and \\(\\mu\\) on \\(B_1\\), and there are disjoint sets \\(A_2\\) and \\(B_2\\) such that \\(\\lambda_2\\) is concentrated on \\(A_2\\) and \\(\\mu\\) on \\(B_2\\). Hence \\(\\lambda_1+\\lambda_2\\) is concentrated on \\(A=A_1 \\cup\\) \\(A_2, \\mu\\) is concentrated on \\(B=B_1 \\cap B_2\\), and \\(A \\cap B=\\varnothing\\). (d) This is obvious. (e) Suppose \\(\\mu(E)=0\\), and \\(\\left\\{E_j\\right\\}\\) is a partition of \\(E\\). Then \\(\\mu\\left(E_j\\right)=0\\); and since \\(\\lambda \\ll \\mu, \\lambda\\left(E_j\\right)=0\\) for all \\(j\\), hence \\(\\sum\\left|\\lambda\\left(E_j\\right)\\right|=0\\). This implies \\(|\\lambda|(E)=0\\).\nSince \\(\\lambda_2 \\perp \\mu\\), there is a set \\(A\\) with \\(\\mu(A)=0\\) on which \\(\\lambda_2\\) is concentrated. Since \\(\\lambda_1 \\ll \\mu, \\lambda_1(E)=0\\) for every \\(E \\subset A\\). So \\(\\lambda_1\\) is concentrated on the complement of \\(A\\). By \\((f)\\), the hypothesis of \\((g)\\) implies, that \\(\\lambda \\perp \\lambda\\), and this clearly forces \\(\\lambda=0\\). We come now to the principal theorem about absolute continuity. In fact, it is probably the most important theorem in measure theory. Its statement will involve \\(\\sigma\\)-finite measures. The following lemma describes one of their significant properties.\n\\(6.9\\) Lemma If \\(\\mu\\) is a positive \\(\\sigma\\)-finite measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in a set \\(X\\), then there is a function \\(w \\in L^1(\\mu)\\) such that \\(0\u0026lt;w(x)\u0026lt;1\\) for every \\(x \\in X\\).\nProof To say that \\(\\mu\\) is \\(\\sigma\\)-finite means that \\(X\\) is the union of countably many sets \\(E_n \\in \\mathfrak{M}(n=1,2,3, \\ldots)\\) for which \\(\\mu\\left(E_n\\right)\\) is finite. Put \\(w_n(x)=0\\) if \\(x \\in\\) \\(X-E_n\\) and put\n\\[ w_n(x)=2^{-n} /\\left(1+\\mu\\left(E_n\\right)\\right) \\]\nif \\(x \\in E_n\\). Then \\(w=\\sum_1^{\\infty} w_n\\) has the required properties.\nThe point of the lemma is that \\(\\mu\\) can be replaced by a finite measure \\(\\tilde{\\mu}\\) (namely, \\(d \\tilde{\\mu}=w d \\mu\\) ) which, because of the strict positivity of \\(w\\), has precisely the same sets of measure 0 as \\(\\mu\\).\n6.10 The Theorem of Lebesgue-Radon-Nikodym Let \\(\\mu\\) be a positive \\(\\sigma\\)-finite measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in a set \\(X\\), and let \\(\\lambda\\) be a complex measure on \\(\\mathfrak{M}\\).\nThere is then a unique pair of complex measures \\(\\lambda_a\\) and \\(\\lambda_s\\) on \\(\\mathfrak{M}\\) such that \\[ \\lambda=\\lambda_a+\\lambda_s, \\quad \\lambda_a \\ll \\mu, \\quad \\lambda_s \\perp \\mu . \\]\nIf \\(\\lambda\\) is positive and finite, then so are \\(\\lambda_a\\) and \\(\\lambda_s\\).\nThere is a unique \\(h \\in L^1(\\mu)\\) such that \\[ \\lambda_a(E)=\\int_E h d \\mu \\]\nfor every set \\(E \\in \\mathfrak{M}\\). The pair \\(\\left(\\lambda_a, \\lambda_s\\right)\\) is called the Lebesgue decomposition of \\(\\lambda\\) relative to \\(\\mu\\). The uniqueness of the decomposition is easily seen, for if \\(\\left(\\lambda_a^{\\prime}, \\lambda_s^{\\prime}\\right)\\) is another pair which satisfies (1), then\n\\[ \\lambda_a^{\\prime}-\\lambda_a=\\lambda_s-\\lambda_s^{\\prime}, \\]\n\\(\\lambda_a^{\\prime}-\\lambda_\\alpha \\ll \\mu\\), and \\(\\lambda_s-\\lambda_s^{\\prime} \\perp \\mu\\), hence both sides of (3) are 0 ; we have used \\(6.8(c)\\), \\(6.8(d)\\), and \\(6.8(g)\\).\nThe existence of the decomposition is the significant part of \\((a)\\). Assertion (b) is known as the Radon-Nikodym theorem. Again, uniqueness of \\(h\\) is immediate, from Theorem \\(1.39(b)\\). Also, if \\(h\\) is any member of \\(L^1(\\mu)\\), the integral in (2) defines a measure on \\(\\mathfrak{M}\\) (Theorem 1.29) which is clearly absolutely continuous with respect to \\(\\mu\\). The point of the Radon-Nikodym theorem is the converse: Every \\(\\lambda \\ll \\mu\\) (in which case \\(\\lambda_a=\\lambda\\) ) is obtained in this way.\nThe function \\(h\\) which occurs in (2) is called the Radon-Nikodym derivative of \\(\\lambda_a\\) with respect to \\(\\mu\\). As noted after Theorem \\(1.29\\), we may express (2) in the form \\(d \\lambda_a=h d \\mu\\), or even in the form \\(h=d \\lambda_a / d \\mu\\).\nThe idea of the following proof, which yields both \\((a)\\) and \\((b)\\) at one stroke, is due to von Neumann.\nProof Assume first that \\(\\lambda\\) is a positive bounded measure on \\(\\mathfrak{M}\\). Associate \\(w\\) to \\(\\mu\\) as in Lemma 6.9. Then \\(d \\varphi=d \\lambda+w d \\mu\\) defines a positive bounded measure \\(\\varphi\\) on \\(\\mathfrak{M}\\). The definition of the sum of two measures shows that\n\\[ \\int_X f d \\varphi=\\int_X f d \\lambda+\\int_X f w d \\mu \\]\nfor \\(f=\\chi_E\\), hence for simple \\(f\\), hence for any nonnegative measurable \\(f\\). If \\(f \\in L^2(\\varphi)\\), the Schwarz inequality gives\n\\[ \\left|\\int_X f d \\lambda\\right| \\leq \\int_X|f| d \\lambda \\leq \\int_X|f| d \\varphi \\leq\\left\\{\\int_X|f|^2 d \\varphi\\right\\}^{1 / 2}\\{\\varphi(X)\\}^{1 / 2} \\]\nSince \\(\\varphi(X)\u0026lt;\\infty\\), we see that\n\\[ f \\rightarrow \\int_{\\boldsymbol{X}} f d \\lambda \\]\nis a bounded linear functional on \\(L^2(\\varphi)\\). We know that every bounded linear functional on a Hilbert space \\(H\\) is given by an inner product with an element of \\(H\\). Hence there exists a \\(g \\in L^2(\\varphi)\\) such that\n\\[ \\int_x f d \\lambda=\\int_X f g d \\varphi \\]\nfor every \\(f \\in L^2(\\varphi)\\).\nObserve how the completeness of \\(L^2(\\varphi)\\) was used to guarantee the existence of \\(g\\). Observe also that although \\(g\\) is defined uniquely as an element of \\(L^2(\\varphi), g\\) is determined only a.e. \\([\\varphi]\\) as a point function on \\(X\\).\nPut \\(f=\\chi_E\\) in (6), for any \\(E \\in \\mathfrak{M}\\) with \\(\\varphi(E)\u0026gt;0\\). The left side of (6) is then \\(\\lambda(E)\\), and since \\(0 \\leq \\lambda \\leq \\varphi\\), we have\n\\[ 0 \\leq \\frac{1}{\\varphi(E)} \\int_E g d \\varphi=\\frac{\\lambda(E)}{\\varphi(E)} \\leq 1 . \\]\nHence \\(g(x) \\in[0,1]\\) for almost all \\(x\\) (with respect to \\(\\varphi\\) ), by Theorem \\(1.40\\). We may therefore assume that \\(0 \\leq g(x) \\leq 1\\) for every \\(x \\in X\\), without affecting (6), and we rewrite (6) in the form\n\\[ \\int_X(1-g) f d \\lambda=\\int_X f g w d \\mu \\]\nPut\n\\[ A=\\{x: 0 \\leq g(x)\u0026lt;1\\}, \\quad B=\\{x: g(x)=1\\} \\]\nand define measures \\(\\lambda_a\\) and \\(\\lambda_s\\) by\n\\[ \\lambda_a(E)=\\lambda(A \\cap E), \\quad \\lambda_s(E)=\\lambda(B \\cap E), \\]\nfor all \\(E \\in \\mathfrak{M}\\).\nIf \\(f=\\chi_B\\) in (8), the left side is 0 , the right side is \\(\\int_B w d \\mu\\). Since \\(w(x)\u0026gt;0\\) for all \\(x\\), we conclude that \\(\\mu(B)=0\\). Thus \\(\\lambda_s \\perp \\mu\\).\nSince \\(g\\) is bounded, (8) holds if \\(f\\) is replaced by\n\\[ \\left(1+g+\\cdots+g^n\\right) \\chi_E \\]\nfor \\(n=1,2,3, \\ldots, E \\in \\mathfrak{M}\\). For \\(\\operatorname{such} f\\), (8) becomes\n\\[ \\int_E\\left(1-g^{n+1}\\right) d \\lambda=\\int_E g\\left(1+g+\\cdots+g^n\\right) w d \\mu . \\]\nAt every point of \\(B, g(x)=1\\), hence \\(1-g^{n+1}(x)=0\\). At every point of \\(A\\), \\(g^{n+1}(x) \\rightarrow 0\\) monotonically. The left side of (11) converges therefore to \\(\\lambda(A \\cap E)=\\lambda_a(E)\\) as \\(n \\rightarrow \\infty\\).\nThe integrands on the right side of (11) increase monotonically to a nonnegative measurable limit \\(h\\), and the monotone convergence theorem shows that the right side of (11) tends to \\(\\int_E h d \\mu\\) as \\(n \\rightarrow \\infty\\).\nWe have thus proved that (2) holds for every \\(E \\in \\mathfrak{M}\\). Taking \\(E=X\\), we see that \\(h \\in L^1(\\mu)\\), since \\(\\lambda_a(X)\u0026lt;\\infty\\).\nFinally, (2) shows that \\(\\lambda_a \\ll \\mu\\), and the proof is complete for positive \\(\\lambda\\). If \\(\\lambda\\) is a complex measure on \\(\\mathfrak{M}\\), then \\(\\lambda=\\lambda_1+i \\lambda_2\\), with \\(\\lambda_1\\) and \\(\\lambda_2\\) real, and we can apply the preceding case to the positive and negative variations of \\(\\lambda_1\\) and \\(\\lambda_2\\).\nIf both \\(\\mu\\) and \\(\\lambda\\) are positive and \\(\\sigma\\)-finite, most of Theorem \\(6.10\\) is still true. We can now write \\(X=\\bigcup X_n\\), where \\(\\mu\\left(X_n\\right)\u0026lt;\\infty\\) and \\(\\lambda\\left(X_n\\right)\u0026lt;\\infty\\), for \\(n=1,2,3\\), … The Lebesgue decompositions of the measures \\(\\lambda\\left(E \\cap X_n\\right)\\) still give us a Lebesgue decomposition of \\(\\lambda\\), and we still get a function \\(h\\) which satisfies Eq. 6.10(2); however, it is no longer true that \\(h \\in L^1(\\mu)\\), although \\(h\\) is “locally in \\(L^1\\),” i.e., \\(\\int_{X_n} h d \\mu\u0026lt;\\infty\\) for each \\(n\\).\nFinally, if we go beyond \\(\\sigma\\)-finiteness, we meet situations where the two theorems under consideration actually fail. For example, let \\(\\mu\\) be Lebesgue measure on \\((0,1)\\), and let \\(\\lambda\\) be the counting measure on the \\(\\sigma\\)-algebra of all Lebesgue\n6.11 Theorem Suppose \\(\\mu\\) and \\(\\lambda\\) are measures on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}, \\mu\\) is positive, and \\(\\lambda\\) is complex. Then the following two conditions are equivalent:\n\\(\\lambda \\ll \\mu\\). To every \\(\\epsilon\u0026gt;0\\) corresponds a \\(\\delta\u0026gt;0\\) such that \\(|\\lambda(E)|\u0026lt;\\epsilon\\) for all \\(E \\in \\mathfrak{M}\\) with \\(\\mu(E)\u0026lt;\\delta\\). Property \\((b)\\) is sometimes used as the definition of absolute continuity.However, \\((a)\\) does not imply \\((b)\\) if \\(\\lambda\\) is a positive unbounded measure. For instance, let \\(\\mu\\) be Lebesgue measure on \\((0,1)\\), and put\n\\[ \\lambda(E)=\\int_E t^{-1} d t \\]\nfor every Lebesgue measurable set \\(E \\subset(0,1)\\).\nProof Suppose \\((b)\\) holds. If \\(\\mu(E)=0\\), then \\(\\mu(E)\u0026lt;\\delta\\) for every \\(\\delta\u0026gt;0\\), hence \\(|\\lambda(E)|\u0026lt;\\epsilon\\) for every \\(\\epsilon\u0026gt;0\\), so \\(\\lambda(E)=0\\). Thus \\((b)\\) implies \\((a)\\).\nSuppose \\((b)\\) is false. Then there exists an \\(\\epsilon\u0026gt;0\\) and there exist sets \\(E_n \\in\\) \\(\\mathfrak{M}(n=1,2,3, \\ldots)\\) such that \\(\\mu\\left(E_n\\right)\u0026lt;2^{-n}\\) but \\(\\left|\\lambda\\left(E_n\\right)\\right| \\geq \\epsilon\\). Hence \\(|\\lambda|\\left(E_n\\right) \\geq \\epsilon\\). Put\n\\[ A_n=\\bigcup_{i=n}^{\\infty} E_i, \\quad A=\\bigcap_{n=1}^{\\infty} A_n . \\]\nThen \\(\\mu\\left(A_n\\right)\u0026lt;2^{-n+1}, A_n \\supset A_{n+1}\\), and so Theorem 1.19(e) shows that \\(\\mu(A)=0\\) and that\n\\[ |\\lambda|(A)=\\lim _{n \\rightarrow \\infty}|\\lambda|\\left(A_n\\right) \\geq \\epsilon\u0026gt;0, \\]\nsince \\(|\\lambda|\\left(A_n\\right) \\geq|\\lambda|\\left(E_n\\right)\\). It follows that we do not have \\(|\\lambda| \\ll \\mu\\), hence \\((a)\\) is false, by Proposition 6.8(e).\n","date":"2022-12-19T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-6/2-abosolute-continuity/","section":"papa rudin","tags":null,"title":"Abosolute Continuity"},{"categories":["ML"],"contents":"This blog mainly refers to Independent Component Analysis: Algorithms and Applications which wrote by Aapo Hyvärinen and Erkki Oja.\nSome useful reference\nElements of Information Theory What is projection pursuit - Entropy Estimation New approximations of differential entropy for independent component analysis and projection pursuit - Entropy Estimation Definition Cocktail party problem Imagine that you are in a room where two people are speaking simultaneously. You have two microphones, which you hold in different locations. The microphones give you two recorded time signals, which we could denote by \\(x_1(t)\\) and \\(x_2(t)\\), with \\(x_1\\) and \\(x_2\\) the amplitudes, and \\(t\\) the time index. Each of these recorded signals is a weighted sum of the speech signals emitted by the two speakers, which we denote by \\(s_1(t)\\) and \\(s_2(t)\\). We could express this as a linear equation: \\[ \\begin{aligned} \u0026amp; x_1(t)=a_{11} s_1+a_{12} s_2 \\\\ \u0026amp; x_2(t)=a_{21} s_1+a_{22} s_2 \\end{aligned} \\] where \\(a_{11}, a_{12}, a_{21}\\), and \\(a_{22}\\) are some parameters that depend on the distances of the microphones from the speakers. It would be very useful if you could now estimate the two original speech signals \\(s_1(t)\\) and \\(s_2(t)\\), using only the recorded signals \\(x_1(t)\\) and \\(x_2(t)\\). This is called the cocktail-party problem. For the time being, we omit any time delays or other extra factors from our simplified mixing model.\nAssumption \\(s_1(t)\\) and \\(s_2(t)\\), at each time instant \\(t\\), are statistically independent. This is not an unrealistic assumption in many cases, and it need not be exactly true in practice.\nDefinition and notation of ICA We assume mixture variables \\(\\mathbf{x}\\) and the independent components have zero mean.\n\\(\\mathbf{x}\\) = \\([x_1, x_2, \\ldots, x_n]^T\\), whose element are the mixture \\(x_1, x_2, \\dots, x_n\\). \\(\\mathbf{s}\\) = \\([s_1, s_2, \\ldots, s_n]^T\\), whose element are the input signals \\(s_1, s_2, \\dots, s_n\\).\n\\(\\mathbf{A}\\) is a matrix with entries \\(a_{ij}\\)\nSupport 𝐀 is a square matrix and non-singular. So after we estimate 𝐀, we can compute its inverse 𝐖, and obtain the independent components by:\n\\[ 𝐬 = 𝐖𝐱 \\]\nAmbiguities of ICA We cannot determine the variance (energies) of the independent components. We cannot determine the order the independent components. (permutation) Because both 𝐀 and 𝐬 is unknown.\nWhat is independence Definition Technically, independence can be defined by the probability densities. Let us denote by \\(p\\left(y_1, y_2\\right)\\) the joint probability density function (pdf) of \\(y_1\\) and \\(y_2\\). Let us further denote by \\(p_1\\left(y_1\\right)\\) the marginal pdf of \\(y_1\\), i.e. the pdf of \\(y_1\\) when it is considered alone: \\[ p_1\\left(y_1\\right)=\\int p\\left(y_1, y_2\\right) d y_2, \\] and similarly for \\(y_2\\). Then we define that \\(y_1\\) and \\(y_2\\) are independent if and only if the joint pdf is factorizable in the following way: \\[ p\\left(y_1, y_2\\right)=p_1\\left(y_1\\right) p_2\\left(y_2\\right) . \\] This definition extends naturally for any number \\(n\\) of random variables, in which case the joint density must be a product of \\(n\\) terms.\nUncorrelated variables are only partly independent A weaker form of independence is uncorrelatedness. Two random variables \\(y_1\\) and \\(y_2\\) are said to be uncorrelated, if their covariance is zero: \\[ E\\left\\{y_1 y_2\\right\\}-E\\left\\{y_1\\right\\} E\\left\\{y_2\\right\\}=0 \\] If the variables are independent, they are uncorrelated. On the other hand, uncorrelatedness does not imply independence.\nGaussian variables are forbidden The fundamental restriction in ICA is that the independent components must be nongaussian for ICA to be possible. To see why gaussian variables make ICA impossible, assume that the mixing matrix is orthogonal and the \\(s_i\\) are gaussian. Then \\(x_1\\) and \\(x_2\\) are gaussian, uncorrelated, and of unit variance. Their joint density is given by \\[ p\\left(x_1, x_2\\right)=\\frac{1}{2 \\pi} \\exp \\left(-\\frac{x_1^2+x_2^2}{2}\\right) \\] This distribution is illustrated in Fig. 7. The Figure shows that the density is completely symmetric. Therefore, it does not contain any information on the directions of the columns of the mixing matrix \\(\\mathbf{A}\\). This is why \\(\\mathbf{A}\\) cannot be estimated.\nMore rigorously, one can prove that the distribution of any orthogonal transformation of the gaussian \\(\\left(x_1, x_2\\right)\\) has exactly the same distribution as \\(\\left(x_1, x_2\\right)\\), and that \\(x_1\\) and \\(x_2\\) are independent. Thus, in the case of gaussian variables, we can only estimate the ICA model up to an orthogonal transformation. In other words, the matrix A is not identifiable for gaussian independent components. (Actually, if just one of the independent components is gaussian, the ICA model can still be estimated.)\nPrinciple of ICA estimation “Nongaussian is independent” The Central Limit Theorem, a classical result in probability theory, tells that the distribution of a sum of independent random variables tends toward a gaussian distribution, under certain conditions. Thus, a sum of two independent random variables usually has a distribution that is closer to gaussian than any of the two original random variables.\nSo our estimation is maximize nongaussian of \\(\\mathbf{s}\\).\nMeasure of nongaussianity kurtosis The classical measure of nongaussianity is kurtosis or the fourth-order cumulant. The kurtosis of \\(y\\) is classically defined by \\[ \\operatorname{kurt}(y)=E\\left\\{y^4\\right\\}-3\\left(E\\left\\{y^2\\right\\}\\right)^2 \\] Actually, since we assumed that \\(y\\) is of unit variance, the right-hand side simplifies to \\(E\\left\\{y^4\\right\\}-3\\). This shows that kurtosis is simply a normalized version of the fourth moment \\(E\\left\\{y^4\\right\\}\\). For a gaussian \\(y\\), the fourth moment equals \\(3\\left(E\\left\\{y^2\\right\\}\\right)^2\\). Thus, kurtosis is zero for a gaussian random variable. For most (but not quite all) nongaussian random variables, kurtosis is nonzero.\nNegentropy A second very important measure of nongaussianity is given by negentropy. Negentropy is based on the informationtheoretic quantity of (differential) entropy.\nEntropy is the basic concept of information theory. The entropy of a random variable can be interpreted as the degree of information that the observation of the variable gives. The more “random”, i.e. unpredictable and unstructured the variable is, the larger its entropy.\nEntropy \\(H\\) is defined for a discrete random variable \\(Y\\) as \\[ H(Y)=-\\sum_i P\\left(Y=a_i\\right) \\log P\\left(Y=a_i\\right) \\] where the \\(a_i\\) are the possible values of \\(Y\\).\nThis very well-known definition can be generalized for continuous-valued random variables and vectors, in which case it is often called differential entropy. The differential entropy \\(H\\) of a random vector \\(\\mathbf{y}\\) with density \\(f(\\mathbf{y})\\) is defined as: \\[ H(\\mathbf{y})=-\\int f(\\mathbf{y}) \\log f(\\mathbf{y}) \\mathrm{d} \\mathbf{y} . \\] A fundamental result of information theory is that a gaussian variable has the largest entropy among all random variables of equal variance. This means that entropy could be used as a measure of nongaussianity. In fact, this shows that the gaussian distribution is the “most random” or the least structured of all distributions. Entropy is small for distributions that are clearly concentrated on certain values, i.e., when the variable is clearly clustered, or has a pdf that is very “spiky”.\nTo obtain a measure of nongaussianity that is zero for a gaussian variable and always nonnegative, one often uses a slightly modified version of the definition of differential entropy, called negentropy. Negentropy \\(J\\) is defined as follows \\[ J(\\mathbf{y})=H\\left(\\mathbf{y}_{\\text {gauss }}\\right)-H(\\mathbf{y}) \\] where \\(\\mathbf{y}_{\\text {gauss }}\\) is a Gaussian random variable of the same covariance matrix as \\(\\mathbf{y}\\). Due to the above-mentioned properties, negentropy is always non-negative, and it is zero if and only if \\(\\mathbf{y}\\) has a Gaussian distribution. Negentropy has the additional interesting property that it is invariant for invertible linear transformations (Comon, 1994; Hyvärinen, \\(1999 \\mathrm{e}\\) ).\nThe advantage of using negentropy, or, equivalently, differential entropy, as a measure of nongaussianity is that it is well justified by statistical theory. In fact, negentropy is in some sense the optimal estimator of nongaussianity, as far as statistical properties are concerned. The problem in using negentropy is, however, that it is computationally very difficult. Estimating negentropy using the definition would require an estimate (possibly nonparametric) of the pdf. Therefore, simpler approximations of negentropy are very useful, as will be discussed next.\nApproximations of negentropy Using higher-order moments, for example as follows (Jones and Sibson, 1987): \\[ J(y) \\approx \\frac{1}{12} E\\left\\{y^3\\right\\}^2+\\frac{1}{48} \\operatorname{kurt}(y)^2 \\] The random variable \\(y\\) is assumed to be of zero mean and unit variance. However, the validity of such approximations may be rather limited. In particular, these approximations suffer from the nonrobustness encountered with kurtosis.\n\\[ J(y) \\approx \\sum_{i=1}^p k_i\\left[E\\left\\{G_i(y)\\right\\}-E\\left\\{G_i(v)\\right\\}\\right]^2, \\label{neg_est} \\] where \\(k_i\\) are some positive constants, and \\(v\\) is a Gaussian variable of zero mean and unit variance (i.e., standardized). The variable \\(y\\) is assumed to be of zero mean and unit variance, and the functions \\(G_i\\) are some nonquadratic functions (Hyvärinen, 1998b).\nNote that even in cases where this approximation is not very accurate, (12) can be used to construct a measure of nongaussianity that is consistent in the sense that it is always non-negative, and equal to zero if \\(y\\) has a Gaussian distribution. In the case where we use only one nonquadratic function \\(G\\), the approximation becomes \\[ J(y) \\propto[E\\{G(y)\\}-E\\{G(v)\\}]^2 \\] for practically any non-quadratic function \\(G\\). This is clearly a generalization of the moment-based approximation in (23), if \\(y\\) is symmetric. Indeed, taking \\(G(y)=y^4\\), one then obtains exactly (23), i.e. a kurtosis-based approximation. But the point here is that by choosing \\(G\\) wisely, one obtains approximations of negentropy that are much better than the one given by (23). In particular, choosing \\(G\\) that does not grow too fast, one obtains more robust estimators. The following choices of \\(G\\) have proved very useful: \\[ G_1(u)=\\frac{1}{a_1} \\log \\cosh a_1 u, \\quad G_2(u)=-\\exp \\left(-u^2 / 2\\right) \\label{eq_G} \\] where \\(1 \\leq a_1 \\leq 2\\) is some suitable constant.\nThus we obtain approximations of negentropy that give a very good compromise between the properties of the two classical nongaussianity measures given by kurtosis and negentropy. They are conceptually simple, fast to compute, yet have appealing statistical properties, especially robustness.\nPreProcessing for ICA Centering (zero mean)\n\\[ \\tilde{\\mathbf{x}} = 𝐱 - E\\{𝐱\\} \\]\nFollowing will assume the mean of 𝐱 is zero;\nWhitening (make the component of 𝐱 is uncorrelated)\ncalculate covariance matrix \\[ C = E\\{𝐱𝐱_{T}\\} \\]\neigen-value decomposition\n\\[ C = E\\{𝐱𝐱_{T}\\} = 𝐄𝐃𝐄_{T} \\]\nWhitening \\[ \\tilde{\\mathbf{x}}=\\mathbf{E D}^{-1 / 2} \\mathbf{E}^T \\mathbf{x} \\]\nverify \\[ E\\left\\{\\tilde{\\mathbf{x}} \\tilde{\\mathbf{x}}^T\\right\\}=\\mathbf{I} \\]\nfurther Processing Apply filter in signal x\nThe FastICA Algorithm FastICA for one unit Here we use equation \\(\\ref{neg_est}\\) two to maximize the nongaussianity. The \\(G\\) use equation \\(\\ref{eq_G}\\). The derivative of \\(\\ref{eq_G}\\) is \\[ \\begin{array}{r} g_1(u)=\\tanh \\left(a_1 u\\right) \\\\ g_2(u)=u \\exp \\left(-u^2 / 2\\right) \\end{array} \\]\nThe processure of FastICA\nChoose an initial (e.g. random) weight vector w.\nLet \\(\\mathbf{w}^{+}=E\\left\\{\\mathbf{x} g\\left(\\mathbf{w}^T \\mathbf{x}\\right)\\right\\}-E\\left\\{g^{\\prime}\\left(\\mathbf{w}^T \\mathbf{x}\\right)\\right\\} \\mathbf{w}\\)\nLet \\(\\mathbf{w}=\\mathbf{w}^{+} /\\left\\|\\mathbf{w}^{+}\\right\\|\\)\nIf not converged, go back to 2 .\nNote that convergence means that the old and new values of \\(\\mathbf{w}\\) point in the same direction, i.e. their dot-product is (almost) equal to 1 . It is not necessary that the vector converges to a single point, since \\(\\mathbf{w}\\) and \\(-\\mathbf{w}\\) define the same direction. This is again because the independent components can be defined only up to a multiplicative sign. Note also that it is here assumed that the data is prewhitened.\nFastICA for serveral unit The one-unit algorithm of the preceding subsection estimaes just one of the independent components, or one projection pursuit derection.\n","date":"2022-12-17T00:00:00Z","permalink":"https://zongpitt.com/posts/independent-component-analysis/","section":"posts","tags":["ML"],"title":"Independent Component Analysis"},{"categories":null,"contents":"6.1 Introduction Let \\(\\mathfrak{M}\\) be a \\(\\sigma\\)-algebra in a set \\(X\\). Call a countable collection \\(\\left\\{E_i\\right\\}\\) of members of \\(\\mathfrak{M}\\) a partition of \\(E\\) if \\(E_i \\cap E_j=\\varnothing\\) whenever \\(i \\neq j\\), and if \\(E=\\) \\(\\bigcup E_i\\). A complex measure \\(\\mu\\) on \\(\\mathfrak{M}\\) is then a complex function on \\(\\mathfrak{M}\\) such that \\[ \\mu(E)=\\sum_{i=1}^{\\infty} \\mu\\left(E_i\\right) \\quad(E \\in \\mathfrak{M}) \\] for every partition \\(\\left\\{E_i\\right\\}\\) of \\(E\\).\nObserve that the convergence of the series in (1) is now part of the requirement (unlike for positive measures, where the series could either converge or diverge to \\(\\infty\\) ). Since the union of the sets \\(E_i\\) is not changed if the subscripts are permuted, every rearrangement of the series (1) must also converge. Hence ([26], Theorem 3.56) the series actually converges absolutely.\nLet us consider the problem of finding a positive measure \\(\\lambda\\) which dominates a given complex measure \\(\\mu\\) on \\(\\mathfrak{M}\\), in the sense that \\(|\\mu(E)| \\leq \\lambda(E)\\) for every \\(E \\in \\mathfrak{M}\\), and let us try to keep \\(\\lambda\\) as small as we can. Every solution to our problem (if there is one at all) must satisfy\n\\[ \\lambda(E)=\\sum_{i=1}^{\\infty} \\lambda\\left(E_i\\right) \\geq \\sum_1^{\\infty}\\left|\\mu\\left(E_i\\right)\\right|, \\]\nfor every partition \\(\\left\\{E_i\\right\\}\\) of any set \\(E \\in \\mathfrak{M}\\), so that \\(\\lambda(E)\\) is at least equal to the supremum of the sums on the right of (2), taken over all partitions of \\(E\\). This suggests that we define a set function \\(|\\mu|\\) on \\(\\mathfrak{M}\\) by \\[ |\\mu|(E)=\\sup \\sum_{i=1}^{\\infty}\\left|\\mu\\left(E_i\\right)\\right| \\quad(E \\in \\mathfrak{M}), \\] the supremum being taken over all partitions \\(\\left\\{E_i\\right\\}\\) of \\(E\\).\nThis notation is perhaps not the best, but it is the customary one. Note that \\(|\\mu|(E) \\geq|\\mu(E)|\\), but that in general \\(|\\mu|(E)\\) is not equal to \\(|\\mu(E)|\\).\nIt turns out, as will be proved below, that \\(|\\mu|\\) actually is a measure, so that our problem does have a solution. The discussion which led to (3) shows then clearly that \\(|\\mu|\\) is the minimal solution, in the sense that any other solution \\(\\lambda\\) has the property \\(\\lambda(E) \\geq|\\mu|(E)\\) for all \\(E \\in \\mathfrak{M}\\).\nThe set function \\(|\\mu|\\) is called the total variation of \\(\\mu\\), or sometimes, to avoid misunderstanding, the total variation measure. The term “total variation of \\(\\mu\\)” is also frequently used to denote the number \\(|\\mu|(X)\\).\nIf \\(\\mu\\) is a positive measure, then of course \\(|\\mu|=\\mu\\).\nBesides being a measure, \\(|\\mu|\\) has another unexpected property: \\(|\\mu|(X)\u0026lt;\\infty\\). Since \\(|\\mu(E)| \\leq|\\mu|(E) \\leq|\\mu|(X)\\), this implies that every complex measure \\(\\mu\\) on any \\(\\sigma\\)-algebra is bounded: If the range of \\(\\mu\\) lies in the complex plane, then it actually lies in some disc of finite radius. This property (proved in Theorem 6.4) is sometimes expressed by saying that \\(\\mu\\) is of bounded variation.\n6.2 Theorem The total variation \\(|\\mu|\\) of a complex measure \\(\\mu\\) on \\(\\mathfrak{M}\\) is a positive measure on \\(\\mathfrak{M}\\).\nProOF Let \\(\\left\\{E_i\\right\\}\\) be a partition of \\(E \\in \\mathfrak{M}\\). Let \\(t_i\\) be real numbers such that \\(t_i\u0026lt;|\\mu|\\left(E_i\\right)\\). Then each \\(E_i\\) has a partition \\(\\left\\{A_{i j}\\right\\}\\) such that \\[ \\sum_j\\left|\\mu\\left(A_{i j}\\right)\\right|\u0026gt;t_i \\quad(i=1,2,3, \\ldots) . \\] Since \\(\\left\\{A_{i j}\\right\\}(i, j=1,2,3, \\ldots)\\) is a partition of \\(E\\), it follows that \\[ \\sum_i t_i \\leq \\sum_{i, j}\\left|\\mu\\left(A_{i j}\\right)\\right| \\leq|\\mu|(E) . \\] Taking the supremum of the left side of \\((2)\\), over all admissible choices of \\(\\left\\{t_i\\right\\}\\), we see that \\[ \\sum_i|\\mu|\\left(E_i\\right) \\leq|\\mu|(E) . \\] To prove the opposite inequality, let \\(\\left\\{A_j\\right\\}\\) be any partition of \\(E\\). Then for any fixed \\(j,\\left\\{A_j \\cap E_i\\right\\}\\) is a partition of \\(A_j\\), and for any fixed \\(i,\\left\\{A_j \\cap E_i\\right\\}\\) is a partition of \\(E_i\\). Hence \\[ \\begin{aligned} \\sum_j\\left|\\mu\\left(A_j\\right)\\right| \u0026amp; =\\sum_j\\left|\\sum_i \\mu\\left(A_j \\cap E_i\\right)\\right| \\\\ \u0026amp; \\leq \\sum_j \\sum_i\\left|\\mu\\left(A_j \\cap E_i\\right)\\right| \\\\ \u0026amp; =\\sum_i \\sum_j\\left|\\mu\\left(A_j \\cap E_i\\right)\\right| \\leq \\sum_i|\\mu|\\left(E_i\\right) \\end{aligned} \\] Since (4) holds for every partition \\(\\left\\{A_j\\right\\}\\) of \\(E\\), we have \\[ |\\mu|(E) \\leq \\sum_i|\\mu|\\left(E_i\\right) . \\] By (3) and (5), \\(|\\mu|\\) is countably additive.\nNote that the Corollary to Theorem \\(1.27\\) was used in (2) and (4). That \\(|\\mu|\\) is not identically \\(\\infty\\) is a trivial consequence of Theorem \\(6.4\\) but can also be seen right now, since \\(|\\mu|(\\varnothing)=0\\).\n6.3 Lemma If \\(z_1, \\ldots, z_N\\) are complex numbers then there is a subset \\(S\\) of \\(\\{1, \\ldots, N\\}\\) for which \\[ \\left|\\sum_{k \\in S} z_k\\right| \\geq \\frac{1}{\\pi} \\sum_{k=1}^N\\left|z_k\\right| . \\] Proof Write \\(z_k=\\left|z_k\\right| e^{i a_k}\\). For \\(-\\pi \\leq \\theta \\leq \\pi\\), let \\(S(\\theta)\\) be the set of all \\(k\\) for which \\(\\cos \\left(\\alpha_k-\\theta\\right)\u0026gt;0\\). Then \\[ \\left|\\sum_{S(\\theta)} z_k\\right|=\\left|\\sum_{S(\\theta)} e^{-i \\theta} z_k\\right| \\geq \\operatorname{Re} \\sum_{S(\\theta)} e^{-i \\theta} z_k=\\sum_{k=1}^N\\left|z_k\\right| \\cos ^{+}\\left(\\alpha_k-\\theta\\right) . \\] Choose \\(\\theta_0\\) so as to maximize the last sum, and put \\(S=S\\left(\\theta_0\\right)\\). This maximum is at least as large as the average of the sum over \\([-\\pi, \\pi]\\), and this average is \\(\\pi^{-1} \\sum\\left|z_k\\right|\\), because \\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi \\cos ^{+}(\\alpha-\\theta) d \\theta=\\frac{1}{\\pi} \\] for every \\(\\alpha\\).\n6.4 Theorem If \\(\\mu\\) is a complex measure on \\(X\\), then \\[ |\\mu|(X)\u0026lt;\\infty . \\] Proof Suppose first that some set \\(E \\in \\mathfrak{M}\\) has \\(|\\mu|(E)=\\infty\\). Put \\(t=\\pi(1+|\\mu(E)|)\\). Since \\(|\\mu|(E)\u0026gt;t\\), there is a partition \\(\\left\\{E_i\\right\\}\\) of \\(E\\) such that \\[ \\sum_{i=1}^N\\left|\\mu\\left(E_i\\right)\\right|\u0026gt;t \\] for some \\(N\\). Apply Lemma 6.3, with \\(z_i=\\mu\\left(E_i\\right)\\), to conclude that there is a set \\(A \\subset E\\) (a union of some of the sets \\(\\left.E_i\\right)\\) for which \\[ |\\mu(A)|\u0026gt;t / \\pi\u0026gt;1 \\text {. } \\] Setting \\(B=E-A\\), it follows that \\[ |\\mu(B)|=|\\mu(E)-\\mu(A)| \\geq|\\mu(A)|-|\\mu(E)|\u0026gt;\\frac{t}{\\pi}-|\\mu(E)|=1 . \\] We have thus split \\(E\\) into disjoint sets \\(A\\) and \\(B\\) with \\(|\\mu(A)|\u0026gt;1\\) and \\(|\\mu(B)|\u0026gt;1\\). Evidently, at least one of \\(|\\mu|(A)\\) and \\(|\\mu|(B)\\) is \\(\\infty\\), by Theorem 6.2. Now if \\(|\\mu|(X)=\\infty\\), split \\(X\\) into \\(A_1, B_1\\), as above, with \\(\\left|\\mu\\left(A_1\\right)\\right|\u0026gt;1\\), \\(|\\mu|\\left(B_1\\right)=\\infty\\). Split \\(B_1\\) into \\(A_2, B_2\\), with \\(\\left|\\mu\\left(A_2\\right)\\right|\u0026gt;1,|\\mu|\\left(B_2\\right)=\\infty\\). Continuing in this way, we get a countably infinite disjoint collection \\(\\left\\{A_i\\right\\}\\), with \\(\\left|\\mu\\left(A_i\\right)\\right|\u0026gt;1\\) for each \\(i\\). The countable additivity of \\(\\mu\\) implies that \\[ \\mu\\left(\\bigcup_i A_i\\right)=\\sum_i \\mu\\left(A_i\\right) . \\] But this series cannot converge, since \\(\\mu\\left(A_i\\right)\\) does not tend to 0 as \\(i \\rightarrow \\infty\\). This contradiction shows that \\(|\\mu|(X)\u0026lt;\\infty\\).\n6.5 If \\(\\mu\\) and \\(\\lambda\\) are complex measures on the same \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\), we define \\(\\mu+\\lambda\\) and \\(c \\mu\\) by \\[ \\begin{aligned} (\\mu+\\lambda)(E) \u0026amp; =\\mu(E)+\\lambda(E) \\\\ (c \\mu)(E) \u0026amp; =c \\mu(E) \\quad(E \\in \\mathfrak{M}) \\end{aligned} \\] for any scalar \\(c\\), in the usual manner. It is then trivial to verify that \\(\\mu+\\lambda\\) and \\(c \\mu\\) are complex measures. The collection of all complex measures on \\(\\mathfrak{M}\\) is thus a vector space. If we put \\[ \\|\\mu\\|=|\\mu|(X), \\] it is easy to verify that all axioms of a normed linear space are satisfied.\n6.6 Positive and Negative Variations Let us now specialize and consider a real measure \\(\\mu\\) on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\). (Such measures are frequently called signed measures.) Define \\(|\\mu|\\) as before, and define \\[ \\mu^{+}=\\frac{1}{2}(|\\mu|+\\mu), \\quad \\mu^{-}=\\frac{1}{2}(|\\mu|-\\mu) . \\] Then both \\(\\mu^{+}\\)and \\(\\mu^{-}\\)are positive measures on \\(\\mathfrak{M}\\), and they are bounded, by Theorem 6.4. Also, \\[ \\mu=\\mu^{+}-\\mu^{-}, \\quad|\\mu|=\\mu^{+}+\\mu^{-} . \\]\nThe measures \\(\\mu^{+}\\)and \\(\\mu^{-}\\)are called the positive and negative variations of \\(\\mu\\), respectively. This representation of \\(\\mu\\) as the difference of the positive measures \\(\\mu^{+}\\) and \\(\\mu^{-}\\)is known as the Jordan decomposition of \\(\\mu\\). Among all representations of \\(\\mu\\) as a difference of two positive measures, the Jordan decomposition has a certain minimum property which will be established as a corollary to Theorem \\(6.14\\).\n","date":"2022-12-11T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-6/1-total-variation/","section":"papa rudin","tags":null,"title":"Total Variation"},{"categories":null,"contents":"Banach Spaces\n5.1 In the preceding chapter we saw how certain analytic facts about trigonometric series can be made to emerge from essentially goemetric considerations about general Hilbert spaces, involving the notions of convexity, subspaces, orthogonality, and completeness. There are many problems in analysis that can be attacked with greater ease when they are placed within a suitably chosen abstract framework. The theory of Hilbert spaces is not always suitable since orthogonality is something rather special. The class of all Banach spaces affords greater variety. In this chapter we shall develop some of the basic properties of Banach spaces and illustrate them by applications to concrete problems.\n5.2 Definition A complex vector space \\(X\\) is said to be a normed linear space if to each \\(x \\in X\\) there is associated a nonnegative real number \\(\\|x\\|\\), called the norm of \\(x\\), such that\n\\(\\|x+y\\| \\leq\\|x\\|+\\|y\\|\\) for all \\(x\\) and \\(y \\in X\\), \\(\\|a x\\|=|\\alpha|\\|x\\|\\) if \\(x \\in X\\) and \\(\\alpha\\) is a scalar, \\(\\|x\\|=0\\) implies \\(x=0\\). By \\((a)\\), the triangle inequality \\[ \\|x-y\\| \\leq\\|x-z\\|+\\|z-y\\| \\quad(x, y, z \\in X) \\]\nholds. Combined with \\((b)\\) (take \\(\\alpha=0, \\alpha=-1\\) ) and \\((c)\\) this shows that every normed linear space may be regarded as a metric space, the distance between \\(x\\) and \\(y\\) being \\(\\|x-y\\|\\).\nA Banach space is a normed linear space which is complete in the metric defined by its norm.\nFor instance, every Hilbert space is a Banach space, so is every \\(L^p(\\mu)\\) normed by \\(\\|f\\|_p\\) (provided we identify functions which are equal a.e.) if \\(1 \\leq p \\leq \\infty\\), and so is \\(C_0(X)\\) with the supremum norm. The simplest Banach space is of course the complex field itself, normed by \\(\\|x\\|=|x|\\).\nOne can equally well discuss real Banach spaces; the definition is exactly the same, except that all scalars are assumed to be real.\n5.3 Definition Consider a linear transformation \\(\\Lambda\\) from a normed linear space \\(X\\) into a normed linear space \\(Y\\), and define its norm by\n\\[ \\|\\Lambda\\|=\\sup \\{\\|\\Lambda x\\|: x \\in X,\\|x\\| \\leq 1\\} . \\]\nIf \\(\\|\\Lambda\\|\u0026lt;\\infty\\), then \\(\\Lambda\\) is called a bounded linear transformation.\nIn (1), \\(\\|x\\|\\) is the norm of \\(x\\) in \\(X,\\|\\Lambda x\\|\\) is the norm of \\(\\Lambda x\\) in \\(Y\\); it will frequently happen that several norms occur together, and the context will make it clear which is which.\nObserve that we could restrict ourselves to unit vectors \\(x\\) in (1), i.e., to \\(x\\) with \\(\\|x\\|=1\\), without changing the supremum, since\n\\[ \\|\\Lambda(\\alpha x)\\|=\\|\\alpha \\Lambda x\\|=|\\alpha|\\|\\Lambda x\\| . \\]\nObserve also that \\(\\|\\Lambda\\|\\) is the smallest number such that the inequality\n\\[ \\|\\Lambda x\\| \\leq\\|\\Lambda\\|\\|x\\| \\]\nholds for every \\(x \\in X\\). The following geometric picture is helpful: \\(\\Lambda\\) maps the closed unit ball in \\(X\\), i.e., the set\n\\[ \\{x \\in X:\\|x\\| \\leq 1\\}, \\]\ninto the closed ball in \\(Y\\) with center at 0 and radius \\(\\|\\Lambda\\|\\). An important special case is obtained by taking the complex field for \\(Y\\); in that case we talk about bounded linear functionals.\n5.4 Theorem For a linear transformation \\(\\Lambda\\) of a normed linear space \\(X\\) into \\(a\\) normed linear space \\(Y\\), each of the following three conditions implies the other two:\n\\(\\Lambda\\) is bounded. \\(\\Lambda\\) is continuous. \\(\\Lambda\\) is continuous at one point of \\(X\\). Proof Since \\(\\left\\|\\Lambda\\left(x_1-x_2\\right)\\right\\| \\leq\\|\\Lambda\\|\\left\\|x_1-x_2\\right\\|\\), it is clear that \\((a)\\) implies \\((b)\\), and \\((b)\\) implies \\((c)\\) trivially. Suppose \\(\\Lambda\\) is continuous at \\(x_0\\). To each \\(\\epsilon\u0026gt;0\\) one can then find a \\(\\delta\u0026gt;0\\) so that \\(\\left\\|x-x_0\\right\\|\u0026lt;\\delta\\) implies \\(\\left\\|\\Lambda x-\\Lambda x_0\\right\\|\u0026lt;\\epsilon\\). In other words, \\(\\|x\\|\u0026lt;\\delta\\) implies\n\\[ \\left\\|\\Lambda\\left(x_0+x\\right)-\\Lambda x_0\\right\\|\u0026lt;\\epsilon . \\]\nBut then the linearity of \\(\\Lambda\\) shows that \\(\\|\\Lambda x\\|\u0026lt;\\epsilon\\). Hence \\(\\|\\Lambda\\| \\leq \\epsilon / \\delta\\), and \\((c)\\) implies \\((a)\\).\n","date":"2022-12-06T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/1-examples-of-banach-space-techniques/","section":"papa rudin","tags":null,"title":"EXAMPLES OF BANACH SPACE TECHNIQUES"},{"categories":null,"contents":"5.22 Successful applications of the Hahn-Banach theorem to concrete problems depend of course on a knowledge of the bounded linear functionals on the normed linear space under consideration. So far we have only determined the bounded linear functionals on a Hilbert space (where a much simpler proof of the Hahn-Banach theorem exists; see Exercise 6), and we know the positive linear functionals on \\(C_c(X)\\).\nWe shall now describe a general situation in which the last-mentioned functionals occur naturally.\nLet \\(K\\) be a compact Hausdorff space, let \\(H\\) be a compact subset of \\(K\\), and let \\(A\\) be a subspace of \\(C(K)\\) such that \\(1 \\in A\\) (1 denotes the function which assigns the number 1 to each \\(x \\in K)\\) and such that \\[ \\|f\\|_K=\\|f\\|_H \\quad(f \\in A) . \\] Here we used the notation \\[ \\|f\\|_E=\\sup \\{|f(x)|: x \\in E\\} . \\] Because of the example discussed in Sec. 5.23, \\(H\\) is sometimes called a boundary of \\(K\\), corresponding to the space \\(A\\).\nIf \\(f \\in A\\) and \\(x \\in K\\), (1) says that \\[ |f(x)| \\leq\\|f\\|_H \\text {. } \\] In particular, if \\(f(y)=0\\) for every \\(y \\in H\\), then \\(f(x)=0\\) for all \\(x \\in K\\). Hence if \\(f_1\\) and \\(f_2 \\in A\\) and \\(f_1(y)=f_2(y)\\) for every \\(y \\in H\\), then \\(f_1=f_2\\); to see this, put \\(f=\\) \\(f_1-f_2\\).\nLet \\(M\\) be the set of all functions on \\(H\\) that are restrictions to \\(H\\) of members of \\(A\\). It is clear that \\(M\\) is a subspace of \\(C(H)\\). The preceding remark shows that each member of \\(M\\) has a unique extension to a member of \\(A\\). Thus we have a natural one-to-one correspondence between \\(M\\) and \\(A\\), which is also normpreserving, by (1). Hence it will cause no confusion if we use the same letter to designate a member of \\(A\\) and its restriction to \\(H\\).\nFix a point \\(x \\in K\\). The inequality (3) shows that the mapping \\(f \\rightarrow f(x)\\) is a bounded linear functional on \\(M\\), of norm 1 [since equality holds in (3) if \\(f=1\\) ]. By the Hahn-Banach theorem there is a linear functional \\(\\Lambda\\) on \\(C(H)\\), of norm 1 , such that \\[ \\Lambda f=f(x) \\quad(f \\in M) . \\] We claim that the properties \\[ \\Lambda 1=1, \\quad\\|\\Lambda\\|=1 \\] imply that \\(\\Lambda\\) is a positive linear functional on \\(C(H)\\).\nTo prove this, suppose \\(f \\in C(H), 0 \\leq f \\leq 1\\), put \\(g=2 f-1\\), and put \\(\\Lambda g=\\alpha+i \\beta\\), where \\(\\alpha\\) and \\(\\beta\\) are real. Note that \\(-1 \\leq g \\leq 1\\), so that \\(|g+i r|^2 \\leq 1+r^2\\) for every real constant \\(r\\). Hence (5) implies that\n\\[ (\\beta+r)^2 \\leq|\\alpha+i(\\beta+r)|^2=|\\Lambda(g+i r)|^2 \\leq 1+r^2 . \\]\nThus \\(\\beta^2+2 r \\beta \\leq 1\\) for every real \\(r\\), which forces \\(\\beta=0\\). Since \\(\\|g\\|_H \\leq 1\\), we have \\(|\\alpha| \\leq 1\\); hence \\[ \\Lambda f=\\frac{1}{2} \\Lambda(1+g)=\\frac{1}{2}(1+\\alpha) \\geq 0 . \\] Now Theorem \\(2.14\\) can be applied. It shows that there is a regular positive Borel measure \\(\\mu_x\\) on \\(H\\) such that \\[ \\Lambda f=\\int_H f d \\mu_x \\quad(f \\in C(H)) . \\] In particular, we get the representation formula \\[ f(x)=\\int_H f d \\mu_x \\quad(f \\in A) . \\] What we have proved is that to each \\(x \\in K\\) there corresponds a positive measure \\(\\mu_x\\) on the “boundary” \\(H\\) which “represents” \\(x\\) in the sense that (9) holds for every \\(f \\in A\\).\nNote that \\(\\Lambda\\) determines \\(\\mu_x\\) uniquely; but there is no reason to expect the Hahn-Banach extension to be unique. Hence, in general, we cannot say much about the uniqueness of the representing measures. Under special circumstances we do get uniqueness, as we shall see presently.\n5.23 To see an example of the preceding situation, let \\(U=\\{z:|z|\u0026lt;1\\}\\) be the open unit disc in the complex plane, put \\(K=\\bar{U}\\) (the closed unit disc), and take for \\(H\\) the boundary \\(T\\) of \\(U\\). We claim that every polynomial \\(f\\), i.e., every function of the form\n\\[ f(z)=\\sum_{n=0}^N a_n z^n \\] where \\(a_0, \\ldots, a_N\\) are complex numbers, satisfies the relation \\[ \\|f\\|_U=\\|f\\|_T . \\] (Note that the continuity of \\(f\\) shows that the supremum of \\(|f|\\) over \\(U\\) is the same as that over \\(\\bar{U}\\).)\nSince \\(\\bar{U}\\) is compact, there exists a \\(z_0 \\in \\bar{U}\\) such that \\(\\left|f\\left(z_0\\right)\\right| \\geq|f(z)|\\) for all \\(z \\in \\bar{U}\\). Assume \\(z_0 \\in U\\). Then \\[ f(z)=\\sum_{n=0}^N b_n\\left(z-z_0\\right)^n \\] and if \\(0\u0026lt;r\u0026lt;1-\\left|z_0\\right|\\), we obtain \\[ \\sum_{n=0}^N\\left|b_n\\right|^2 r^{2 n}=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi\\left|f\\left(z_0+r e^{i \\theta}\\right)\\right|^2 d \\theta \\leq \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi\\left|f\\left(z_0\\right)\\right|^2 d \\theta=\\left|b_0\\right|^2 \\] so that \\(b_1=b_2=\\cdots=b_N=0\\); i.e., \\(f\\) is constant. Thus \\(z_0 \\in T\\) for every nonconstant polynomial \\(f\\), and this proves (2).\n(We have just proved a special case of the maximum modulus theorem; we shall see later that this is an important property of all holomorphic functions.)\n5.24 The Poisson Integral Let \\(A\\) be any subspace of \\(C(\\bar{U})\\) (where \\(\\bar{U}\\) is the closed unit disc, as above) such that \\(A\\) contains all polynomials and such that \\[ \\|f\\|_U=\\|f\\|_T \\] holds for every \\(f \\in A\\). We do not exclude the possibility that \\(A\\) consists of precisely the polynomials, but \\(A\\) might be larger.\nThe general result obtained in Sec. \\(5.22\\) applies to \\(A\\) and shows that to each \\(z \\in U\\) there corresponds a positive Borel measure \\(\\mu_z\\) on \\(T\\) such that \\[ f(z)=\\int_T f d \\mu_z \\quad(f \\in A) . \\] (This also holds for \\(z \\in T\\), but is then trivial: \\(\\mu_z\\) is simply the unit mass concentrated at the point \\(z\\).)\nWe now fix \\(z \\in U\\) and write \\(z=r e^{i \\theta}, 0 \\leq r\u0026lt;1, \\theta\\) real. If \\(u_n(w)=w^n\\), then \\(u_n \\in A\\) for \\(n=0,1,2, \\ldots\\); hence ( 2 ) shows that \\[ r^n e^{i n \\theta}=\\int_T u_n d \\mu_z \\quad(n=0,1,2, \\ldots) \\] Since \\(u_{-n}=\\bar{u}_n\\) on \\(T,(3)\\) leads to \\[ \\int_T u_n d \\mu_z=r^{|n|} e^{i n \\theta} \\quad(n=0, \\pm 1, \\pm 2, \\ldots) . \\] This suggests that we look at the real function \\[ P_r(\\theta-t)=\\sum_{n=-\\infty}^{\\infty} r^{|n|} e^{i n(\\theta-t)} \\quad(t \\text { real }) \\] since \\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi P_r(\\theta-t) e^{i n t} d t=r^{|n|} e^{i n \\theta} \\quad(n=0, \\pm 1, \\pm 2, \\ldots) \\] Note that the series (5) is dominated by the convergent geometric series \\(\\sum r^{|n|}\\), so that it is legitimate to insert the series into the integral (6) and to integrate term by term, which gives (6). Comparison of (4) and (6) gives \\[ \\int_T f d \\mu_z=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f\\left(e^{i t}\\right) P_r(\\theta-t) d t \\] for \\(f=u_n\\), hence for every trigonometric polynomial \\(f\\), and Theorem \\(4.25\\) now implies that (7) holds for every \\(f \\in C(T)\\). [This shows that \\(\\mu_z\\) was uniquely determined by (2). Why?] In particular, (7) holds if \\(f \\in A\\), and then (2) gives the representation \\[ f(z)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f\\left(e^{i t}\\right) P_r(\\theta-t) d t \\quad(f \\in A) . \\] The series (5) can be summed explicitly, since it is the real part of \\[ 1+2 \\sum_1^{\\infty}\\left(z e^{-i t}\\right)^n=\\frac{e^{i t}+z}{e^{i t}-z}=\\frac{1-r^2+2 i r \\sin (\\theta-t)}{\\left|1-z e^{-i t}\\right|^2} \\] Thus \\[ P_r(\\theta-t)=\\frac{1-r^2}{1-2 r \\cos (\\theta-t)+r^2} . \\] This is the so-called “Poisson kernel.” Note that \\(P_r(\\theta-t) \\geq 0\\) if \\(0 \\leq r\u0026lt;1\\). We now summarize what we have proved:\n5.25 Theorem Suppose \\(A\\) is a vector space of continuous complex functions on the closed unit disc \\(\\bar{U}\\). If \\(A\\) contains all polynomials, and if \\[ \\sup _{z \\in U}|f(z)|=\\sup _{z \\in T}|f(z)| \\] for every \\(f \\in A\\) (where \\(T\\) is the unit circle, the boundary of \\(U\\) ), then the Poisson integral representation \\[ f(z)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi \\frac{1-r^2}{1-2 r \\cos (\\theta-t)+r^2} f\\left(e^{i t}\\right) d t \\quad\\left(z=r e^{i \\theta}\\right) \\] is valid for every \\(f \\in A\\) and every \\(z \\in U\\).\n","date":"2022-11-19T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/6-an-abstract-approach-to-the-poisson-integral/","section":"papa rudin","tags":null,"title":"An Abstract Approach to the Poisson Integral"},{"categories":null,"contents":"5.14 As in Sec. 4.26, we associate to every \\(f \\in L^1(T)\\) a function \\(\\hat{f}\\) on \\(Z\\) defined by \\[ \\hat{f}(n)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) e^{-i n t} d t \\quad(n \\in Z) . \\] It is easy to prove that \\(\\hat{f}(n) \\rightarrow 0\\) as \\(|n| \\rightarrow \\infty\\), for every \\(f \\in L^1\\). For we know that \\(C(T)\\) is dense in \\(L^1(T)\\) (Theorem 3.14) and that the trigonometric polynomials are dense in \\(C(T)\\) (Theorem 4.25). If \\(\\epsilon\u0026gt;0\\) and \\(f \\in L^1(T)\\), this says that there is a \\(g \\in C(T)\\) and a trigonometric polynomial \\(P\\) such that \\(\\|f-g\\|_1\u0026lt;\\epsilon\\) and \\(\\|g-P\\|_{\\infty}\u0026lt;\\epsilon\\). Since \\[ \\|g-P\\|_1 \\leq\\|g-P\\|_{\\infty} \\] if follows that \\(\\|f-P\\|_1\u0026lt;2 \\epsilon\\); and if \\(|n|\\) is large enough (depending on \\(P\\) ), then \\[ |\\hat{f}(n)|=\\left|\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi\\{f(t)-P(t)\\} e^{-i n t} d t\\right| \\leq\\|f-P\\|_1\u0026lt;2 \\epsilon . \\] Thus \\(\\hat{f}(n) \\rightarrow 0\\) as \\(n \\rightarrow \\pm \\infty\\). This is known as the Riemann-Lebesgue lemma.\nThe question we wish to raise is whether the converse is true. That is to say, if \\(\\left\\{a_n\\right\\}\\) is a sequence of complex numbers such that \\(a_n \\rightarrow 0\\) as \\(n \\rightarrow \\pm \\infty\\), does it follow that there is an \\(f \\in L^1(T)\\) such that \\(\\hat{f}(n)=a_n\\) for all \\(n \\in Z\\) ? In other words, is something like the Riesz-Fischer theorem true in this situation?\nThis can easily be answered (negatively) with the aid of the open mapping theorem.\nLet \\(c_0\\) be the space of all complex functions \\(\\varphi\\) on \\(Z\\) such that \\(\\varphi(n) \\rightarrow 0\\) as \\(n \\rightarrow \\pm \\infty\\), with the supremum norm \\[ \\|\\varphi\\|_{\\infty}=\\sup \\{|\\varphi(n)|: n \\in Z\\} . \\] Then \\(c_0\\) is easily seen to be a Banach space. In fact, if we declare every subset of \\(Z\\) to be open, then \\(Z\\) is a locally compact Hausdorff space, and \\(c_0\\) is nothing but \\(C_0(Z)\\). The following theorem contains the answer to our question:\n5.15 Theorem The mapping \\(f \\rightarrow \\hat{f}\\) is a one-to-one bounded linear transformation of \\(L^1(T)\\) into (but not onto) \\(c_0\\).\nProof Define \\(\\Lambda\\) by \\(\\Lambda f=\\hat{f}\\). It is clear that \\(\\Lambda\\) is linear. We have just proved that \\(\\Lambda\\) maps \\(L^1(T)\\) into \\(c_0\\), and formula 5.14(1) shows that \\(|\\hat{f}(n)| \\leq\\|f\\|_1\\), so that \\(\\|\\Lambda\\| \\leq 1\\). (Actually, \\(\\|\\Lambda\\|=1\\); to see this, take \\(f=1\\).) Let us now prove that \\(\\Lambda\\) is one-to-one. Suppose \\(f \\in L^1(T)\\) and \\(\\hat{f}(n)=0\\) for every \\(n \\in Z\\). Then\n\\[ \\int_{-\\pi}^\\pi f(t) g(t) d t=0 \\]\nif \\(g\\) is any trigonometric polynomial. By Theorem \\(4.25\\) and the dominated convergence theorem, (1) holds for every \\(g \\in C(T)\\). Apply the dominated convergence theorem once more, in conjunction with the Corollary to Lusin’s theorem, to conclude that (1) holds if \\(g\\) is the characteristic function of any measurable set in \\(T\\). Now Theorem 1.39(b) shows that \\(f=0\\) a.e.\nIf the range of \\(\\Lambda\\) were all of \\(c_0\\), Theorem \\(5.10\\) would imply the existence of a \\(\\delta\u0026gt;0\\) such that\n\\[ \\|\\hat{f}\\|_{\\infty} \\geq \\delta\\|f\\|_1 \\]\nfor every \\(f \\in L^1(T)\\). But if \\(D_n(t)\\) is defined as in Sec. 5.11, then \\(D_n \\in L^1(T)\\), \\(\\left\\|\\hat{D}_n\\right\\|_{\\infty}=1\\) for \\(n=1,2,3, \\ldots\\), and \\(\\left\\|D_n\\right\\|_1 \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\). Hence there is no \\(\\delta\u0026gt;0\\) such that the inequalities\n\\[ \\left\\|\\hat{D}_n\\right\\|_{\\infty} \\geq \\delta\\left\\|D_n\\right\\|_1 \\]\nhold for every \\(n\\).\nThis completes the proof.\n","date":"2022-11-19T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/4-fourier-coefficients-of-l1-functions/","section":"papa rudin","tags":null,"title":"Fourier Coefficients of L¹-functions"},{"categories":null,"contents":"5.16 Theorem If \\(M\\) is a subspace of a normed linear space \\(X\\) and if \\(f\\) is a bounded linear functional on \\(M\\), then \\(f\\) can be extended to a bounded linear functional \\(F\\) on \\(X\\) so that \\(\\|F\\|=\\|f\\|\\).\nNote that \\(M\\) need not be closed.\nBefore we turn to the proof, some comments seem called for. First, to say (in the most general situation) that a function \\(F\\) is an extension of \\(f\\) means that the domain of \\(F\\) includes that of \\(f\\) and that \\(F(x)=f(x)\\) for all \\(x\\) in the domain of \\(f\\). Second, the norms \\(\\|F\\|\\) and \\(\\|f\\|\\) are computed relative to the domains of \\(F\\) and \\(f\\); explicitly,\n\\[ \\|f\\|=\\sup \\{|f(x)|: x \\in M,\\|x\\| \\leq 1\\}, \\quad\\|F\\|=\\sup \\{|F(x)|: x \\in X,\\|x\\| \\leq 1\\}, \\]\nThe third comment concerns the field of scalars. So far everything has been stated for complex scalars, but the complex field could have been replaced by the real field without any changes in statements or proofs. The Hahn-Banach theorem is also true in both cases; nevertheless, it appears to be essentially a “real” theorem. The fact that the complex case was not yet proved when Banach wrote his classical book “Opérations linéaires” may be the main reason that real scalars are the only ones considered in his work.\nIt will be helpful to introduce some temporary terminology. Recall that \\(V\\) is a complex (real) vector space if \\(x+y \\in V\\) for \\(x\\) and \\(y \\in V\\), and if \\(a x \\in V\\) for all complex (real) numbers \\(\\alpha\\). It follows trivially that every complex vector space is also a real vector space. A complex function \\(\\varphi\\) on a complex vector space \\(V\\) is a complex-linear functional if\n\\[ \\varphi(x+y)=\\varphi(x)+\\varphi(y) \\quad \\text { and } \\quad \\varphi(\\alpha x)=\\alpha \\varphi(x) \\]\nfor all \\(x\\) and \\(y \\in V\\) and all complex \\(\\alpha\\). A real-valued function \\(\\varphi\\) on a complex (real) vector space \\(V\\) is a real-linear functional if (1) holds for all real \\(\\alpha\\).\nIf \\(u\\) is the real part of a complex-linear functional \\(f\\), i.e., if \\(u(x)\\) is the real part of the complex number \\(f(x)\\) for all \\(x \\in V\\), it is easily seen that \\(u\\) is a real-linear functional. The following relations hold between \\(f\\) and \\(u\\) :\n5.17 Proposition Let \\(V\\) be a complex vector space. If \\(u\\) is the real part of a complex-linear functional \\(f\\) on \\(V\\), then \\[ f(x)=u(x)-i u(i x) \\quad(x \\in V) . \\] If \\(u\\) is a real-linear functional on \\(V\\) and if \\(f\\) is defined by (1), then \\(f\\) is \\(a\\) complex-linear functional on \\(V\\). If \\(V\\) is a normed linear space and \\(f\\) and \\(u\\) are related as in (1), then \\(\\|f\\|=\\|u\\|\\). Proof If \\(\\alpha\\) and \\(\\beta\\) are real numbers and \\(z=\\alpha+i \\beta\\), the real part of \\(i z\\) is \\(-\\beta\\). This gives the identity \\[ z=\\operatorname{Re} z-i \\operatorname{Re}(i z) \\] for all complex numbers \\(z\\). Since \\[ \\operatorname{Re}(i f(x))=\\operatorname{Re} f(i x)=u(i x) \\text {, } \\] (1) follows from (2) with \\(z=f(x)\\).\nUnder the hypotheses \\((b)\\), it is clear that \\(f(x+y)=f(x)+f(y)\\) and that \\(f(\\alpha x)=\\alpha f(x)\\) for all real \\(\\alpha\\). But we also have\n\\[ f(i x)=u(i x)-i u(-x)=u(i x)+i u(x)=i f(x), \\]\nwhich proves that \\(f\\) is complex-linear. Since \\(|u(x)| \\leq|f(x)|\\), we have \\(\\|u\\| \\leq\\|f\\|\\). On the other hand, to every \\(x \\in V\\) there corresponds a complex number \\(\\alpha,|\\alpha|=1\\), so that \\(\\alpha f(x)=|f(x)|\\). Then\n\\[ |f(x)|=f(\\alpha x)=u(\\alpha x) \\leq\\|u\\| \\cdot\\|\\alpha x\\|=\\|u\\| \\cdot\\|x\\|, \\]\nwhich proves that \\(\\|f\\| \\leq\\|u\\|\\).\n5.18 Proof of Theorem \\(5.16\\) We first assume that \\(X\\) is a real normed linear space and, consequently, that \\(f\\) is a real-linear bounded functional on \\(M\\). If \\(\\|f\\|=0\\), the desired extension is \\(F=0\\). Omitting this case, there is no loss of generality in assuming that \\(\\|f\\|=1\\).\nChoose \\(x_0 \\in X, x_0 \\notin M\\), and let \\(M_1\\) be the vector space spanned by \\(M\\) and \\(x_0\\). Then \\(M_1\\) consists of all vectors of the form \\(x+\\lambda x_0\\), where \\(x \\in M\\) and \\(\\lambda\\) is a real scalar. If we define \\(f_1\\left(x+\\lambda x_0\\right)=f(x)+\\lambda \\alpha\\), where \\(\\alpha\\) is any fixed real number, it is trivial to verify that an extension of \\(f\\) to a linear functional on \\(M_1\\) is obtained. The problem is to choose \\(\\alpha\\) so that the extended functional still has norm 1 . This will be the case provided that\n\\[ |f(x)+\\lambda \\alpha| \\leq\\left\\|x+\\lambda x_0\\right\\| \\quad(x \\in M, \\lambda \\text { real }) . \\]\nReplace \\(x\\) by \\(-\\lambda x\\) and divide both sides of (1) by \\(|\\lambda|\\). The requirement is then that\n\\[ |f(x)-\\alpha| \\leq\\left\\|x-x_0\\right\\| \\quad(x \\in M) \\] i.e., that \\(A_x \\leq \\alpha \\leq B_x\\) for all \\(x \\in M\\), where \\[ A_x=f(x)-\\left\\|x-x_0\\right\\| \\quad \\text { and } \\quad B_x=f(x)+\\left\\|x-x_0\\right\\| \\text {. } \\] There exists such an \\(\\alpha\\) if and only if all the intervals \\(\\left[A_x, B_x\\right]\\) have a common point, i.e., if and only if \\[ A_x \\leq B_y \\] for all \\(x\\) and \\(y \\in M\\). But \\[ f(x)-f(y)=f(x-y) \\leq\\|x-y\\| \\leq\\left\\|x-x_0\\right\\|+\\left\\|y-x_0\\right\\| \\] and so (4) follows from (3).\nWe have now proved that there exists a norm-preserving extension \\(f_1\\) of \\(f\\) on \\(M_1\\).\nLet \\(\\mathscr{P}\\) be the collection of all ordered pairs \\(\\left(M^{\\prime}, f^{\\prime}\\right)\\), where \\(M^{\\prime}\\) is a subspace of \\(X\\) which contains \\(M\\) and where \\(f^{\\prime}\\) is a real-linear extension of \\(f\\) to \\(M^{\\prime}\\), with \\(\\left\\|f^{\\prime}\\right\\|=1\\). Partially order \\(\\mathscr{P}\\) by declaring \\(\\left(M^{\\prime}, f^{\\prime}\\right) \\leq\\left(M^{\\prime \\prime}, f^{\\prime \\prime}\\right)\\) to mean that \\(M^{\\prime} \\subset M^{\\prime \\prime}\\) and \\(f^{\\prime \\prime}(x)=f^{\\prime}(x)\\) for all \\(x \\in M^{\\prime}\\). The axioms of a partial order\nare clearly satisfied, \\(\\mathscr{P}\\) is not empty since it contains \\((M, f)\\), and so the Hausdorff maximality theorem asserts the existence of a maximal totally ordered subcollection \\(\\Omega\\) of \\(\\mathscr{P}\\).\nLet \\(\\Phi\\) be the collection of all \\(M^{\\prime}\\) such that \\(\\left(M^{\\prime}, f^{\\prime}\\right) \\in \\Omega\\). Then \\(\\Phi\\) is totally ordered, by set inclusion, and therefore the union \\(\\tilde{M}\\) of all members of \\(\\Phi\\) is a subspace of \\(X\\). (Note that in general the union of two subspaces is not a subspace. An example is two planes through the origin in \\(R^3\\).) If \\(x \\in \\tilde{M}\\), then \\(x \\in M^{\\prime}\\) for some \\(M^{\\prime} \\in \\Phi\\); define \\(F(x)=f^{\\prime}(x)\\), where \\(f^{\\prime}\\) is the function which occurs in the pair \\(\\left(M^{\\prime}, f^{\\prime}\\right) \\in \\Omega\\). Our definition of the partial order in \\(\\Omega\\) shows that it is immaterial which \\(M^{\\prime} \\in \\Phi\\) we choose to define \\(F(x)\\), as long as \\(M^{\\prime}\\) contains \\(x\\).\nIt is now easy to check that \\(F\\) is a linear functional on \\(\\tilde{M}\\), with \\(\\|F\\|=1\\). If \\(\\tilde{M}\\) were a proper subspace \\(X\\), the first part of the proof would give us a further extension of \\(F\\), and this would contradict the maximality of \\(\\Omega\\). Thus \\(\\tilde{M}=X\\), and the proof is complete for the case of real scalars.\nIf now \\(f\\) is a complex-linear functional on the subspace \\(M\\) of the complex normed linear space \\(X\\), let \\(u\\) be the real part of \\(f\\), use the real Hahn-Banach theorem to extend \\(u\\) to a real-linear functional \\(U\\) on \\(X\\), with \\(\\|U\\|=\\|u\\|\\), and define\n\\[ F(x)=U(x)-i U(i x) \\quad(x \\in X) . \\]\nBy Proposition 5.17, \\(F\\) is a complex-linear extension of \\(f\\), and\n\\[ \\|F\\|=\\|U\\|=\\|u\\|=\\|f\\| . \\]\nThis completes the proof. Let us mention two important consequences of the Hahn-Banach theorem:\n5.19 Theorem Let \\(M\\) be a linear subspace of a normed linear space \\(X\\), and let \\(x_0 \\in X\\). Then \\(x_0\\) is in the closure \\(\\bar{M}\\) of \\(M\\) if and only if there is no bounded linear functional \\(f\\) on \\(X\\) such that \\(f(x)=0\\) for all \\(x \\in M\\) but \\(f\\left(x_0\\right) \\neq 0\\).\nProof If \\(x_0 \\in \\bar{M}, f\\) is a bounded linear functional on \\(X\\), and \\(f(x)=0\\) for all \\(x \\in M\\), the continuity of \\(f\\) shows that we also have \\(f\\left(x_0\\right)=0\\).\nConversely, suppose \\(x_0 \\notin \\bar{M}\\). Then there exists a \\(\\delta\u0026gt;0\\) such that \\(\\left\\|x-x_0\\right\\|\u0026gt;\\delta\\) for all \\(x \\in M\\). Let \\(M^{\\prime}\\) be the subspace generated by \\(M\\) and \\(x_0\\), and define \\(f\\left(x+\\lambda x_0\\right)=\\lambda\\) if \\(x \\in M\\) and \\(\\lambda\\) is a scalar. Since\n\\[ \\delta|\\lambda| \\leq|\\lambda|\\left\\|x_0+\\lambda^{-1} x\\right\\|=\\left\\|\\lambda x_0+x\\right\\|, \\]\nwe see that \\(f\\) is a linear functional on \\(M^{\\prime}\\) whose norm is at most \\(\\delta^{-1}\\). Also \\(f(x)=0\\) on \\(M, f\\left(x_0\\right)=1\\). The Hahn-Banach theorem allows us to extend this \\(f\\) from \\(M^{\\prime}\\) to \\(X\\).\n5.20 Theorem If \\(X\\) is a normed linear space and if \\(x_0 \\in X, x_0 \\neq 0\\), there is a bounded linear functional \\(f\\) on \\(X\\), of norm 1 , so that \\(f\\left(x_0\\right)=\\left\\|x_0\\right\\|\\).\nProof Let \\(M=\\left\\{\\lambda x_0\\right\\}\\), and define \\(f\\left(\\lambda x_0\\right)=\\lambda\\left\\|x_0\\right\\|\\). Then \\(f\\) is a linear functional of norm 1 on \\(M\\), and the Hahn-Banach theorem can again be applied. I//I\n5.21 Remarks If \\(X\\) is a normed linear space, let \\(X^*\\) be the collection of all bounded linear functionals on \\(X\\). If addition and scalar multiplication of linear functionals are defined in the obvious manner, it is easy to see that \\(X^*\\) is again a normed linear space. In fact, \\(X^*\\) is a Banach space; this follows from the fact that the field of scalars is a complete metric space. We leave the verification of these properties of \\(X^*\\) as an exercise.\nOne of the consequences of Theorem \\(5.20\\) is that \\(X^*\\) is not the trivial vector space (i.e., \\(X^*\\) consists of more than 0 ) if \\(X\\) is not trivial. In fact, \\(X^*\\) separates points on \\(X\\). This means that if \\(x_1 \\neq x_2\\) in \\(X\\) there exists an \\(f \\in X^*\\) such that \\(f\\left(x_1\\right) \\neq f\\left(x_2\\right)\\). To prove this, merely take \\(x_0=x_2-x_1\\) in Theorem \\(5.20\\). Another consequence is that, for \\(x \\in X\\),\n\\[ \\|x\\|=\\sup \\left\\{|f(x)|: f \\in X^*,\\|f\\|=1\\right\\} . \\]\nHence, for fixed \\(x \\in X\\), the mapping \\(f \\rightarrow f(x)\\) is a bounded linear functional on \\(X^*\\), of norm \\(\\|x\\|\\).\nThis interplay between \\(X\\) and \\(X^*\\) (the so-called “dual space” of \\(X\\) ) forms the basis of a large portion of that part of mathematics which is known as functional analysis.\n","date":"2022-11-19T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/5-the-hahn-banach-theorem/","section":"papa rudin","tags":null,"title":"The Hahn-Banach Theorem"},{"categories":null,"contents":"5.11 A Convergence Problem Is it true for every \\(f \\in C(T)\\) that the Fourier series of \\(f\\) converges to \\(f(x)\\) at every point \\(x\\) ?\nLet us recall that the \\(n\\)th partial sum of the Fourier series of \\(f\\) at the point \\(x\\) is given by \\[ s_n(f ; x)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) D_n(x-t) d t \\quad(n=0,1,2, \\ldots), \\] where \\[ D_n(t)=\\sum_{k=-n}^n e^{i k t} . \\] This follows directly from formulas \\(4.26(1)\\) and \\(4.26(3)\\). The problem is to determine whether \\[ \\lim _{n \\rightarrow \\infty} s_n(f ; x)=f(x) \\] for every \\(f \\in C(T)\\) and for every real \\(x\\). We observed in Sec. \\(4.26\\) that the partial sums do converge to \\(f\\) in the \\(L^2\\)-norm, and Theorem \\(3.12\\) implies therefore that each \\(f \\in L^2(T)\\) [hence also each \\(f \\in C(T)\\) ] is the pointwise limit a.e. of some subsequence of the full sequence of the partial sums. But this does not answer the present question.\nWe shall see that the Banach-Steinhaus theorem answers the question negatively. Put \\[ s^*(f ; x)=\\sup _n\\left|s_n(f ; x)\\right| . \\] To begin with, take \\(x=0\\), and define \\[ \\Lambda_n f=s_n(f ; 0) \\quad(f \\in C(T) ; n=1,2,3, \\ldots) . \\] We know that \\(C(T)\\) is a Banach space, relative to the supremum norm \\(\\|f\\|_{\\infty}\\). It follows from (1) that each \\(\\Lambda_n\\) is a bounded linear functional on \\(C(T)\\), of norm \\[ \\left\\|\\Lambda_n\\right\\| \\leq \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi\\left|D_n(t)\\right| d t=\\left\\|D_n\\right\\|_1 . \\] We claim that \\[ \\left\\|\\Lambda_n\\right\\| \\rightarrow \\infty \\quad \\text { as } n \\rightarrow \\infty . \\] This will be proved by showing that equality holds in (6) and that \\[ \\left\\|D_n\\right\\|_1 \\rightarrow \\infty \\quad \\text { as } n \\rightarrow \\infty . \\]\n\\(\\left\\|D_n\\right\\|_1 \\rightarrow \\infty \\quad\\) as \\(n \\rightarrow \\infty\\) Multiply (2) by \\(e^{i t / 2}\\) and by \\(e^{-i t / 2}\\) and subtract one of the resulting two equations from the other, to obtain \\[ D_n(t)=\\frac{\\sin \\left(n+\\frac{1}{2}\\right) t}{\\sin (t / 2)} . \\]\nSince \\(|\\sin x| \\leq|x|\\) for all real \\(x,(9)\\) shows that \\[ \\begin{aligned} \\left\\|D_n\\right\\|_1 \u0026amp;\u0026gt;\\frac{2}{\\pi} \\int_0^\\pi\\left|\\sin \\left(n+\\frac{1}{2}\\right) t\\right| \\frac{d t}{t}=\\frac{2}{\\pi} \\int_0^{(n+1 / 2) \\pi}|\\sin t| \\frac{d t}{t} \\\\ \u0026amp;\u0026gt;\\frac{2}{\\pi} \\sum_{k=1}^n \\frac{1}{k \\pi} \\int_{(k-1) \\pi}^{k \\pi}|\\sin t| d t=\\frac{4}{\\pi^2} \\sum_{k=1}^n \\frac{1}{k} \\rightarrow \\infty, \\end{aligned} \\] which proves (8).\nNext, fix \\(n\\), and put \\(g(t)=1\\) if \\(D_n(t) \\geq 0, g(t)=-1\\) if \\(D_n(t)\u0026lt;0\\). There exist \\(f_j \\in C(T)\\) such that \\(-1 \\leq f_j \\leq 1\\) and \\(f_j(t) \\rightarrow g(t)\\) for every \\(t\\), as \\(j \\rightarrow \\infty\\). By the dominated convergence theorem, \\[ \\lim _{j \\rightarrow \\infty} \\Lambda_n\\left(f_j\\right)=\\lim _{j \\rightarrow \\infty} \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f_j(-t) D_n(t) d t=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi g(-t) D_n(t) d t=\\left\\|D_n\\right\\|_1 . \\] Thus equality holds in (6), and we have proved (7).\nSince (7) holds, the Banach-Steinhaus theorem asserts now that \\(s^*(f ; 0)=\\infty\\) for every \\(f\\) in some dense \\(G_\\delta\\)-set in \\(C(T)\\).\nWe chose \\(x=0\\) just for convenience. It is clear that the same result holds for every other \\(x\\) :\nTo each real number \\(x\\) there corresponds a set \\(E_x \\subset C(T)\\) which is a dense \\(G_\\delta\\) in \\(C(T)\\), such that \\(s^*(f ; x)=\\infty\\) for every \\(f \\in E_x\\).\nIn particular, the Fourier series of each \\(f \\in E_x\\) diverges at \\(x\\), and we have a negative answer to our question. (Exercise 22 shows the answer is positive if mere continuity is replaced by a somewhat stronger smoothness assumption.)\nIt is interesting to observe that the above result can be strengthened by another application of Baire’s theorem. Let us take countably many points \\(x_i\\), and let \\(E\\) be the intersection of the corresponding sets \\[ E_{x_i} \\subset C(T) . \\] By Baire’s theorem, \\(E\\) is a dense \\(G_\\delta\\) in \\(C(T)\\). Every \\(f \\in E\\) has \\[ s^*\\left(f ; x_i\\right)=\\infty \\] at every point \\(x_i\\).\nFor each \\(f, s^*(f ; x)\\) is a lower semicontinuous function of \\(x\\), since (4) exhibits it as the supremum of a collection of continuous functions. Hence \\(\\left\\{x: s^*(f ; x)=\\infty\\right\\}\\) is a \\(G_\\delta\\) in \\(R^1\\), for each \\(f\\). If the above points \\(x_i\\) are taken so that their union is dense in \\((-\\pi, \\pi)\\), we obtain the following result:\n5.12 Theorem There is a set \\(E \\subset C(T)\\) which is a dense \\(G_\\delta\\) in \\(C(T)\\) and which has the following property: For each \\(f \\in E\\), the set\n\\[ Q_f=\\left\\{x: s^*(f ; x)=\\infty\\right\\} \\] is a dense \\(G_\\delta\\) in \\(R^1\\).\nThis gains in interest if we realize that \\(E\\), as well as each \\(Q_f\\), is an uncountable set:\n5.13 Theorem In a complete metric space \\(X\\) which has no isolated points, no countable dense set is a \\(G_\\delta\\).\nProof Let \\(x_k\\) be the points of a countable dense set \\(E\\) in \\(X\\). Assume that \\(E\\) is a \\(G_\\delta\\). Then \\(E=\\bigcap V_n\\), where each \\(V_n\\) is dense and open. Let \\[ W_n=V_n-\\bigcup_{k=1}^n\\left\\{x_k\\right\\} . \\] Then each \\(W_n\\) is still a dense open set, but \\(\\bigcap W_n=\\varnothing\\), in contradiction to Baire’s theorem.\nNote: A slight change in the proof of Baire’s theorem shows actually that every dense \\(G_\\delta\\) contains a perfect set if \\(X\\) is as above.\n","date":"2022-11-18T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/3-fourier-series-of-continuous-functions/","section":"papa rudin","tags":null,"title":"Fourier Series of Continuous Functions"},{"categories":null,"contents":"4.23 Definitions Let \\(T\\) be the unit circle in the complex plane, i.e., the set of all complex numbers of absolute value 1 . If \\(F\\) is any function on \\(T\\) and if \\(f\\) is defined on \\(R^1\\) by\n\\[ \\begin{equation} f(t)=F\\left(e^{i t}\\right), \\end{equation} \\]\nthen \\(f\\) is a periodic function of period \\(2 \\pi\\). This means that \\(f(t+2 \\pi)=f(t)\\) for all real \\(t\\). Conversely, if \\(f\\) is a function on \\(R^1\\), with period \\(2 \\pi\\), then there is a function \\(F\\) on \\(T\\) such that (1) holds. Thus we may identify functions on \\(T\\) with \\(2 \\pi\\)-periodic functions on \\(R^1\\); and, for simplicity of notation, we shall sometimes write \\(f(t)\\) rather than \\(f\\left(e^{i t}\\right)\\), even if we think of \\(f\\) as being defined on \\(T\\).\nWith these conventions in mind, we define \\(L^p(T)\\), for \\(1 \\leq p\u0026lt;\\infty\\), to be the class of all complex, Lebesgue measurable, \\(2 \\pi\\)-periodic functions on \\(R^1\\) for which the norm \\[ \\begin{equation} \\|f\\|_p=\\left\\{\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi|f(t)|^p d t\\right\\}^{1 / p} \\end{equation} \\] is finite.\nIn other words, we are looking at \\(L^p(\\mu)\\), where \\(\\mu\\) is Lebesgue measure on \\([0,2 \\pi]\\) (or on \\(T\\) ), divided by \\(2 \\pi\\). \\(L^{\\infty}(T)\\) will be the class of all \\(2 \\pi\\)-periodic members of \\(L^{\\infty}\\left(R^1\\right)\\), with the essential supremum norm, and \\(C(T)\\) consists of all continuous complex functions on \\(T\\) (or, equivalently, of all continuous, complex, \\(2 \\pi\\)-periodic functions on \\(R^1\\) ), with norm \\[ \\begin{equation} \\|f\\|_{\\infty}=\\sup _t|f(t)|, \\end{equation} \\]\nThe factor \\(1 /(2 \\pi)\\) in (2) simplifies the formalism we are about to develop. For instance, the \\(L^p\\)-norm of the constant function 1 is 1 .\nA trigonometric polynomial is a finite sum of the form\n\\[ \\begin{equation} f(t)=a_0+\\sum_{n=1}^N\\left(a_n \\cos n t+b_n \\sin n t\\right) \\quad\\left(t \\in R^1\\right) \\end{equation} \\]\nwhere \\(a_0, a_1, \\ldots, a_N\\) and \\(b_1, \\ldots, b_N\\) are complex numbers. On account of the Euler identities, (4) can also be written in the form\n\\[ \\begin{equation} f(t)=\\sum_{n=-N}^N c_n e^{i n t} \\end{equation} \\]\nwhich is more convenient for most purposes. It is clear that every trigonometric polynomial has period \\(2 \\pi\\).\nWe shall denote the set of all integers (positive, zero, and negative) by \\(Z\\), and put\n\\[ u_n(t)=e^{i n t} \\quad(n \\in Z) . \\]\nIf we define the inner product in \\(L^2(T)\\) by\n\\[ (f, g)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) \\overline{g(t)} d t \\]\n[note that this is in agreement with (2)], an easy computation shows that\n\\[ \\left(u_n, u_m\\right)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi e^{i(n-m) t} d t= \\begin{cases}1 \u0026amp; \\text { if } n=m, \\\\ 0 \u0026amp; \\text { if } n \\neq m .\\end{cases} \\]\nThus \\(\\left\\{u_n: n \\in Z\\right\\}\\) is an orthonormal set in \\(L^2(T)\\), usually called the trigonometric system. We shall now prove that this system is maximal, and shall then derive concrete versions of the abstract theorems previously obtained in the Hilbert space context.\n4.24 The Completeness of the Trigonometric System Theorem \\(4.18\\) shows that the maximality (or completeness) of the trigonometric system will be proved as soon as we can show that the set of all trigonometric polynomials is dense in \\(L^2(T)\\). Since \\(C(T)\\) is dense in \\(L^2(T)\\), by Theorem \\(3.14\\) (note that \\(T\\) is compact), it suffices to show that to every \\(f \\in C(T)\\) and to every \\(\\epsilon\u0026gt;0\\) there is a trigonometric polynomial \\(P\\) such that \\(\\|f-P\\|_2\u0026lt;\\epsilon\\). Since \\(\\|g\\|_2 \\leq\\|g\\|_{\\infty}\\) for every \\(g \\in C(T)\\), the estimate \\(\\|f-P\\|_2\u0026lt;\\epsilon\\) will follow from \\(\\|f-P\\|_{\\infty}\u0026lt;\\epsilon\\), and it is this estimate which we shall prove.\nSuppose we had trigonometric polynomials \\(Q_1, Q_2, Q_3, \\ldots\\), with the following properties:\n\\(Q_k(t) \\geq 0\\) for \\(t \\in R^1\\). \\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi Q_k(t) d t=1 . \\]\nIf \\(\\eta_k(\\delta)=\\sup \\left\\{Q_k(t): \\delta \\leq|t| \\leq \\pi\\right\\}\\), then \\[ \\lim _{k \\rightarrow \\infty} \\eta_k(\\delta)=0 \\]\nfor every \\(\\delta\u0026gt;0\\).\nAnother way of stating \\((c)\\) is to say: for every \\(\\delta\u0026gt;0, Q_k(t) \\rightarrow 0\\) uniformly on \\([-\\pi,-\\delta] \\cup[\\delta, \\pi]\\)\nTo each \\(f \\in C(T)\\) we associate the functions \\(P_k\\) defined by\n\\[ P_k(t)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t-s) Q_k(s) d s \\quad(k=1,2,3, \\ldots) . \\]\nIf we replace \\(s\\) by \\(-s\\) (using Theorem \\(2.20(e)\\) ) and then by \\(s-t\\), the periodicity of \\(f\\) and \\(Q_k\\) shows that the value of the integral is not affected. Hence\n\\[ P_k(t)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(s) Q_k(t-s) d s \\quad(k=1,2,3, \\ldots) . \\]\nSince each \\(Q_k\\) is a trigonometric polynomial, \\(Q_k\\) is of the form\n\\[ Q_k(t)=\\sum_{n=-N_k}^{N_k} a_{n, k} e^{i n t}, \\]\nand if we replace \\(t\\) by \\(t-s\\) in (3) and substitute the result in (2), we see that each \\(P_k\\) is a trigonometric polynomial.\nLet \\(\\epsilon\u0026gt;0\\) be given. Since \\(f\\) is uniformly continuous on \\(T\\), there exists a \\(\\delta\u0026gt;0\\) such that \\(|f(t)-f(s)|\u0026lt;\\epsilon\\) whenever \\(|t-s|\u0026lt;\\delta\\). By \\((b)\\), we have\n\\[ P_k(t)-f(t)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi\\{f(t-s)-f(t)\\} Q_k(s) d s, \\]\nand \\((a)\\) implies, for all \\(t\\), that\n\\[ \\left|P_k(t)-f(t)\\right| \\leq \\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi|f(t-s)-f(t)| Q_k(s) d s=A_1+A_2, \\]\nwhere \\(A_1\\) is the integral over \\([-\\delta, \\delta]\\) and \\(A_2\\) is the integral over \\([-\\pi,-\\delta] \\cup\\) \\([\\delta, \\pi]\\). In \\(A_1\\), the integrand is less than \\(\\epsilon Q_k(s)\\), so \\(A_1\u0026lt;\\epsilon\\), by \\((b)\\). In \\(A_2\\), we have \\(Q_k(s) \\leq \\eta_k(\\delta)\\), hence\n\\[ A_2 \\leq 2\\|f\\|_{\\infty} \\cdot \\eta_k(\\delta)\u0026lt;\\epsilon \\]\nfor sufficiently large \\(k\\), by \\((c)\\). Since these estimates are independent of \\(t\\), we have proved that\n\\[ \\lim _{k \\rightarrow \\infty}\\left\\|f-P_k\\right\\|_{\\infty}=0 . \\]\nIt remains to construct the \\(Q_k\\). This can be done in many ways. Here is a simple one. Put\n\\[ Q_k(t)=c_k\\left\\{\\frac{1+\\cos t}{2}\\right\\}^k, \\]\nwhere \\(c_k\\) is chosen so that \\((b)\\) holds. Since \\((a)\\) is clear, we only need to show \\((c)\\). Since \\(Q_k\\) is even, \\((b)\\) shows that\n\\[ 1=\\frac{c_k}{\\pi} \\int_0^\\pi\\left\\{\\frac{1+\\cos t}{2}\\right\\}^k d t\u0026gt;\\frac{c_k}{\\pi} \\int_0^\\pi\\left\\{\\frac{1+\\cos t}{2}\\right\\}^k \\sin t d t=\\frac{2 c_k}{\\pi(k+1)} . \\]\nSince \\(Q_k\\) is decreasing on \\([0, \\pi]\\), it follows that\n\\[ Q_k(t) \\leq Q_k(\\delta) \\leq \\frac{\\pi(k+1)}{2}\\left(\\frac{1+\\cos \\delta}{2}\\right)^k \\quad(0\u0026lt;\\delta \\leq|t| \\leq \\pi) \\]\nThis implies \\((c)\\), since \\(1+\\cos \\delta\u0026lt;2\\) if \\(0\u0026lt;\\delta \\leq \\pi\\).\nWe have proved the following important result:\n4.25 Theorem If \\(f \\in C(T)\\) and \\(\\epsilon\u0026gt;0\\), there is a trigonometric polynomial \\(P\\) such that\n\\[ |f(t)-P(t)|\u0026lt;\\epsilon \\]\nfor every real \\(t\\).\nA more precise result was proved by Fejér (1904): The arithmetic means of the partial sums of the Fourier series of any \\(f \\in C(T)\\) converge uniformly to \\(f\\). For a proof (quite similar to the above) see Theorem \\(3.1\\) of \\([45]\\), or p. 89 of [36], vol. I.\n4.26 Fourier Series For any \\(f \\in L^1(T)\\), we define the Fourier coefficients of \\(f\\) by the formula\n\\[ \\hat{f}(n)=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) e^{-i n t} d t \\quad(n \\in Z) \\]\nwhere, we recall, \\(Z\\) is the set of all integers. We thus associate with each \\(f \\in L^1(T)\\) a function \\(\\hat{f}\\) on \\(Z\\). The Fourier series of \\(f\\) is\n\\[ \\sum_{-\\infty}^{\\infty} \\hat{f}(n) e^{i n t} \\]\nand its partial sums are\n\\[ s_N(t)=\\sum_{-N}^N \\hat{f}(n) e^{i n t} \\quad(N=0,1,2, \\ldots) \\]\nSince \\(L^2(T) \\subset L^1(T)\\), (1) can be applied to every \\(f \\in L^2(T)\\). Comparing the definitions made in Secs. \\(4.23\\) and \\(4.13\\), we can now restate Theorems \\(4.17\\) and \\(4.18\\) in concrete terms:\nThe Riesz-Fischer theorem asserts that if \\(\\left\\{c_n\\right\\}\\) is a sequence of complex numbers such that\n\\[ \\sum_{n=-\\infty}^{\\infty}\\left|c_n\\right|^2\u0026lt;\\infty \\]\nthen there exists an \\(f \\in L^2(T)\\) such that\n\\[ c_n=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) e^{-i n t} d t \\quad(n \\in Z) \\]\nThe Parseval theorem asserts that\n\\[ \\sum_{n=-\\infty}^{\\infty} \\hat{f}(n) \\overline{\\hat{g}(n)}=\\frac{1}{2 \\pi} \\int_{-\\pi}^\\pi f(t) \\overline{g(t)} d t \\]\nwhenever \\(f \\in L^2(T)\\) and \\(g \\in L^2(T)\\); the series on the left of (6) converges absolutely; and if \\(s_N\\) is as in \\((3)\\), then\n\\[ \\lim _{N \\rightarrow \\infty}\\left\\|f-s_N\\right\\|_2=0, \\]\nsince a special case of \\((6)\\) yields\n\\[ \\left\\|f-s_N\\right\\|_2^2=\\sum_{|n|\u0026gt;N}|\\hat{f}(n)|^2 . \\]\nNote that (7) says that every \\(f \\in L^2(T)\\) is the \\(L^2\\)-limit of the partial sums of its Fourier series; i.e., the Fourier series of \\(f\\) converges to \\(f\\), in the \\(L^2\\)-sense. Pointwise convergence presents a more delicate problem, as we shall see in Chap. 5.\nThe Riesz-Fischer theorem and the Parseval theorem may be summarized by saying that the mapping \\(f \\leftrightarrow \\hat{f}\\) is a Hilbert space isomorphism of \\(L^2(T)\\) onto \\(\\ell^2(Z)\\).\nThe theory of Fourier series in other function spaces, for instance in \\(L^1(T)\\), is much more difficult than in \\(L^2(T)\\), and we shall touch only a few aspects of it.\nObserve that the crucial ingredient in the proof of the Riesz-Fischer theorem is the fact that \\(L^2\\) is complete. This is so well recognized that the name “RieszFischer theorem” is sometimes given to the theorem which asserts the completeness of \\(L^2\\), or even of any \\(L^p\\).\n","date":"2022-11-07T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-4/3-trigonometric-series/","section":"papa rudin","tags":null,"title":"Trigonometric Series"},{"categories":null,"contents":"4.13 Definitions If \\(V\\) is a vector space, if \\(x_1, \\ldots, x_k \\in V\\), and if \\(c_1, \\ldots, c_k\\) are scalars, then \\(c_1 x_1+\\cdots+c_k x_k\\) is called a linear combination of \\(x_1, \\ldots, x_k\\). The set \\(\\left\\{x_1, \\ldots, x_k\\right\\}\\) is called independent if \\(c_1 x_1+\\cdots+c_k x_k=0\\) implies that \\(c_1=\\cdots=c_k=0\\). A set \\(S \\subset V\\) is independent if every finite subset of \\(S\\) is independent. The set \\([S]\\) of all linear combinations of all finite subsets of \\(S\\) (also called the set of all finite linear combinations of members of \\(S\\) ) is clearly a vector space; \\([S]\\) is the smallest subspace of \\(V\\) which contains \\(S ;[S]\\) is called the span of \\(S\\), or the space spanned by \\(S\\).\nA set of vectors \\(u_\\alpha\\) in a Hilbert space \\(H\\), where \\(\\alpha\\) runs through some index set \\(A\\), is called orthonormal if it satisfies the orthogonality relations \\(\\left(u_\\alpha, u_\\beta\\right)=0\\) for all \\(\\alpha \\neq \\beta, \\alpha \\in A\\), and \\(\\beta \\in A\\), and if it is normalized so that \\(\\left\\|u_\\alpha\\right\\|=1\\) for each \\(\\alpha \\in A\\). In other words, \\(\\left\\{u_\\alpha\\right\\}\\) is orthonormal provided that\n\\[ \\left(u_\\alpha, u_\\beta\\right)= \\begin{cases}1 \u0026amp; \\text { if } \\alpha=\\beta, \\\\ 0 \u0026amp; \\text { if } \\alpha \\neq \\beta .\\end{cases} \\]\nIf \\(\\left\\{u_\\alpha: \\alpha \\in A\\right\\}\\) is orthonormal, we associate with each \\(x \\in H\\) a complex function \\(\\hat{x}\\) on the index set \\(A\\), defined by\n\\[ \\hat{x}(\\alpha)=\\left(x, u_\\alpha\\right) \\quad(\\alpha \\in A) . \\]\nOne sometimes calls the numbers \\(\\hat{x}(\\alpha)\\) the Fourier coefficients of \\(x\\), relative to the set \\(\\left\\{u_\\alpha\\right\\}\\).\nWe begin with some simple facts about finite orthonormal sets.\n4.14 Theorem Suppose that \\(\\left\\{u_\\alpha: \\alpha \\in A\\right\\}\\) is an orthonormal set in \\(H\\) and that \\(F\\) is a finite subset of \\(A\\). Let \\(M_F\\) be the span of \\(\\left\\{u_\\alpha: \\alpha \\in F\\right\\}\\).\nIf \\(\\varphi\\) is a complex function on \\(A\\) that is 0 outside \\(F\\), then there is a vector \\(y \\in M_F\\), namely \\[ y=\\sum_{\\alpha \\in F} \\varphi(\\alpha) u_\\alpha \\]\nthat has \\(\\hat{y}(\\alpha)=\\varphi(\\alpha)\\) for every \\(\\alpha \\in A\\). Also,\n\\[ \\|y\\|^2=\\sum_{\\alpha \\in F}|\\varphi(\\alpha)|^2 . \\]\nIf \\(x \\in H\\) and \\[ s_F(x)=\\sum_{\\alpha \\in F} \\hat{x}(\\alpha) u_\\alpha \\]\nthen\n\\[ \\left\\|x-s_F(x)\\right\\|\u0026lt;\\|x-s\\| \\]\nfor every \\(s \\in M_F\\), except for \\(s=s_F(x)\\), and\n\\[ \\sum_{\\alpha \\in F}|\\hat{x}(\\alpha)|^2 \\leq\\|x\\|^2 \\]\nProof Part \\((a)\\) is an immediate consequence of the orthogonality relations \\(4.13(1)\\).\nIn the proof of \\((b)\\), let us write \\(s_F\\) in place of \\(s_F(x)\\), and note that \\(\\hat{s}_F(\\alpha)=\\) \\(\\hat{x}(\\alpha)\\) for all \\(\\alpha \\in F\\). This says that \\(\\left(x-s_F\\right) \\perp u_\\alpha\\) if \\(\\alpha \\in F\\), hence \\(\\left(x-s_F\\right) \\perp\\) \\(\\left(s_F-s\\right)\\) for every \\(s \\in M_F\\), and therefore\n\\[ \\|x-s\\|^2=\\left\\|\\left(x-s_F\\right)+\\left(s_F-s\\right)\\right\\|^2=\\left\\|x-s_F\\right\\|^2+\\left\\|s_F-s\\right\\|^2 . \\]\nThis gives (4). With \\(s=0\\), (6) gives \\(\\left\\|s_F\\right\\|^2 \\leq\\|x\\|^2\\), which is the same as (5), because of (2).\nThe inequality (4) states that the “partial sum” \\(s_F(x)\\) of the “Fourier series” \\(\\sum \\hat{x}(\\alpha) u_\\alpha\\) of \\(x\\) is the unique best approximation to \\(x\\) in \\(M_F\\), relative to the metric defined by the Hilbert space norm.\n4.15 We want to drop the finiteness condition that appears in Theorem \\(4.14\\) (thus obtaining Theorems \\(4.17\\) and 4.18) without even restricting ourselves to sets that are necessarily countable. For this reason it seems advisable to clarify the meaning of the symbol \\(\\sum_{\\alpha \\in 4} \\varphi(\\alpha)\\) when \\(\\alpha\\) ranges over an arbitrary set \\(A\\).\nAssume \\(0 \\leq \\varphi(\\alpha) \\leq \\infty\\) for each \\(\\alpha \\in A\\). Then\n\\[ \\sum_{\\alpha \\in A} \\varphi(\\alpha) \\]\ndenotes the supremum of the set of all finite sums \\(\\varphi\\left(\\alpha_1\\right)+\\cdots+\\varphi\\left(\\alpha_n\\right)\\), where \\(\\alpha_1, \\ldots, \\alpha_n\\) are distinct members of \\(A\\).\nA moment’s consideration will show that the sum (1) is thus precisely the Lebesgue integral of \\(\\varphi\\) relative to the counting measure \\(\\mu\\) on \\(A\\).\nIn this context one usually writes \\(\\ell^p(A)\\) for \\(L^p(\\mu)\\). A complex function \\(\\varphi\\) with domain \\(A\\) is thus in \\(\\ell^2(A)\\) if and only if\n\\[ \\sum_{\\alpha \\in A}|\\varphi(\\alpha)|^2\u0026lt;\\infty . \\]\nExample 4.5(b) shows that \\(\\ell^2(A)\\) is a Hilbert space, with inner product\n\\[ (\\varphi, \\psi)=\\sum_{\\alpha \\in A} \\varphi(\\alpha) \\overline{\\psi(\\alpha)} . \\]\nHere, again, the sum over \\(A\\) stands for the integral of \\(\\varphi \\bar{\\psi}\\) with respect to the counting measure; note that \\(\\varphi \\bar{\\psi} \\in \\ell^1(A)\\) because \\(\\varphi\\) and \\(\\psi\\) are in \\(\\ell^2(A)\\).\nTheorem \\(3.13\\) shows that the functions \\(\\varphi\\) that are zero except on some finite subset of \\(A\\) are dense in \\(\\ell^2(A)\\).\nMoreover, if \\(\\varphi \\in \\ell^2(A)\\), then \\(\\{\\alpha \\in A: \\varphi(\\alpha) \\neq 0\\}\\) is at most countable. For if \\(A_n\\) is the set of all \\(\\alpha\\) where \\(|\\varphi(\\alpha)|\u0026gt;1 / n\\), then the number of elements of \\(A\\) is at most\n\\[ \\sum_{\\alpha \\in A_n}|n \\varphi(\\alpha)|^2 \\leq n^2 \\sum_{\\alpha \\in A}|\\varphi(\\alpha)|^2 . \\]\nEach \\(A_n(n=1,2,3, \\ldots)\\) is thus a finite set. The following lemma about complete metric spaces will make it easy to pass from finite orthonormal sets to infinite ones.\n4.16 Lemma Suppose that \\(X\\) and \\(Y\\) are metric spaces, \\(X\\) is complete, \\(f: X \\rightarrow Y\\) is continuous, \\(X\\) has a dense subset \\(X_0\\) on which \\(f\\) is an isometry, and \\(f\\left(X_0\\right)\\) is dense in \\(Y\\). Then \\(f\\) is an isometry of \\(X\\) onto \\(Y\\). The most important part of the conclusion is that \\(f\\) maps \\(X\\) onto all of \\(Y\\). Recall that an isometry is simply a mapping that preserves distances. Thus, by assumption, the distance between \\(f\\left(x_1\\right)\\) and \\(f\\left(x_2\\right)\\) in \\(Y\\) is equal to that between \\(x_1\\) and \\(x_2\\) in \\(X\\), for all points \\(x_1, x_2\\) in \\(X_0\\). Proof The fact that \\(f\\) is an isometry on \\(X\\) is an immediate consequence of the continuity of \\(f\\), since \\(X_0\\) is dense in \\(X\\).\nPick \\(y \\in Y\\). Since \\(f\\left(X_0\\right)\\) is dense in \\(Y\\), there is a sequence \\(\\left\\{x_n\\right\\}\\) in \\(X_0\\) such that \\(f\\left(x_n\\right) \\rightarrow y\\) as \\(n \\rightarrow \\infty\\). Thus \\(\\left\\{f\\left(x_n\\right)\\right\\}\\) is a Cauchy sequence in \\(Y\\). Since \\(f\\) is an isometry on \\(X_0\\), it follows that \\(\\left\\{x_n\\right\\}\\) is also a Cauchy sequence. The completeness of \\(X\\) implies now that \\(\\left\\{x_n\\right\\}\\) converges to some \\(x \\in X\\), and the continuity of \\(f\\) shows that \\(f(x)=\\lim f\\left(x_n\\right)=y\\).\n4.17 Theorem Let \\(\\left\\{u_\\alpha: \\alpha \\in A\\right\\}\\) be an orthonormal set in \\(H\\), and let \\(P\\) be the space of all finite linear combinations of the vectors \\(u_\\alpha\\).\nThe inequality\n\\[ \\sum_{\\alpha \\in A}|\\hat{x}(\\alpha)|^2 \\leq\\|x\\|^2 \\]\nholds then for every \\(x \\in H\\), and \\(x \\rightarrow \\hat{x}\\) is a continuous linear mapping of \\(H\\) onto \\(\\ell^2(A)\\) whose restriction to the closure \\(\\bar{P}\\) of \\(P\\) is an isometry of \\(\\bar{P}\\) onto \\(\\ell^2(A)\\).\nProof Since the inequality \\(4.14(5)\\) holds for every finite set \\(F \\subset A\\), we have (1), the so-called Bessel inequality.\nDefine \\(f\\) on \\(H\\) by \\(f(x)=\\hat{x}\\). Then (1) shows explicitly that \\(f\\) maps \\(H\\) into \\(\\ell^2(A)\\). The linearity of \\(f\\) is obvious. If we apply (1) to \\(x-y\\) we see that\n\\[ \\|f(y)-f(x)\\|_2=\\|\\hat{y}-\\hat{x}\\|_2 \\leq\\|y-x\\| . \\]\nThus \\(f\\) is continuous. Theorem 4.14(a) shows that \\(f\\) is an isometry of \\(P\\) onto the dense subspace of \\(\\ell^2(A)\\) consisting of those functions whose support is a finite set \\(F \\subset A\\). The theorem follows therefore from Lemma 4.16, applied with \\(X=\\bar{P}, X_0=P, Y=\\ell^2(A)\\); note that \\(\\bar{P}\\), being a closed subset of the complete metric spate \\(H\\), is itself complete.\nThe fact that the mapping \\(x \\rightarrow \\hat{x}\\) carries \\(H\\) onto \\(\\ell^2(A)\\) is known as the RieszFischer theorem.\n4.18 Theorem Let \\(\\left\\{u_\\alpha: \\alpha \\in A\\right\\}\\) be an orthonormal set in \\(H\\). Each of the following four conditions on \\(\\left\\{u_\\alpha\\right\\}\\) implies the other three:\n\\(\\left\\{u_\\alpha\\right\\}\\) is a maximal orthonormal set in \\(H\\). The set \\(P\\) of all finite linear combinations of members of \\(\\left\\{u_\\alpha\\right\\}\\) is dense in \\(H\\). The equality \\[ \\sum_{\\alpha \\in A}|\\hat{x}(\\alpha)|^2=\\|x\\|^2 \\]\nholds for every \\(x \\in H\\). (iv) The equality\n\\[ \\sum_{\\alpha \\in A} \\hat{x}(\\alpha) \\overline{\\hat{y}(\\alpha)}=(x, y) \\]\nholds for all \\(x \\in H\\) and \\(y \\in H\\). The last formula is known as Parseval’s identity. Observe that \\(\\hat{x}\\) and \\(\\hat{y}\\) are in \\(\\ell^2(A)\\), hence \\(\\hat{x} \\hat{y}\\) is in \\(\\ell^1(A)\\), so that the sum in (iv) is well defined. Of course, (iii) is the special case \\(x=y\\) of (iv).\nMaximal orthonormal sets are often called complete orthoriormal sets or orthonormal bases.\nProof To say that \\(\\left\\{u_\\alpha\\right\\}\\) is maximal means simply that no vector of \\(H\\) can be adjoined to \\(\\left\\{u_\\alpha\\right\\}\\) in such a way that the resulting set is still orthonormal. This happens precisely when there is no \\(x \\neq 0\\) in \\(H\\) that is orthogonal to every \\(u_\\alpha\\).\nWe shall prove that (i) \\(\\rightarrow\\) (ii) \\(\\rightarrow\\) (iii) \\(\\rightarrow\\) (iv) \\(\\rightarrow\\) (i).\nIf \\(P\\) is not dense in \\(H\\), then its closure \\(\\bar{P}\\) is not all of \\(H\\), and the corollary to Theorem \\(4.11\\) implies that \\(P^{\\perp}\\) contains a nonzero vector. Thus \\(\\left\\{u_\\alpha\\right\\}\\) is not maximal when \\(P\\) is not dense, and (i) implies (ii).\nIf (ii) holds, so does (iii), by Theorem 4.17.\nThe implication (iii) \\(\\rightarrow\\) (iv) follows from the easily proved Hilbert space identity (sometimes called the “polarization identity”)\n\\[ 4(x, y)=\\|x+y\\|^2-\\|x-y\\|^2+i\\|x+i y\\|^2-i\\|x-i y\\|^2 \\]\nwhich expresses the inner product \\((x, y)\\) in terms of norms and which is equally valid with \\(\\hat{x}, \\hat{y}\\) in place of \\(x, y\\), simply because \\(\\ell^2(A)\\) is also a Hilbert space. (See Exercise 19 for other identities of this type.) Note that the sums in (iii) and (iv) are \\(\\|\\hat{x}\\|_2^2\\) and \\((\\hat{x}, \\hat{y})\\), respectively.\nFinally, if (i) is false, there exists \\(u \\neq 0\\) in \\(H\\) so that \\(\\left(u, u_\\alpha\\right)=0\\) for all \\(\\alpha \\in A\\). If \\(x=y=u\\), then \\((x, y)=\\|u\\|^2\u0026gt;0\\) but \\(\\hat{x}(\\alpha)=0\\) for all \\(\\alpha \\in A\\), hence (iv) fails. Thus (iv) implies (i), and the proof is complete.\n4.19 Isomorphisms Speaking informally, two algebraic systems of the same nature are said to be isomorphic if there is a one-to-one mapping of one onto the other which preserves all relevant properties. For instance, we may ask whether two groups are isomorphic or whether two fields are isomorphic. Two vector spaces are isomorphic if there is a one-to-one linear mapping of one onto the other. The linear mappings are the ones which preserve the relevant concepts in a vector space, namely, addition and scalar multiplication.\nIn the same way, two Hilbert spaces \\(H_1\\) and \\(H_2\\) are isomorphic if there is a one-to-one linear mapping \\(\\Lambda\\) of \\(H_1\\) onto \\(H_2\\) which also preserves inner products: \\((\\Lambda x, \\Lambda y)=(x, y)\\) for all \\(x\\) and \\(y \\in H_1\\). Such a \\(\\Lambda\\) is an isomorphism (or, more specifically, a Hilbert space isomorphism) of \\(H_1\\) onto \\(H_2\\). Using this terminology, Theorems \\(4.17\\) and \\(4.18\\) yield the following statement:\nIf \\(\\left\\{u_\\alpha: \\alpha \\in A\\right\\}\\) is a maximal orthonormal set in a Hilbert space \\(H\\), and if \\(\\hat{x}(\\alpha)=\\) \\(\\left(x, u_\\alpha\\right)\\), then the mapping \\(x \\rightarrow \\hat{x}\\) is a Hilbert space isomorphism of \\(H\\) onto \\(\\ell^2(A)\\).\nOne can prove (we shall omit this) that \\(\\ell^2(A)\\) and \\(\\ell^2(B)\\) are isomorphic if and only if the sets \\(A\\) and \\(B\\) have the same cardinal number. But we shall prove that every nontrivial Hilbert space (this means that the space does not consist of 0 alone) is isomorphic to some \\(\\ell^2(A)\\), by proving that every such space contains a maximal orthonormal set (Theorem 4.22). The proof will depend on a property of partially ordered sets which is equivalent to the axiom of choice.\n4.20 Partially Ordered Sets A set \\(\\mathscr{P}\\) is said to be partially ordered by a binary relation \\(\\leq\\) if\n\\(a \\leq b\\) and \\(b \\leq c\\) implies \\(a \\leq c\\). \\(a \\leq a\\) for every \\(\\alpha \\in \\mathscr{P}\\). \\(a \\leq b\\) and \\(b \\leq a\\) implies \\(a=b\\). A subset \\(\\mathscr{2}\\) of a partially ordered set \\(\\mathscr{P}\\) is said to be totally ordered (or linearly ordered) if every pair \\(a, b \\in \\mathscr{2}\\) satisfies either \\(\\alpha \\leq b\\) or \\(b \\leq a\\).\nFor example, every collection of subsets of a given set is partially ordered by the inclusion relation \\(\\subset\\).\nTo give a more specific example, let \\(\\mathscr{P}\\) be the collection of all open subsets of the plane, partially ordered by set inclusion, and let \\(\\mathscr{2}\\) be the collection of all open circular discs with center at the origin. Then \\(\\mathscr{Q} \\subset \\mathscr{P}, \\mathscr{Q}\\) is totally ordered by \\(\\subset\\), and \\(\\mathscr{2}\\) is a maximal totally ordered subset of \\(\\mathscr{P}\\). This means that if any member of \\(\\mathscr{P}\\) not in \\(\\mathscr{Q}\\) is adjoined to \\(\\mathscr{2}\\), the resulting collection of sets is no longer totally ordered by \\(\\subset\\).\n4.21 The Hausdorff Maximality Theorem Every nonempty partially ordered set contains a maximal totally ordered subset.\nThis is a consequence of the axiom of choice and is, in fact, equivalent to it ; another (very similar) form of it is known as Zorn’s lemma. We give the proof in the Appendix.\nIf now \\(H\\) is a nontrivial Hilbert space, then there exists a \\(u \\in H\\) with \\(\\|u\\|=1\\), so that there is a nonempty orthonormal set in \\(H\\). The existence of a maximal orthonormal set is therefore a consequence of the following theorem:\n4.22 Theorem Every orthonormal set \\(B\\) in a Hilbert space \\(H\\) is contained in a maximal orthonormal set in \\(H\\).\nProof Let \\(\\mathscr{P}\\) be the class of all orthonormal sets in \\(H\\) which contain the given set \\(B\\). Partially order \\(\\mathscr{P}\\) by set inclusion. Since \\(B \\in \\mathscr{P}, \\mathscr{P} \\neq \\varnothing\\). Hence \\(\\mathscr{P}\\) contains a maximal totally ordered class \\(\\Omega\\). Let \\(S\\) be the union of all members of \\(\\Omega\\). It is clear that \\(B \\subset S\\). We claim that \\(S\\) is a maximal orthonormal set:\nIf \\(u_1\\) and \\(u_2 \\in S\\), then \\(u_1 \\in A_1\\) and \\(u_2 \\in A_2\\) for some \\(A_1\\) and \\(A_2 \\in \\Omega\\). Since \\(\\Omega\\) is total ordered, \\(A_1 \\subset A_2\\) (or \\(A_2 \\subset A_1\\) ), so that \\(u_1 \\in A_2\\) and \\(u_2 \\in A_2\\). Since \\(A_2\\) is orthonormal, \\(\\left(u_1, u_2\\right)=0\\) if \\(u_1 \\neq u_2,\\left(u_1, u_2\\right)=1\\) if \\(u_1=u_2\\). Thus \\(S\\) is an orthonormal set.\nSuppose \\(S\\) is not maximal. Then \\(S\\) is a proper subset of an orthonormal set \\(S^*\\). Clearly, \\(S^* \\notin \\Omega\\), and \\(S^*\\) contains every member of \\(\\Omega\\). Hence we may adjoin \\(S^*\\) to \\(\\Omega\\) and still have a total order. This contradicts the maximality of \\(\\Omega\\).\n","date":"2022-10-31T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-4/2-orthonormal-sets/","section":"papa rudin","tags":null,"title":"Orthonormal Sets"},{"categories":null,"contents":"4.1 Definition A complex vector space \\(H\\) is called an inner product space (or unitary space) if to each ordered pair of vectors \\(x\\) and \\(y \\in H\\) there is associated a complex number \\((x, y)\\), the so-called “inner product” (or “scalar product”) of \\(x\\) and \\(y\\), such that the following rules hold:\n\\((y, x)=(\\overline{x, y)}\\). (The bar denotes complex conjugation.) \\((x+y, z)=(x, z)+(y, z)\\) if \\(x, y\\), and \\(z \\in H\\). \\((\\alpha x, y)=\\alpha(x, y)\\) if \\(x\\) and \\(y \\in H\\) and \\(\\alpha\\) is a scalar. \\((x, x) \\geq 0\\) for all \\(x \\in H\\). \\((x, x)=0\\) only if \\(x=0\\). Let us list some immediate consequences of these axioms:\nimplies that \\((0, y)=0\\) for all \\(y \\in H\\). and \\((c)\\) may be combined into the statement: For every \\(y \\in H\\), the mapping \\(x \\rightarrow(x, y)\\) is a linear functional on \\(H\\). \\((a)\\) and \\((c)\\) show that \\((x, \\alpha y)=\\bar{\\alpha}(x, y)\\). \\((a)\\) and \\((b)\\) imply the second distributive law: \\[ (z, x+y)=(z, x)+(z, y) \\] By \\((d)\\), we may define \\(\\|x\\|\\), the norm of the vector \\(x \\in H\\), to be the nonnegative square root of \\((x, x)\\). Thus \\((f)\\)\n\\[ \\|x\\|^2=(x, x) . \\]\n4.2 The Schwarz Inequality The properties \\(4.1(\\) a) to \\((d)\\) imply that\n\\[ |(x, y)| \\leq\\|x\\|\\|y\\| \\]\nfor all \\(x\\) and \\(y \\in H\\). Proof Put \\(A=\\|x\\|^2, B=|(x, y)|\\), and \\(C=\\|y\\|^2\\). There is a complex number \\(\\alpha\\) such that \\(|\\alpha|=1\\) and \\(\\alpha(y, x)=B\\). For any real \\(r\\), we then have\n\\[ (x-r \\alpha y, x-r \\alpha y)=(x, x)-r \\alpha(y, x)-r \\bar{\\alpha}(x, y)+r^2(y, y) . \\]\nThe expression on the left is real and not negative. Hence\n\\[ A-2 B r+C r^2 \\geq 0 \\]\nfor every real \\(r\\). If \\(C=0\\), we must have \\(B=0\\); otherwise (2) is false for large positive \\(r\\). If \\(C\u0026gt;0\\), take \\(r=B / C\\) in (2), and obtain \\(B^2 \\leq A C\\).\n4.3 The Triangle Inequality For \\(x\\) and \\(y \\in H\\), we have \\[ \\|x+y\\| \\leq\\|x\\|+\\|y\\| \\text {. } \\] Proof By the Schwarz inequality, \\[ \\begin{aligned} \\|x+y\\|^2 \u0026amp;=(x+y, x+y)=(x, x)+(x, y)+(y, x)+(y, y) \\\\ \u0026amp; \\leq\\|x\\|^2+2\\|x\\|\\|y\\|+\\|y\\|^2=(\\|x\\|+\\|y\\|)^2 . \\end{aligned} \\] ### 4.4 Definition\nIt follows from the triangle inequality that \\[ \\|x-z\\| \\leq\\|x-y\\|+\\|y-z\\| \\quad(x, y, z \\in H) . \\]\nIf we define the distance between \\(x\\) and \\(y\\) to be \\(\\|x-y\\|\\), all the axioms for a metric space are satisfied; here, for the first time, we use part \\((e)\\) of Definition 4.1.\nThus \\(H\\) is now a metric space. If this metric space is complete, i.e., if every Cauchy sequence converges in \\(H\\), then \\(H\\) is called a Hilbert space.\nThroughout the rest of this chapter, the letter \\(H\\) will denote a Hilbert space.\n4.5 Examples For any fixed \\(n\\), the set \\(C^n\\) of all \\(n\\)-tuples \\[ x=\\left(\\xi_1, \\ldots, \\xi_n\\right), \\] where \\(\\xi_1, \\ldots, \\xi_n\\) are complex numbers, is a Hilbert space if addition and scalar multiplication are defined componentwise, as usual, and if \\[ (x, y)=\\sum_{j=1}^n \\xi_j \\bar{\\eta}_j \\quad\\left(y=\\left(\\eta_1, \\ldots, \\eta_n\\right)\\right) . \\] If \\(\\mu\\) is any positive measure, \\(L^2(\\mu)\\) is a Hilbert space, with inner product \\[ (f, g)=\\int_X f \\bar{g} d \\mu . \\] The integrand on the right is in \\(L^1(\\mu)\\), by Theorem \\(3.8\\), so that \\((f, g)\\) is well defined. Note that \\[ \\|f\\|=(f, f)^{1 / 2}=\\left\\{\\int_X|f|^2 d \\mu\\right\\}^{1 / 2}=\\|f\\|_2 . \\] The completeness of \\(L^2(\\mu)\\) (Theorem 3.11) shows that \\(L^2(\\mu)\\) is indeed a Hilbert space. [We recall that \\(L^2(\\mu)\\) should be regarded as a space of equivalence classes of functions; compare the discussion in Sec. 3.10.] For \\(H=L^2(\\mu)\\), the inequalities \\(4.2\\) and \\(4.3\\) turn out to be special cases of the inequalities of Hölder and Minkowski.\nNote that Example \\((a)\\) is a special case of \\((b)\\). What is the measure in (a)?\nThe vector space of all continuous compiex functions on \\([0,1]\\) is an inner product space if \\[ (f, g)=\\int_0^1 f(t) \\overline{g(t)} d t \\] but is not a Hilbert space. 4.6 Theorem For any fixed \\(y \\in H\\), the mappings \\[ x \\rightarrow(x, y), \\quad x \\rightarrow(y, x), \\quad x \\rightarrow\\|x\\| \\] are continuous functions on \\(H\\). Proof The Schwarz inequality implies that \\[ \\left|\\left(x_1, y\\right)-\\left(x_2, y\\right)\\right|=\\left|\\left(x_1-x_2, y\\right)\\right| \\leq\\left\\|x_1-x_2\\right\\|\\|y\\|, \\] which proves that \\(x \\rightarrow(x, y)\\) is, in fact, uniformly continuous, and the same is true for \\(x \\rightarrow(y, x)\\). The triangle inequality \\(\\left\\|x_1\\right\\| \\leq\\left\\|x_1-x_2\\right\\|+\\left\\|x_2\\right\\|\\) yields \\[ \\left\\|x_1\\right\\|-\\left\\|x_2\\right\\| \\leq\\left\\|x_1-x_2\\right\\| \\text {, } \\] and if we interchange \\(x_1\\) and \\(x_2\\) we see that \\[ \\left|\\left\\|x_1\\right\\|-\\left\\|x_2\\right\\|\\right| \\leq\\left\\|x_1-x_2\\right\\| \\] for all \\(x_1\\) and \\(x_2 \\in H\\). Thus \\(x \\rightarrow\\|x\\|\\) is also uniformly continuous.\n4.7 Subspaces A subset \\(M\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if \\(M\\) is itself a vector space, relative to the addition and scalar multiplication which are defined in \\(V\\). A necessary and sufficient condition for a set \\(M \\subset V\\) to be a subspace is that \\(x+y \\in M\\) and \\(\\alpha x \\in M\\) whenever \\(x\\) and \\(y \\in M\\) and \\(\\alpha\\) is a scalar.\nIn the vector space context, the word “subspace” will always have this meaning. Sometimes, for emphasis, we may use the term “linear subspace” in place of subspace.\nFor example, if \\(V\\) is the vector space of all complex functions on a set \\(S\\), the set of all bounded complex functions on \\(S\\) is a subspace of \\(V\\), but the set of all \\(f \\in V\\) with \\(|f(x)| \\leq 1\\) for all \\(x \\in S\\) is not. The real vector space \\(R^3\\) has the following subspaces, and no others: \\((a) R^3,(b)\\) all planes through the origin \\(0,(c)\\) all straight lines through 0 , and \\((d)\\{0\\}\\).\nA closed subspace of \\(H\\) is a subspace that is a closed set relative to the topology induced by the metric of \\(H\\).\nNote that if \\(M\\) is a subspace of \\(H\\), so is its closure \\(\\bar{M}\\). To see this, pick \\(x\\) and \\(y\\) in \\(\\bar{M}\\) and let \\(\\alpha\\) be a scalar. There are sequences \\(\\left\\{x_n\\right\\}\\) and \\(\\left\\{y_n\\right\\}\\) in \\(M\\) that converge to \\(x\\) and \\(y\\), respectively. It is then easy to verify that \\(x_n+y_n\\) and \\(\\alpha x_n\\) converge to \\(x+y\\) and \\(\\alpha x\\), respectively. Thus \\(x+y \\in \\bar{M}\\) and \\(\\alpha x \\in \\bar{M}\\).\n4.8 Convex Sets A set \\(E\\) in a vector space \\(V\\) is said to be convex if it has the following geometric property: Whenever \\(x \\in E, y \\in E\\), and \\(0\u0026lt;t\u0026lt;1\\), the point \\[ z_t=(1-t) x+t y \\] also lies in \\(E\\). As \\(t\\) runs from 0 to 1, one may visualize \\(z_t\\) as describing a straight line segment in \\(V\\), from \\(x\\) to \\(y\\). Convexity requires that \\(E\\) contain the segments between any two of its points. It is clear that every subspace of \\(V\\) is convex. Also, if \\(E\\) is convex, so is each of its translates \\[ E+x=\\{y+x: y \\in E\\} \\]\n4.9 Orthogonality If \\((x, y)=0\\) for some \\(x\\) and \\(y \\in H\\), we say that \\(x\\) is orthogonal to \\(y\\), and sometimes write \\(x \\perp y\\). Since \\((x, y)=0\\) implies \\((y, x)=0\\), the relation \\(\\perp\\) is symmetric.\nLet \\(x^{\\perp}\\) denote the set of all \\(y \\in H\\) which are orthogonal to \\(x\\); and if \\(M\\) is a subspace of \\(H\\), let \\(M^{\\perp}\\) be the set of all \\(y \\in H\\) which are orthogonal to every \\(x \\in M\\).\nNote that \\(x^{\\perp}\\) is a subspace of \\(H\\), since \\(x \\perp y\\) and \\(x \\perp y^{\\prime}\\) implies \\(x \\perp\\left(y+y^{\\prime}\\right)\\) and \\(x \\perp \\alpha y\\). Also, \\(x^{\\perp}\\) is precisely the set of points where the continuous function \\(y \\rightarrow(x, y)\\) is 0 . Hence \\(x^{\\perp}\\) is a closed subspace of \\(H\\). Since \\[ M^{\\perp}=\\bigcap_{x \\in M} x^{\\perp}, \\] \\(M^{\\perp}\\) is an intersection of closed subspaces, and it follows that \\(M^{\\perp}\\) is a closed subspace of \\(H\\).\n4.10 Theorem Every nonempty, closed, convex set \\(E\\) in a Hilbert space \\(H\\) contains a unique element of smallest norm.\nIn other words, there is one and only one \\(x_0 \\in E\\) such that \\(\\left\\|x_0\\right\\| \\leq\\|x\\|\\) for every \\(x \\in E\\).\nProof An easy computation, using only the properties listed in Definition 4.1, establishes the identity \\[ \\|x+y\\|^2+\\|x-y\\|^2=2\\|x\\|^2+2\\|y\\|^2 \\quad(x \\text { and } y \\in H) . \\] This is known as the parallelogram law: If we interpret \\(\\|x\\|\\) to be the length of the vector \\(x\\), (1) says that the sum of the squares of the diagonals of a parallelogram is equal to the sum of the squares of its sides, a familiar proposition in plane geometry.\nLet \\(\\delta=\\inf \\{\\|x\\|: x \\in E\\}\\). For any \\(x\\) and \\(y \\in E\\), we apply (1) to \\(\\frac{1}{2} x\\) and \\(\\frac{1}{2} y\\) and obtain \\[ \\frac{1}{4}\\|x-y\\|^2=\\frac{1}{2}\\|x\\|^2+\\frac{1}{2}\\|y\\|^2-\\left\\|\\frac{x+y}{2}\\right\\|^2 . \\] Since \\(E\\) is convex, \\((x+y) / 2 \\in E\\). Hence \\[ \\|x-y\\|^2 \\leq 2\\|x\\|^2+2\\|y\\|^2-4 \\delta^2 \\quad(x \\text { and } y \\in E) . \\] If also \\(\\|x\\|=\\|y\\|=\\delta\\), then (3) implies \\(x=y\\), and we have proved the uniqueness assertion of the theorem.\nThe definition of \\(\\delta\\) shows that there is a sequence \\(\\left\\{y_n\\right\\}\\) in \\(E\\) so that \\(\\left\\|y_n\\right\\| \\rightarrow \\delta\\) as \\(n \\rightarrow \\infty\\). Replace \\(x\\) and \\(y\\) in (3) by \\(y_n\\) and \\(y_m\\). Then, as \\(n \\rightarrow \\infty\\) and \\(m \\rightarrow \\infty\\), the right side of (3) will tend to 0 . This shows that \\(\\left\\{y_n\\right\\}\\) is a Cauchy sequence. Since \\(H\\) is complete, there exists an \\(x_0 \\in H\\) so that \\(y_n \\rightarrow x_0\\), i.e., \\(\\left\\|y_n-x_0\\right\\| \\rightarrow 0\\), as \\(n \\rightarrow \\infty\\). Since \\(y_n \\in E\\) and \\(E\\) is closed, \\(x_0 \\in E\\). Since the norm is a continuous function on \\(H\\) (Theorem 4.6), it follows that \\[ \\left\\|x_0\\right\\|=\\lim _{n \\rightarrow \\infty}\\left\\|y_n\\right\\|=\\delta . \\]\n4.11 Theorem Let \\(M\\) be a closed subspace of a Hilbert space \\(H\\).\nEvery \\(x \\in H\\) has then a unique decomposition \\[ x=P x+Q x \\] into a sum of \\(P x \\in M\\) and \\(Q x \\in M^{\\perp}\\). Px and \\(Q x\\) are the nearest points to \\(x\\) in \\(M\\) and in \\(M^{\\perp}\\), respectively. The mappings \\(P: H \\rightarrow M\\) and \\(Q: H \\rightarrow M^{\\perp}\\) are linear. \\(\\|x\\|^2=\\|P x\\|^2+\\|Q x\\|^2\\). Corollary If \\(M \\neq H\\), then there exists \\(y \\in H, y \\neq 0\\), such that \\(y \\perp M\\). \\(P\\) and \\(Q\\) are called the orthogonal projections of \\(H\\) onto \\(M\\) and \\(M^{\\perp}\\).\nProof As regards the uniqueness in \\((a)\\), suppose that \\(x^{\\prime}+y^{\\prime}=x^{\\prime \\prime}+y^{\\prime \\prime}\\) for some vectors \\(x^{\\prime}, x^{\\prime \\prime}\\) in \\(M\\) and \\(y^{\\prime}, y^{\\prime \\prime}\\) in \\(M^{\\perp}\\). Then \\[ x^{\\prime}-x^{\\prime \\prime}=y^{\\prime \\prime}-y^{\\prime} . \\] Since \\(x^{\\prime}-x^{\\prime \\prime} \\in M, y^{\\prime \\prime}-y^{\\prime} \\in M^{\\perp}\\), and \\(M \\cap M^{\\perp}=\\{0\\}\\) [an immediate consequence of the fact that \\((x, x)=0\\) implies \\(x=0]\\), we have \\(x^{\\prime \\prime}=x^{\\prime}, y^{\\prime \\prime}=y^{\\prime}\\).\nTo prove the existence of the decomposition, note that the set \\[ x+M=\\{x+y: y \\in M\\} \\] is closed and convex. Define \\(Q x\\) to be the element of smallest norm in \\(x+M\\); this exists, by Theorem 4.10. Define \\(P x=x-Q x\\).\nSince \\(Q x \\in x+M\\), it is clear that \\(P x \\in M\\). Thus \\(P\\) maps \\(H\\) into \\(M\\).\nTo prove that \\(Q\\) maps \\(H\\) into \\(M^{\\perp}\\) we show that \\((Q x, y)=0\\) for all \\(y \\in M\\). Assume \\(\\|y\\|=1\\), without loss of generality, and put \\(z=Q x\\). The minimizing property of \\(Q x\\) shows that \\[ (z, z)=\\|z\\|^2 \\leq\\|z-\\alpha y\\|^2=(z-\\alpha y, z-\\alpha y) \\] for every scalar \\(\\alpha\\). This simplifies to \\[ 0 \\leq-\\alpha(y, z)-\\bar{\\alpha}(z, y)+\\alpha \\bar{\\alpha} . \\] With \\(\\alpha=(z, y)\\), this gives \\(0 \\leq-|(z, y)|^2\\), so that \\((z, y)=0\\). Thus \\(Q x \\in M^{\\perp}\\). We have already seen that \\(P x \\in M\\). If \\(y \\in M\\), it follows that \\[ \\|x-y\\|^2=\\|Q x+(P x-y)\\|^2=\\|Q x\\|^2+\\|P x-y\\|^2 \\] which is obviously minimized when \\(y=P x\\). We have now proved \\((a)\\) and \\((b)\\). If we apply \\((a)\\) to \\(x\\), to \\(y\\), and to \\(\\alpha x+\\beta y\\), we obtain \\[ P(\\alpha x+\\beta y)-\\alpha P x-\\beta P y=\\alpha Q x+\\beta Q y-Q(\\alpha x+\\beta y) . \\] The left side is in \\(M\\), the right side in \\(M^{\\perp}\\). Hence both are 0 , so \\(P\\) and \\(Q\\) are linear. Since \\(P x \\perp Q x,(d)\\) follows from \\((a)\\). To prove the corollary, take \\(x \\in H, x \\notin M\\), and put \\(y=Q x\\). Since \\(P x \\in M, x \\neq P x\\), hence \\(y=x-P x \\neq 0\\).\nWe have already observed that \\(x \\rightarrow(x, y)\\) is, for each \\(y \\in H\\), a continuous linear functional on \\(H\\). It is a very important fact that all continuous linear functionals on \\(H\\) are of this type.\n4.12 Theorem If \\(L\\) is a continuous linear functional on \\(H\\), then there is \\(a\\) unique \\(y \\in H\\) such that \\[ L x=(x, y) \\quad(x \\in H) . \\] PROOF If \\(L x=0\\) for all \\(x\\), take \\(y=0\\). Otherwise, define \\[ M=\\{x: L x=0\\} \\] The linearity of \\(L\\) shows that \\(M\\) is a subspace. The continuity of \\(L\\) shows that \\(M\\) is closed. Since \\(L x \\neq 0\\) for some \\(x \\in H\\), Theorem \\(4.11\\) shows that \\(M^{\\perp}\\) does not consist of 0 alone.\nHence there exists \\(z \\in M^{\\perp}\\), with \\(\\|z\\|=1\\). Put \\[ u=(L x) z-(L z) x \\] Since \\(L u=(L x)(L z)-(L z)(L x)=0\\), we have \\(u \\in M\\). Thus \\((u, z)=0\\). This gives \\[ L x=(L x)(z, z)=(L z)(x, z) . \\] Thus (1) holds with \\(y=\\alpha z\\), where \\(\\bar{\\alpha}=L z\\).\nThe uniqueness of \\(y\\) is easily proved, for if \\((x, y)=\\left(x, y^{\\prime}\\right)\\) for al \\(x \\in H\\), set \\(z=y-y^{\\prime} ;\\) then \\((x, z)=0\\) for all \\(x \\in H ;\\) in particular, \\((z, z)=0\\), hence \\(z=0\\).\n","date":"2022-10-30T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-4/1-inner-products-and-linear-functionals/","section":"papa rudin","tags":null,"title":"1 Inner Products and Linear Functionals"},{"categories":null,"contents":"Many of the most common inequalities in analysis have their origin in the notion of convexity.\n3.1 Definition A real function \\(\\varphi\\) defined on a segment \\((a, b)\\), where \\(-\\infty \\leq a\u0026lt;b \\leq \\infty\\), is called convex if the inequality\n\\[ \\begin{equation} \\varphi((1-\\lambda) x+\\lambda y) \\leq(1-\\lambda) \\varphi(x)+\\lambda \\varphi(y) \\end{equation} \\]\nholds whenever \\(a\u0026lt;x\u0026lt;b, a\u0026lt;y\u0026lt;b\\), and \\(0 \\leq \\lambda \\leq 1\\). Graphically, the condition is that if \\(x\u0026lt;t\u0026lt;y\\), then the point \\((t, \\varphi(t))\\) should lie below or on the line connecting the points \\((x, \\varphi(x))\\) and \\((y, \\varphi(y))\\) in the plane. Also, (1) is equivalent to the requirement that\n\\[ \\begin{equation} \\frac{\\varphi(t)-\\varphi(s)}{t-s} \\leq \\frac{\\varphi(u)-\\varphi(t)}{u-t} \\end{equation} \\]\nwhenever \\(a\u0026lt;s\u0026lt;t\u0026lt;u\u0026lt;b\\).\nThe mean value theorem for differentiation, combined with (2), shows immediately that a real differentiable function \\(\\varphi\\) is convex in \\((a, b)\\) if and only if \\(a\u0026lt;s\u0026lt;t\u0026lt;b\\) implies \\(\\varphi^{\\prime}(s) \\leq \\varphi^{\\prime}(t)\\), i.e., if and only if the derivative \\(\\varphi^{\\prime}\\) is a monotonically increasing function.\nFor example, the exponential function is convex on \\((-\\infty, \\infty)\\).\n3.2 Theorem If \\(\\varphi\\) is convex on \\((a, b)\\) then \\(\\varphi\\) is continuous on \\((a, b)\\).\nProof The idea of the proof is most easily conveyed in geometric language. Those who may worry that this is not “rigorous” are invited to transcribe it in terms of epsilons and deltas.\nSuppose \\(a\u0026lt;s\u0026lt;x\u0026lt;y\u0026lt;t\u0026lt;b\\). Write \\(S\\) for the point \\((s, \\varphi(s))\\) in the plane, and deal similarly with \\(x, y\\), and \\(t\\). Then \\(X\\) is on or below the line \\(S Y\\), hence \\(Y\\) is on or above the line through \\(S\\) and \\(X\\); also, \\(Y\\) is on or below \\(X T\\). As \\(y \\rightarrow x\\), it follows that \\(Y \\rightarrow X\\), i.e., \\(\\varphi(y) \\rightarrow \\varphi(x)\\). Left-hand limits are handled in the same manner, and the continuity of \\(\\varphi\\) follows.\nNote that this theorem depends on the fact that we are working on an open segment. For instance, if \\(\\varphi(x)=0\\) on \\([0,1)\\) and \\(\\varphi(1)=1\\), then \\(\\varphi\\) satisfies 3.1(1) on \\([0,1]\\) without being continuous.\n3.3 Theorem (Jensen’s Inequality) Let \\(\\mu\\) be a positive measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in a set \\(\\Omega\\), so that \\(\\mu(\\Omega)=1\\). Iff is a real function in \\(L^1(\\mu)\\), if \\(a\u0026lt;f(x)\u0026lt;b\\) for all \\(x \\in \\Omega\\), and if \\(\\varphi\\) is convex on \\((a, b)\\), then\n\\[ \\varphi\\left(\\int_{\\Omega} f d \\mu\\right) \\leq \\int_{\\Omega}(\\varphi \\circ f) d \\mu . \\]\nNote: The cases \\(a=-\\infty\\) and \\(b=\\infty\\) are not excluded. It may happen that \\(\\varphi \\circ f\\) is not in \\(L^1(\\mu)\\); in that case, as the proof will show, the integral of \\(\\varphi \\circ f\\) exists in the extended sense described in Sec. \\(1.31\\), and its value is \\(+\\infty\\).\nProof Put \\(t=\\int_{\\Omega} f d \\mu\\). Then \\(a\u0026lt;t\u0026lt;b\\). If \\(\\beta\\) is the supremum of the quotients on the left of \\(3.1(2)\\), where \\(a\u0026lt;s\u0026lt;t\\), then \\(\\beta\\) is no larger than any of the quotients on the right of \\(3.1(2)\\), for any \\(u \\in(t, b)\\). It follows that\n\\[ \\varphi(s) \\geq \\varphi(t)+\\beta(s-t) \\quad(a\u0026lt;s\u0026lt;b) . \\]\nHence\n\\[ \\varphi(f(x))-\\varphi(t)-\\beta(f(x)-t) \\geq 0 \\]\nfor every \\(x \\in \\Omega\\). Since \\(\\varphi\\) is continuous, \\(\\varphi \\circ f\\) is measurable. If we integrate both sides of (3) with respect to \\(\\mu\\), (1) follows from our choice of \\(t\\) and the assumption \\(\\mu(\\Omega)=1\\).\nTo give an example, take \\(\\varphi(x)=e^x\\). Then (1) becomes\n\\[ \\exp \\left\\{\\int_{\\Omega} f d \\mu\\right\\} \\leq \\int_{\\Omega} e^f d \\mu . \\]\nIf \\(\\Omega\\) is a finite set, consisting of points \\(p_1, \\ldots, p_n\\), say, and if\n\\[ \\mu\\left(\\left\\{p_i\\right\\}\\right)=1 / n, \\quad f\\left(p_i\\right)=x_i, \\]\nbecomes \\[ \\exp \\left\\{\\frac{1}{n}\\left(x_1+\\cdots+x_n\\right)\\right\\} \\leq \\frac{1}{n}\\left(e^{x_1}+\\cdots+e^{x_n}\\right), \\]\nfor real \\(x_i\\). Putting \\(y_i=e^{x_i}\\), we obtain the familiar inequality between the arithmetic and geometric means of \\(n\\) positive numbers:\n\\[ \\left(y_1 y_2 \\cdots y_n\\right)^{1 / n} \\leq \\frac{1}{n}\\left(y_1+y_2+\\cdots+y_n\\right) . \\]\nGoing back from this to (4), it should become clear why the left and right sides of\n\\[ \\exp \\left\\{\\int_{\\Omega} \\log g d \\mu\\right\\} \\leq \\int_{\\Omega} g d \\mu \\]\nare often called the geometric and arithmetic means, respectively, of the positive function \\(g\\). If we take \\(\\mu\\left(\\left\\{p_i\\right\\}\\right)=\\alpha_i\u0026gt;0\\), where \\(\\sum \\alpha_i=1\\), then we obtain\n\\[ y_1^{\\alpha_1} y_2^{\\alpha_2} \\cdots y_n^{\\alpha_n} \\leq \\alpha_1 y_1+\\alpha_2 y_2+\\cdots+\\alpha_n y_n \\]\nin place of (6). These are just a few samples of what is contained in Theorem 3.3.\nFor a converse, see Exercise 20.\n3.4 Definition If \\(p\\) and \\(q\\) are positive real numbers such that \\(p+q=p q\\), or equivalently\n\\[ \\frac{1}{p}+\\frac{1}{q}=1 \\]\nthen we call \\(p\\) and \\(q\\) a pair of conjugate exponents. It is clear that (1) implies \\(1\u0026lt;p\u0026lt;\\infty\\) and \\(1\u0026lt;q\u0026lt;\\infty\\). An important special case is \\(p=q=2\\).\nAs \\(p \\rightarrow 1,(1)\\) forces \\(q \\rightarrow \\infty\\). Consequently 1 and \\(\\infty\\) are also regarded as a pair of conjugate exponents. Many analysts denote the exponent conjugate to \\(p\\) by \\(p^{\\prime}\\), often without saying so explicitly.\n3.5 Theorem Let \\(p\\) and \\(q\\) be conjugate exponents, \\(1\u0026lt;p\u0026lt;\\infty\\). Let \\(X\\) be a measure space, with measure \\(\\mu\\). Let \\(f\\) and \\(g\\) be measurable functions on \\(X\\), with range in \\([0, \\infty]\\). Then\n\\[ \\int_X f g d \\mu \\leq\\left\\{\\int_X f^p d \\mu\\right\\}^{1 / p}\\left\\{\\int_X g^q d \\mu\\right\\}^{1 / q} \\]\nand\n\\[ \\left\\{\\int_X(f+g)^p d \\mu\\right\\}^{1 / p} \\leq\\left\\{\\int_X f^p d \\mu\\right\\}^{1 / p}+\\left\\{\\int_X g^p d \\mu\\right\\}^{1 / p} . \\]\nThe inequality (1) is Hölder’s; (2) is Minkowski’s. If \\(p=q=2\\), (1) is known as the Schwarz inequality.\nProof Let \\(A\\) and \\(B\\) be the two factors on the right of (1). If \\(A=0\\), then \\(f=0\\) a.e. (by Theorem 1.39); hence \\(f g=0\\) a.e., so (1) holds. If \\(A\u0026gt;0\\) and \\(B=\\infty\\), (1) is again trivial. So we need consider only the case \\(0\u0026lt;A\u0026lt;\\infty, 0\u0026lt;B\u0026lt;\\infty\\). Put\n\\[ F=\\frac{f}{A}, \\quad G=\\frac{g}{B} . \\]\nThis gives\n\\[ \\int_X F^p d \\mu=\\int_X G^q d \\mu=1 . \\]\nIf \\(x \\in X\\) is such that \\(0\u0026lt;F(x)\u0026lt;\\infty\\) and \\(0\u0026lt;G(x)\u0026lt;\\infty\\), there are real numbers \\(s\\) and \\(t\\) such that \\(F(x)=e^{s / p}, G(x)=e^{t / q}\\). Since \\(1 / p+1 / q=1\\), the convexity of the exponential function implies that\n\\[ e^{s / p+t / q} \\leq p^{-1} e^s+q^{-1} e^t . \\]\nIt follows that\n\\[ F(x) G(x) \\leq p^{-1} F(x)^p+q^{-1} G(x)^q \\]\nfor every \\(x \\in X\\). Integration of (6) yields\n\\[ \\int_X F G d \\mu \\leq p^{-1}+q^{-1}=1, \\]\nby (4); inserting (3) into (7), we obtain (1). Note that (6) could also have been obtained as a special case of the inequality 3.3(8). To prove (2), we write\n\\[ (f+g)^p=f \\cdot(f+g)^{p-1}+g \\cdot(f+g)^{p-1} . \\]\nHölder’s inequality gives\n\\[ \\int f \\cdot(f+g)^{p-1} \\leq\\left\\{\\int f^p\\right\\}^{1 / p}\\left\\{\\int(f+g)^{(p-1) q}\\right\\}^{1 / q} . \\]\nLet \\(\\left(9^{\\prime}\\right)\\) be the inequality (9) with \\(f\\) and \\(g\\) interchanged. Since \\((p-1) q=p\\), addition of \\((9)\\) and \\(\\left(9^{\\prime}\\right)\\) gives\n\\[ \\int(f+g)^p \\leq\\left\\{\\int(f+g)^p\\right\\}^{1 / q}\\left[\\left\\{\\int f^p\\right\\}^{1 / p}+\\left\\{\\int g^p\\right\\}^{1 / p}\\right] . \\]\nClearly, it is enough to prove (2) in the case that the left side is greater than 0 and the right side is less than \\(\\infty\\). The convexity of the function \\(t^p\\) for \\(0\u0026lt;t\u0026lt;\\infty\\) shows that\n\\[ \\left(\\frac{f+g}{2}\\right)^p \\leq \\frac{1}{2}\\left(f^p+g^p\\right) . \\]\nHence the left side of \\((2)\\) is less than \\(\\infty\\), and (2) follows from (10) if we divide by the first factor on the right of \\((10)\\), bearing in mind that \\(1-1 / q=1 / p\\). This completes the proof.\nIt is sometimes useful to know the conditions under which equality can hold in an inequality. In many cases this information may be obtained by examining the proof of the inequality.\nFor instance, equality holds in (7) if and only if equality holds in (6) for almost every \\(x\\). In (5), equality holds if and only if \\(s=t\\). Hence ” \\(F^p=G^q\\) a.e.” is a necessary and sufficient condition for equality in (7), if (4) is assumed. In terms of the original functions \\(f\\) and \\(g\\), the following result is then obtained:\nAssuming \\(A\u0026lt;\\infty\\) and \\(B\u0026lt;\\infty\\), equality holds in (1) if and only if there are constants \\(\\alpha\\) and \\(\\beta\\), not both 0 , such that \\(\\alpha f^p=\\beta g^q\\) a.e. We leave the analogous discussion of equality in (2) as an exercise.\n","date":"2022-10-28T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-3/1-convex-functons-and-inequalities/","section":"papa rudin","tags":null,"title":"1 Convex Functions and Inequalities"},{"categories":null,"contents":"In this section, \\(X\\) will be an arbitrary measure space with a positive measure \\(\\mu\\).\n3.6 Definition If \\(0\u0026lt;p\u0026lt;\\infty\\) and if \\(f\\) is a complex measurable function on \\(X\\), define\n\\[ \\|f\\|_p=\\left\\{\\int_X|f|^p d \\mu\\right\\}^{1 / p} \\]\nand let \\(L^p(\\mu)\\) consist of all \\(f\\) for which\n\\[ \\|f\\|_p\u0026lt;\\infty . \\]\nWe call \\(\\|f\\|_p\\) the \\(L^p\\)-norm of \\(f\\).\nIf \\(\\mu\\) is Lebesgue measure on \\(R^k\\), we write \\(L^p\\left(R^k\\right)\\) instead of \\(L^p(\\mu)\\), as in Sec. \\(2.21\\). If \\(\\mu\\) is the counting measure on a set \\(A\\), it is customary to denote the corresponding \\(L^p\\)-space by \\(\\ell^p(A)\\), or simply by \\(\\ell^p\\), if \\(A\\) is countable. An element of \\(\\ell^p\\) may be regarded as a complex sequence \\(x=\\left\\{\\xi_n\\right\\}\\), and\n\\[ \\|x\\|_p=\\left\\{\\sum_{n=1}^{\\infty}\\left|\\xi_n\\right|^p\\right\\}^{1 / p} . \\]\n3.7 Definition Suppose \\(g: X \\rightarrow[0, \\infty]\\) is measurable. Let \\(S\\) be the set of all real \\(\\alpha\\) such that\n\\[ \\mu\\left(g^{-1}((\\alpha, \\infty])\\right)=0 . \\]\nIf \\(S=\\varnothing\\), put \\(\\beta=\\infty\\). If \\(S \\neq \\varnothing\\), put \\(\\beta=\\inf S\\). Since\n\\[ g^{-1}((\\beta, \\infty])=\\bigcup_{n=1}^{\\infty} g^{-1}\\left(\\left(\\beta+\\frac{1}{n}, \\infty\\right]\\right), \\]\nand since the union of a countable collection of sets of measure 0 has measure 0 , we see that \\(\\beta \\in S\\). We call \\(\\beta\\) the essential supremum of \\(g\\).\nIf \\(f\\) is a complex measurable function on \\(X\\), we define \\(\\|f\\|_{\\infty}\\) to be the essential supremum of \\(|f|\\), and we let \\(L^{\\infty}(\\mu)\\) consist of all \\(f\\) for which \\(\\|f\\|_{\\infty}\u0026lt;\\infty\\). The members of \\(L^{\\infty}(\\mu)\\) are sometimes called essentially bounded measurable functions on \\(X\\).\nIt follows from this definition that the inequality \\(|f(x)| \\leq \\lambda\\) holds for almost all \\(x\\) if and only if \\(\\lambda \\geq\\|f\\|_{\\infty}\\).\nAs in Definition 3.6, \\(L^{\\infty}\\left(R^k\\right)\\) denotes the class of all essentially bounded (with respect to Lebesgue measure) functions on \\(R^k\\), and \\(\\ell^{\\infty}(A)\\) is the class of all bounded functions on \\(A\\). (Here bounded means the same as essentially bounded, since every nonempty set has positive measure!)\n3.8 Theorem If \\(p\\) and \\(q\\) are conjugate exponents, \\(1 \\leq p \\leq \\infty\\), and if \\(f \\in L^p(\\mu)\\) and \\(g \\in L^q(\\mu)\\), then \\(f g \\in L^1(\\mu)\\), and\n\\[ \\|f g\\|_1 \\leq\\|f\\|_p\\|g\\|_q . \\]\nProof For \\(1\u0026lt;p\u0026lt;\\infty,(1)\\) is simply Hölder’s inequality, applied to \\(|f|\\) and \\(|g|\\). If \\(p=\\infty\\), note that \\[ |f(x) g(x)| \\leq\\|f\\|_{\\infty}|g(x)| \\] for almost all \\(x\\); integrating (2), we obtain (1). If \\(p=1\\), then \\(q=\\infty\\), and the same argument applies.\n3.9 Theorem Suppose \\(1 \\leq p \\leq \\infty\\), and \\(f \\in L^p(\\mu), g \\in L^p(\\mu)\\). Then \\(f+g \\in L^p(\\mu)\\), and\n\\[ \\|f+g\\|_p \\leq\\|f\\|_p+\\|g\\|_p . \\]\nProof For \\(1\u0026lt;p\u0026lt;\\infty\\), this follows from Minkowski’s inequality, since\n\\[ \\int_X|f+g|^p d \\mu \\leq \\int_X(|f|+|g|)^p d \\mu . \\]\nFor \\(p=1\\) or \\(p=\\infty\\), (1) is a trivial consequence of the inequality \\(|f+g| \\leq|f|+|g|\\).\n3.10 Remarks Fix \\(p, 1 \\leq p \\leq \\infty\\). If \\(f \\in L^p(\\mu)\\) and \\(\\alpha\\) is a complex number, it is clear that \\(\\alpha f \\in L^p(\\mu)\\). In fact,\n\\[ \\|\\alpha f\\|_p=|\\alpha|\\|f\\|_p . \\]\nIn conjunction with Theorem \\(3.9\\), this shows that \\(L^p(\\mu)\\) is a complex vector space.\nSuppose \\(f, g\\), and \\(h\\) are in \\(L^p(\\mu)\\). Replacing \\(f\\) by \\(f-g\\) and \\(g\\) by \\(g-h\\) in Theorem 3.9, we obtain\n\\[ \\|f-h\\|_p \\leq\\|f-g\\|_p+\\|g-h\\|_p . \\]\nThis suggests that a metric may be introduced in \\(L^p(\\mu)\\) by defining the distance between \\(f\\) and \\(g\\) to be \\(\\|f-g\\|_p\\). Call this distance \\(d(f, g)\\) for the moment. Then \\(0 \\leq d(f, g)\u0026lt;\\infty, d(f, f)=0, d(f, g)=d(g, f)\\), and (2) shows that the triangle inequality \\(d(f, h) \\leq d(f, g)+d(g, h)\\) is satisfied. The only other property which \\(d\\) should have to define a metric space is that \\(d(f, g)=0\\) should imply that \\(f=g\\). In our present situation this need not be so; we have \\(d(f, g)=0\\) precisely when \\(f(x)=g(x)\\) for almost all \\(x\\).\nLet us write \\(f \\sim g\\) if and only if \\(d(f, g)=0\\). It is clear that this is an equivalence relation in \\(L^p(\\mu)\\) which partitions \\(L^p(\\mu)\\) into equivalence classes; each class consists of all functions which are equivalent to a given one. If \\(F\\) and \\(G\\) are two equivalence classes, choose \\(f \\in F\\) and \\(g \\in G\\), and define \\(d(F, G)=d(f, g)\\); note that \\(f \\sim f_1\\) and \\(g \\sim g_1\\) implies\n\\[ d(f, g)=d\\left(f_1, g_1\\right) \\]\nWith this definition, the set of equivalence classes is now a metric space. Note that it is also a vector space, since \\(f \\sim f_1\\) and \\(g \\sim g_1\\) implies \\(f+g \\sim\\) \\(f_1+g_1\\) and \\(\\alpha f \\sim \\alpha f_1\\).\nWhen \\(L^p(\\mu)\\) is regarded as a metric space, then the space which is really under consideration is therefore not a space whose elements are functions, but a space whose elements are equivalence classes of functions. For the sake of simplicity of language, it is, however, customary to relegate this distinction to the status of a tacit understanding and to continue to speak of \\(L^p(\\mu)\\) as a space of functions. We shall follow this custom.\nIf \\(\\left\\{f_n\\right\\}\\) is a sequence in \\(L^p(\\mu)\\), if \\(f \\in L^p(\\mu)\\), and if \\(\\lim _{n \\rightarrow \\infty}\\left\\|f_n-f\\right\\|_p=0\\), we say that \\(\\left\\{f_n\\right\\}\\) converges to \\(f\\) in \\(L^p(\\mu)\\) (or that \\(\\left\\{f_n\\right\\}\\) converges to \\(f\\) in the mean of order \\(p\\), or that \\(\\left\\{f_n\\right\\}\\) is \\(L^p\\)-convergent to \\(f\\) ). If to every \\(\\epsilon\u0026gt;0\\) there corresponds an integer \\(N\\) such that \\(\\left\\|f_n-f_m\\right\\|_p\u0026lt;\\epsilon\\) as soon as \\(n\u0026gt;N\\) and \\(m\u0026gt;N\\), we call \\(\\left\\{f_n\\right\\}\\) a Cauchy sequence in \\(L^p(\\mu)\\). These definitions are exactly as in any metric space.\nIt is a very important fact that \\(L^p(\\mu)\\) is a complete metric space, i.e., that every Cauchy sequence in \\(L^p(\\mu)\\) converges to an element of \\(L^p(\\mu)\\) :\n3.11 Theorem \\(L^p(\\mu)\\) is a complete metric space, for \\(1 \\leq p \\leq \\infty\\) and for every positive measure \\(\\mu\\).\nProof Assume first that \\(1 \\leq p\u0026lt;\\infty\\). Let \\(\\left\\{f_n\\right\\}\\) be a Cauchy sequence in \\(L^p(\\mu)\\). There is a subsequence \\(\\left\\{f_{n_i}\\right\\}, n_1\u0026lt;n_2\u0026lt;\\cdots\\), such that\n\\[ \\left\\|f_{n_{i+1}}-f_{n_i}\\right\\|_p\u0026lt;2^{-i} \\quad(i=1,2,3, \\ldots) \\]\nPut\n\\[ g_k=\\sum_{i=1}^k\\left|f_{n_{i+1}}-f_{n_i}\\right|, \\quad g=\\sum_{i=1}^{\\infty}\\left|f_{n_{i+1}}-f_{n_i}\\right| . \\]\nSince (1) holds, the Minkowski inequality shows that \\(\\left\\|g_k\\right\\|_p\u0026lt;1\\) for \\(k=1\\), \\(2,3, \\ldots\\). Hence an application of Fatou’s lemma to \\(\\left\\{g_k^p\\right\\}\\) gives \\(\\|g\\|_p \\leq 1\\). In particular, \\(g(x)\u0026lt;\\infty\\) a.e., so that the series\n\\[ f_{n_1}(x)+\\sum_{i=1}^{\\infty}\\left(f_{n_{i+1}}(x)-f_{n_i}(x)\\right) \\]\nconverges absolutely for almost every \\(x \\in X\\). Denote the sum of (3) by \\(f(x)\\), for those \\(x\\) at which (3) converges; put \\(f(x)=0\\) on the remaining set of measure zero. Since\n\\[ f_{n_1}+\\sum_{i=1}^{k-1}\\left(f_{n_{i+1}}-f_{n_i}\\right)=f_{n_k}, \\]\nwe see that\n\\[ f(x)=\\lim _{i \\rightarrow \\infty} f_{n_i}(x) \\quad \\text { a.e. } \\]\nHaving found a function \\(f\\) which is the pointwise limit a.e. of \\(\\left\\{f_{n i}\\right\\}\\), we now have to prove that this \\(f\\) is the \\(L^p\\)-limit of \\(\\left\\{f_n\\right\\}\\). Choose \\(\\epsilon\u0026gt;0\\). There exists an \\(N\\) such that \\(\\left\\|f_n-f_m\\right\\|_p\u0026lt;\\epsilon\\) if \\(n\u0026gt;N\\) and \\(m\u0026gt;N\\). For every \\(m\u0026gt;N\\), Fatou’s lemma shows therefore that\n\\[ \\int_X\\left|f-f_m\\right|^p d \\mu \\leq \\liminf _{i \\rightarrow \\infty} \\int_X\\left|f_{n_i}-f_m\\right|^p d \\mu \\leq \\epsilon^p . \\]\nWe conclude from (6) that \\(f-f_m \\in L^p(\\mu)\\), hence that \\(f \\in L^p(\\mu)\\) [since \\(f=\\) \\(\\left.\\left(f-f_m\\right)+f_m\\right]\\), and finally that \\(\\left\\|f-f_m\\right\\|_p \\rightarrow 0\\) as \\(m \\rightarrow \\infty\\). This completes the proof for the case \\(1 \\leq p\u0026lt;\\infty\\).\nIn \\(L^{\\infty}(\\mu)\\) the proof is much easier. Suppose \\(\\left\\{f_n\\right\\}\\) is a Cauchy sequence in \\(L^{\\infty}(\\mu)\\), let \\(A_k\\) and \\(B_{m, n}\\) be the sets where \\(\\left|f_k(x)\\right|\u0026gt;\\left\\|f_k\\right\\|_{\\infty}\\) and where \\(\\left|f_n(x)-f_m(x)\\right|\u0026gt;\\left\\|f_n-f_m\\right\\|_{\\infty}\\), and let \\(E\\) be the union of these sets, for \\(k, m\\), \\(n=1,2,3, \\ldots\\). Then \\(\\mu(E)=0\\), and on the complement of \\(E\\) the sequence \\(\\left\\{f_n\\right\\}\\) converges uniformly to a bounded function \\(f\\). Define \\(f(x)=0\\) for \\(x \\in E\\). Then \\(f \\in L^{\\infty}(\\mu)\\), and \\(\\left\\|f_n-f\\right\\|_{\\infty} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\).\nThe preceding proof contains a result which is interesting enough to be stated separately:\n3.12 Theorem If \\(1 \\leq p \\leq \\infty\\) and if \\(\\left\\{f_n\\right\\}\\) is a Cauchy sequence in \\(L^p(\\mu)\\), with limit \\(f\\), then \\(\\left\\{f_n\\right\\}\\) has a subsequence which converges pointwise almost everywhere to \\(f(x)\\).\nThe simple functions play an interesting role in \\(L^p(\\mu)\\) :\n3.13 Theorem Let \\(S\\) be the class of all complex, measurable, simple functions on \\(X\\) such that \\[ \\mu(\\{x: s(x) \\neq 0\\})\u0026lt;\\infty . \\] If \\(1 \\leq p\u0026lt;\\infty\\), then \\(S\\) is dense in \\(L^p(\\mu)\\).\nProof First, it is clear that \\(S \\subset L^p(\\mu)\\). Suppose \\(f \\geq 0, f \\in L^p(\\mu)\\), and let \\(\\left\\{s_n\\right\\}\\) be as in Theorem 1.17. Since \\(0 \\leq s_n \\leq f\\), we have \\(s_n \\in L^p(\\mu)\\), hence \\(s_n \\in S\\). Since \\(\\left|f-s_n\\right|^p \\leq f^p\\), the dominated convergence theorem shows that \\(\\left\\|f-s_n\\right\\|_p \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). Thus \\(f\\) is in the \\(L^p\\)-closure of \\(S\\). The general case \\((f\\) complex) follows from this.\n","date":"2022-10-28T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-3/2-the-lp-sapce/","section":"papa rudin","tags":null,"title":"2 The $L^p$-spaces"},{"categories":null,"contents":"So far we have considered \\(L^p(\\mu)\\) on any measure space. Now let \\(X\\) be a locally compact Hausdorff space, and let \\(\\mu\\) be a measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in \\(X\\), with the properties stated in Theorem 2.14. For example, \\(X\\) might be \\(R^k\\), and \\(\\mu\\) might be Lebesgue measure on \\(R^k\\).\nUnder these circumstances, we have the following analogue of Theorem 3.13:\n3.14 Theorem For \\(1 \\leq p\u0026lt;\\infty, C_c(X)\\) is dense in \\(L^p(\\mu)\\).\nProof Define \\(S\\) as in Theorem 3.13. If \\(s \\in S\\) and \\(\\epsilon\u0026gt;0\\), there exists a \\(g \\epsilon\\) \\(C_c(X)\\) such that \\(g(x)=s(x)\\) except on a set of measure \\(\u0026lt;\\epsilon\\), and \\(|g| \\leq\\|s\\|_{\\infty}\\) (Lusin’s theorem). Hence\n\\[ \\|g-s\\|_p \\leq 2 \\epsilon^{1 / p}\\|s\\|_{\\infty} . \\]\nSince \\(S\\) is dense in \\(L^p(\\mu)\\), this completes the proof.\n3.15 Remarks Let us discuss the relations between the spaces \\(L^p\\left(R^k\\right)\\) (the \\(L^p-\\) spaces in which the underlying measure is Lebesgue measure on \\(R^k\\) ) and the space \\(C_c\\left(R^k\\right)\\) in some detail. We consider a fixed dimension \\(k\\).\nFor every \\(p \\in[1, \\infty]\\) we have a metric on \\(C_c\\left(R^k\\right)\\); the distance between \\(f\\) and \\(g\\) is \\(\\|f-g\\|_p\\). Note that this is a genuine metric, and that we do not have to pass to equivalence classes. The point is that if two continuous functions on \\(R^k\\) are not identical, then they differ on some nonempty open set \\(V\\), and \\(m(V)\u0026gt;0\\), since \\(V\\) contains a \\(k\\)-cell. Thus if two members of \\(C_c\\left(R^k\\right)\\) are equal a.e., they are equal. It is also of interest to note that in \\(C_c\\left(R^k\\right)\\) the essential supremum is the same as the actual supremum: for \\(f \\in C_c\\left(R^k\\right)\\)\n\\[ \\|f\\|_{\\infty}=\\sup _{x \\in R^k}|f(x)| . \\]\nIf \\(1 \\leq p\u0026lt;\\infty\\), Theorem \\(3.14\\) says that \\(C_c\\left(R^k\\right)\\) is dense in \\(L^p\\left(R^k\\right)\\), and Theorem \\(3.11\\) shows that \\(L^p\\left(R^k\\right)\\) is complete. Thus \\(L^p\\left(R^k\\right)\\) is the completion of the metric space which is obtained by endowing \\(C_c\\left(R^k\\right)\\) with the \\(L^p\\)-metric.\nThe cases \\(p=1\\) and \\(p=2\\) are the ones of greatest interest. Let us state once more, in different words, what the preceding result says if \\(p=1\\) and \\(k=1\\); the statement shows that the Lebesgue integral is indeed the “right” generalization of the Riemann integral:\nIf the distance between two continuous functions \\(f\\) and \\(g\\), with compact supports in \\(R^1\\), is defined to be\n\\[ \\int_{-\\infty}^{\\infty}|f(t)-g(t)| d t \\]\nthe completion of the resulting metric space consists precisely of the Lebesgue integrable functions on \\(R^1\\), provided we identify any two that are equal almost everywhere.\nIf \\(1 \\leq p\u0026lt;\\infty\\), Theorem \\(3.14\\) says that \\(C_c\\left(R^k\\right)\\) is dense in \\(L^p\\left(R^k\\right)\\), and Theorem \\(3.11\\) shows that \\(L^p\\left(R^k\\right)\\) is complete. Thus \\(L^p\\left(R^k\\right)\\) is the completion of the metric space which is obtained by endowing \\(C_c\\left(R^k\\right)\\) with the \\(L^p\\)-metric.\nThe cases \\(p=1\\) and \\(p=2\\) are the ones of greatest interest. Let us state once more, in different words, what the preceding result says if \\(p=1\\) and \\(k=1\\); the statement shows that the Lebesgue integral is indeed the “right” generalization of the Riemann integral:\nIf the distance between two continuous functions \\(f\\) and \\(g\\), with compact supports in \\(R^1\\), is defined to be\n\\[ \\int_{-\\infty}^{\\infty}|f(t)-g(t)| d t \\]\nthe completion of the resulting metric space consists precisely of the Lebesgue integrable functions on \\(R^1\\), provided we identify any two that are equal almost everywhere.\n3.16 Definition A complex function \\(f\\) on a locally compact Hausdorff space \\(X\\) is said to vanish at infinity if to every \\(\\epsilon\u0026gt;0\\) there exists a compact set \\(K \\subset X\\) such that \\(|f(x)|\u0026lt;\\epsilon\\) for all \\(x\\) not in \\(K\\).\nThe class of all continuous \\(f\\) on \\(X\\) which vanish at infinity is called \\(C_0(X)\\)\nIt is clear that \\(C_c(X) \\subset C_0(X)\\), and that the two classes coincide if \\(X\\) is compact. In that case we write \\(C(X)\\) for either of them.\n3.17 Theorem If \\(X\\) is a locally compact Hausdorff space, then \\(C_0(X)\\) is the completion of \\(C_c(X)\\), relative to the metric defined by the supremum norm\n\\[ \\|f\\|=\\sup _{x \\in X}|f(x)| . \\]\nProof An elementary verification shows that \\(C_0(X)\\) satisfies the axioms of a metric space if the distance between \\(f\\) and \\(g\\) is taken to be \\(\\|f-g\\|\\). We have to show that \\((a) C_c(X)\\) is dense in \\(C_0(X)\\) and \\((b) C_0(X)\\) is a complete metric space.\nGiven \\(f \\in C_0(X)\\) and \\(\\epsilon\u0026gt;0\\), there is a compact set \\(K\\) so that \\(|f(x)|\u0026lt;\\epsilon\\) outside \\(K\\). Urysohn’s lemma gives us a function \\(g \\in C_c(X)\\) such that \\(0 \\leq g \\leq 1\\) and \\(g(x)=1\\) on \\(K\\). Put \\(h=f g\\). Then \\(h \\in C_c(X)\\) and \\(\\|f-h\\|\u0026lt;\\epsilon\\). This proves \\((a)\\)\nTo prove \\((b)\\), let \\(\\left\\{f_n\\right\\}\\) be a Cauchy sequence in \\(C_0(X)\\), i.e., assume that \\(\\left\\{f_n\\right\\}\\) converges uniformly. Then its pointwise limit function \\(f\\) is continuous. Given \\(\\epsilon\u0026gt;0\\), there exists an \\(n\\) so that \\(\\left\\|f_n-f\\right\\|\u0026lt;\\epsilon / 2\\) and there is a compact set \\(K\\) so that \\(\\left|f_n(x)\\right|\u0026lt;\\epsilon / 2\\) outside \\(K\\). Hence \\(|f(x)|\u0026lt;\\epsilon\\) outside \\(K\\), and we have proved that \\(f\\) vanishes at infinity. Thus \\(C_0(X)\\) is complete.\n","date":"2022-10-28T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-3/3-approximation-by-continuous-functions/","section":"papa rudin","tags":null,"title":"3 Approximation by Continuous Functions"},{"categories":null,"contents":"Since the continuous functions played such a prominent role in our construction of Borel measures, and of Lebesgue measure in particular, it seems reasonable to expect that there are some interesting relations between continuous functions and measurable functions. In this section we shall give two theorems of this kind.\nWe shall assume, in both of them, that \\(\\mu\\) is a measure on a locally compact Hausdorff space \\(X\\) which has the properties stated in Theorem 2.14. In particular, \\(\\mu\\) could be Lebesgue measure on some \\(R^k\\).\n2.24 Lusin’s Theorem Suppose \\(f\\) is a complex measurable function on \\(X\\), \\(\\mu(A)\u0026lt;\\infty, f(x)=0\\) if \\(x \\notin A\\), and \\(\\epsilon\u0026gt;0\\). Then there exists a \\(g \\in C_c(X)\\) such that\n\\[ \\mu(\\{x: f(x) \\neq g(x)\\})\u0026lt;\\epsilon . \\]\nFurthermore, we may arrange it so that\n\\[ \\sup _{x \\in X}|g(x)| \\leq \\sup _{x \\in X}|f(x)| . \\]\nProof Assume first that \\(0 \\leq f\u0026lt;1\\) and that \\(A\\) is compact. Attach a sequence \\(\\left\\{s_n\\right\\}\\) to \\(f\\), as in the proof of Theorem 1.17, and put \\(t_1=s_1\\) and \\(t_n=s_n-s_{n-1}\\) for \\(n=2,3,4, \\ldots\\). Then \\(2^n t_n\\) is the characteristic function of a set \\(T_n \\subset A\\), and\n\\[ f(x)=\\sum_{n=1}^{\\infty} t_n(x) \\quad(x \\in X) . \\]\nFix an open set \\(V\\) such that \\(A \\subset V\\) and \\(\\bar{V}\\) is compact. There are compact sets \\(K_n\\) and open sets \\(V_n\\) such that \\(K_n \\subset T_n \\subset V_n \\subset V\\) and \\(\\mu\\left(V_n-K_n\\right)\u0026lt;2^{-n} \\epsilon\\). By Urysohn’s lemma, there are functions \\(h_n\\) such that \\(K_n \\prec h_n \\prec V_n\\). Define\n\\[ g(x)=\\sum_{n=1}^{\\infty} 2^{-n} h_n(x) \\quad(x \\in X) . \\]\nThis series converges uniformly on \\(X\\), so \\(g\\) is continuous. Also, the support of \\(g\\) lies in \\(\\bar{V}\\). Since \\(2^{-n} h_n(x)=t_n(x)\\) except in \\(V_n-K_n\\), we have \\(g(x)=f(x)\\) except in \\(\\bigcup\\left(V_n-K_n\\right)\\), and this latter set has measure less than \\(\\epsilon\\). Thus (1) holds if \\(A\\) is compact and \\(0 \\leq f \\leq 1\\).\nIt follows that (1) holds if \\(A\\) is compact and \\(f\\) is a bounded measurable function. The compactness of \\(A\\) is easily removed, for if \\(\\mu(A)\u0026lt;\\infty\\) then \\(A\\) contains a compact set \\(K\\) with \\(\\mu(A-K)\\) smaller than any preassigned positive number. Next, if \\(f\\) is a complex measurable function and if \\(B_n=\\) \\(\\{x:|f(x)|\u0026gt;n\\}\\), then \\(\\bigcap B_n=\\varnothing\\), so \\(\\mu\\left(B_n\\right) \\rightarrow 0\\), by Theorem 1.19(e). Since \\(f\\) coincides with the bounded function \\(\\left(1-\\chi_{B_n}\\right) \\cdot f\\) except on \\(B_n,(1)\\) follows in the general case.\nFinally, let \\(R=\\sup \\{|f(x)|: x \\in X\\}\\), and define \\(\\varphi(z)=z\\) if \\(|z| \\leq R\\), \\(\\varphi(z)=R z /|z|\\) if \\(|z|\u0026gt;R\\). Then \\(\\varphi\\) is a continuous mapping of the complex plane onto the disc of radius \\(R\\). If \\(g\\) satisfies (1) and \\(g_1=\\varphi \\circ g\\), then \\(g_1\\) satisfies (1) and (2).\nCorollary Assume that the hypotheses of Lusin’s theorem are satisfied and that \\(|f| \\leq 1\\). Then there is a sequence \\(\\left\\{g_n\\right\\}\\) such that \\(g_n \\in C_c(X),\\left|g_n\\right| \\leq 1\\), and\n\\[ f(x)=\\lim _{n \\rightarrow \\infty} g_n(x) \\quad \\text { a.e. } \\]\nProof The theorem implies that to each \\(n\\) there corresponds a \\(g_n \\in C_c(X)\\), with \\(\\left|g_n\\right| \\leq 1\\), such that \\(\\mu\\left(E_n\\right) \\leq 2^{-n}\\), where \\(E_n\\) is the set of all \\(x\\) at which \\(f(x) \\neq g_n(x)\\). For almost every \\(x\\) it is then true that \\(x\\) lies in at most finitely many of the sets \\(E_n\\) (Theorem 1.41). For any such \\(x\\), it follows that \\(f(x)=\\) \\(g_n(x)\\) for all large enough \\(n\\). This gives (5).\n2.25 The Vitali-Carathéodory Theorem Suppose \\(f \\in L^1(\\mu), f\\) is real-valued, and \\(\\epsilon\u0026gt;0\\). Then there exist functions \\(u\\) and \\(v\\) on \\(X\\) such that \\(u \\leq f \\leq v, u\\) is upper semicontinuous and bounded above, \\(v\\) is lower semicontinuous and bounded below, and\n\\[ \\int_X(v-u) d \\mu\u0026lt;\\epsilon \\]\nProof Assume first that \\(f \\geq 0\\) and that \\(f\\) is not identically 0 . Since \\(f\\) is the pointwise limit of an increasing sequence of simple functions \\(s_n, f\\) is the sum of the simple functions \\(t_n=s_n-s_{n-1}\\) (taking \\(s_0=0\\) ), and since \\(t_n\\) is a linear combination of characteristic functions, we see that there are measurable sets \\(E_i\\) (not necessarily disjoint) and constants \\(c_i\u0026gt;0\\) such that\n\\[ f(x)=\\sum_{i=1}^{\\infty} c_i \\chi_{E_i}(x) \\quad(x \\in X) \\]\nSince\n\\[ \\int_X f d \\mu=\\sum_{i=1}^{\\infty} c_i \\mu\\left(E_i\\right), \\]\nthe series in (3) converges. There are compact sets \\(K_i\\) and open sets \\(V_i\\) such that \\(K_i \\subset E_i \\subset V_i\\) and\n\\[ c_i \\mu\\left(V_i-K_i\\right)\u0026lt;2^{-i-1} \\epsilon \\quad(i=1,2,3, \\ldots) . \\]\nPut\n\\[ v=\\sum_{i=1}^{\\infty} c_i \\chi_{V_i}, \\quad u=\\sum_{i=1}^N c_i \\chi_{K_i}, \\]\nwhere \\(N\\) is chosen so that\n\\[ \\sum_{N+1}^{\\infty} c_i \\mu\\left(E_i\\right)\u0026lt;\\frac{\\epsilon}{2} . \\]\nThen \\(v\\) is lower semicontinuous, \\(u\\) is upper semicontinuous, \\(u \\leq f \\leq v\\), and\n\\[ \\begin{aligned} v-u \u0026amp;=\\sum_{i=1}^N c_i\\left(\\chi_{V_i}-\\chi_{K_i}\\right)+\\sum_{N+1}^{\\infty} c_i \\chi_{V_i} \\\\ \u0026amp; \\leq \\sum_{i=1}^{\\infty} c_i\\left(\\chi_{V_i}-\\chi_{K_i}\\right)+\\sum_{N+1}^{\\infty} c_i \\chi_{E_i} \\end{aligned} \\]\nso that (4) and (6) imply (1).\nIn the general case, write \\(f=f^{+}-f^{-}\\), attach \\(u_1\\) and \\(v_1\\) to \\(f^{+}\\), attach \\(u_2\\) and \\(v_2\\) to \\(f^{-}\\), as above, and put \\(u=u_1-v_2, v=v_1-u_2\\). Since \\(-v_2\\) is upper semicontinuous and since the sum of two upper semicontinuous functions is upper semicontinuous (similarly for lower semicontinuous; we leave the proof of this as an exercise), \\(u\\) and \\(v\\) have the desired properties.\n","date":"2022-10-27T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/5-continuity-properties-of-measurable-functions/","section":"papa rudin","tags":null,"title":"5 Continuity Properties of Measurable Functions"},{"categories":null,"contents":"2.19 Euclidean Spaces Euclidean \\(k\\)-dimensional space \\(R^k\\) is the set of all points \\(x=(ξ_1, \\ldots, ξ_k)\\) whose coordinates \\(ξ_i\\) are real numbers, with the following algebraic and topological structure:\nIf \\(x=\\left( ξ_1, \\ldots, ξ_k\\right), y=\\left( \\eta_1, \\ldots, \\eta_k\\right)\\), and \\(\\alpha\\) is a real number, \\(x+y\\) and \\(\\alpha x\\) are defined by \\[ x+y=\\left(ξ_1+\\eta_1, \\ldots, ξ_k+\\eta_k\\right), \\quad \\alpha x=\\left(\\alpha ξ_1, \\ldots, \\alpha ξ_k\\right) . \\] This makes \\(R^k\\) into a real vector space. If \\(x \\cdot y=\\sum ξ_i \\eta_i\\) and \\(|x|=(x \\cdot x)^{1 / 2}\\), the Schwarz inequality \\(|x \\cdot y| \\leq|x||y|\\) leads to the triangle inequality \\[ |x-y| \\leq|x-z|+|z-y| ; \\] hence we obtain a metric by setting \\(\\rho(x, y)=|x-y|\\). We assume that these facts are familiar and shall prove them in greater generality in Chap. 4.\nIf \\(E \\subset R^k\\) and \\(x \\in R^k\\), the translate of \\(E\\) by \\(x\\) is the set \\[ E+x=\\{y+x: y \\in E\\} \\] A set of the form \\[ W=\\left\\{x: \\alpha_i\u0026lt;\\xi_i\u0026lt;\\beta_i, 1 \\leq i \\leq k\\right\\}, \\] or any set obtained by replacing any or all of the \\(\u0026lt;\\) signs in (4) by \\(\\leq\\), is called a \\(k\\)-cell; its volume is defined to be \\[ \\operatorname{vol}(W)=\\prod_{i=1}^k\\left(\\beta_i-\\alpha_i\\right) \\] If \\(a \\in R^k\\) and \\(\\delta\u0026gt;0\\), we shall call the set \\[ Q(a ; \\delta)=\\left\\{x: \\alpha_i \\leq \\xi_i\u0026lt;\\alpha_i+\\delta, 1 \\leq i \\leq k\\right\\} \\] the \\(\\delta\\)-box with corner at \\(a\\). Here \\(a=\\left(\\alpha_1, \\ldots, \\alpha_k\\right)\\).\nFor \\(n=1,2,3, \\ldots\\), we let \\(P_n\\) be the set of all \\(x \\in R^k\\) whose coordinates are integer multiples of \\(2^{-n}\\), and we let \\(\\Omega_n\\) be the collection of all \\(2^{-n}\\) boxes with corners at points of \\(P_n\\). We shall need the following four properties of \\(\\left\\{\\Omega_n\\right\\}\\). The first three are obvious by inspection.\nIf \\(n\\) is fixed, each \\(x \\in R^k\\) lies in one and only one member of \\(\\Omega_n\\). If \\(Q^{\\prime} \\in \\Omega_n, Q^{\\prime \\prime} \\in \\Omega_r\\), and \\(r\u0026lt;n\\), then either \\(Q^{\\prime} \\subset Q^{\\prime \\prime}\\) or \\(Q^{\\prime} \\cap Q^{\\prime \\prime}=\\varnothing\\). If \\(Q \\in \\Omega_r\\), then \\(\\mathrm{vol}(Q)=2^{-r k}\\); and if \\(n\u0026gt;r\\), the set \\(P_n\\) has exactly \\(2^{(n-r) k}\\) points in \\(Q\\). Every nonempty open set in \\(R^k\\) is a countable union of disjoint boxes belonging to \\(\\Omega_1 \\cup \\Omega_2 \\cup \\Omega_3 \\cup \\cdots\\). Proof of \\((d)\\) If \\(V\\) is open, every \\(x \\in V\\) lies in an open ball which lies in \\(V\\); hence \\(x \\in Q \\subset V\\) for some \\(Q\\) belonging to some \\(\\Omega_n\\). In other words, \\(V\\) is the union of all boxes which lie in \\(V\\) and which belong to some \\(\\Omega_n\\). From this collection of boxes, select those which belong to \\(\\Omega_1\\), and remove those in \\(\\Omega_2\\), \\(\\Omega_3, \\ldots\\) which lie in any of the selected boxes. From the remaining collection, select those boxes of \\(\\Omega_2\\) which lie in \\(V\\), and remove those in \\(\\Omega_3, \\Omega_4, \\ldots\\) which lie in any of the selected boxes. If we proceed in this way, \\((a)\\) and (b) show that \\((d)\\) holds.\n2.20 Theorem There exists a positive complete measure \\(m\\) defined on a \\(\\sigma\\) algebra \\(\\mathfrak{M}\\) in \\(R^k\\), with the following properties:\n\\(m(W)=\\operatorname{vol}(W)\\) for every \\(k\\)-cell \\(W\\). \\(\\mathfrak{M}\\) contains all Borel sets in \\(R^k\\); more precisely, \\(E \\in \\mathfrak{M}\\) if and only if there are sets \\(A\\) and \\(B \\subset R^k\\) such that \\(A \\subset E \\subset B, A\\) is an \\(F_\\sigma, B\\) is a \\(G_\\delta\\), and \\(m(B-A)=0\\). Also, \\(m\\) is regular. \\(m\\) is translation-invariant, i.e., \\[ m(E+x)=m(E) \\]\nfor every \\(E \\in \\mathfrak{M}\\) and every \\(x \\in R^k\\).\nIf \\(\\mu\\) is any positive translation-invariant Borel measure on \\(R^k\\) such that \\(\\mu(K)\u0026lt;\\infty\\) for every compact set \\(K\\), then there is a constant \\(c\\) such that \\(\\mu(E)=c m(E)\\) for all Borel sets \\(E \\subset R^k\\). To every linear transformation \\(T\\) of \\(R^k\\) into \\(R^k\\) corresponds a real number \\(\\Delta(T)\\) such that \\[ m(T(E))=\\Delta(T) m(E) \\] for every \\(E \\in \\mathfrak{M}\\). In particular, \\(m(T(E))=m(E)\\) when \\(T\\) is a rotation. The members of \\(\\mathfrak{M}\\) are the Lebesgue measurable sets in \\(R^k ; m\\) is the Lebesgue measure on \\(R^k\\). When clarity requires it, we shall write \\(m_k\\) in place of \\(m\\).\nProof If \\(f\\) is any complex function on \\(R^k\\), with compact support, define\n\\[ \\begin{equation} \\Lambda_n f=2^{-n k} \\sum_{x \\in P_n} f(x) \\quad(n=1,2,3, \\ldots), \\end{equation} \\]\nwhere \\(P_n\\) is as in Sec. \\(2.19\\).\nNow suppose \\(f \\in C_c\\left(R^k\\right), f\\) is real, \\(W\\) is an open \\(k\\)-cell which contains the support of \\(f\\), and \\(\\epsilon\u0026gt;0\\). The uniform continuity of \\(f([26]\\), Theorem 4.19) shows that there is an integer \\(N\\) and that there are functions \\(g\\) and \\(h\\) with support in \\(W\\), such that (i) \\(g\\) and \\(h\\) are constant on each box belonging to \\(\\Omega_N\\), (ii) \\(g \\leq f \\leq h\\), and (iii) \\(h-g\u0026lt;\\epsilon\\). If \\(n\u0026gt;N\\), Property \\(2.19(c)\\) shows that\n\\[ \\begin{equation} \\Lambda_N g=\\Lambda_n g \\leq \\Lambda_n f \\leq \\Lambda_n h=\\Lambda_N h . \\end{equation} \\]\nThus the upper and lower limits of \\(\\left\\{\\Lambda_n f\\right\\}\\) differ by at most \\(\\epsilon\\) vol \\((W)\\), and since \\(\\epsilon\\) was arbitrary, we have proved the existence of\n\\[ \\begin{equation} \\Lambda f=\\lim _{n \\rightarrow \\infty} \\Lambda_n f \\quad\\left(f \\in C_c\\left(R^k\\right)\\right) . \\end{equation} \\]\nIt is immediate that \\(\\Lambda\\) is a positive linear functional on \\(C_c\\left(R^k\\right)\\). (In fact, \\(\\Lambda f\\) is precisely the Riemann integral of \\(f\\) over \\(R^k\\). We went through the preceding construction in order not to have to rely on any theorems about Riemann integrals in several variables.)\n\\(W e\\) define \\(m\\) and \\(\\mathfrak{M}\\) to be the measure and \\(\\sigma\\)-algebra associated with this \\(\\Lambda\\) as in Theorem \\(2.14\\).\nSince Theorem \\(2.14\\) gives us a complete measure and since \\(R^k\\) is \\(\\sigma\\) compact, Theorem \\(2.17\\) implies assertion \\((b)\\) of Theorem \\(2.20\\).\nTo prove \\((a)\\), let \\(W\\) be the open cell \\(2.19(4)\\), let \\(E_r\\) be the union of those boxes belonging to \\(\\Omega_r\\) whose closures lie in \\(W\\), choose \\(f_r\\) so that \\(\\bar{E}_r \\prec\\) \\(f_r \\prec W\\), and put \\(g_r=\\max \\left\\{f_1, \\ldots, f_r\\right\\}\\). Our construction of \\(\\Lambda\\) shows that\n\\[ \\begin{equation} \\operatorname{vol}\\left(E_r\\right) \\leq \\Lambda f_r \\leq \\Lambda g_r \\leq \\operatorname{vol} W . \\end{equation} \\]\nAs \\(r \\rightarrow \\infty, \\operatorname{vol}\\left(E_r\\right) \\rightarrow \\operatorname{vol}(W)\\), and\n\\[ \\begin{equation} \\Lambda g_r=\\int g_r d m \\rightarrow m(W) \\end{equation} \\]\nby the monotone convergence theorem, since \\(g_r(x) \\rightarrow \\chi_W(x)\\) for all \\(x \\in R^k\\). Thus \\(m(W)=\\operatorname{vol}(W)\\) for every open cell \\(W\\), and since every \\(k\\)-cell is the intersection of a decreasing sequence of open \\(k\\)-cells, we obtain \\((a)\\).\nThe proofs of \\((c),(d)\\), and \\((e)\\) will use the following observation: If \\(\\lambda\\) is a positive Borel measure on \\(R^k\\) and \\(\\lambda(E)=m(E)\\) for all boxes \\(E\\), then the same equality holds for all open sets \\(E\\), by property \\(2.19(d)\\), and therefore for all Borel sets \\(E\\), since \\(\\lambda\\) and \\(m\\) are regular (Theorem 2.18).\nTo prove \\((c)\\), fix \\(x \\in R^k\\) and define \\(\\lambda(E)=m(E+x)\\). It is clear that \\(\\lambda\\) is then a measure; by \\((a), \\lambda(E)=m(E)\\) for all boxes, hence \\(m(E+x)=m(E)\\) for all Borel sets \\(E\\). The same equality holds for every \\(E \\in \\mathfrak{M}\\), because of \\((b)\\).\nSuppose next that \\(\\mu\\) satisfies the hypotheses of \\((d)\\). Let \\(Q_0\\) be a 1-box, put \\(c=\\mu\\left(Q_0\\right)\\). Since \\(Q_0\\) is the union of \\(2^{n k}\\) disjoint \\(2^{-n}\\) boxes that are translates of each other, we have\n\\[ 2^{n k} \\mu(Q)=\\mu\\left(Q_0\\right)=c m\\left(Q_0\\right)=c \\cdot 2^{n k} m(Q) \\]\nfor every \\(2^{-n}\\)-box \\(Q\\). Property \\(2.19(d)\\) implies now that \\(\\mu(E)=c m(E)\\) for all open sets \\(E \\subset R^k\\). This proves \\((d)\\).\nTo prove (e), let \\(T: R^k \\rightarrow R^k\\) be linear. If the range of \\(T\\) is a subspace \\(Y\\) of lower dimension, then \\(m(Y)=0\\) and the desired conclusion holds with \\(\\Delta(T)=0\\). In the other case, elementary linear algebra tells us that \\(T\\) is a one-to-one map of \\(R^k\\) onto \\(R^k\\) whose inverse is also linear. Thus \\(T\\) is a homeomorphism of \\(R^k\\) onto \\(R^k\\), so that \\(T(E)\\) is a Borel set for every Borel set \\(E\\), and we can therefore define a positive Borel measure \\(\\mu\\) on \\(R^k\\) by\n\\[ \\begin{equation} \\mu(E)=m(T(E)) . \\end{equation} \\]\nThe linearity of \\(T\\), combined with the translation-invariance of \\(m\\), gives\n\\[ \\begin{equation} \\mu(E+x)=m(T(E+x))=m(T(E)+T x)=m(T(E))=\\mu(E) . \\end{equation} \\]\nThus \\(\\mu\\) is translation-invariant, and the first assertion of \\((e)\\) follows from (d), first for Borel sets \\(E\\), then for all \\(E \\in \\mathfrak{M}\\) by \\((b)\\).\nTo find \\(\\Delta(T)\\), we merely need to know \\(m(T(E)) / m(E)\\) for one set \\(E\\) with \\(0\u0026lt;m(E)\u0026lt;\\infty\\). If \\(T\\) is a rotation, let \\(E\\) be the unit ball of \\(R^k\\); then \\(T(E)=E\\), and \\(\\Delta(T)=1\\).\n2.21 Remarks If \\(m\\) is the Lebesgue measure on \\(R^k\\), it is customary to write \\(L^1\\left(R^k\\right)\\) in place of \\(L^1(m)\\). If \\(E\\) is a Lebesgue measurable subset of \\(R^k\\), and if \\(m\\) is restricted to the measurable subsets of \\(E\\), a new measure space is obtained in an obvious fashion. The phrase ” \\(f \\in L^1\\) on \\(E\\) ” or ” \\(f \\in L^1(E)\\) ” is used to indicate that \\(f\\) is integrable on this measure space.\nIf \\(k=1\\), if \\(I\\) is any of the sets \\((a, b),(a, b],[a, b),[a, b]\\), and if \\(f \\in L^1(I)\\), it is customary to write\n\\[ \\int_a^b f(x) d x \\text { in place of } \\int_I f d m \\text {. } \\]\nSince the Lebesgue measure of any single point is 0 , it makes no difference over which of these four sets the integral is extended.\nEverything learned about integration in elementary Calculus courses is still useful in the present context, for if \\(f\\) is a continuous complex function on \\([a, b]\\), then the Riemann integral of \\(f\\) and the Lebesgue integral of \\(f\\) over \\([a, b]\\) coincide. This is obvious from our construction if \\(f(a)=f(b)=0\\) and if \\(f(x)\\) is defined to be 0 for \\(x\u0026lt;a\\) and for \\(x\u0026gt;b\\). The general case follows without difficulty. Actually the same thing is true for every Riemann integrable \\(f\\) on \\([a, b]\\). Since we shall have no occasion to discuss Riemann integrable functions in the sequel, we omit the proof and refer to Theorem \\(11.33\\) of [26].\nTwo natural questions may have occurred to some readers by now: Is every Lebesgue measurable set a Borel set? Is every subset of \\(R^k\\) Lebesgue measurable? The answer is negative in both cases, even when \\(k=1\\).\nThe first question can be settled by a cardinality argument which we sketch briefly. Let \\(c\\) be the cardinality of the continuum (the real line or, equivalently, the collection of all sets of integers). We know that \\(R^k\\) has a countable base (open balls with rational radii and with centers in some countable dense subset of \\(R^k\\) ), and that \\(\\mathscr{B}_k\\) (the collection of all Borel sets of \\(\\left.R^k\\right)\\) is the \\(\\sigma\\)-algebra generated by this base. It follows from this (we omit the proof) that \\(\\mathscr{B}_k\\) has cardinality \\(c\\). On the other hand, there exist Cantor sets \\(E \\subset R^1\\) with \\(m(E)=0\\). (Exercise 5.) The completeness of \\(m\\) implies that each of the \\(2^c\\) subsets of \\(E\\) is Lebesgue measurable. Since \\(2^c\u0026gt;c\\), most subsets of \\(E\\) are not Borel sets.\nThe following theorem answers the second question.\n2.22 Theorem If \\(A \\subset R^1\\) and every subset of \\(A\\) is Lebesgue measurable then \\(m(A)=0\\)\nCorollary Every set of positive measure has nonmeasurable subsets.\nProof We shall use the fact that \\(R^1\\) is a group, relative to addition. Let \\(Q\\) be the subgroup that consists of the rational numbers, and let \\(E\\) be a set that contains exactly one point from each coset of \\(Q\\) in \\(R^1\\). (The assertion that\nthere is such a set is a direct application of the axiom of choice.) Then \\(E\\) has the following two properties.\n\\((E+r) \\cap(E+s)=\\varnothing\\) if \\(r \\in Q, s \\in Q, r \\neq s\\). Every \\(x \\in R^1\\) lies in \\(E+r\\) for some \\(r \\in Q\\). To prove (a), suppose \\(x \\in(E+r) \\cap(E+s)\\). Then \\(x=y+r=z+s\\) for some \\(y \\in E, z \\in E, y \\neq z\\). But \\(y-z=s-r \\in Q\\), so that \\(y\\) and \\(z\\) lie in the same coset of \\(Q\\), a contradiction. To prove (b), let \\(y\\) be the point of \\(E\\) that lies in the same coset as \\(x\\), put \\(r=x-y\\).\nFix \\(t \\in Q\\), for the moment, and put \\(A_t=A \\cap(E+t)\\). By hypothesis, \\(A_t\\) is measurable. Let \\(K \\subset A_t\\) be compact, let \\(H\\) be the union of the translates \\(K+r\\), where \\(r\\) ranges over \\(Q \\cap[0,1]\\). Then \\(H\\) is bounded, hence \\(m(H)\u0026lt;\\infty\\). Since \\(K \\subset E+t,(a)\\) shows that the sets \\(K+r\\) are pairwise disjoint. Thus \\(m(H)=\\sum_r m(K+r)\\). But \\(m(K+r)=m(K)\\). It follows that \\(m(K)=0\\). This holds for every compact \\(K \\subset A_t\\). Hence \\(m\\left(A_t\\right)=0\\).\nFinally, \\((b)\\) shows that \\(A=\\bigcup A_t\\), where \\(t\\) ranges over \\(Q\\). Since \\(Q\\) is countable, we conclude that \\(m(A)=0\\).\n2.23 Determinants The scale factors \\(\\Delta(T)\\) that occur in Theorem \\(2.20(e)\\) can be interpreted algebraically by means of determinants.\nLet \\(\\left\\{e_1, \\ldots, e_k\\right\\}\\) be the standard basis for \\(R^k:\\) the \\(i\\) th coordinate of \\(e_j\\) is 1 if \\(i=j, 0\\) if \\(i \\neq j\\). If \\(T: R^k \\rightarrow R^k\\) is linear and\n\\[ T e_j=\\sum_{i=1}^k \\alpha_{i j} e_i \\quad(1 \\leq j \\leq k) \\]\nthen det \\(T\\) is, by definition, the determinant of the matrix \\([T]\\) that has \\(\\alpha_{i j}\\) in row \\(i\\) and column \\(j\\).\nWe claim that\n\\[ \\Delta(T)=|\\operatorname{det} T| \\]\nIf \\(T=T_1 T_2\\), it is clear that \\(\\Delta(T)=\\Delta\\left(T_1\\right) \\Delta\\left(T_2\\right)\\). The multiplication theorem for determinants shows therefore that if (2) holds for \\(T_1\\) and \\(T_2\\), then (2) also holds for \\(T\\). Since every linear operator on \\(R^k\\) is a product of finitely many linear operators of the following three types, it suffices to establish (2) for each of these:\n\\(\\left\\{T e_1, \\ldots, T e_k\\right\\}\\) is a permutation of \\(\\left\\{e_1, \\ldots, e_k\\right\\}\\). \\(T e_1=\\alpha e_1, T e_i=e_i\\) for \\(i=2, \\ldots, k\\). \\(T e_1=e_1+e_2, T e_i=e_i\\) for \\(i=2, \\ldots, k\\). Let \\(Q\\) be the cube consisting of all \\(x=\\left(\\xi_1, \\ldots, \\xi_k\\right)\\) with \\(0 \\leq \\xi_i\u0026lt;1\\) for \\(i=1, \\ldots, k\\).\nIf \\(T\\) is of type (I), then \\([T]\\) has exactly one 1 in each row and each column and has 0 in all other places. So det \\(T=\\pm 1\\). Also, \\(T(Q)=Q\\). So \\(\\Delta(T)=1=|\\operatorname{det} T|\\)\nIf \\(T\\) is of type (II), then clearly \\(\\Delta(T)=|\\alpha|=\\mid\\) det \\(T \\mid\\).\nIf \\(T\\) is of type (III), then det \\(T=1\\) and \\(T(Q)\\) is the set of all points \\(\\sum \\xi_i e_i\\) whose coordinates satisfy\n\\[ \\xi_1 \\leq \\xi_2\u0026lt;\\xi_1+1, \\quad 0 \\leq \\xi_i\u0026lt;1 \\quad \\text { if } \\quad i \\neq 2 . \\]\nIf \\(S_1\\) is the set of points in \\(T(Q)\\) that have \\(\\xi_2\u0026lt;1\\) and if \\(S_2\\) is the rest of \\(T(Q)\\), then\n\\[ S_1 \\cup\\left(S_2-e_2\\right)=Q, \\]\nand \\(S_1 \\cap\\left(S_2-e_2\\right)\\) is empty. Hence \\(\\Delta(T)=m\\left(S_1 \\cup S_2\\right)=m\\left(S_1\\right)+m\\left(S_2-e_2\\right)=\\) \\(m(Q)=1\\), so that we again have \\(\\Delta(T)=\\mid\\) det \\(T \\mid\\).\n","date":"2022-10-16T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/4-lebesgue-measure/","section":"papa rudin","tags":null,"title":"4 Lebesgue Measure"},{"categories":null,"contents":"2.15 Definition A measure \\(\\mu\\) defined on the \\(\\sigma\\)-algebra of all Borel sets in a locally compact Hausdorff space \\(X\\) is called a Borel measure on \\(X\\). If \\(\\mu\\) is positive, a Borel set \\(E \\subset X\\) is outer regular or inner regular, respectively, if \\(E\\) has property \\((c)\\) or \\((d)\\) of Theorem 2.14. If every Borel set in \\(X\\) is both outer and inner regular, \\(\\mu\\) is called regular.\nIn our proof of the Riesz theorem, outer regularity of every set \\(E\\) was built into the construction, but inner regularity was proved only for the open sets and for those \\(E \\in \\mathfrak{M}\\) for which \\(\\mu(E)\u0026lt;\\infty\\). It turns out that this flaw is in the nature of things. One cannot prove regularity of \\(\\mu\\) under the hypothesis of Theorem 2.14; an example is described in Exercise 17.\nHowever, a slight strengthening of the hypotheses does give us a regular measure. Theorem \\(2.17\\) shows this. And if we specialize a little more, Theorem \\(2.18\\) shows that all regularity problems neatly disappear.\n2.16 Definition A set \\(E\\) in a topological space is called \\(\\sigma\\)-compact if \\(E\\) is a countable union of compact sets.\nA set \\(E\\) in a measure space (with measure \\(\\mu\\) ) is said to have \\(\\sigma\\)-finite measure if \\(E\\) is a countable union of sets \\(E_i\\) with \\(\\mu\\left(E_i\\right)\u0026lt;\\infty\\).\nFor example, in the situation described in Theorem 2.14, every \\(\\sigma\\) compact set has \\(\\sigma\\)-finite measure. Also, it is easy to see that if \\(E \\in \\mathfrak{M}\\) and \\(E\\) has \\(\\sigma\\)-finite measure, then \\(E\\) is inner regular.\n2.17 theorem Suppose \\(X\\) is a locally compact, \\(\\sigma\\)-compact Hausdorff space. If \\(\\mathfrak{M}\\) and \\(\\mu\\) are as described in the statement of Theorem 2.14, then \\(\\mathfrak{M}\\) and \\(\\mu\\) have the following properties:\nIf \\(E \\in \\mathfrak{M}\\) and \\(\\epsilon\u0026gt;0\\), there is a closed set \\(F\\) and an open set \\(V\\) such that \\(F \\subset E \\subset V\\) and \\(\\mu(V-F)\u0026lt;\\epsilon\\). \\(\\mu\\) is a regular Borel measure on \\(X\\). If \\(E \\in \\mathfrak{M}\\), there are sets \\(A\\) and \\(B\\) such that \\(A\\) is an \\(F_\\sigma, B\\) is a \\(G_\\delta\\), \\(A \\subset E \\subset B\\), and \\(\\mu(B-A)=0\\) As a corollary of \\((c)\\) we see that every \\(E \\in \\mathfrak{M}\\) is the union of an \\(F_\\sigma\\) and a set of measure 0 .\nProof Let \\(X=K_1 \\cup K_2 \\cup K_3 \\cup \\cdots\\), where each \\(K_n\\) is compact. If \\(E \\in \\mathfrak{M}\\) and \\(\\epsilon\u0026gt;0\\), then \\(\\mu\\left(K_n \\cap E\\right)\u0026lt;\\infty\\), and there are open sets \\(V_n \\supset K_n \\cap E\\) such that\n\\[ \\mu\\left(V_n-\\left(K_n \\cap E\\right)\\right)\u0026lt;\\frac{\\epsilon}{2^{n+1}} \\quad(n=1,2,3, \\ldots) \\]\nIf \\(V=\\bigcup V_n\\), then \\(V-E \\subset \\bigcup\\left(V_n-\\left(K_n \\cap E\\right)\\right)\\), so that\n\\[ \\mu(V-E)\u0026lt;\\frac{\\epsilon}{2} \\]\n2.18 Theorem Let \\(X\\) be a locally compact Hausdorff space in which every open set is \\(\\sigma\\)-compact. Let \\(\\lambda\\) be any positive Borel measure on \\(X\\) such that \\(\\lambda(K)\u0026lt;\\infty\\) for every compact set \\(K\\). Then \\(\\lambda\\) is regular.\nNote that every euclidean space \\(R^k\\) satisfies the present hypothesis, since every open set in \\(R^k\\) is a countable union of closed balls.\nProof Put \\(\\Lambda f=\\int_X f d \\lambda\\), for \\(f \\in C_c(X)\\). Since \\(\\lambda(K)\u0026lt;\\infty\\) for every compact \\(K\\) \\(\\Lambda\\) is a positive linear functional on \\(C_c(X)\\), and there is a regular measure \\(\\mu\\), satisfying the conclusions of Theorem \\(2.17\\), such that\n\\[ \\begin{equation} \\int_X f d \\lambda=\\int_X f d \\mu \\quad\\left(f \\in C_c(X)\\right) \\end{equation} \\]\nWe will show that \\(\\lambda=\\mu\\).\nLet \\(V\\) be open in \\(X\\). Then \\(V=\\bigcup K_i\\), where \\(K_i\\) is compact, \\(i=1,2,3, \\ldots\\) By Urysohn’s lemma we can choose \\(f_i\\) so that \\(K_i \\prec f_i \\prec V\\). Let \\(g_n=\\max \\left(f_1, \\ldots, f_n\\right)\\). Then \\(g_n \\in C_c(X)\\) and \\(g_n(x)\\) increases to \\(\\chi_V(x)\\) at every point \\(x \\in X\\). Hence (1) and the monotone convergence theorem imply\n\\[ \\begin{equation} \\lambda(V)=\\lim _{n \\rightarrow \\infty} \\int_X g_n d \\lambda=\\lim _{n \\rightarrow \\infty} \\int_X g_n d \\mu=\\mu(V) . \\end{equation} \\]\nNow let \\(E\\) be a Borel set in \\(X\\), and choose \\(\\epsilon\u0026gt;0\\). Since \\(\\mu\\) satisfies Theorem \\(2.17\\), there is a closed set \\(F\\) and an open set \\(V\\) such that \\(F \\subset E \\subset V\\) and \\(\\mu(V-F)\u0026lt;\\epsilon\\). Hence \\(\\mu(V) \\leq \\mu(F)+\\epsilon \\leq \\mu(E)+\\epsilon\\).\nSince \\(V-F\\) is open, (2) shows that \\(\\lambda(V-F)\u0026lt;\\epsilon\\), hence \\(\\lambda(V) \\leq \\lambda(E)+\\epsilon\\). Consequently \\(\\lambda(E) \\leq \\lambda(V)=\\mu(V) \\leq \\mu(E)+\\epsilon\\) and \\(\\mu(E) \\leq \\mu(V)=\\lambda(V) \\leq \\lambda(E)+\\epsilon\\), so that \\(|\\lambda(E)-\\mu(E)|\u0026lt;\\epsilon\\) for every \\(\\epsilon\u0026gt;0\\). Hence \\(\\lambda(E)=\\mu(E)\\).\n","date":"2022-10-15T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/3-regularity-properties-of-borel-measures/","section":"papa rudin","tags":null,"title":"3  Regularity Properties of borel Measures"},{"categories":null,"contents":"Guangzong Chen\nProblem 1 First we use equation\n\\[ G_{0, N} \\equiv \\sum_{i=0}^{N-1} A^{N-i-1} B R^{-1} B^{\\mathrm{T}}\\left(A^{\\mathrm{T}}\\right)^{N-i-1} \\]\nget \\(G_{0, N} = 5.3281\\). Then we use follwing equation to get optimal control\n\\[ u_k^*=-R^{-1} B^{\\mathrm{T}} \\lambda_{k+1}=R^{-1} B^{\\mathrm{T}}\\left(A^{\\mathrm{T}}\\right)^{N-k-1} G_{0, N}^{-1}\\left(r_N-A^N x_0\\right) \\]\nThe control value I have is \\(u_0 = 0.1173\\) \\(u_1 = 0.2346\\) \\(u_2 = 0.4692\\) \\(u_3 = 0.9384\\) \\(u_4 = 1.8768\\)\nProblem 2 Use c2d to discretized the system. Then we have following parameters for discrete system.\n\\[ \\begin{aligned} \u0026amp;A=\\\\ \u0026amp;\\begin{array}{rrr} \u0026amp; x 1 \u0026amp; x 2 \\\\ x 1 \u0026amp; 1 \u0026amp; 0.5 \\\\ x 2 \u0026amp; 0 \u0026amp; 1 \\end{array}\\\\ \u0026amp;B=\\\\ \u0026amp;\\begin{array}{rr} u 1 \\\\ x 1 \u0026amp; 0.125 \\\\ \\text { x2 } \u0026amp; 0.5 \\end{array}\\\\ \u0026amp;C=\\\\ \u0026amp;\\begin{array}{rr} x 1 \u0026amp; x 2 \\\\ 1 \u0026amp; 0 \\end{array}\\\\ \u0026amp;D=\\\\ \u0026amp;\\text { u1 }\\\\ \u0026amp;\\text { y1 } 0 \\end{aligned} \\]\nThen solve Riccati difference equation \\[ S_k=A_k^{\\mathrm{T}}\\left[S_{k+1}-S_{k+1} B_k\\left(B_k^{\\mathrm{T}} S_{k+1} B_k+R_k\\right)^{-1} B_k^{\\mathrm{T}} S_{k+1}\\right] A_k+Q_ks \\]\nwe get s11: 4.0368 4.0435 4.0678 4.1417 4.3448 4.8755 6.2840 10.4679 26.4338 95.3311 100.0000 s22: 4.1499 4.1632 4.1976 4.2775 4.4464 4.7800 5.4147 6.6293 9.0036 11.2041 100.0000\nuse equation \\[ K_k=\\left(B_k^{\\mathrm{T}} S_{k+1} B_k+R_k\\right)^{-1} B_k^{\\mathrm{T}} S_{k+1} A_k \\]\nWe get kalman gain K: 0.6527 0.6566 0.6680 0.6972 0.7644 0.9085 1.2052 1.8094 2.8929 0.4535 1.3169 1.3223 1.3359 1.3663 1.4276 1.5418 1.7409 2.0664 2.4838 2.0408\nwe use state equation and \\(u_k = -K_k * x_k\\), we get \\(x\\) and \\(u\\) x1: 10.0000 12.5380 11.5599 9.2410 6.7202 4.5113 2.7813 1.5272 0.6841 0.1878 0.0126\nx2: 10.0000 12.5380 11.5599 9.2410 6.7202 4.5113 2.7813 1.5272 0.6841 0.1878 0.0126\nu_k: -19.6958 -8.4336 -2.2920 0.6764 1.8188 2.0119 1.7964 1.4914 1.2829 1.2860 0\nhw2_01 ","date":"2022-10-12T00:00:00Z","permalink":"https://zongpitt.com/optimal-control/hw2/","section":"optimal control","tags":null,"title":"Optimal Control HW2"},{"categories":null,"contents":" youtube bilibili\nnote pdf xopp ","date":"2022-10-08T00:00:00Z","permalink":"https://zongpitt.com/random-process/1-set-theory/","section":"random process","tags":null,"title":"1 set theory"},{"categories":null,"contents":" youtube part1 part2 bilibili part1 part2 note pdf xopp ","date":"2022-10-08T00:00:00Z","permalink":"https://zongpitt.com/random-process/2-basic-concepts/","section":"random process","tags":null,"title":"2 Basic Concepts"},{"categories":null,"contents":"2.3 Definitions Let \\(X\\) be a topological space, as defined in Sec. \\(1.2\\).\nA set \\(E \\subset X\\) is closed if its complement \\(E^c\\) is open. (Hence \\(\\varnothing\\) and \\(X\\) are closed, finite unions of closed sets are closed, and arbitrary intersections of closed sets are closed.)\nThe closure \\(\\bar{E}\\) of a set \\(E \\subset X\\) is the smallest closed set in \\(X\\) which contains \\(E\\). (The following argument proves the existence of \\(\\bar{E}\\) : The collection \\(\\Omega\\) of all closed subsets of \\(X\\) which contain \\(E\\) is not empty, since \\(X \\in \\Omega\\); let \\(\\bar{E}\\) be the intersection of all members of \\(\\Omega\\).)\nA set \\(K \\subset X\\) is compact if every open cover of \\(K\\) contains a finite subcover. More explicitly, the requirement is that if \\(\\left\\{V_\\alpha\\right\\}\\) is a collection of open sets whose union contains \\(K\\), then the union of some finite subcollection of \\(\\left\\{V_\\alpha\\right\\}\\) also contains \\(K\\). In particular, if \\(X\\) is itself compact, then \\(X\\) is called a compact space.\nA neighborhood of a point \\(p \\in X\\) is any open subset of \\(X\\) which contains \\(p\\). (The use of this term is not quite standardized; some use “neighborhood of \\(p\\)” for any set which contains an open set containing p.)\n\\(X\\) is a Hausdorff space if the following is true: If \\(p \\in X, q \\in X\\), and \\(p \\neq q\\), then \\(p\\) has a neighborhood \\(U\\) and \\(q\\) has a neighborhood \\(V\\) such that \\(U \\cap V=\\varnothing\\)\n\\(X\\) is locally compact if every point of \\(X\\) has a neighborhood whose closure is compact.\nObviously, every compact space is locally compact.\nWe recall the Heine-Borel theorem: The compact subsets of a euclidean space \\(R^n\\) are precisely those that are closed and bounded \\(([26], \\dagger\\) Theorem 2.41). From this it follows easily that \\(R^n\\) is a locally compact Hausdorff space. Also, every metric space is a Hausdorff space.\n2.4 theorem Suppose \\(K\\) is compact and \\(F\\) is closed, in a topological space \\(X\\). If \\(F \\subset K\\), then \\(F\\) is compact.\nPROOF If \\(\\left\\{V_\\alpha\\right\\}\\) is an open cover of \\(F\\) and \\(W=F^c\\), then \\(W \\cup \\bigcup_\\alpha V_\\alpha\\) covers \\(X\\); hence there is a finite collection \\(\\left\\{V_{\\alpha_i}\\right\\}\\) such that\n\\[ \\begin{equation} K \\subset W \\cup V_{\\alpha_1} \\cup \\cdots \\cup V_{\\alpha_n} . \\end{equation} \\]\nThen \\(F \\subset V_{\\alpha_1} \\cup \\cdots \\cup V_{\\alpha_n}\\).\nCorollary If \\(A \\subset B\\) and if \\(B\\) has compact closure, so does \\(A\\).\n2.5 Theorem Suppose \\(X\\) is a Hausdorff space, \\(K \\subset X, K\\) is compact, and \\(p \\in K^c\\). Then there are open sets \\(U\\) and \\(W\\) such that \\(p \\in U, K \\subset W\\), and \\(U \\cap W=\\varnothing\\)\nPROOF If \\(q \\in K\\), the Hausdorff separation axiom implies the existence of disjoint open sets \\(U_q\\) and \\(V_q\\), such that \\(p \\in U_q\\) and \\(q \\in V_q\\). Since \\(K\\) is compact, there are points \\(q_1, \\ldots, q_n \\in K\\) such that\n\\[ \\begin{equation} K \\subset V_{q_1} \\cup \\cdots \\cup V_{q_n} . \\end{equation} \\]\nOur requirements are then satisfied by the sets\n\\[ \\begin{equation} U=U_{q_1} \\cap \\cdots \\cap U_{q_n} \\text { and } W=V_{q_1} \\cup \\cdots \\cup V_{q_n} . \\quad \\text { //// } \\end{equation} \\]\nCorollaries\nCompact subsets of Hausdorff spaces are closed. If \\(F\\) is closed and \\(K\\) is compact in a Hausdorff space, then \\(F \\cap K\\) is compact. Corollary \\((b)\\) follows from \\((a)\\) and Theorem \\(2.4\\).\n2.6 Theorem If \\(\\left\\{K_\\alpha\\right\\}\\) is a collection of compact subsets of a Hausdorff space and if \\(\\bigcap_\\alpha K_\\alpha=\\varnothing\\), then some finite subcollection of \\(\\left\\{K_\\alpha\\right\\}\\) also has empty intersection.\nProof Put \\(V_\\alpha=K_\\alpha^c\\). Fix a member \\(K_1\\) of \\(\\left\\{K_\\alpha\\right\\}\\). Since no point of \\(K_1\\) belongs to every \\(K_\\alpha,\\left\\{V_\\alpha\\right\\}\\) is an open cover of \\(K_1\\). Hence \\(K_1 \\subset V_{\\alpha_1} \\cup \\cdots \\cup V_{\\alpha_n}\\) for some finite collection \\(\\left\\{V_{\\alpha_i}\\right\\}\\). This implies that\n\\[ \\begin{equation} K_1 \\cap K_{\\alpha_1} \\cap \\cdots \\cap K_{\\alpha_n}=\\varnothing . \\end{equation} \\]\n2.7 Theorem Suppose \\(U\\) is open in a locally compact Hausdorff space \\(X\\), \\(K \\subset U\\), and \\(K\\) is compact. Then there is an open set \\(V\\) with compact closure such that\n\\[ \\begin{equation} K \\subset V \\subset \\bar{V} \\subset U . \\end{equation} \\]\nProof Since every point of \\(K\\) has a neighborhood with compact closure, and since \\(K\\) is covered by the union of finitely many of these neighborhoods, \\(K\\) lies in an open set \\(G\\) with compact closure. If \\(U=X\\), take \\(V=G\\).\nOtherwise, let \\(C\\) be the complement of \\(U\\). Theorem \\(2.5\\) shows that to each \\(p \\in C\\) there corresponds an open set \\(W_p\\) such that \\(K \\subset W_p\\) and \\(p \\notin \\bar{W}_p\\). Hence \\(\\left\\{C \\cap \\bar{G} \\cap \\bar{W}_p\\right\\}\\), where \\(p\\) ranges over \\(C\\), is a collection of compact sets with empty intersection. By Theorem \\(2.6\\) there are points \\(p_1, \\ldots, p_n \\in C\\) such that\n\\[ \\begin{equation} C \\cap \\bar{G} \\cap \\bar{W}_{p_1} \\cap \\cdots \\cap \\bar{W}_{p_n}=\\varnothing . \\end{equation} \\]\nThe set\n\\[ \\begin{equation} V=G \\cap W_{p_1} \\cap \\cdots \\cap W_{p_n} \\end{equation} \\]\nthen has the required properties, since\n\\[ \\begin{equation} \\bar{V} \\subset \\bar{G} \\cap \\bar{W}_{p_1} \\cap \\cdots \\cap \\bar{W}_{p_n} . \\end{equation} \\]\n2.8 Definition Let \\(f\\) be a real (or extended-real) function on a topological space. If\n\\[ \\begin{equation} \\{x: f(x)\u0026gt;\\alpha\\} \\end{equation} \\]\nis open for every real \\(\\alpha, f\\) is said to be lower semicontinuous. If\n\\[ \\begin{equation} \\{x: f(x)\u0026lt;\\alpha\\} \\end{equation} \\]\nis open for every real \\(\\alpha, f\\) is said to be upper semicontinuous.\nA real function is obviously continuous if and only if it is both upper and lower semicontinuous.\nThe simplest examples of semicontinuity are furnished by characteristic functions:\nCharacteristic functions of open sets are lower semicontinuous.\nCharacteristic functions of closed sets are upper semicontinuous.\nThe following property is an almost immediate consequence of the definitions:\nThe supremum of any collection of lower semicontinuous functions is lower semicontinuous. The infimum of any collection of upper semicontinuous functions is upper semicontinuous. 2.9 Definition The support of a complex function \\(f\\) on a topological space \\(X\\) is the closure of the set\n\\[ \\begin{equation} \\{x: f(x) \\neq 0\\} . \\end{equation} \\]\nThe collection of all continuous complex functions on \\(X\\) whose support is compact is denoted by \\(C_c(X)\\).\nObserve that \\(C_c(X)\\) is a vector space. This is due to two facts:\nThe support of \\(f+g\\) lies in the union of the support of \\(f\\) and the support of \\(g\\), and any finite union of compact sets is compact.\nThe sum of two continuous complex functions is continuous, as are scalar multiples of continuous functions.\n(Statement and proof of Theorem \\(1.8\\) hold verbatim if “measurable function” is replaced by “continuous function,” ” measurable space” by “topological space”; take \\(\\Phi(s, t)=s+t\\), or \\(\\Phi(s, t)=s t\\), to prove that sums and products of continuous functions are continuous.)\n2.10 Theorem Let \\(X\\) and \\(Y\\) be topological spaces, and let \\(f: X \\rightarrow Y\\) be continuous. If \\(K\\) is a compact subset of \\(X\\), then \\(f(K)\\) is compact.\nProof If \\(\\left\\{V_\\alpha\\right\\}\\) is an open cover of \\(f(K)\\), then \\(\\left\\{f^{-1}\\left(V_\\alpha\\right)\\right\\}\\) is an open cover of \\(K\\), hence \\(K \\subset f^{-1}\\left(V_{\\alpha_1}\\right) \\cup \\cdots \\cup f^{-1}\\left(V_{\\alpha_n}\\right)\\) for some \\(\\alpha_1, \\ldots, \\alpha_n\\), and therefore \\(f(K) \\subset V_{\\alpha_1} \\cup \\cdots \\cup V_{\\alpha_n}\\).\nCorollary The range of any \\(f \\in C_c(X)\\) is a compact subset of the complex plane.\nIn fact, if \\(K\\) is the support of \\(f \\in C_c(X)\\), then \\(f(X) \\subset f(K) \\cup\\{0\\}\\). If \\(X\\) is not compact, then \\(0 \\in f(X)\\), but 0 need not lie in \\(f(K)\\), as is seen by easy examples.\n2.11 Notation In this chapter the following conventions will be used. The notation\n\\[ \\begin{equation} K \\prec f \\end{equation} \\]\nwill mean that \\(K\\) is a compact subset of \\(X\\), that \\(f \\in C_c(X)\\), that \\(0 \\leq f(x) \\leq 1\\) for all \\(x \\in X\\), and that \\(f(x)=1\\) for all \\(x \\in K\\). The notation\n\\[ f \\prec V \\]\nwill mean that \\(V\\) is open, that \\(f \\in C_c(X), 0 \\leq f \\leq 1\\), and that the support of \\(f\\) lies in \\(V\\). The notation\n\\[ \\begin{equation} K \\prec f \\prec V \\end{equation} \\]\nwill be used to indicate that both (1) and (2) hold.\n2.12 Urysohn’s Lemma Suppose \\(X\\) is a locally compact Hausdorff space, \\(V\\) is open in \\(X, K \\subset V\\), and \\(K\\) is compact. Then there exists an \\(f \\in C_c(X)\\), such that\n\\[ \\begin{equation} K \\prec f \\prec V . \\end{equation} \\]\nIn terms of characteristic functions, the conclusion asserts the existence of a continuous function \\(f\\) which satisfies the inequalities \\(\\chi_K \\leq f \\leq \\chi_V\\). Note that it is easy to find semicontinuous functions which do this; examples are \\(\\chi_K\\) and \\(\\chi_V\\).\nProof Put \\(r_1=0, r_2=1\\), and let \\(r_3, r_4, r_5, \\ldots\\) be an enumeration of the rationals in \\((0,1)\\). By Theorem \\(2.7\\), we can find open sets \\(V_0\\) and then \\(V_1\\) such that \\(\\bar{V}_0\\) is compact and\n\\[ \\begin{equation} K \\subset V_1 \\subset \\bar{V}_1 \\subset V_0 \\subset \\bar{V}_0 \\subset V . \\end{equation} \\]\nSuppose \\(n \\geq 2\\) and \\(V_{r_1}, \\ldots, V_{r_n}\\) have been chosen in such a manner that \\(r_i\u0026lt;r_j\\) implies \\(\\bar{V}_{r_j} \\subset V_{r_i}\\). Then one of the numbers \\(r_1, \\ldots, r_n\\), say \\(r_i\\), will be the largest one which is smaller than \\(r_{n+1}\\), and another, say \\(r_j\\), will be the smallest one larger than \\(r_{n+1}\\). Using Theorem \\(2.7\\) again, we can find \\(V_{r_{n+1}}\\) so that\n\\[ \\begin{equation} \\bar{V}_{r_j} \\subset V_{r_{n+1}} \\subset \\bar{V}_{r_{n+1}} \\subset V_{r_i} . \\end{equation} \\]\nContinuing, we obtain a collection \\(\\left\\{V_r\\right\\}\\) of open sets, one for every rational \\(r \\in[0,1]\\), with the following properties: \\(K \\subset V_1, \\bar{V}_0 \\subset V, e a c h \\bar{V}_r\\) is compact, and\n\\[ \\begin{equation} s\u0026gt;r \\text { implies } \\bar{V}_s \\subset V_r \\text {. } \\end{equation} \\]\nDefine\n\\[ \\begin{equation} f_r(x)=\\left\\{\\begin{array}{ll} r \u0026amp; \\text { if } x \\in V_r, \\\\ 0 \u0026amp; \\text { otherwise }, \\end{array} \\quad g_s(x)= \\begin{cases}1 \u0026amp; \\text { if } x \\in \\bar{V}_s, \\\\ s \u0026amp; \\text { otherwise }\\end{cases}\\right. \\end{equation} \\]\nand\n\\[ \\begin{equation} f=\\sup _r f_r, \\quad g=\\inf _s g_s \\end{equation} \\]\nThe remarks following Definition \\(2.8\\) show that \\(f\\) is lower semicontinuous and that \\(g\\) is upper semicontinuous. It is clear that \\(0 \\leq f \\leq 1\\), that \\(f(x)=1\\) if \\(x \\in K\\), and that \\(f\\) has its support in \\(\\bar{V}_0\\). The proof will be completed by showing that \\(f=g\\).\nThe inequality \\(f_r(x)\u0026gt;g_s(x)\\) is possible only if \\(r\u0026gt;s, x \\in V_r\\), and \\(x \\notin \\bar{V}_s\\). But \\(r\u0026gt;s\\) implies \\(V_r \\subset V_s\\). Hence \\(f_r \\leq g_s\\) for all \\(r\\) and \\(s\\), so \\(f \\leq g\\).\nSuppose \\(f(x)\u0026lt;g(x)\\) for some \\(x\\). Then there are rationals \\(r\\) and \\(s\\) such that \\(f(x)\u0026lt;r\u0026lt;s\u0026lt;g(x)\\). Since \\(f(x)\u0026lt;r\\), we have \\(x \\notin V_r\\); since \\(g(x)\u0026gt;s\\), we have \\(x \\in \\bar{V}_s\\). By (3), this is a contradiction. Hence \\(f=g\\).∎\n2.13 Theorem Suppose \\(V_1, \\ldots, V_n\\) are open subsets of a locally compact Hausdorff space \\(X, K\\) is compact, and\n\\[ \\begin{equation} K \\subset V_1 \\cup \\cdots \\cup V_n . \\end{equation} \\]\nThen there exist functions \\(h_i \\prec V_i(i=1, \\ldots, n)\\) such that\n\\[ \\begin{equation} h_1(x)+\\cdots+h_n(x)=1 \\quad(x \\in K) . \\end{equation} \\]\nBecause of (1), the collection \\(\\left\\{h_1, \\ldots, h_n\\right\\}\\) is called a partition of unity on \\(K\\), subordinate to the cover \\(\\left\\{V_1, \\ldots, V_n\\right\\}\\).\nProof By Theorem 2.7, each \\(x \\in K\\) has a neighborhood \\(W_x\\) with compact closure \\(\\bar{W}_x \\subset V_i\\) for some \\(i\\) (depending on \\(x\\) ). There are points \\(x_1, \\ldots, x_m\\) such that \\(W_{x_1} \\cup \\cdots \\cup W_{x_m} \\supset K\\). If \\(1 \\leq i \\leq n\\), let \\(H_i\\) be the union of those \\(W_{x_j}\\) which lie in \\(V_i\\). By Urysohn’s lemma, there are functions \\(g_i\\) such that \\(H_i \\prec g_i \\prec V_i\\). Define\n\\[ \\begin{equation} \\begin{aligned} \u0026amp;h_1=g_1 \\\\ \u0026amp;h_2=\\left(1-g_1\\right) g_2 \\\\ \u0026amp;\\cdots \\ldots \\ldots \\\\ \u0026amp;h_n=\\left(1-g_1\\right)\\left(1-g_2\\right) \\cdots\\left(1-g_{n-1}\\right) g_n . \\end{aligned} \\end{equation} \\]\nThen \\(h_i \\prec V_i\\). It is easily verified, by induction, that\n\\[ \\begin{equation} h_1+h_2+\\cdots+h_n=1-\\left(1-g_1\\right)\\left(1-g_2\\right) \\cdots\\left(1-g_n\\right) \\text {. } \\end{equation} \\]\nSince \\(K \\subset H_1 \\cup \\cdots \\cup H_n\\), at least one \\(g_i(x)=1\\) at each point \\(x \\in K\\); hence (3) shows that (1) holds.\n","date":"2022-10-04T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/1-topological-preliminaries/","section":"papa rudin","tags":null,"title":"1 Topological Preliminaries"},{"categories":null,"contents":"2.14 Theorem Let \\(X\\) be a locally compact Hausdorff space, and let \\(\\Lambda\\) be a positive linear functional on \\(C_c(X)\\). Then there exists a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in \\(X\\) which contains all Borel sets in \\(X\\), and there exists a unique positive measure \\(\\mu\\) on \\(\\mathfrak{M}\\) which represents \\(\\Lambda\\) in the sense that\n\\(Λ f=\\int_X f d μ\\) for every \\(f \\in C_c(X)\\), and which has the following additional properties: \\(\\mu(K)\u0026lt;\\infty\\) for every compact set \\(K \\subset X\\). For every \\(E \\in \\mathfrak{M}\\), we have \\[ \\begin{equation} \\mu(E)=\\inf \\{\\mu(V): E \\subset V, V \\text { open }\\} . \\end{equation} \\]\nThe relation \\[ \\begin{equation} \\mu(E)=\\sup \\{\\mu(K): K \\subset E, K \\text { compact }\\} \\end{equation} \\]\nholds for every open set \\(E\\), and for every \\(E \\in \\mathfrak{M}\\) with \\(\\mu(E)\u0026lt;\\infty\\).\nIf \\(E \\in \\mathfrak{M}, A \\subset E\\), and \\(\\mu(E)=0\\), then \\(A \\in \\mathfrak{M}\\). For the sake of clarity, let us be more explicit about the meaning of the word “positive” in the hypothesis: \\(\\Lambda\\) is assumed to be a linear functional on the complex vector space \\(C_c(X)\\), with the additional property that \\(\\Lambda f\\) is a nonnegative real number for every \\(f\\) whose range consists of nonnegative real numbers. Briefly, if \\(f(X) \\subset[0, \\infty)\\) then \\(\\Lambda f \\in[0, \\infty)\\).\nProperty \\((a)\\) is of course the one of greatest interest. After we define \\(\\mathfrak{M}\\) and \\(\\mu\\), (b) to \\((d)\\) will be established in the course of proving that \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra and that \\(\\mu\\) is countably additive. We shall see later (Theorem 2.18) that in “reasonable” spaces \\(X\\) every Borel measure which satisfies \\((b)\\) also satisfies \\((c)\\) and \\((d)\\) and that \\((d)\\) actually holds for every \\(E \\in \\mathfrak{M}\\), in those cases. Property \\((e)\\) merely says that \\((X, \\mathfrak{M}, \\mu)\\) is a complete measure space, in the sense of Theorem \\(1.36{ }_2\\)\nThroughout the proof of this theorem, the letter \\(K\\) will stand for a compact subset of \\(X\\), and \\(V\\) will denote an open set in \\(X\\).\nLet us begin by proving the uniqueness of \\(\\mu\\). If \\(\\mu\\) satisfies \\((c)\\) and \\((d)\\), it is clear that \\(\\mu\\) is determined on \\(\\mathfrak{M}\\) by its values on compact sets. Hence it suffices to prove that \\(\\mu_1(K)=\\mu_2(K)\\) for all \\(K\\), whenever \\(\\mu_1\\) and \\(\\mu_2\\) are measures for which the theorem holds. So, fix \\(K\\) and \\(\\epsilon\u0026gt;0\\). By \\((b)\\) and \\((c)\\), there exists a \\(V \\supset K\\) with \\(\\mu_2(V)\u0026lt;\\mu_2(K)+\\epsilon\\); by Urysohn’s lemma, there exists an \\(f\\) so that \\(K \\prec f\u0026lt;V\\); hence\n\\[ \\begin{equation} \\begin{aligned} \\mu_1(K) \u0026amp;=\\int_X \\chi_K d \\mu_1 \\leq \\int_X f d \\mu_1=\\Lambda f=\\int_X f d \\mu_2 \\\\ \u0026amp; \\leq \\int_X \\chi_V d \\mu_2=\\mu_2(V)\u0026lt;\\mu_2(K)+\\epsilon \\end{aligned} \\end{equation} \\]\nThus \\(\\mu_1(K) \\leq \\mu_2(K)\\). If we interchange the roles of \\(\\mu_1\\) and \\(\\mu_2\\), the opposite inequality is obtained, and the uniqueness of \\(\\mu\\) is proved.\nIncidentally, the above computation shows that \\((a)\\) forces \\((b)\\).\nConstruction of \\(\\mu\\) and \\(\\mathfrak{M}\\) For every open set \\(V\\) in \\(X\\), define\n\\[ \\begin{equation} \\mu(V)=\\sup \\{\\Lambda f: f\u0026lt;V\\} \\end{equation} \\]\nIf \\(V_1 \\subset V_2\\), it is clear that (1) implies \\(\\mu\\left(V_1\\right) \\leq \\mu\\left(V_2\\right)\\). Hence\n\\[ \\begin{equation} \\mu(E)=\\inf \\{\\mu(V): E \\subset V, V \\text { open }\\}, \\end{equation} \\]\nif \\(E\\) is an open set, and it is consistent with (1) to define \\(\\mu(E)\\) by (2), for every \\(E \\subset X\\).\nNote that although we have defined \\(\\mu(E)\\) for every \\(E \\subset X\\), the countable additivity of \\(\\mu\\) will be proved only on a certain \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) in \\(X\\).\nLet \\(\\mathfrak{M}_F\\) be the class of all \\(E \\subset X\\) which satisfy two conditions: \\(\\mu(E)\u0026lt;\\infty\\), and\n\\[ \\begin{equation} \\mu(E)=\\sup \\{\\mu(K): K \\subset E, K \\text { compact }\\} . \\end{equation} \\]\nFinally, let \\(\\mathfrak{M}\\) be the class of all \\(E \\subset X\\) such that \\(E \\cap K \\in \\mathfrak{M}_F\\) for every compact \\(K\\).\nProof that \\(\\mu\\) and \\(\\mathfrak{M}\\) have the required properties It is evident that \\(\\mu\\) is monotone, i.e., that \\(\\mu(A) \\leq \\mu(B)\\) if \\(A \\subset B\\) and that \\(\\mu(E)=0\\) implies \\(E \\in \\mathfrak{M}_F\\) and \\(E \\in \\mathfrak{M}\\). Thus (e) holds, and so does (c), by definition.\nSince the proof of the other assertions is rather long, it will be convenient to divide it into several steps.\nObserve that the positivity of \\(\\Lambda\\) implies that \\(\\Lambda\\) is monotone: \\(f \\leq g\\) implies \\(\\Lambda f \\leq \\Lambda g\\). This is clear, since \\(\\Lambda g=\\Lambda f+\\Lambda(g-f)\\) and \\(g-f \\geq 0\\). This monotonicity will be used in Steps II and \\(\\mathrm{X}\\).\nSTEP I If \\(E_1, E_2, E_3, \\ldots\\) are arbitrary subsets of \\(X\\), then \\[ \\begin{equation} \\mu\\left(\\bigcup_{i=1}^{\\infty} E_i\\right) \\leq \\sum_{i=1}^{\\infty} \\mu\\left(E_i\\right) \\end{equation} \\]\nProof We first show that\n\\[ \\begin{equation} \\mu\\left(V_1 \\cup V_2\\right) \\leq \\mu\\left(V_1\\right)+\\mu\\left(V_2\\right) \\end{equation} \\]\nif \\(V_1\\) and \\(V_2\\) are open. Choose \\(g \\prec V_1 \\cup V_2\\). By Theorem \\(2.13\\) there are functions \\(h_1\\) and \\(h_2\\) such that \\(h_i\u0026lt;V_i\\) and \\(h_1(x)+h_2(x)=1\\) for all \\(x\\) in the support of \\(g\\). Hence \\(h_i g \\prec V_i, g=h_1 g+h_2 g\\), and so\n\\[ \\begin{equation} \\Lambda g=\\Lambda\\left(h_1 g\\right)+\\Lambda\\left(h_2 g\\right) \\leq \\mu\\left(V_1\\right)+\\mu\\left(V_2\\right) \\end{equation} \\]\nSince (6) holds for every \\(g\u0026lt;V_1 \\cup V_2\\), (5) follows.\nIf \\(\\mu\\left(E_i\\right)=\\infty\\) for some \\(i\\), then (4) is trivially true. Suppose therefore that \\(\\mu\\left(E_i\\right)\u0026lt;\\infty\\) for every \\(i\\). Choose \\(\\epsilon\u0026gt;0\\). By (2) there are open sets \\(V_i \\supset E_i\\) such that\n\\[ \\begin{equation} \\mu\\left(V_i\\right)\u0026lt;\\mu\\left(E_i\\right)+2^{-i} \\epsilon \\quad(i=1,2,3, \\ldots) \\end{equation} \\]\nPut \\(V=\\bigcup_1^{\\infty} V_i\\), and choose \\(f \\prec V\\). Since \\(f\\) has compact support, we see that \\(f \\prec V_1 \\cup \\cdots \\cup V_n\\) for some \\(n\\). Applying induction to (5), we therefore obtain\n\\[ \\begin{equation} \\Lambda f \\leq \\mu\\left(V_1 \\cup \\cdots \\cup V_n\\right) \\leq \\mu\\left(V_1\\right)+\\cdots+\\mu\\left(V_n\\right) \\leq \\sum_{i=1}^{\\infty} \\mu\\left(E_i\\right)+\\epsilon . \\end{equation} \\]\nSince this holds for every \\(f \\prec V\\), and since \\(\\bigcup E_i \\subset V\\), it follows that\n\\[ \\begin{equation} \\mu\\left(\\bigcup_{i=1}^{\\infty} E_i\\right) \\leq \\mu(V) \\leq \\sum_{i=1}^{\\infty} \\mu\\left(E_i\\right)+\\epsilon, \\end{equation} \\]\nwhich proves (4), since \\(\\epsilon\\) was arbitrary.\nSTEP II If \\(K\\) is compact, then \\(K \\in \\mathfrak{M}_F\\) and \\[ \\begin{equation} \\mu(K)=\\inf \\{\\Lambda f: K \\prec f\\} . \\end{equation} \\]\nThis implies assertion \\((b)\\) of the theorem.\nProof If \\(K \\prec f\\) and \\(0\u0026lt;\\alpha\u0026lt;1\\), let \\(V_\\alpha=\\{x: f(x)\u0026gt;\\alpha\\}\\). Then \\(K \\subset V_\\alpha\\), and \\(\\alpha g \\leq f\\) whenever \\(g \\prec V_\\alpha\\). Hence\n\\[ \\begin{equation} \\mu(K) \\leq \\mu\\left(V_\\alpha\\right)=\\sup \\left\\{\\Lambda g: g \\prec V_\\alpha\\right\\} \\leq \\alpha^{-1} \\Lambda f . \\end{equation} \\]\nLet \\(\\alpha \\rightarrow 1\\), to conclude that\n\\[ \\begin{equation} \\mu(K) \\leq \\Lambda f \\text {. } \\end{equation} \\]\nThus \\(\\mu(K)\u0026lt;\\infty\\). Since \\(K\\) evidently satisfies (3), \\(K \\in \\mathfrak{M}_F\\).\nIf \\(\\epsilon\u0026gt;0\\), there exists \\(V \\supset K\\) with \\(\\mu(V)\u0026lt;\\mu(K)+\\epsilon\\). By Urysohn’s lemma, \\(K \\prec f \\prec V\\) for some \\(f\\). Thus\n\\[ \\begin{equation} \\Lambda f \\leq \\mu(V)\u0026lt;\\mu(K)+\\epsilon, \\end{equation} \\]\nwhich, combined with (8), gives (7). STEP III Every open set satisfies (3). Hence \\(\\mathfrak{M}_F\\) contains every open set \\(V\\) with \\(\\mu(V)\u0026lt;\\infty\\).\nProof Let \\(\\alpha\\) be a real number such that \\(\\alpha\u0026lt;\\mu(V)\\). There exists an \\(f\u0026lt;V\\) with \\(\\alpha\u0026lt;\\Lambda f\\). If \\(W\\) is any open set which contains the support \\(K\\) of \\(f\\), then \\(f \\prec W\\), hence \\(\\Lambda f \\leq \\mu(W)\\). Thus \\(\\Lambda f \\leq \\mu(K)\\). This exhibits a compact \\(K \\subset V\\) with \\(\\alpha\u0026lt;\\mu(K)\\), so that (3) holds for \\(V\\).\nSTEP IV Suppose \\(E=\\bigcup_{i=1}^{\\infty} E_i\\), where \\(E_1, E_2, E_3, \\ldots\\) are pairwise disjoint members of \\(\\mathfrak{M}_F\\). Then\n\\[ \\begin{equation} \\mu(E)=\\sum_{i=1}^{\\infty} \\mu\\left(E_i\\right) . \\end{equation} \\]\nIf, in addition, \\(\\mu(E)\u0026lt;\\infty\\), then also \\(E \\in \\mathfrak{M}_F\\).\nProof We first show that \\[ \\mu\\left(K_1 \\cup K_2\\right)=\\mu\\left(K_1\\right)+\\mu\\left(K_2\\right) \\] if \\(K_1\\) and \\(K_2\\) are disjoint compact sets. Choose \\(\\epsilon\u0026gt;0\\). By Urysohn’s lemma, there exists \\(f \\in C_c(X)\\) such that \\(f(x)=1\\) on \\(K_1, f(x)=0\\) on \\(K_2\\), and \\(0 \\leq f \\leq 1\\). By Step II there exists \\(g\\) such that \\[ K_1 \\cup K_2 \\prec g \\text { and } \\Lambda g\u0026lt;\\mu\\left(K_1 \\cup K_2\\right)+\\epsilon . \\] Note that \\(K_1 \\prec f g\\) and \\(K_2 \\prec(1-f) g\\). Since \\(\\Lambda\\) is linear, it follows from (8) that \\[ \\mu\\left(K_1\\right)+\\mu\\left(K_2\\right) \\leq \\Lambda(f g)+\\Lambda(g-f g)=\\Lambda g\u0026lt;\\mu\\left(K_1 \\cup K_2\\right)+\\epsilon . \\] Since \\(\\epsilon\\) was arbitrary, (10) follows now from Step I. If \\(\\mu(E)=\\infty\\), (9) follows from Step I. Assume therefore that \\(\\mu(E)\u0026lt;\\infty\\), and choose \\(\\epsilon\u0026gt;0\\). Since \\(E_i \\in \\mathfrak{M}_F\\), there are compact sets \\(H_i \\subset E_i\\) with \\[ \\mu\\left(H_i\\right)\u0026gt;\\mu\\left(E_i\\right)-2^{-i} \\epsilon \\quad(i=1,2,3, \\ldots) . \\] Putting \\(K_n=H_1 \\cup \\cdots \\cup H_n\\) and using induction on (10), we obtain \\[ \\mu(E) \\geq \\mu\\left(K_n\\right)=\\sum_{i=1}^n \\mu\\left(H_i\\right)\u0026gt;\\sum_{i=1}^n \\mu\\left(E_i\\right)-\\epsilon . \\] Since (12) holds for every \\(n\\) and every \\(\\epsilon\u0026gt;0\\), the left side of (9) is not smaller than the right side, and so (9) follows from Step I. But if \\(\\mu(E)\u0026lt;\\infty\\) and \\(\\epsilon\u0026gt;0\\), (9) shows that \\[ \\mu(E) \\leq \\sum_{i=1}^N \\mu\\left(E_i\\right)+\\epsilon \\] for some \\(N\\). By (12), it follows that \\(\\mu(E) \\leq \\mu\\left(K_N\\right)+2 \\epsilon\\), and this shows that \\(E\\) satisfies (3); hence \\(E \\in \\mathfrak{M}_F\\).\nSTEP \\(\\vee\\) If \\(E \\in \\mathfrak{M}_F\\) and \\(\\epsilon\u0026gt;0\\), there is a compact \\(K\\) and an open \\(V\\) such that \\(K \\subset E \\subset V\\) and \\(\\mu(V-K)\u0026lt;\\epsilon\\). Proof Our definitions show that there exist \\(K \\subset E\\) and \\(V \\supset E\\) so that \\[ \\mu(V)-\\frac{\\epsilon}{2}\u0026lt;\\mu(E)\u0026lt;\\mu(K)+\\frac{\\epsilon}{2} . \\] Since \\(V-K\\) is open, \\(V-K \\in \\mathfrak{M}_F\\), by Step III. Hence Step IV implies that \\[ \\mu(K)+\\mu(V-K)=\\mu(V)\u0026lt;\\mu(K)+\\epsilon . \\] STEP VI If \\(A \\in \\mathfrak{M}_F\\) and \\(B \\in \\mathfrak{M}_F\\), then \\(A-B, A \\cup B\\), and \\(A \\cap B\\) belong to \\(\\mathfrak{M}_F\\).\nPROOF If \\(\\epsilon\u0026gt;0\\), Step \\(\\mathrm{V}\\) shows that there are sets \\(K_i\\) and \\(V_i\\) such that \\(K_1 \\subset A \\subset V_1, K_2 \\subset B \\subset V_2\\), and \\(\\mu\\left(V_i-K_i\\right)\u0026lt;\\epsilon\\), for \\(i=1,2\\). Since \\[ A-B \\subset V_1-K_2 \\subset\\left(V_1-K_1\\right) \\cup\\left(K_1-V_2\\right) \\cup\\left(V_2-K_2\\right), \\] Step I shows that \\[ \\mu(A-B) \\leq \\epsilon+\\mu\\left(K_1-V_2\\right)+\\epsilon . \\] Since \\(K_1-V_2\\) is a compact subset of \\(A-B\\), (14) shows that \\(A-B\\) satisfies (3), so that \\(A-B \\in \\mathfrak{M}_F\\).\nSince \\(A \\cup B=(A-B) \\cup B\\), an application of Step IV shows that \\(A \\cup B \\in \\mathfrak{M}_F\\). Since \\(A \\cap B=A-(A-B)\\), we also have \\(A \\cap B \\in \\mathfrak{M}_F\\). I/// STEP VII \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra in \\(X\\) which contains all Borel sets. PROOF Let \\(K\\) be an arbitrary compact set in \\(X\\). If \\(A \\in \\mathfrak{M}\\), then \\(A^c \\cap K=K-(A \\cap K)\\), so that \\(A^c \\cap K\\) is a difference of two members of \\(\\mathscr{M}_F\\). Hence \\(A^c \\cap K \\in \\mathfrak{M}_F\\), and we conclude: \\(A \\in \\mathfrak{M}\\) implies \\(A^c \\in \\mathfrak{M}\\).\nNext, suppose \\(A=\\bigcup_1^{\\infty} A_i\\), where each \\(A_i \\in \\mathfrak{M}\\). Put \\(B_1=A_1 \\cap K\\), and \\[ B_n=\\left(A_n \\cap K\\right)-\\left(B_1 \\cup \\cdots \\cup B_{n-1}\\right) \\quad(n=2,3,4, \\ldots) . \\] Then \\(\\left\\{B_n\\right\\}\\) is a disjoint sequence of members of \\(\\mathfrak{M}_F\\), by Step VI, and \\(A \\cap K=\\bigcup_1^{\\infty} B_n\\). It follows from Step IV that \\(A \\cap K \\in \\mathfrak{M}_F\\). Hence \\(A \\in \\mathfrak{M}\\). Finally, if \\(C\\) is closed, then \\(C \\cap K\\) is compact, hence \\(C \\cap K \\in \\mathfrak{M}_F\\), so \\(C \\in \\mathfrak{M}\\). In particular, \\(X \\in \\mathfrak{M}\\).\nWe have thus proved that \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra in \\(X\\) which contains all closed subsets of \\(X\\). Hence \\(\\mathfrak{M}\\) contains all Borel sets in \\(X\\). STEP VIII \\(\\mathfrak{M}_F\\) consists of precisely those sets \\(E \\in \\mathfrak{M}\\) for which \\(\\mu(E)\u0026lt;\\infty\\). This implies assertion \\((d)\\) of the theorem. Proof If \\(E \\in \\mathfrak{M}_F\\), Steps II and VI imply that \\(E \\cap K \\in \\mathfrak{M}_F\\) for every compact \\(K\\), hence \\(E \\in \\mathfrak{M}\\).\nConversely, suppose \\(E \\in \\mathfrak{M}\\) and \\(\\mu(E)\u0026lt;\\infty\\), and choose \\(\\epsilon\u0026gt;0\\). There is an open set \\(V \\supset E\\) with \\(\\mu(V)\u0026lt;\\infty\\); by III and V, there is a compact \\(K \\subset V\\) with \\(\\mu(V-K)\u0026lt;\\epsilon\\). Since \\(E \\cap K \\in \\mathfrak{M}_F\\), there is a compact set \\(H \\subset E \\cap K\\) with \\[ \\mu(E \\cap K)\u0026lt;\\mu(H)+\\epsilon . \\] Since \\(E \\subset(E \\cap K) \\cup(V-K)\\), it follows that \\[ \\mu(E) \\leq \\mu(E \\cap K)+\\mu(V-K)\u0026lt;\\mu(H)+2 \\epsilon, \\] which implies that \\(E \\in \\mathfrak{M}_F\\). STEP IX \\(\\mu\\) is a measure on \\(\\mathfrak{M}\\).\nProof The countable additivity of \\(\\mu\\) on \\(\\mathfrak{M}\\) follows immediately from Steps IV and VIII. STEP X For every \\(f \\in C_c(X), \\Lambda f=\\int_X f d \\mu\\). This proves \\((a)\\), and completes the theorem. Proof Clearly, it is enough to prove this for real \\(f\\). Also, it is enough to prove the inequality \\[ \\Lambda f \\leq \\int_{\\boldsymbol{X}} f d \\mu \\] for every real \\(f \\in C_c(X)\\). For once (16) is established, the linearity of \\(\\Lambda\\) shows that \\[ -\\Lambda f=\\Lambda(-f) \\leq \\int_X(-f) d \\mu=-\\int_X f d \\mu \\] which, together with (16), shows that equality holds in (16). Let \\(K\\) be the support of a real \\(f \\in C_c(X)\\), let \\([a, b]\\) be an interval which contains the range of \\(f\\) (note the Corollary to Theorem 2.10), choose \\(\\epsilon\u0026gt;0\\), and choose \\(y_i\\), for \\(i=0,1, \\ldots, n\\), so that \\(y_i-y_{i-1}\u0026lt;\\epsilon\\) and \\[ y_0\u0026lt;a\u0026lt;y_1\u0026lt;\\cdots\u0026lt;y_n=b . \\] Put \\[ E_i=\\left\\{x: y_{i-1}\u0026lt;f(x) \\leq y_i\\right\\} \\cap K \\quad(i=1, \\ldots, n) . \\] Since \\(f\\) is continuous, \\(f\\) is Borel measurable, and the sets \\(E_i\\) are therefore disjoint Borel sets whose union is \\(K\\). There are open sets \\(V_i \\supset E_i\\) such that \\[ \\mu\\left(V_i\\right)\u0026lt;\\mu\\left(E_i\\right)+\\frac{\\epsilon}{n} \\quad(i=1, \\ldots, n) \\] and such that \\(f(x)\u0026lt;y_i+\\epsilon\\) for all \\(x \\in V_i\\). By Theorem \\(2.13\\), there are functions \\(h_i \\prec V_i\\) such that \\(\\sum h_i=1\\) on \\(K\\). Hence \\(f=\\sum h_i f\\), and Step II shows that \\[ \\mu(K) \\leq \\Lambda\\left(\\sum h_i\\right)=\\sum \\Lambda h_i . \\]\nSince \\(h_i f \\leq\\left(y_i+\\epsilon\\right) h_i\\), and since \\(y_i-\\epsilon\u0026lt;f(x)\\) on \\(E_i\\), we have \\[ \\begin{aligned} \\Lambda f \u0026amp;=\\sum_{i=1}^n \\Lambda\\left(h_i f\\right) \\leq \\sum_{i=1}^n\\left(y_i+\\epsilon\\right) \\Lambda h_i \\\\ \u0026amp;=\\sum_{i=1}^n\\left(|a|+y_i+\\epsilon\\right) \\Lambda h_i-|a| \\sum_{i=1}^n \\Lambda h_i \\\\ \u0026amp; \\leq \\sum_{i=1}^n\\left(|a|+y_i+\\epsilon\\right)\\left[\\mu\\left(E_i\\right)+\\epsilon / n\\right]-|a| \\mu(K) \\\\ \u0026amp;=\\sum_{i=1}^n\\left(y_i-\\epsilon\\right) \\mu\\left(E_i\\right)+2 \\epsilon \\mu(K)+\\frac{\\epsilon}{n} \\sum_{i=1}^n\\left(|a|+y_i+\\epsilon\\right) \\\\ \u0026amp; \\leq \\int_X f d \\mu+\\epsilon[2 \\mu(K)+|a|+b+\\epsilon] \\end{aligned} \\] Since \\(\\epsilon\\) was arbitrary, (16) is established, and the proof of the theorem is complete.\n","date":"2022-10-04T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/2-the-riesz-representation-theorem/","section":"papa rudin","tags":null,"title":"2 The Riesz Representation Theorem"},{"categories":null,"contents":"2.1 Definition A complex vector space (or a vector space over the complex field) is a set \\(V\\), whose elements are called vectors and in which two operations, called addition and scalar multiplication, are defined, with the following familiar algebraic properties:\nTo every pair of vectors \\(x\\) and \\(y\\) there corresponds a vector \\(x+y\\), in such a way that \\(x+y=y+x\\) and \\(x+(y+z)=(x+y)+z\\); \\(V\\) contains a unique vector 0 (the zero vector or origin of \\(V\\) ) such that \\(x+0=x\\) for every \\(x \\in V\\); and to each \\(x \\in V\\) there corresponds a unique vector \\(-x\\) such that \\(x+(-x)=0\\).\nTo each pair \\((\\alpha, x)\\), where \\(x \\in V\\) and \\(\\alpha\\) is a scalar (in this context, the word scalar means complex number), there is associated a vector \\(\\alpha x \\in V\\), in such a way that \\(1 x=x, \\alpha(\\beta x)=(\\alpha \\beta) x\\), and such that the two distributive laws\n\\[ \\begin{equation} \\alpha(x+y)=\\alpha x+\\alpha y,(\\alpha+\\beta) x=\\alpha x+\\beta x \\end{equation} \\]\nhold. A linear transformation of a vector space \\(V\\) into a vector space \\(V_1\\) is a mapping \\(\\Lambda\\) of \\(V\\) into \\(V_1\\) such that\n\\[ \\begin{equation} \\Lambda(\\alpha x+\\beta y)=\\alpha \\Lambda x+\\beta \\Lambda y \\end{equation} \\]\nfor all \\(x\\) and \\(y \\in V\\) and for all scalars \\(\\alpha\\) and \\(\\beta\\). In the special case in which \\(V_1\\) is the field of scalars (this is the simplest example of a vector space, except for the trivial one consisting of 0 alone), \\(\\Lambda\\) is called a linear functional. A linear functional is thus a complex function on \\(V\\) which satisfies (2).\nNote that one often writes \\(\\Lambda x\\), rather than \\(\\Lambda(x)\\), if \\(\\Lambda\\) is linear.\nThe preceding definitions can of course be made equally well with any field whatsoever in place of the complex field. Unless the contrary is explicitly stated, however, all vector spaces occurring in this book will be complex, with one notable exception: the euclidean spaces \\(R^k\\) are vector spaces over the real field.\n2.2 Integration as a Linear Functional Analysis is full of vector spaces and linear transformations, and there is an especially close relationship between integration on the one hand and linear functionals on the other.\nFor instance, Theorem \\(1.32\\) shows that \\(L^1(\\mu)\\) is a vector space, for any positive measure \\(\\mu\\), and that the mapping\n\\[ \\begin{equation} f \\rightarrow \\int_{\\boldsymbol{X}} f d \\mu \\end{equation} \\]\nis a linear functional on \\(L^1(\\mu)\\). Similarly, if \\(g\\) is any bounded measurable function, the mapping\n\\[ \\begin{equation} f \\rightarrow \\int_x f g d \\mu \\end{equation} \\]\nis a linear functional on \\(L^1(\\mu)\\); we shall see in Chap. 6 that the functionals (2) are, in a sense, the only interesting ones on \\(L^1(\\mu)\\).\nFor another example, let \\(C\\) be the set of all continuous complex functions on the unit interval \\(I=[0,1]\\). The sum of the two continuous functions is continuous, and so is any scalar multiple of a continuous function. Hence \\(C\\) is a vector space, and if\n\\[ \\begin{equation} \\Lambda f=\\int_0^1 f(x) d x \\quad(f \\in C), \\end{equation} \\]\nthe integral being the ordinary Riemann integral, then \\(\\Lambda\\) is clearly a linear functional on \\(C ; \\Lambda\\) has an additional interesting property: it is a positive linear functional. This means that \\(\\Lambda f \\geq 0\\) whenever \\(f \\geq 0\\).\nOne of the tasks which is still ahead of us is the construction of the Lebesgue measure. The construction can be based on the linear functional (3), by the following observation: Consider a segment \\((a, b) \\subset I\\) and consider the class of all \\(f \\in C\\) such that \\(0 \\leq f \\leq 1\\) on \\(I\\) and \\(f(x)=0\\) for all \\(x\\) not in \\((a, b)\\). We have \\(\\Lambda f\u0026lt;b-a\\) for all such \\(f\\), but we can choose \\(f\\) so that \\(\\Lambda f\\) is as close to \\(b-a\\) as desired. Thus the length (or measure) of \\((a, b)\\) is intimately related to the values of the functional \\(\\Lambda\\).\nThe preceding observation, when looked at from a more general point of view, leads to a remarkable and extremely important theorem of \\(F\\). Riesz:\nTo every positive linear functional \\(\\Lambda\\) on \\(C\\) corresponds a finite positive Borel measure \\(\\mu\\) on I such that\n\\[ \\begin{equation} \\Lambda f=\\int_I f d \\mu \\quad(f \\in C) . \\end{equation} \\]\n[The converse is obvious: if \\(\\mu\\) is a finite positive Borel measure on \\(I\\) and if \\(\\Lambda\\) is defined by (4), then \\(\\Lambda\\) is a positive linear functional on \\(C\\).]\nIt is clearly of interest to replace the bounded interval \\(I\\) by \\(R^1\\). We can do this by restricting attention to those continuous functions on \\(R^1\\) which vanish outside some bounded interval. (These functions are Riemann integrable, for instance.) Next, functions of several variables occur frequently in analysis. Thus we ought to move from \\(R^1\\) to \\(R^n\\). It turns out that the proof of the Riesz theorem still goes through, with hardly any changes. Moreover, it turns out that the euclidean properties of \\(R^n\\) (coordinates, orthogonality, etc.) play no role in the proof; in fact, if one thinks of them too much they just get in the way. Essential to the proof are certain topological properties of \\(R^n\\). (Naturally. We are now dealing with continuous functions.) The crucial property is that of local compactness: Each point of \\(R^n\\) has a neighborhood whose closure is compact.\nWe shall therefore establish the Riesz theorem in a very general setting (Theorem 2.14). The existence of Lebesgue measure follows then as a special case. Those who wish to concentrate on a more concrete situation may skip lightly over the following section on topological preliminaries (Urysohn’s lemma is the item of greatest interest there; see Exercise 3) and may replace locally compact Hausdorff spaces by locally compact metric spaces, or even by euclidean spaces, without missing any of the principal ideas.\nIt should also be mentioned that there are situations, especially in probability theory, where measures occur naturally on spaces without topology, or on topological spaces that are not locally compact. An example is the so-called Wiener measure which assigns numbers to certain sets of continuous functions and which is a basic tool in the study of Brownian motion. These topics will not be discussed in this book.\n","date":"2022-10-03T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-2/0-vector-spaces/","section":"papa rudin","tags":null,"title":"0 Vector Spaces"},{"categories":null,"contents":"Problem 1 Let \\(t_1 = (x_3 - x_1)^2 + (y_3 - y_1)^2\\), \\(t_2 = (x_3 - x_2)^2 + (y_3 - y_2)^2\\), then \\(d_1 = \\sqrt{t_1}\\), \\(d_2 = \\sqrt{t_2}\\).\nThe problem formula as follow:\n\\[ \\begin{equation} \\begin{aligned} \\min \u0026amp; d_1 \\\\ \\textrm{s.t.} \u0026amp; \\quad d_1 = d_2 \\end{aligned} \\end{equation} \\]\nSince \\(t_1, t_2 \u0026gt; 0\\), so we can solve (2) problem instead (1).\n\\[ \\begin{equation} \\begin{aligned} \\min \u0026amp; \\quad t_1 \\\\ \\textrm{s.t.} \u0026amp; \\quad t_1 = t_2 \\end{aligned} \\end{equation} \\]\nDefine Hamiltonian function\n\\[ H = t_1 + λ(t_1 - t_2) \\]\nNecessary condition for optimization problem\n\\[ \\begin{equation} \\begin{aligned} \\frac{ ∂ H}{∂ λ} \u0026amp; =t_1 - t_2 = 0\\\\ \\frac{ ∂ H}{∂ x_3} \u0026amp; = 2(x_3 - x_1) + 2 λ(x_3 - x_1) - 2 λ (x_3 - x_2) = 0 \\\\ \\frac{ ∂ H}{∂ y_3} \u0026amp; = 2(y_3 - x_1) + 2 λ(y_3 - x_1) - 2 λ (y_3 - y_2) = 0 \\\\ \\end{aligned} \\end{equation} \\]\nSolving the equations, we get\n\\[ \\begin{equation} \\begin{aligned} λ = -\\frac{1}{2} \\\\ x_3 = (x_1 + x_2) / 2 \\\\ y_3 = (y_1 + y_2) / 2 \\\\ \\end{aligned} \\end{equation} \\]\nSo \\(P3 = (\\frac{x_1 + x_2}{2}, \\frac{y_1 + y_2}{2})\\)\nProblem 2 Formulation problems\n\\[ \\begin{equation} \\begin{aligned} \\min \u0026amp; \\quad L=\\frac{1}{2} x^{\\mathrm{T}}\\left[\\begin{array}{ll} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \\end{array}\\right] x+\\frac{1}{2} u^{\\mathrm{T}}\\left[\\begin{array}{ll} 2 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \\end{array}\\right] u \\\\ \\textrm {s.t.} \u0026amp; \\quad x=\\left[\\begin{array}{l} 1 \\\\ 3 \\end{array}\\right]+\\left[\\begin{array}{ll} 2 \u0026amp; 2 \\\\ 1 \u0026amp; 0 \\end{array}\\right] u \\end{aligned} \\end{equation} \\]\nDefine Hamiltonian function\n\\[ H = \\frac{1}{2} x^{\\mathrm{T}}\\left[\\begin{array}{ll} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \\end{array}\\right] x+\\frac{1}{2} u^{\\mathrm{T}}\\left[\\begin{array}{ll} 2 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \\end{array}\\right] u + λ^T (x - \\left[\\begin{array}{l} 1 \\\\ 3 \\end{array}\\right]+\\left[\\begin{array}{ll} 2 \u0026amp; 2 \\\\ 1 \u0026amp; 0 \\end{array}\\right] u) \\]\nNecessary conditions\n\\[ \\begin{equation} \\begin{array}{l} \\frac{ ∂ H}{∂ λ}=x-\\left[\\begin{array}{l} 1 \\\\ 3 \\end{array}\\right]- \\left[\\begin{array}{ll} 2 \u0026amp; 2 \\\\ 1 \u0026amp; 0 \\end{array}\\right] u=0 \\\\ \\frac{ ∂ H}{∂ x}=\\left[\\begin{array}{ll} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \\end{array}\\right] x+λ=0 \\\\ \\frac{ ∂ H}{∂ u}=\\left[\\begin{array}{ll} 4 \u0026amp; 2 \\\\ 2 \u0026amp; 2 \\end{array}\\right] u +\\left[\\begin{array}{ll} 2 \u0026amp; 1 \\\\ 2 \u0026amp; 0 \\end{array}\\right] λ=0 \\end{array} \\label{2_nec} \\end{equation} \\]\nSolve \\((\\ref{2_nec})\\) we get\n\\[ u=\\left[\\begin{array}{c} -2 \\\\ \\frac{8}{5} \\end{array}\\right] \\]\n\\[ x=\\left[\\begin{array}{r} -\\frac{1}{5} \\\\ 1 \\end{array}\\right] \\]\n\\[ \\lambda=\\left[\\begin{array}{l} \\frac{1}{5} \\\\ 2 \\end{array}\\right] \\]\n\\[ L = 1.02 + 2.08 = 3.1 \\]\nProblem 3 Formulation problem \\[ \\begin{array}{ll} \\min \u0026amp; J=\\frac{1}{2} \\sum_{k=0}^{N-1} u_{k}{ }^{2} \\\\ \\text { s.t. } \u0026amp; x_{k + 1}=x_{k} u_{k}+1 \\end{array} \\]\nWhere \\(N = 2\\), \\(x_2 = 0\\) and \\(x_0\\) is given.\nDefine Hamiltonian function\n\\[ \\begin{equation} H=\\frac{1}{2} \\sum_{k=0}^{N-1} u_{k}^{2}+ \\sum_{k=0}^{N-1} λ_{k+1} \\left(x_{k+1}-x_{k} u_{k}-1\\right) \\end{equation} \\]\nNecessary conditions\n\\[ \\frac{ ∂ H}{∂ λ_{k+1}}=x_{k+1}-x_{k} u_{k}-1=0 \\quad(k=0,1) \\]\n\\[ \\frac{ ∂ H}{∂ x_{k}}=λ_{k}-λ_{k+1} u_{k}=0 \\quad(k=1) \\] and define \\(λ_{0} = 0\\) for convenient\n\\[ \\frac{ ∂ H}{ ∂ u_{k}}=u_{k}-λ_{k+1} x_{k}=0 \\quad(k=0,1) \\]\nFrom necessary conditions we get state and costate equations\n\\[ λ_k = λ_{k+1}^2 x_k \\quad (k = 1) \\]\n\\[ x_{k+1} = λ_{k+1} x_k^2 + 1 k = (0, 1) \\]\nfrom state and costate function we get\n\\[ \\begin{aligned} λ_1 = λ_2^2 x_1 \\\\ x_2 = λ_2 x_1^2 + 1 \\\\ x_1 = λ_1 x_0^2 + 1 \\end{aligned} \\]\nThen we can \\(λ_1\\)\n\\[ \\begin{aligned} λ_1 = \\frac{ λ_2^2 } {1-λ_2^2 x_0^2 } \\\\ x_1 = \\frac{λ_1}{λ_2^2} = \\frac{1} {(1-λ_2^2 x_0 ^2) } \\\\ \\end{aligned} \\]\nThen we find \\(x_2 = \\frac{ λ_2 } {(1-λ_2^2 x_0 ^2)^2 } + 1\\) and we have equation \\(\\frac{ λ_2 } {(1-λ_2^2 x_0 ^2)^2 } + 1 = 0\\)\nIf \\(x_0 = 1\\), we have following equations:\n\\(λ_2 ≈ -1.49021\\) or \\(λ_2 ≈ -0.52488\\), \\(λ_1 = λ_2^2 / (1 - λ_2^2) = -1.819\\) or \\(0.380\\) \\(x_1 = (-1.819) / (-1.49021) / (-1.49021) = -0.81910\\) or \\(x_1 = 0.380 / -0.52488 / -0.52488 = 1.379\\)\n\\(u_0 = λ_1 = -1.819\\) or \\(0.380\\) \\(u_1 = λ_2 * x_1 = -1.49021 * -0.81910 = 1.2206\\) or $ -0.52488 * 1.379 = -0.72380$\nWe have \\(L = 4.79862\\) or \\(0.6682\\)\n0.6682 is the solution for the optimization problem.\n","date":"2022-09-27T00:00:00Z","permalink":"https://zongpitt.com/optimal-control/hw1/","section":"optimal control","tags":null,"title":"Optimal Control HW1"},{"categories":null,"contents":"As before, \\(\\mu\\) will in this section be a positive measure on an arbitrary measurable space \\(X\\).\n1.30 Definition We define \\(L^1(\\mu)\\) to be the collection of all complex measurable functions \\(f\\) on \\(X\\) for which\n\\[ \\begin{equation} \\int_X|f| d \\mu\u0026lt;\\infty . \\end{equation} \\]\nNote that the measurability of \\(f\\) implies that of \\(|f|\\), as we saw in Proposition \\(1.9(b)\\); hence the above integral is defined.\nThe members of \\(L^1(\\mu)\\) are called Lebesgue integrable functions (with respect to \\(\\mu\\) ) or summable functions. The significance of the exponent 1 will become clear in Chap. 3 .\n1.31 Definition If \\(f=u+i v\\), where \\(u\\) and \\(v\\) are real measurable functions on \\(X\\), and if \\(f \\in L^1(\\mu)\\), we define\n\\[ \\begin{equation} \\int_E f d \\mu=\\int_E u^{+} d \\mu-\\int_E u^{-} d \\mu+i \\int_E v^{+} d \\mu-i \\int_E v^{-} d \\mu \\label{eq: 1.31_1} \\end{equation} \\]\nfor every measurable set \\(E\\).\nHere \\(u^{+}\\)and \\(u^{-}\\)are the positive and negative parts of \\(u\\), as defined in Sec. \\(1.15 ; v^{+}\\)and \\(v^{-}\\)are similarly obtained from \\(v\\). These four functions are measurable, real, and nonnegative; hence the four integrals on the right of \\((\\ref{eq: 1.31_1})\\) exist, by Definition 1.23. Furthermore, we have \\(u^{+} \\leq|u|\u0026lt;|f|\\), etc., so that each of these four integrals is finite. Thus \\((\\ref{eq: 1.31_1})\\) defines the integral on the left as a complex number.\nOccasionally it is desirable to define the integral of a measurable function \\(f\\) with range in \\([-\\infty, \\infty]\\) to be\n\\[ \\begin{equation} \\int_E f d \\mu=\\int_E f^{+} d \\mu-\\int_E f^{-} d \\mu, \\label{eq: 1.31_2} \\end{equation} \\]\nprovided that at least one of the integrals on the right of \\((\\ref{eq: 1.31_2})\\) is finite. The left side of \\((\\ref{eq: 1.31_2})\\) is then a number in \\([-\\infty, \\infty]\\).\n1.32 Theorem Suppose \\(f\\) and \\(g \\in L^1(\\mu)\\) and \\(\\alpha\\) and \\(\\beta\\) are complex numbers. Then \\(\\alpha f+\\beta g \\in L^1(\\mu)\\), and\n\\[ \\begin{equation} \\int_X(\\alpha f+\\beta g) d \\mu=\\alpha \\int_X f d \\mu+\\beta \\int_X g d \\mu . \\label{1_32_1} \\end{equation} \\]\nProof The measurability of \\(\\alpha f+\\beta g\\) follows from Proposition 1.9(c). By Sec. \\(1.24\\) and Theorem 1.27,\n\\[ \\begin{equation} \\begin{aligned} \\int_X|\\alpha f+\\beta g| d \\mu \u0026amp; \\leq \\int_X(|\\alpha||f|+|\\beta||g|) d \\mu \\\\ \u0026amp;=|\\alpha| \\int_X|f| d \\mu+|\\beta| \\int_X|g| d \\mu\u0026lt;\\infty . \\end{aligned} \\label{1_32_2} \\end{equation} \\]\nThus \\(\\alpha f+\\beta g \\in L^1(\\mu)\\).\nTo prove \\((\\ref{1_32_1})\\), it is clearly sufficient to prove\n\\[ \\begin{equation} \\int_X(f+g) d \\mu=\\int_X f d \\mu+\\int_X g d \\mu \\label{1_32_3} \\end{equation} \\]\nand\n\\[ \\begin{equation} \\int_X(\\alpha f) d \\mu=\\alpha \\int_X f d \\mu, \\label{1_32_4} \\end{equation} \\]\nand the general case of \\((\\ref{1_32_3})\\) will follow if we prove \\((\\ref{1_32_3})\\) for real \\(f\\) and \\(g\\) in \\(L^1(\\mu)\\). Assuming this, and setting \\(h=f+g\\), we have\n\\[ \\begin{equation} h^{+}-h^{-}=f^{+}-f^{-}+g^{+}-g^{-} \\label{1_32_5} \\end{equation} \\]\nor\n\\[ \\begin{equation} h^{+}+f^{-}+g^{-}=f^{+}+g^{+}+h^{-} . \\label{1_32_6} \\end{equation} \\]\nBy Theorem 1.27,\n\\[ \\begin{equation} \\int h^{+}+\\int f^{-}+\\int g^{-}=\\int f^{+}+\\int g^{+}+\\int h^{-}, \\label{1_32_7} \\end{equation} \\]\nand since each of these integrals is finite, we may transpose and obtain \\((\\ref{1_32_3})\\).\nThat \\((\\ref{1_32_4})\\) holds if \\(\\alpha \\geq 0\\) follows from Proposition \\(1.24(c)\\). It is easy to verify that \\((\\ref{1_32_3})\\) holds if \\(\\alpha=-1\\), using relations like \\((-u)^{+}=u^{-}\\). The case \\(\\alpha=i\\) is also easy: If \\(f=u+i v\\), then\n\\[ \\begin{equation} \\begin{aligned} \\int(i f) \u0026amp;=\\int(i u-v)=\\int(-v)+i \\int u=-\\int v+i \\int u=i\\left(\\int u+i \\int v\\right) \\\\ \u0026amp;=i \\int f . \\end{aligned} \\label{1_32_8} \\end{equation} \\]\nCombining these cases with \\((\\ref{1_32_3})\\), we obtain \\((\\ref{1_32_4})\\) for any complex \\(\\alpha\\). \\(\\blacksquare\\)\n1.33 Theorem If \\(f \\in L^1(\\mu)\\), then\n\\[ \\begin{equation} \\left|\\int_X f d \\mu\\right| \\leq \\int_X|f| d \\mu . \\end{equation} \\]\nProof Put \\(z=\\int_X f d \\mu\\). Since \\(z\\) is a complex number, there is a complex number \\(\\alpha\\), with \\(|\\alpha|=1\\), such that \\(\\alpha z=|z|\\). Let \\(u\\) be the real part of \\(\\alpha f\\). Then \\(u \\leq|\\alpha f|=|f|\\). Hence\n\\[ \\begin{equation} \\left|\\int_X f d \\mu\\right|=\\alpha \\int_X f d \\mu=\\int_X \\alpha f d \\mu=\\int_X u d \\mu \\leq \\int_X|f| d \\mu . \\end{equation} \\]\nThe third of the above equalities holds since the preceding ones show that \\(\\int \\alpha f d \\mu\\) is real. \\(\\blacksquare\\)\nWe conclude this section with another important convergence theorem.\n1.34 Lebesgue’s Dominated Convergence Theorem Suppose \\(\\left\\{f_n\\right\\}\\) is a sequence of complex measurable functions on \\(X\\) such that\n\\[ \\begin{equation} f(x)=\\lim _{n \\rightarrow \\infty} f_n(x) \\end{equation} \\]\nexists for every \\(x \\in X\\). If there is a function \\(g \\in L^1(\\mu)\\) such that\n\\[ \\begin{equation} \\left|f_n(x)\\right| \\leq g(x) \\quad(n=1,2,3, \\ldots ; x \\in X), \\end{equation} \\]\nthen \\(f \\in L^1(\\mu)\\)\n\\[ \\begin{equation} \\lim _{n \\rightarrow \\infty} \\int_X\\left|f_n-f\\right| d \\mu=0 \\label{1_34_3} \\end{equation} \\]\nand\n\\[ \\begin{equation} \\lim _{n \\rightarrow \\infty} \\int_X f_n d \\mu=\\int_X f d \\mu . \\label{1_34_4} \\end{equation} \\]\nProof Since \\(|f| \\leq g\\) and \\(f\\) is measurable, \\(f \\in L^1(\\mu)\\). Since \\(\\left|f_n-f\\right| \\leq 2 g\\) Fatou’s lemma applies to the functions \\(2 g-\\left|f_n-f\\right|\\) and yields\n\\[ \\begin{equation} \\begin{aligned} \\int_X 2 g d \\mu \u0026amp; \\leq \\liminf _{n \\rightarrow \\infty} \\int_X\\left(2 g-\\left|f_n-f\\right|\\right) d \\mu \\\\ \u0026amp;=\\int_X 2 g d \\mu+\\liminf _{n \\rightarrow \\infty}\\left(-\\int_X\\left|f_n-f\\right| d \\mu\\right) \\\\ \u0026amp;=\\int_X 2 g d \\mu-\\operatorname{limiup}_{n \\rightarrow \\infty} \\int_X\\left|f_n-f\\right| d \\mu \\end{aligned} \\label{1_34_5} \\end{equation} \\]\nSince \\(\\int 2 g d \\mu\\) is finite, we may subtract it and obtain\n\\[ \\begin{equation} \\lim _{n \\rightarrow \\infty} \\sup _X\\left|f_n-f\\right| d \\mu \\leq 0 \\label{1_34_6} \\end{equation} \\]\nIf a sequence of nonnegative real numbers fails to converge to 0 , then its upper limit is positive. Thus \\((\\ref{1_34_6})\\) implies \\((\\ref{1_34_3})\\). By Theorem 1.33, applied to \\(f_n-f,(\\ref{1_34_3})\\) implies \\((\\ref{1_34_4})\\). \\(\\blacksquare\\)\n","date":"2022-09-24T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/7-integration-of-complex-functions/","section":"papa rudin","tags":null,"title":"7 Integration of Complex Functions"},{"categories":null,"contents":"1.35 Definition Let \\(P\\) be a property which a point \\(x\\) may or may not have. For instance, \\(P\\) might be the property ” \\(f(x)\u0026gt;0\\) ” if \\(f\\) is a given function, or it might be ” \\(\\left\\{f_n(x)\\right\\}\\) converges” if \\(\\left\\{f_n\\right\\}\\) is a given sequence of functions.\nIf \\(\\mu\\) is a measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) and if \\(E \\in \\mathfrak{M}\\), the statement “\\(P\\) holds almost everywhere on \\(E\\)” (abbreviated to “\\(P\\) holds a.e. on \\(E\\)”) means that there exists an \\(N \\in \\mathfrak{M}\\) such that \\(\\mu(N)=0, N \\subset E\\), and \\(P\\) holds at every point of \\(E-N\\). This concept of a.e. depends of course very strongly on the given measure, and we shall write “a.e. \\([\\mu]\\)” whenever clarity requires that the measure be indicated.\nFor example, if \\(f\\) and \\(g\\) are measurable functions and if\n\\[ \\begin{equation} \\mu(\\{x: f(x) \\neq g(x)\\})=0, \\label{eq_1_35_1} \\end{equation} \\]\nwe say that \\(f=g\\) a.e. \\([\\mu]\\) on \\(X\\), and we may write \\(f \\sim g\\). This is easily seen to be an equivalence relation. The transitivity \\((f \\sim g\\) and \\(g \\sim h\\) implies \\(f \\sim h)\\) is a consequence of the fact that the union of two sets of measure 0 has measure 0 .\nNote that if \\(f \\sim g\\), then, for every \\(E \\in \\mathfrak{M}\\),\n\\[ \\begin{equation} \\int_E f d \\mu=\\int_E g d \\mu . \\label{eq_1_35_2} \\end{equation} \\]\nTo see this, let \\(N\\) be the set which appears in \\((\\ref{eq_1_35_1})\\); then \\(E\\) is the union of the disjoint sets \\(E-N\\) and \\(E \\cap N\\); on \\(E-N, f=g\\), and \\(\\mu(E \\cap N)=0\\).\nThus, generally speaking, sets of measure 0 are negligible in integration. It ought to be true that every subset of a negligible set is negligible. But it may happen that some set \\(N \\in \\mathfrak{M}\\) with \\(\\mu(N)=0\\) has a subset \\(E\\) which is not a member of \\(\\mathfrak{M}\\). Of course we can define \\(\\mu(E)=0\\) in this case. But will this extension of \\(\\mu\\) still be a measure, i.e., will it still be defined on a \\(\\sigma\\)-algebra? It is a pleasant fact that the answer is affirmative:\n1.36 Theorem Let \\((X, \\mathfrak{M}, \\mu)\\) be a measure space, let \\(\\mathfrak{M}^*\\) be the collection of all \\(E \\subset X\\) for which there exist sets \\(A\\) and \\(B \\in \\mathfrak{M}\\) such that \\(A \\subset E \\subset B\\) and \\(\\mu(B-A)=0\\), and define \\(\\mu(E)=\\mu(A)\\) in this situation. Then \\(\\mathfrak{M}^*\\) is a \\(\\sigma\\)-algebra, and \\(\\mu\\) is a measure on \\(\\mathfrak{M}^*\\).\nThis extended measure \\(\\mu\\) is called complete, since all subsets of sets of measure 0 are now measurable; the \\(\\sigma\\)-algebra \\(\\mathfrak{M}^*\\) is called the \\(\\mu\\)-completion of \\(\\mathfrak{M}\\). The theorem says that every measure can be completed, so, whenever it is convenient, we may assume that any given measure is complete; this just gives us more measurable sets, hence more measurable functions. Most measures that one meets in the ordinary course of events are already complete, but there are exceptions; one of these will occur in the proof of Fubini’s theorem in Chap. 8.\nProof We begin by checking that \\(\\mu\\) is well defined for every \\(E \\in \\mathfrak{M}^*\\). Suppose \\(A \\subset E \\subset B, A_1 \\subset E \\subset B_1\\), and \\(\\mu(B-A)=\\mu\\left(B_1-A_1\\right)=0\\). (The letters \\(A\\) and \\(B\\) will denote members of \\(\\mathfrak{M}\\) throughout this proof.) Since\n\\[ A-A_1 \\subset E-A_1 \\subset B_1-A_1 \\]\nwe have \\(\\mu\\left(A-A_1\\right)=0\\), hence \\(\\mu(A)=\\mu\\left(A \\cap A_1\\right)\\). For the same reason, \\(\\mu\\left(A_1\\right)=\\mu\\left(A_1 \\cap A\\right)\\). We conclude that indeed \\(\\mu\\left(A_1\\right)=\\mu(A)\\).\nNext, let us verify that \\(\\mathfrak{M}^*\\) has the three defining properties of a \\(\\sigma\\) algebra.\n\\(X \\in \\mathfrak{M}^*\\), because \\(X \\in \\mathfrak{M}^{\\text {and }} \\mathfrak{M} \\subset \\mathfrak{M}^*\\). If \\(A \\subset E \\subset B\\) then \\(B^c \\subset E^c \\subset A^c\\). Thus \\(E \\in \\mathfrak{M}^*\\) implies \\(E^c \\in \\mathfrak{M}^*\\), because \\(A^c-B^c=A^c \\cap B=B-A\\) If \\(A_i \\subset E_i \\subset B_i, E=\\bigcup E_i, A=\\bigcup A_i, B=\\bigcup B_i\\), then \\(A \\subset E \\subset B\\) and \\[ B-A=\\bigcup_1^{\\infty}\\left(B_i-A\\right) \\subset \\bigcup_1^{\\infty}\\left(B_i-A_i\\right) \\]\nSince countable unions of sets of measure zero have measure zero, it follows that \\(E \\in \\mathfrak{M}^*\\) if \\(E_i \\in \\mathfrak{M}^*\\) for \\(i=1,2,3, \\ldots\\)\nFinally, if the sets \\(E_i\\) are disjoint in step (iii), the same is true of the sets \\(A_i\\), and we conclude that\n\\[ \\mu(E)=\\mu(A)=\\sum_1^{\\infty} \\mu\\left(A_i\\right)=\\sum_1^{\\infty} \\mu\\left(E_i\\right) \\]\nThis proves that \\(\\mu\\) is countably additive on \\(\\mathfrak{M}^*\\). \\(\\blacksquare\\)\n1.37 The fact that functions which are equal a.e. are indistinguishable as far as integration is concerned suggests that our definition of measurable function might profitably be enlarged. Let us call a function \\(f\\) defined on a set \\(E \\in \\mathfrak{M}\\) measurable on \\(X\\) if \\(\\mu\\left(E^c\\right)=0\\) and if \\(f^{-1}(V) \\cap E\\) is measurable for every open set \\(V\\). If we define \\(f(x)=0\\) for \\(x \\in E^c\\), we obtain a measurable function on \\(X\\), in the old sense. If our measure happens to be complete, we can define \\(f\\) on \\(E^c\\) in a perfectly arbitrary manner, and we still get a measurable function. The integral of \\(f\\) over any set \\(A \\in \\mathfrak{M}\\) is independent of the definition of \\(f\\) on \\(E^c\\); therefore this definition need not even be specified at all.\nThere are many situations where this occurs naturally. For instance, a function \\(f\\) on the real line may be differentiable only almost everywhere (with respect to Lebesgue measure), but under certain conditions it is still true that \\(f\\) is the integral of its derivative; this will be discussed in Chap. 7. Or a sequence \\(\\left\\{f_n\\right\\}\\) of measurable functions on \\(X\\) may converge only almost everywhere; with our new definition of measurability, the limit is still a measurable function on \\(X\\), and we do not have to cut down to the set on which convergence actually occurs.\nTo illustrate, let us state a corollary of Lebesgue’s dominated convergence theorem in a form in which exceptional sets of measure zero are admitted:\n\\(1.38\\) Theorem Suppose \\(\\left\\{f_n\\right\\}\\) is a sequence of complex measurable functions defined a.e. on \\(X\\) such that\n\\[ \\begin{equation} \\sum_{n=1}^{\\infty} \\int_X\\left|f_n\\right| d \\mu\u0026lt;\\infty \\label{eq_1_38_1} \\end{equation} \\]\nThen the series\n\\[ \\begin{equation} f(x)=\\sum_{n=1}^{\\infty} f_n(x) \\label{eq_1_38_2} \\end{equation} \\]\nconverges for almost all \\(x, f \\in L^1(\\mu)\\), and\n\\[ \\begin{equation} \\int_X f d \\mu=\\sum_{n=1}^{\\infty} \\int_X f_n d \\mu \\label{eq_1_38_3} \\end{equation} \\]\nProof Let \\(S_n\\) be the set on which \\(f_n\\) is defined, so that \\(\\mu\\left(S_n^c\\right)=0\\). Put \\(\\varphi(x)=\\) \\(\\sum\\left|f_n(x)\\right|\\), for \\(x \\in S=\\bigcap S_n\\). Then \\(\\mu\\left(S^c\\right)=0\\). By (1) and Theorem 1.27,\n\\[ \\begin{equation} \\int_s \\varphi d \\mu\u0026lt;\\infty . \\label{eq_1_38_4} \\end{equation} \\]\nIf \\(E=\\{x \\in S: \\varphi(x)\u0026lt;\\infty\\}\\), it follows from \\((\\ref{eq_1_38_4})\\) that \\(\\mu\\left(E^c\\right)=0\\). The series \\((\\ref{eq_1_38_2})\\) converges absolutely for every \\(x \\in E\\), and if \\(f(x)\\) is defined by \\((\\ref{eq_1_38_2})\\) for \\(x \\in E\\), then \\(|f(x)| \\leq \\varphi(x)\\) on \\(E\\), so that \\(f \\in L^1(\\mu)\\) on \\(E\\), by \\((\\ref{eq_1_38_4})\\). If \\(g_n=f_1+\\cdots+f_n\\), then \\(\\left|g_n\\right| \\leq \\varphi, g_n(x) \\rightarrow f(x)\\) for all \\(x \\in E\\), and Theorem \\(1.34\\) gives \\((\\ref{eq_1_38_3})\\) with \\(E\\) in place of \\(X\\). This is equivalent to \\((\\ref{eq_1_38_3})\\), since \\(\\mu\\left(E^c\\right)=0\\). \\(\\blacksquare\\)\n","date":"2022-09-24T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/8-the-role-played-by-sets-of-measure-zero/","section":"papa rudin","tags":null,"title":"8 The Role Played by Sets of Measure Zero"},{"categories":null,"contents":"1.18 Definition A positive measure is a function \\(\\mu\\), defined on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\), whose range is in \\([0, \\infty]\\) and which is countably additive. This means that if \\(\\left\\{A_i\\right\\}\\) is a disjoint countable collection of members of \\(\\mathfrak{M}\\), then \\[ \\begin{equation} \\mu\\left(\\bigcup_{i=1}^{\\infty} A_i\\right)=\\sum_{i=1}^{\\infty} \\mu\\left(A_i\\right) . \\end{equation} \\]\nTo avoid trivialities, we shall also assume that \\(\\mu(A)\u0026lt;\\infty\\) for at least one \\(A \\in \\mathfrak{M}\\).\nA measure space is a measurable space which has a positive measure defined on the \\(\\sigma\\)-algebra of its measurable sets.\nA complex measure is a complex-valued countably additive function defined on a \\(\\sigma\\)-algebra.\nNote: What we have called a positive measure is frequently just called a measure; w add the word “positive” for emphasis. If \\(\\mu(E)=0\\) for every \\(E \\in \\mathfrak{M}\\), then \\(\\mu\\) is a positive measure, by our definition. The value \\(\\infty\\) is admissible for a positive measure; but when we talk of a complex measure \\(\\mu\\), it is understood that \\(\\mu(E)\\) is a complex number, for every \\(E \\in \\mathfrak{M}\\). The real measures form a subclass of the complex ones, of course.\n1.19 Theorem Let \\(\\mu\\) be a positive measure on a \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\). Then\n\\(\\mu(\\varnothing)=0\\). \\(\\mu\\left(A_1 \\cup \\cdots \\cup A_n\\right)=\\mu\\left(A_1\\right)+\\cdots+\\mu\\left(A_n\\right)\\) if \\(A_1, \\ldots, A_n\\) are pairwise disjoint members of \\(\\mathfrak{M}\\). \\(A \\subset B\\) implies \\(\\mu(A) \\leq \\mu(B)\\) if \\(A \\in \\mathfrak{M}, B \\in \\mathfrak{M}\\). \\(\\mu\\left(A_n\\right) \\rightarrow \\mu(A)\\) as \\(n \\rightarrow \\infty\\) if \\(A=\\bigcup_{n=1}^{\\infty} A_n, A_n \\in \\mathfrak{M}\\), and \\[ A_1 \\subset A_2 \\subset A_3 \\subset \\cdots . \\] \\(\\mu\\left(A_n\\right) \\rightarrow \\mu(A)\\) as \\(n \\rightarrow \\infty\\) if \\(A=\\bigcap_{n=1}^{\\infty} A_n, A_n \\in \\mathfrak{M}\\), \\[ A_1 \\supset A_2 \\supset A_3 \\supset \\cdots \\text {, } \\] and \\(\\mu\\left(A_1\\right)\\) is finite. As the proof will show, these properties, with the exception of \\((c)\\), also hold for complex measures; \\((b)\\) is called finite additivity; \\((c)\\) is called monotonicity.\nProof\nTake \\(A \\in \\mathfrak{M}\\) so that \\(\\mu(A)\u0026lt;\\infty\\), and take \\(A_1=A\\) and \\(A_2=A_3=\\cdots=\\) \\(\\varnothing\\) in \\(1.18(1)\\). Take \\(A_{n+1}=A_{n+2}=\\cdots=\\varnothing\\) in \\(1.18(1)\\). Since \\(B=A \\cup(B-A)\\) and \\(A \\cap(B-A)=\\varnothing\\), we see that (b) implies \\(\\mu(B)=\\mu(A)+\\mu(B-A) \\geq \\mu(A)\\). Put \\(B_1=A_1\\), and put \\(B_n=A_n-A_{n-1}\\) for \\(n=2,3,4, \\ldots\\) Then \\(B_n \\in \\mathfrak{M}\\), \\(B_i \\cap B_j=\\varnothing\\) if \\(i \\neq j, A_n=B_1 \\cup \\cdots \\cup B_n\\), and \\(A=\\bigcup_{i=1}^{\\infty} B_i\\). Hence \\[ \\mu\\left(A_n\\right)=\\sum_{i=1}^n \\mu\\left(B_i\\right) \\quad \\text { and } \\mu(A)=\\sum_{i=1}^{\\infty} \\mu\\left(B_i\\right) . \\] Now \\((d)\\) follows, by the definition of the sum of an infinite series. Put \\(C_n=A_1-A_n\\). Then \\(C_1 \\subset C_2 \\subset C_3 \\subset \\cdots\\), \\[ \\mu\\left(C_n\\right)=\\mu\\left(A_1\\right)-\\mu\\left(A_n\\right), \\] \\(A_1-A=\\bigcup C_n\\), and so \\((d)\\) shows that \\[ \\mu\\left(A_1\\right)-\\mu(A)=\\mu\\left(A_1-A\\right)=\\lim _{n \\rightarrow \\infty} \\mu\\left(C_n\\right)=\\mu\\left(A_1\\right)-\\lim _{n \\rightarrow \\infty} \\mu\\left(A_n\\right) . \\] This implies \\((e)\\). \\(\\blacksquare\\) 1.20 Examples The construction of interesting measure spaces requires some labor, as we shall see. However, a few simple-minded examples can be given immediately:\nFor any \\(E \\subset X\\), where \\(X\\) is any set, define \\(\\mu(E)=\\infty\\) if \\(E\\) is an infinite set, and let \\(\\mu(E)\\) be the number of points in \\(E\\) if \\(E\\) is finite. This \\(\\mu\\) is called the counting measure on \\(X\\).\nFix \\(x_0 \\in X\\), define \\(\\mu(E)=1\\) if \\(x_0 \\in E\\) and \\(\\mu(E)=0\\) if \\(x_0 \\notin E\\), for any \\(E \\subset X\\). This \\(\\mu\\) may be called the unit mass concentrated at \\(x_0\\).\nLet \\(\\mu\\) be the counting measure on the set \\(\\{1,2,3, \\ldots\\}\\), let \\(A_n=\\{n, n+1\\), \\(n+2, \\ldots\\}\\). Then \\(\\bigcap A_n=\\varnothing\\) but \\(\\mu\\left(A_n\\right)=\\infty\\) for \\(n=1,2,3, \\ldots\\) This shows that the hypothesis\n\\[ \\begin{equation} \\mu\\left(A_1\\right)\u0026lt;\\infty \\end{equation} \\]\nis not superfluous in Theorem \\(1.19(e)\\).\n1.21 A Comment on Terminology One frequently sees measure spaces referred to as “ordered triples” \\((X, \\mathfrak{M}, \\mu)\\) where \\(X\\) is a set, \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra in \\(X\\), and \\(\\mu\\) is a measure defined on \\(\\mathfrak{M}\\). Similarly, measurable spaces are “ordered pairs.” \\((X, \\mathfrak{M})\\).\nThis is logically all right, and often convenient, though somewhat redundant. For instance, in \\((X, \\mathfrak{M})\\) the set \\(X\\) is merely the largest member of \\(\\mathfrak{M}\\), so if we know \\(\\mathfrak{M}\\) we also know \\(X\\). Similarly, every measure has a \\(\\sigma\\)-algebra for its domain, by definition, so if we know a measure \\(\\mu\\) we also know the \\(\\sigma\\)-algebra \\(\\mathfrak{M}\\) on which \\(\\mu\\) is defined and we know the set \\(X\\) in which \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra.\nIt is therefore perfectly legitimate to use expressions like “Let \\(\\mu\\) be a measure”or, if we wish to emphasize the \\(\\sigma\\)-algebra or the set in question, to say “Let \\(\\mu\\) be a measure on \\(\\mathfrak{M}\\)” or “Let \\(\\mu\\) be a measure on \\(X\\).”\nWhat is logically rather meaningless but customary (and we shall often follow mathematical custom rather than logic) is to say “Let \\(X\\) be a measure space”; the emphasis should not be on the set, but on the measure. Of course, when this wording is used, it is tacitly understood that there is a measure defined on some \\(\\sigma\\)-algebra in \\(X\\) and that it is this measure which is really under discussion.\nSimilarly, a topological space is an ordered pair \\((X, \\tau)\\), where \\(\\tau\\) is a topology in the set \\(X\\), and the significant data are contained in \\(\\tau\\), not in \\(X\\), but “the topological space \\(X\\)” is what one talks about.\nThis sort of tacit convention is used throughout mathematics. Most mathematical systems are sets with some class of distinguished subsets or some binary operations or some relations (which are required to have certain properties), and one can list these and then describe the system as an ordered pair, triple, etc., depending on what is needed. For instance, the real line may be described as a quadruple \\(\\left(R^1,+, \\cdot,\u0026lt;\\right)\\), where \\(+, \\cdot\\), and \\(\u0026lt;\\) satisfy the axioms of a complete archimedean ordered field. But it is a safe bet that very few mathematicians think of the real field as an ordered quadruple.\n","date":"2022-09-23T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/4-elementary-properties-of-measures/","section":"papa rudin","tags":null,"title":"4 Elementary Properties of Measures"},{"categories":null,"contents":"1.22 Throughout integration theory, one inevitably encounters \\(\\infty\\). One reason is that one wants to be able to integrate over sets of infinite measure; after all, the real line has infinite length. Another reason is that even if one is primarily interested in real-valued functions, the lim sup of a sequence of positive real functions or the sum of a sequence of positive real functions may well be \\(\\infty\\) at some points, and much of the elegance of theorems like \\(1.26\\) and \\(1.27\\) would be lost if one had to make some special provisions whenever this occurs. Let us define \\(a+\\infty=\\infty+a=\\infty\\) if \\(0 \\leq a \\leq \\infty\\), and\n\\[ \\begin{equation} a \\cdot \\infty=\\infty \\cdot a= \\begin{cases}\\infty \u0026amp; \\text { if } 0\u0026lt;a \\leq \\infty \\\\ 0 \u0026amp; \\text { if } a=0\\end{cases} \\end{equation} \\]\nsums and products of real numbers are of course defined in the usual way. It may seem strange to define \\(0 \\cdot \\infty=0\\). However, one verifies without difficulty that with this definition the commutative, associative, and distributive laws hold in \\([0, \\infty]\\) without any restriction.\nThe cancellation laws have to be treated with some care: \\(a+b=a+c\\) implies \\(b=c\\) only when \\(a\u0026lt;\\infty\\), and \\(a b=a c\\) implies \\(b=c\\) only when \\(0\u0026lt;a\u0026lt;\\infty\\). Observe that the following useful proposition holds:\nIf \\(0 \\leq a_1 \\leq a_2 \\leq \\cdots, 0 \\leq b_1 \\leq b_2 \\leq \\cdots, a_n \\rightarrow a\\), and \\(b_n \\rightarrow b\\), then \\(a_n b_n \\rightarrow a b\\)\nIf we combine this with Theorems \\(1.17\\) and \\(1.14\\), we see that sums and products of measurable functions into \\([0, \\infty]\\) are measurable.\n","date":"2022-09-23T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/5-arithmetic-in-0-inf/","section":"papa rudin","tags":null,"title":"5 Arithmetic in [0, ∞]"},{"categories":null,"contents":"In this section, \\(\\mathfrak{M}\\) will be a \\(\\sigma\\)-algebra in a set \\(X\\) and \\(\\mu\\) will be a positive measure on \\(\\mathfrak{M}\\).\n1.23 Definition If \\(s: X \\rightarrow[0, \\infty)\\) is a measurable simple function, of the form\n\\[ \\begin{equation} s=\\sum_{i=1}^n \\alpha_i \\chi_{A_i}, \\end{equation} \\]\nwhere \\(\\alpha_1, \\ldots, \\alpha_n\\) are the distinct values of \\(s\\) (compare Definition 1.16), and if \\(E \\in \\mathfrak{M}\\), we define\n\\[ \\begin{equation} \\int_E s d \\mu=\\sum_{i=1}^n \\alpha_i \\mu\\left(A_i \\cap E\\right) . \\end{equation} \\]\nThe convention \\(0 \\cdot \\infty=0\\) is used here; it may happen that \\(\\alpha_i=0\\) for some \\(i\\) and that \\(\\mu\\left(A_i \\cap E\\right)=\\infty\\).\nIf \\(f: X \\rightarrow[0, \\infty]\\) is measurable, and \\(E \\in \\mathfrak{M}\\), we define \\[ \\begin{equation} \\int_E f d \\mu=\\sup \\int_E s d \\mu, \\end{equation} \\] the supremum being taken over all simple measurable functions \\(s\\) such that \\(0 \\leq s \\leq f\\).\nThe left member of (3) is called the Lebesgue integral of \\(f\\) over \\(E\\), with respect to the measure \\(\\mu\\). It is a number in \\([0, \\infty]\\).\nObserve that we apparently have two definitions for \\(\\int_E f d \\mu\\) if \\(f\\) is simple, namely, (2) and (3). However, these assign the same value to the integral, since \\(f\\) is, in this case, the largest of the functions \\(s\\) which occur on the right of (3).\n1.24 The following propositions are immediate consequences of the definitions. The functions and sets occurring in them are assumed to be measurable:\nIf \\(0 \\leq f \\leq g\\), then \\(\\int_E f d \\mu \\leq \\int_E g d \\mu\\). If \\(A \\subset B\\) and \\(f \\geq 0\\), then \\(\\int_A f d \\mu \\leq \\int_B f d \\mu\\). Iff \\(\\geq 0\\) and \\(c\\) is a constant, \\(0 \\leq c\u0026lt;\\infty\\), then \\[ \\int_E c f d \\mu=c \\int_E f d \\mu . \\] If \\(f(x)=0\\) for all \\(x \\in E\\), then \\(\\int_E f d \\mu=0\\), even if \\(\\mu(E)=\\infty\\). If \\(\\mu(E)=0\\), then \\(\\int_E f d \\mu=0\\), even if \\(f(x)=\\infty\\) for every \\(x \\in E\\). If \\(f \\geq 0\\), then \\(\\int_E f d \\mu=\\int_X \\chi_E f d \\mu\\). This last result shows that we could have restricted our definition of integration to integrals over all of \\(X\\), without losing any generality. If we wanted to integrate over subsets, we could then use \\((f)\\) as the definition. It is purely a matter of taste which definition is preferred.\nOne may also remark here that every measurable subset \\(E\\) of a measure space \\(X\\) is again a measure space, in a perfectly natural way: The new measurable sets are simply those measurable subsets of \\(X\\) which lie in \\(E\\), and the measure is unchanged, except that its domain is restricted. This shows again that as soon as we have integration defined over every measure space, we automatically have it defined over every measurable subset of every measure space.\n1.25 Proposition Let \\(s\\) and \\(t\\) be nonnegative measurable simple functions on \\(X\\). For \\(E \\in \\mathfrak{M}\\), define\n\\[ \\begin{equation} \\varphi(E)=\\int_E s d \\mu . \\end{equation} \\]\nThen \\(\\varphi\\) is a measure on \\(\\mathfrak{M}\\). Also\n\\[ \\int_X(s+t) d \\mu=\\int_X s d \\mu+\\int_X t d \\mu . \\] (This proposition contains provisional forms of Theorems \\(1.27\\) and 1.29.)\nProof If \\(s\\) is as in Definition \\(1.23\\), and if \\(E_1, E_2, \\ldots\\) are disjoint members of \\(\\mathfrak{M}\\) whose union is \\(E\\), the countable additivity of \\(\\mu\\) shows that\n\\[ \\begin{equation} \\begin{aligned} \\varphi(E) \u0026amp;=\\sum_{i=1}^n \\alpha_i \\mu\\left(A_i \\cap E\\right)=\\sum_{i=1}^n \\alpha_i \\sum_{r=1}^{\\infty} \\mu\\left(A_i \\cap E_r\\right) \\\\ \u0026amp;=\\sum_{r=1}^{\\infty} \\sum_{i=1}^n \\alpha_i \\mu\\left(A_i \\cap E_r\\right)=\\sum_{r=1}^{\\infty} \\varphi\\left(E_r\\right) . \\end{aligned} \\end{equation} \\]\nAlso, \\(\\varphi(\\varnothing)=0\\), so that \\(\\varphi\\) is not identically \\(\\infty\\).\nNext, let \\(s\\) be as before, let \\(\\beta_1, \\ldots, \\beta_m\\) be the distinct values of \\(t\\), and let \\(B_j=\\left\\{x: t(x)=\\beta_j\\right\\}\\). If \\(E_{i j}=A_i \\cap B_j\\), then and\n\\[ \\begin{equation} \\int_{E_{i j}}(s+t) d \\mu=\\left(\\alpha_i+\\beta_j\\right) \\mu\\left(E_{i j}\\right) \\\\ \\end{equation} \\]\nand\n\\[ \\begin{equation} \\int_{E_{i j}} s d \\mu+\\int_{E_{i j}} t d \\mu=\\alpha_i \\mu\\left(E_{i j}\\right)+\\beta_j \\mu\\left(E_{i j}\\right) . \\end{equation} \\]\nThus (2) holds with \\(E_{i j}\\) in place of \\(X\\). Since \\(X\\) is the disjoint union of the sets \\(E_{i j}(1 \\leq i \\leq n, 1 \\leq j \\leq m)\\), the first half of our proposition implies that (2) holds. \\(\\blacksquare\\)\nWe now come to the interesting part of the theory. One of its most remarkable features is the ease with which it handles limit operations.\n1.26 Lebesgue’s Monotone Convergence Theorem Let \\(\\left\\{f_n\\right\\}\\) be a sequence of measurable functions on \\(X\\), and suppose that\n\\(0 \\leq f_1(x) \\leq f_2(x) \\leq \\cdots \\leq \\infty\\) for every \\(x \\in X\\), \\(f_n(x) \\rightarrow f(x)\\) as \\(n \\rightarrow \\infty\\), for every \\(x \\in X\\). Then \\(f\\) is measurable, and\n\\[ \\begin{equation} \\int_X f_n d \\mu \\rightarrow \\int_X f d \\mu \\quad \\text { as } n \\rightarrow \\infty . \\end{equation} \\]\nProof Since \\(\\int f_n \\leq \\int f_{n+1}\\), there exists an \\(\\alpha \\in[0, \\infty]\\) such that\n\\[ \\begin{equation} \\int_X f_n d \\mu \\rightarrow \\alpha \\text { as } n \\rightarrow \\infty \\text {. } \\end{equation} \\]\nBy Theorem \\(1.14, f\\) is measurable. Since \\(f_n \\leq f\\), we have \\(\\int f_n \\leq \\int f\\) for every \\(n\\), so (1) implies\n\\[ \\begin{equation} \\alpha \\leq \\int_X f d \\mu . \\end{equation} \\]\nLet \\(s\\) be any simple measurable function such that \\(0 \\leq s \\leq f\\), let \\(c\\) be a constant, \\(0\u0026lt;c\u0026lt;1\\), and define\n\\[ \\begin{equation} E_n=\\left\\{x: f_n(x) \\geq c s(x)\\right\\} \\quad(n=1,2,3, \\ldots) . \\end{equation} \\]\nEach \\(E_n\\) is measurable, \\(E_1 \\subset E_2 \\subset E_3 \\subset \\cdots\\), and \\(X=\\bigcup E_n\\). To see this equality, consider some \\(x \\in X\\). If \\(f(x)=0\\), then \\(x \\in E_1 ;\\) if \\(f(x)\u0026gt;0\\), then \\(c s(x)\u0026lt;f(x)\\), since \\(c\u0026lt;1\\); hence \\(x \\in E_n\\) for some \\(n\\). Also\n\\[ \\begin{equation} \\int_X f_n d \\mu \\geq \\int_{E_n} f_n d \\mu \\geq c \\int_{E_n} s d \\mu \\quad(n=1,2,3, \\ldots) . \\end{equation} \\]\nLet \\(n \\rightarrow \\infty\\), applying Proposition \\(1.25\\) and Theorem \\(1.19(d)\\) to the last integral in (4). The result is\n\\[ \\begin{equation} \\alpha \\geq c \\int_X s d \\mu . \\end{equation} \\]\nSince (5) holds for every \\(c\u0026lt;1\\), we have\n\\[ \\begin{equation} \\alpha \\geq \\int_X s d \\mu \\end{equation} \\]\nfor every simple measurable \\(s\\) satisfying \\(0 \\leq s \\leq f\\), so that\n\\[ \\begin{equation} \\alpha \\geq \\int_x f d \\mu . \\end{equation} \\]\nThe theorem follows from (1), (2), and (7).\n1.27 Theorem If \\(f_n: X \\rightarrow[0, \\infty]\\) is measurable, for \\(n=1,2,3, \\ldots\\), and\n\\[ \\begin{equation} f(x)=\\sum_{n=1}^{\\infty} f_n(x) \\quad(x \\in X), \\end{equation} \\]\nthen\n\\[ \\begin{equation} \\int_X f d \\mu=\\sum_{n=1}^{\\infty} \\int_X f_n d \\mu \\end{equation} \\]\nProof First, there are sequences \\(\\left\\{s_i^{\\prime}\\right\\},\\left\\{s_i^{\\prime \\prime}\\right\\}\\) of simple measurable functions such that \\(s_i^{\\prime} \\rightarrow f_1\\) and \\(s_i^{\\prime \\prime} \\rightarrow f_2\\), as in Theorem 1.17. If \\(s_i=s_i^{\\prime}+s_i^{\\prime \\prime}\\), then \\(s_i \\rightarrow f_1+f_2\\), and the monotone convergence theorem, combined with Proposition \\(1.25\\), shows that\n\\[ \\begin{equation} \\int_X\\left(f_1+f_2\\right) d \\mu=\\int_X f_1 d \\mu+\\int_X f_2 d \\mu . \\end{equation} \\]\nNext, put \\(g_N=f_1+\\cdots+f_N\\). The sequence \\(\\left\\{g_N\\right\\}\\) converges monotonically to \\(f\\), and if we apply induction to (3) we see that\n\\[ \\begin{equation} \\int_X g_N d \\mu=\\sum_{n=1}^N \\int_X f_n d \\mu . \\end{equation} \\]\nApplying the monotone convergence theorem once more, we obtain (2), and the proof is complete. \\(\\blacksquare\\)\nIf we let \\(\\mu\\) be the counting measure on a countable set, Theorem \\(1.27\\) is a statement about double series of nonnegative real numbers (which can of course be proved by more elementary means):\nCorollary If \\(a_{i j} \\geq 0\\) for \\(i\\) and \\(j=1,2,3, \\ldots\\), then\n\\[ \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} a_{i j}=\\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} a_{i j} . \\]\n1.28 Fatou’s Lemma If \\(f_n: X \\rightarrow[0, \\infty]\\) is measurable, for each positive integer \\(n\\), then\n\\[ \\begin{equation} \\int_X\\left(\\liminf _{n \\rightarrow \\infty} f_n\\right) d \\mu \\leq \\liminf _{n \\rightarrow \\infty} \\int_X f_n d \\mu . \\end{equation} \\]\nStrict inequality can occur in (1); see Exercise 8.\nProof Put \\[ \\begin{equation} g_k(x)=\\inf _{i \\geq k} f_i(x) \\quad(k=1,2,3, \\ldots ; x \\in X) . \\end{equation} \\]\nThen \\(g_k \\leq f_k\\), so that\n\\[ \\begin{equation} \\int_X g_k d \\mu \\leq \\int_X f_k d \\mu \\quad(k=1,2,3, \\ldots) . \\end{equation} \\]\nAlso, \\(0 \\leq g_1 \\leq g_2 \\leq \\cdots\\), each \\(g_k\\) is measurable, by Theorem 1.14, and \\(g_k(x) \\rightarrow \\lim \\inf f_n(x)\\) as \\(k \\rightarrow \\infty\\), by Definition 1.13. The monotone convergence theorem shows therefore that the left side of (3) tends to the left side of (1), as \\(k \\rightarrow \\infty\\). Hence (1) follows from (3). \\(\\blacksquare\\)\n1.29 Theorem Suppose \\(f: X \\rightarrow[0, \\infty]\\) is measurable, and\n\\[ \\begin{equation} \\varphi(E)=\\int_E f d \\mu \\quad(E \\in \\mathfrak{M}) . \\end{equation} \\]\nThen \\(\\varphi\\) is a measure on \\(\\mathfrak{M}\\), and\n\\[ \\begin{equation} \\int_X g d \\varphi=\\int_X g f d \\mu \\end{equation} \\]\nfor every measurable \\(g\\) on \\(X\\) with range in \\([0, \\infty]\\). Proof Let \\(E_1, E_2, E_3, \\ldots\\) be disjoint members of \\(\\mathfrak{M}\\) whose union is \\(E\\). Observe that\n\\[ \\begin{equation} \\chi_{E} f=\\sum_{j=1}^{\\infty} \\chi_{E_j} f \\end{equation} \\]\nand that\n\\[ \\begin{equation} \\varphi(E)=\\int_X \\chi_E f d \\mu, \\quad \\varphi\\left(E_j\\right)=\\int_X \\chi_{E_j} f d \\mu . \\end{equation} \\]\nIt now follows from Theorem \\(1.27\\) that\n\\[ \\begin{equation} \\varphi(E)=\\sum_{j=1}^{\\infty} \\varphi\\left(E_j\\right) . \\end{equation} \\]\nSince \\(\\varphi(\\varnothing)=0,(5)\\) proves that \\(\\varphi\\) is a measure.\nNext, (1) shows that (2) holds whenever \\(g=\\chi_E\\) for some \\(E \\in \\mathfrak{M}\\). Hence (2) holds for every simple measurable function \\(g\\), and the general case follows from the monotone convergence theorem.\nRemark The second assertion of Theorem \\(1.29\\) is sometimes written in the form\n\\[ \\begin{equation} d \\varphi=f d \\mu . \\end{equation} \\]\nWe assign no independent meaning to the symbols \\(d \\varphi\\) and \\(d \\mu ;(6)\\) merely means that (2) holds for every measurable \\(g \\geq 0\\).\nTheorem 1.29 has a very important converse, the Radon-Nikodym theorem, which will be proved in Chap. 6.\n","date":"2022-09-23T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/6-integration-of-positive-function/","section":"papa rudin","tags":null,"title":"6 Integration of Positive Functions"},{"categories":null,"contents":"11.12 Definition Suppose \\(X\\) is a set, not necessarily a subset of a euclidean space, or indeed of any metric space. \\(X\\) is said to be a measure space if there exists a \\(\\sigma\\)-ring \\(\\mathfrak{M}\\) of subsets of \\(X\\) (which are called measurable sets) and a nonnegative countably additive set function \\(\\mu\\) (which is called a measure), defined on \\(\\mathfrak{M}\\).\nIf, in addition, \\(X \\in \\mathfrak{M}\\), then \\(X\\) is said to be a measurable space.\nFor instance, we can take \\(X=R^{p}, \\mathfrak{M}\\) the collection of all Lebesguemeasurable subsets of \\(R^{p}\\), and \\(\\mu\\) Lebesgue measure.\nOr, let \\(X\\) be the set of all positive integers, \\(M i\\) the collection of all subsets of \\(X\\), and \\(\\mu(E)\\) the number of elements of \\(E\\).\nAnother example is provided by probability theory, where events may be considered as sets, and the probability of the occurrence of events is an additive (or countably additive) set function.\nIn the following sections we shall always deal with measurable spaces. It should be emphasized that the integration theory which we shall soon discuss would not become simpler in any respect if we sacrificed the generality we have now attained and restricted ourselves to Lebesgue measure, say, on an interval of the real line. In fact, the essential features of the theory are brought out with much greater clarity in the more general situation, where it is seen that everything depends only on the countable additivity of \\(\\mu\\) on a \\(\\sigma\\)-ring.\nIt will be convenient to introduce the notation\n\\[ \\{x \\mid P\\} \\]\nfor the set of all elements \\(x\\) which have the property \\(P\\).\n11.13 Definition Let \\(f\\) be a function defined on the measurable space \\(X\\), with values in the extended real number system. The function \\(f\\) is said to be measurable if the set\n\\[ \\{x \\mid f(x)\u0026gt;a\\} \\]\nis measurable for every real \\(a\\).\n11.14 Example If \\(X=R^{p}\\) and \\(\\mathfrak{M}=\\mathfrak{M}(\\mu)\\) as defined in Definition 11.9, every continuous \\(f\\) is measurable, since then (42) is an open set. 11.15 Theorem Each of the following four conditions implies the other three:\n\\[ \\begin{aligned} \u0026amp;\\{x \\mid f(x)\u0026gt;a\\} \\text { is measurable for every real } a . \\\\ \u0026amp;\\{x \\mid f(x) \\geq a\\} \\text { is measurable for every real } a . \\\\ \u0026amp;\\{x \\mid f(x)\u0026lt;a\\} \\text { is measurable for every real a. } \\\\ \u0026amp;\\{x \\mid f(x) \\leq a\\} \\text { is measurable for every real } a . \\end{aligned} \\]\nProof The relations\n\\[ \\begin{aligned} \u0026amp;\\{x \\mid f(x) \\geq a\\}=\\bigcap_{n=1}^{\\infty}\\left\\{x \\mid f(x)\u0026gt;a-\\frac{1}{n}\\right\\} \\\\ \u0026amp;\\{x \\mid f(x)\u0026lt;a\\}=X-\\{x \\mid f(x) \\geq a\\} \\\\ \u0026amp;\\{x \\mid f(x) \\leq a\\}=\\bigcap_{n=1}^{\\infty}\\left\\{x \\mid f(x)\u0026lt;a+\\frac{1}{n}\\right\\} \\\\ \u0026amp;\\{x \\mid f(x)\u0026gt;a\\}=X-\\{x \\mid f(x) \\leq a\\} \\end{aligned} \\]\nshow successively that (43) implies (44), (44) implies (45), (45) implies (46), and (46) implies (43).\nHence any of these conditions may be used instead of (42) to define measurability.\n\\[ \\{x|| f(x) \\mid\u0026lt;a\\}=\\{x \\mid f(x)\u0026lt;a\\} \\cap\\{x \\mid f(x)\u0026gt;-a\\} . \\]\n11.17 Theorem Let \\(\\left\\{f_{n}\\right\\}\\) be a sequence of measurable functions. For \\(x \\in X\\), put\n\\[ \\begin{aligned} \u0026amp;g(x)=\\sup _{n}(x) \\quad(n=1,2,3, \\ldots), \\\\ \u0026amp;h(x)=\\limsup _{n \\rightarrow \\infty} f_{n}(x) . \\end{aligned} \\]\nThen \\(g\\) and \\(h\\) are measurable.\nThe same is of course true of the inf and lim inf.\n\\[ \\begin{aligned} \\{x \\mid g(x)\u0026gt;a\\} \u0026amp;=\\bigcup_{n=1}^{\\infty}\\left\\{x \\mid f_{n}(x)\u0026gt;a\\right\\}, \\\\ h(x) \u0026amp;=\\inf g_{m}(x), \\end{aligned} \\]\nwhere \\(g_{m}(x)=\\sup f_{n}(x)(n \\geq m)\\).\nIff and \\(g\\) are measurable, then \\(\\max (f, g)\\) and \\(\\min (f, g)\\) are measurable. If \\[ f^{+}=\\max (f, 0), \\quad f^{-}=-\\min (f, 0), \\]\nit follows, in particular, that \\(f^{+}\\)and \\(f^{-}\\)are measurable.\nThe limit of a convergent sequence of measurable functions is measurable. 11.18 Theorem Let \\(f\\) and \\(g\\) be measurable real-valued functions defined on \\(X\\), let \\(F\\) be real and continuous on \\(R^{2}\\), and put\n\\[ h(x)=F(f(x), g(x)) \\quad(x \\in X) . \\]\nThen \\(h\\) is measurable.\nIn particular, \\(f+g\\) and \\(f g\\) are measurable.\nProof Let\n\\[ G_{a}=\\{(u, v) \\mid F(u, v)\u0026gt;a\\} . \\]\nThen \\(G_{a}\\) is an open subset of \\(R^{2}\\), and we can write\n\\[ G_{a}=\\bigcup_{n=1}^{\\infty} I_{n}, \\]\nwhere \\(\\left\\{I_{n}\\right\\}\\) is a sequence of open intervals:\n\\[ I_{n}=\\left\\{(u, v) \\mid a_{n}\u0026lt;u\u0026lt;b_{n}, c_{n}\u0026lt;v\u0026lt;d_{n}\\right\\} . \\]\nSince\n\\[ \\left\\{x \\mid a_{n}\u0026lt;f(x)\u0026lt;b_{n}\\right\\}=\\left\\{x \\mid f(x)\u0026gt;a_{n}\\right\\} \\cap\\left\\{x \\mid f(x)\u0026lt;b_{n}\\right\\} \\]\nis measurable, it follows that the set\n\\[ \\left\\{x \\mid(f(x), g(x)) \\in I_{n}\\right\\}=\\left\\{x \\mid a_{n}\u0026lt;f(x)\u0026lt;b_{n}\\right\\} \\cap\\left\\{x \\mid c_{n}\u0026lt;g(x)\u0026lt;d_{n}\\right\\} \\]\nis measurable. Hence the same is true of\n\\[ \\begin{aligned} \\{x \\mid h(x)\u0026gt;a\\} \u0026amp;=\\left\\{x \\mid(f(x), g(x)) \\in G_{a}\\right\\} \\\\ \u0026amp;=\\bigcup_{n=1}^{\\infty}\\left\\{x \\mid(f(x), g(x)) \\in I_{n}\\right\\} . \\end{aligned} \\]\nSumming up, we may say that all ordinary operations of analysis, including limit operations, when applied to measurable functions, lead to measurable functions; in other words, all functions that are ordinarily met with are measurable.\nThat this is, however, only a rough statement is shown by the following example (based on Lebesgue measure, on the real line): If \\(h(x)=f(g(x))\\), where \\(f\\) is measurable and \\(g\\) is continuous, then \\(h\\) is not necessarily measurable. (For the details, we refer to McShane, page 241.)\nThe reader may have noticed that measure has not been mentioned in our discussion of measurable functions. In fact, the class of measurable functions on \\(X\\) depends only on the \\(\\sigma\\)-ring \\(\\mathfrak{M}\\) (using the notation of Definition 11.12). For instance, we may speak of Borel-measurable functions on \\(R^{p}\\), that is, of function \\(f\\) for which\n\\[ \\{x \\mid f(x)\u0026gt;a\\} \\]\nis always a Borel set, without reference to any particular measure.\n11.19 Definition Let \\(s\\) be a real-valued function defined on \\(X\\). If the range of \\(s\\) is finite, we say that \\(s\\) is a simple function.\nLet \\(E \\subset X\\), and put\n\\[ K_{E}(x)= \\begin{cases}1 \u0026amp; (x \\in E), \\\\ 0 \u0026amp; (x \\notin E) .\\end{cases} \\]\n\\(K_{E}\\) is called the characteristic function of \\(E\\).\nSuppose the range of \\(s\\) consists of the distinct numbers \\(c_{1}, \\ldots, c_{n}\\). Let\n\\[ E_{i}=\\left\\{x \\mid s(x)=c_{i}\\right\\} \\quad(i=1, \\ldots, n) . \\]\nThen\n\\[ s=\\sum_{n=1}^{n} c_{i} K_{E_{i}}, \\]\nthat is, every simple function is a finite linear combination of characteristic functions. It is clear that \\(s\\) is measurable if and only if the sets \\(E_{1}, \\ldots, E_{n}\\) are measurable.\nIt is of interest that every function can be approximated by simple functions:\n11.20 Theorem Let \\(f\\) be a real function on \\(X\\). There exists a sequence \\(\\left\\{s_{n}\\right\\}\\) of simple functions such that \\(s_{n}(x) \\rightarrow f(x)\\) as \\(n \\rightarrow \\infty\\), for every \\(x \\in X\\). If \\(f\\) is measurable, \\(\\left\\{s_{n}\\right\\}\\) may be chosen to be a sequence of measurable functions. If \\(f \\geq 0,\\left\\{s_{n}\\right\\}\\) may be chosen to be a monotonically increasing sequence.\nProof If \\(f \\geq 0\\), define\n\\[ E_{n i}=\\left\\{x \\mid \\frac{i-1}{2^{n}} \\leq f(x)\u0026lt;\\frac{i}{2^{n}}\\right\\}, \\quad F_{n}=\\{x \\mid f(x) \\geq n\\} \\]\nfor \\(n=1,2,3, \\ldots, i=1,2, \\ldots, n 2^{n}\\). Put\n\\[ s_{n}=\\sum_{i=1}^{n 2^{n}} \\frac{i-1}{2^{n}} K_{E_{n i}}+n K_{F_{n}} . \\]\nIn the general case, let \\(f=f^{+}-f^{-}\\), and apply the preceding construction to \\(f^{+}\\)and to \\(f^{-}\\).\nIt may be noted that the sequence \\(\\left\\{s_{n}\\right\\}\\) given by (50) converges uniformly to \\(f\\) if \\(f\\) is bounded.\nWe shall define integration on a measurable space \\(X\\), in which \\(\\mathfrak{M}\\) is the \\(\\sigma\\)-ring of measurable sets, and \\(\\mu\\) is the measure. The reader who wishes to visualize a more concrete situation may think of \\(X\\) as the real line, or an interval, and of \\(\\mu\\) as the Lebesgue measure \\(m\\).\n\\[ s(x)=\\sum_{i=1}^{n} c_{i} K_{E_{i}}(x) \\quad\\left(x \\in X, c_{i}\u0026gt;0\\right) \\]\nis measurable, and suppose \\(E \\in \\mathfrak{M}\\). We define\n\\[ I_{E}(s)=\\sum_{i=1}^{n} c_{i} \\mu\\left(E \\cap E_{i}\\right) . \\]\nIf \\(f\\) is measurable and nonnegative, we define\n\\[ \\int_{E} f d \\mu=\\sup I_{E}(s), \\]\nwhere the sup is taken over all measurable simple functions \\(s\\) such that \\(0 \\leq s \\leq f\\).\nThe left member of (53) is called the Lebesgue integral of \\(f\\), with respect to the measure \\(\\mu\\), over the set \\(E\\). It should be noted that the integral may have the value \\(+\\infty\\)\nIt is easily verified that\n\\[ \\int_{E} s d \\mu=I_{E}(s) \\]\nfor every nonnegative simple measurable function \\(s\\).\n11.22 Definition Let \\(f\\) be measurable, and consider the two integrals\n\\[ \\int_{E} f^{+} d \\mu, \\quad \\int_{E} f^{-} d \\mu, \\]\nwhere \\(f^{+}\\)and \\(f^{-}\\)are defined as in (47). If at least one of the integrals (55) is finite, we define\n\\[ \\int_{\\boldsymbol{E}} f d \\mu=\\int_{\\boldsymbol{E}} f^{+} d \\mu-\\int_{\\boldsymbol{E}} f^{-} d \\mu . \\]\nIf both integrals in (55) are finite, then (56) is finite, and we say that \\(f\\) is integrable (or summable) on \\(E\\) in the Lebesgue sense, with respect to \\(\\mu\\); we write \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\). If \\(\\mu=m\\), the usual notation is: \\(f \\in \\mathscr{L}\\) on \\(E\\).\nThis terminology may be a little confusing: If (56) is \\(+\\infty\\) or \\(-\\infty\\), then the integral of \\(f\\) over \\(E\\) is defined, although \\(f\\) is not integrable in the above sense of the word; \\(f\\) is integrable on \\(E\\) only if its integral over \\(E\\) is finite.\nWe shall be mainly interested in integrable functions, although in some cases it is desirable to deal with the more general situation.\n11.23 Remarks The following properties are evident:\nIf \\(f\\) is measurable and bounded on \\(E\\), and if \\(\\mu(E)\u0026lt;+\\infty\\), then \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\)\nIf \\(a \\leq f(x) \\leq b\\) for \\(x \\in E\\), and \\(\\mu(E)\u0026lt;+\\infty\\), then\n\\[ a \\mu(E) \\leq \\int_{E} f d \\mu \\leq b \\mu(E) \\]\nIf \\(f\\) and \\(g \\in \\mathscr{L}(\\mu)\\) on \\(E\\), and if \\(f(x) \\leq g(x)\\) for \\(x \\in E\\), then \\[ \\int_{E} f d \\mu \\leq \\int_{E} g d \\mu . \\]\nIf \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\), then \\(c f \\in \\mathscr{L}(\\mu)\\) on \\(E\\), for every finite constant \\(c\\), and \\[ \\int_{E} c f d \\mu=c \\int_{E} f d \\mu . \\]\nIf \\(\\mu(E)=0\\), and \\(f\\) is measurable, then \\[ \\int_{E} f d \\mu=0 . \\]\nIf \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E, A \\in \\mathfrak{P}\\), and \\(A \\subset E\\), then \\(f \\in \\mathscr{L}(\\mu)\\) on \\(A\\). Suppose \\(f\\) is measurable and nonnegative on \\(X\\). For \\(A \\in \\mathfrak{M}\\), define \\[ \\phi(A)=\\int_{A} f d \\mu . \\]\nThen \\(\\phi\\) is countably additive on \\(\\mathfrak{M}\\).\nThe same conclusion holds if \\(f \\in \\mathscr{L}(\\mu)\\) on \\(X\\). Proof It is clear that \\((b)\\) follows from \\((a)\\) if we write \\(f=f^{+}-f^{-}\\)and apply (a) to \\(f^{+}\\)and to \\(f^{-}\\).\nTo prove \\((a)\\), we have to show that\n\\[ \\phi(A)=\\sum_{n=1}^{\\infty} \\phi\\left(A_{n}\\right) \\]\nif \\(A_{n} \\in \\mathfrak{M}(n=1,2,3, \\ldots), A_{i} \\cap A_{j}=0\\) for \\(i \\neq j\\), and \\(A=\\bigcup_{1}^{\\infty} A_{n}\\).\nIf \\(f\\) is a characteristic function, then the countable additivity of \\(\\phi\\) is precisely the same as the countable additivity of \\(\\mu\\), since\n\\[ \\int_{A} K_{E} d \\mu=\\mu(A \\cap E) . \\] holds.\nIf \\(f\\) is simple, then \\(f\\) is of the form (51), and the conclusion again\nIn the general case, we have, for every measurable simple function \\(s\\) such that \\(0 \\leq s \\leq f\\)\n\\[ \\int_{A} s d \\mu=\\sum_{n=1}^{\\infty} \\int_{A_{n}} s d \\mu \\leq \\sum_{n=1}^{\\infty} \\phi\\left(A_{n}\\right) \\]\nTherefore, by (53),\n\\[ \\phi(A) \\leq \\sum_{n=1}^{\\infty} \\phi\\left(A_{n}\\right) \\]\nNow if \\(\\phi\\left(A_{n}\\right)=+\\infty\\) for some \\(n\\), (58) is trivial, since \\(\\phi(A) \\geq \\phi\\left(A_{n}\\right)\\). Suppose \\(\\phi\\left(A_{n}\\right)\u0026lt;+\\infty\\) for every \\(n\\).\nGiven \\(\\varepsilon\u0026gt;0\\), we can choose a measurable function \\(s\\) such that \\(0 \\leq s \\leq f\\), and such that\n\\[ \\int_{A_{1}} s d \\mu \\geq \\int_{A_{1}} f d \\mu-\\varepsilon, \\quad \\int_{A_{2}} s d \\mu \\geq \\int_{A_{2}} f d \\mu-\\varepsilon . \\]\nHence\n\\[ \\phi\\left(A_{1} \\cup A_{2}\\right) \\geq \\int_{A_{1} \\cup A_{2}} s d \\mu=\\int_{A_{1}} s d \\mu+\\int_{A_{2}} s d \\mu \\geq \\phi\\left(A_{1}\\right)+\\phi\\left(A_{2}\\right)-2 \\varepsilon, \\]\nso that\n\\[ \\phi\\left(A_{1} \\cup A_{2}\\right) \\geq \\phi\\left(A_{1}\\right)+\\phi\\left(A_{2}\\right) \\]\nIt follows that we have, for every \\(n\\),\n\\[ \\phi\\left(A_{1} \\cup \\cdots \\cup A_{n}\\right) \\geq \\phi\\left(A_{1}\\right)+\\cdots+\\phi\\left(A_{n}\\right) \\text {. } \\]\nSince \\(A \\supset A_{1} \\cup \\cdots \\cup A_{n}\\), (61) implies\n\\[ \\phi(A) \\geq \\sum_{n=1}^{\\infty} \\phi\\left(A_{n}\\right) \\]\nand (58) follows from (59) and (62).\nCorollary If \\(A \\in \\mathfrak{M}, B \\in \\mathfrak{M}, B \\subset A\\), and \\(\\mu(A-B)=0\\), then\n\\[ \\int_{\\boldsymbol{A}} f d \\mu=\\int_{\\boldsymbol{B}} f d \\mu . \\]\nSince \\(A=B \\cup(A-B)\\), this follows from Remark 11.23(e).\n11.25 Remarks The preceding corollary shows that sets of ineasure zero are negligible in integration.\nLet us write \\(f \\sim g\\) on \\(E\\) if the set\n\\[ \\{x \\mid f(x) \\neq g(x)\\} \\cap E \\]\nhas measure zero.\nThen \\(f \\sim f ; f \\sim g\\) implies \\(g \\sim f ;\\) and \\(f \\sim g, g \\sim h\\) implies \\(f \\sim h\\). That is, the relation \\(\\sim\\) is an equivalence relation.\nIf \\(f \\sim g\\) on \\(E\\), we clearly have\n\\[ \\int_{A} f d \\mu=\\int_{A} g d \\mu \\]\nprovided the integrals exist, for every measurable subset \\(A\\) of \\(E\\).\nIf a property \\(P\\) holds for every \\(x \\in E-A\\), and if \\(\\mu(A)=0\\), it is customary to say that \\(P\\) holds for almost all \\(x \\in E\\), or that \\(P\\) holds almost everywhere on \\(E\\). (This concept of “almost everywhere” depends of course on the particular measure under consideration. In the literature, unless something is said to the contrary, it usually refers to Lebesgue measure.)\nIf \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\), it is clear that \\(f(x)\\) must be finite almost everywhere on \\(E\\). In most cases we therefore do not lose any generality if we assume the given functions to be finite-valued from the outset.\n11.26 Theorem Iff \\(\\in \\mathscr{L}(\\mu)\\) on \\(E\\), then \\(|f| \\in \\mathscr{L}(\\mu)\\) on \\(E\\), and\n\\[ \\left|\\int_{E} f d \\mu\\right| \\leq \\int_{E}|f| d \\mu . \\]\nProof Write \\(E=A \\cup B\\), where \\(f(x) \\geq 0\\) on \\(A\\) and \\(f(x)\u0026lt;0\\) on \\(B\\). By Theorem 11.24,\n\\[ \\int_{E}|f| d \\mu=\\int_{A}|f| d \\mu+\\int_{B}|f| d \\mu=\\int_{A} f^{+} d \\mu+\\int_{B} f^{-} d \\mu\u0026lt;+\\infty \\]\nso that \\(|f| \\in \\mathscr{L}(\\mu)\\). Since \\(f \\leq|f|\\) and \\(-f \\leq|f|\\), we see that\n\\[ \\int_{E} f d \\mu \\leq \\int_{E}|f| d \\mu, \\quad-\\int_{E} f d \\mu \\leq \\int_{E}|f| d \\mu, \\]\nand \\((63)\\) follows.\nSince the integrability of \\(f\\) implies that of \\(|f|\\), the Lebesgue integral is often called an absolutely convergent integral. It is of course possible to define nonabsolutely convergent integrals, and in the treatment of some problems it is essential to do so. But these integrals lack some of the most useful properties of the Lebesgue integral and play a somewhat less important role in analysis.\n11.27 Theorem Suppose \\(f\\) is measurable on \\(E,|f| \\leq g\\), and \\(g \\in \\mathscr{L}(\\mu)\\) on \\(E\\). Then \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\).\nProof We have \\(f^{+} \\leq g\\) and \\(f^{-} \\leq g\\).\n11.28 Lebesgue’s monotone convergence theorem Suppose \\(E \\in \\mathfrak{M}\\). Let \\(\\left\\{f_{n}\\right\\}\\) be a sequence of measurable functions such that\n\\[ 0 \\leq f_{1}(x) \\leq f_{2}(x) \\leq \\cdots \\quad(x \\in E) . \\]\nLet \\(f\\) be defined by\n\\[ f_{n}(x) \\rightarrow f(x) \\quad(x \\in E) \\]\nas \\(n \\rightarrow \\infty\\). Then\n\\[ \\int_{E} f_{n} d \\mu \\rightarrow \\int_{E} f d \\mu \\quad(n \\rightarrow \\infty) . \\]\nProof By (64) it is clear that, as \\(n \\rightarrow \\infty\\),\n\\[ \\int_{E} f_{n} d \\mu \\rightarrow \\alpha \\]\nfor some \\(\\alpha\\); and since \\(\\int f_{n} \\leq \\int f\\), we have\n\\[ \\alpha \\leq \\int_{E} f d \\mu . \\]\nChoose \\(c\\) such that \\(0\u0026lt;c\u0026lt;1\\), and let \\(s\\) be a simple measurable function such that \\(0 \\leq s \\leq f\\). Put\n\\[ E_{n}=\\left\\{x \\mid f_{n}(x) \\geq c s(x)\\right\\} \\quad(n=1,2,3, \\ldots) \\text {. } \\]\nBy (64), \\(E_{1} \\subset E_{2} \\subset E_{3} \\subset \\cdots ;\\) and by (65),\n\\[ E=\\bigcup_{n=1}^{\\infty} E_{n} \\text {. } \\]\nFor every \\(n\\),\n\\[ \\int_{E} f_{n} d \\mu \\geq \\int_{E_{n}} f_{n} d \\mu \\geq c \\int_{E_{n}} s d \\mu . \\]\nWe let \\(n \\rightarrow \\infty\\) in (70). Since the integral is a countably additive set function (Theorem 11.24), (69) shows that we may apply Theorem \\(11.3\\) to the last integral in (70), and we obtain\n\\[ \\alpha \\geq c \\int_{E} s d \\mu \\]\nLetting \\(c \\rightarrow 1\\), we see that\n\\[ \\alpha \\geq \\int_{E} s d \\mu \\]\nand (53) implies\n\\[ \\alpha \\geq \\int_{E} f d \\mu . \\]\nThe theorem follows from (67), (68), and (72).\n11.29 Theorem Suppose \\(f=f_{1}+f_{2}\\), where \\(f_{i} \\in \\mathscr{L}(\\mu)\\) on \\(E(i=1,2)\\). Then \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\), and\n\\[ \\int_{E} f d \\mu=\\int_{E} f_{1} d \\mu+\\int_{E} f_{2} d \\mu . \\]\nProof First, suppose \\(f_{1} \\geq 0, f_{2} \\geq 0\\). If \\(f_{1}\\) and \\(f_{2}\\) are simple, (73) follows trivially from (52) and (54). Otherwise, choose monotonically increasing sequences \\(\\left\\{s_{n}^{\\prime}\\right\\},\\left\\{s_{n}^{\\prime \\prime}\\right\\}\\) of nonnegative measurable simple functions which converge to \\(f_{1}, f_{2}\\). Theorem \\(11.20\\) shows that this is possible. Put \\(s_{n}=s_{n}^{\\prime}+s_{n}^{\\prime \\prime}\\). Then\n\\[ \\int_{E} s_{n} d \\mu=\\int_{E} s_{n}^{\\prime} d \\mu+\\int_{E} s_{n}^{\\prime \\prime} d \\mu, \\]\nand (73) follows if we let \\(n \\rightarrow \\infty\\) and appeal to Theorem 11.28. Next, suppose \\(f_{1} \\geq 0, f_{2} \\leq 0\\). Put\n\\[ A=\\{x \\mid f(x) \\geq 0\\}, \\quad B=\\{x \\mid f(x)\u0026lt;0\\} . \\]\nThen \\(f, f_{1}\\), and \\(-f_{2}\\) are nonnegative on \\(A\\). Hence\n\\[ \\int_{A} f_{1} d \\mu=\\int_{A} f d \\mu+\\int_{A}\\left(-f_{2}\\right) d \\mu=\\int_{A} f d \\mu-\\int_{A} f_{2} d \\mu . \\]\nSimilarly, \\(-f, f_{1}\\), and \\(-f_{2}\\) are nonnegative on \\(B\\), so that\n\\[ \\int_{B}\\left(-f_{2}\\right) d \\mu=\\int_{B} f_{1} d \\mu+\\int_{B}(-f) d \\mu, \\]\nor\n\\[ \\int_{B} f_{1} d \\mu=\\int_{B} f d \\mu-\\int_{B} f_{2} d \\mu, \\]\nand (73) follows if we add (74) and (75).\nIn the general case, \\(E\\) can be decomposed into four sets \\(E_{i}\\) on each of which \\(f_{1}(x)\\) and \\(f_{2}(x)\\) are of constant sign. The two cases we have proved so far imply\n\\[ \\int_{E_{i}} f d \\mu=\\int_{E_{i}} f_{1} d \\mu+\\int_{E_{i}} f_{2} d \\mu \\quad(i=1,2,3,4), \\]\nand (73) follows by adding these four equations.\nWe are now in a position to reformulate Theorem \\(11.28\\) for series.\n11.30 Theorem Suppose \\(E \\in \\mathfrak{M}\\). If \\(\\left\\{f_{n}\\right\\}\\) is a sequence of nonnegative measurable functions and\n\\[ f(x)=\\sum_{n=1}^{\\infty} f_{n}(x) \\quad(x \\in E), \\]\nthen\n\\[ \\int_{E} f d \\mu=\\sum_{n=1}^{\\infty} \\int_{E} f_{n} d \\mu . \\]\nProof The partial sums of (76) form a monotonically increasing sequence.\n11.31 Fatou’s theorem Suppose \\(E \\in \\mathfrak{M}\\). If \\(\\left\\{f_{n}\\right\\}\\) is a sequence of nonnegative measurable functions and\n\\[ f(x)=\\liminf _{n \\rightarrow \\infty} f_{n}(x) \\quad(x \\in E), \\]\nthen\n\\[ \\int_{E} f d \\mu \\leq \\liminf _{n \\rightarrow \\infty} \\int_{E} f_{n} d \\mu \\]\nStrict inequality may hold in (77). An example is given in Exercise \\(5 .\\)\nProof For \\(n=1,2,3, \\ldots\\) and \\(x \\in E\\), put\n\\[ g_{n}(x)=\\inf f_{i}(x) \\quad(i \\geq n) . \\]\nThen \\(g_{n}\\) is measurable on \\(E\\), and\n\\[ \\begin{aligned} 0 \u0026amp; \\leq g_{1}(x) \\leq g_{2}(x) \\leq \\cdots, \\\\ g_{n}(x) \u0026amp; \\leq f_{n}(x), \\\\ g_{n}(x) \u0026amp; \\rightarrow f(x) \\quad(n \\rightarrow \\infty) . \\end{aligned} \\]\nBy (78), (80), and Theorem \\(11.28\\),\n\\[ \\int_{E} g_{n} d \\mu \\rightarrow \\int_{E} f d \\mu, \\]\nso that (77) follows from (79) and (81).\n11.32 Lebesgue’s dominated convergence theorem Suppose \\(E \\in \\mathfrak{M}\\). Let \\(\\left\\{f_{n}\\right\\}\\) be a sequence of measurable functions such that\n\\[ f_{n}(x) \\rightarrow f(x) \\quad(x \\in E) \\]\nas \\(n \\rightarrow \\infty\\). If there exists a function \\(g \\in \\mathscr{L}(\\mu)\\) on \\(E\\), such that\n\\[ \\left|f_{n}(x)\\right| \\leq g(x) \\quad(n=1,2,3, \\ldots, x \\in E), \\]\nthen\n\\[ \\lim _{n \\rightarrow \\infty} \\int_{E} f_{n} d \\mu=\\int_{E} f d \\mu . \\]\nBecause of \\((83),\\left\\{f_{n}\\right\\}\\) is said to be dominated by \\(g\\), and we talk about dominated convergence. By Remark \\(11.25\\), the conclusion is the same if (82) holds almost everywhere on \\(E\\).\nProof First, (83) and Theorem \\(11.27\\) imply that \\(f_{n} \\in \\mathscr{L}(\\mu)\\) and \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\).\nSince \\(f_{n}+g \\geq 0\\), Fatou’s theorem shows that\n\\[ \\int_{E}(f+g) d \\mu \\leq \\liminf _{n \\rightarrow \\infty} \\int_{E}\\left(f_{n}+g\\right) d \\mu, \\]\nor\n\\[ \\int_{E} f d \\mu \\leq \\liminf _{n \\rightarrow \\infty} \\int_{E} f_{n} d \\mu . \\]\nSince \\(g-f_{n} \\geq 0\\), we see similarly that\n\\[ \\int_{E}(g-f) d \\mu \\leq \\liminf _{n \\rightarrow \\infty} \\int_{E}\\left(g-f_{n}\\right) d \\mu, \\]\nso that\n\\[ -\\int_{E} f d \\mu \\leq \\liminf _{n \\rightarrow \\infty}\\left[-\\int_{E} f_{n} d \\mu\\right], \\]\nwhich is the same as\n\\[ \\int_{E} f d \\mu \\geq \\lim \\sup _{n \\rightarrow \\infty} \\int_{E} f d \\mu . \\]\nThe existence of the limit in (84) and the equality asserted by (84) now follow from (85) and (86).\nCorollary If \\(\\mu(E)\u0026lt;+\\infty,\\left\\{f_{n}\\right\\}\\) is uniformly bounded on \\(E\\), and \\(f_{n}(x) \\rightarrow f(x)\\) on \\(E\\), then (84) holds.\nA uniformly bounded convergent sequence is often said to be boundedly convergent.\nOur next theorem will show that every function which is Riemann-integrable on an interval is also Lebesgue-integrable, and that Riemann-integrable functions are subject to rather stringent continuity conditions. Quite apart from the fact that the Lebesgue theory therefore enables us to integrate a much larger class of functions, its greatest advantage lies perhaps in the ease with which many limit operations can be handled; from this point of view, Lebesgue’s convergence theorems may well be regarded as the core of the Lebesgue theory.\nOne of the difficulties which is encountered in the Riemann theory is that limits of Riemann-integrable functions (or even continuous functions) may fail to be Riemann-integrable. This difficulty is now almost eliminated, since limits of measurable functions are always measurable.\nLet the measure space \\(X\\) be the interval \\([a, b]\\) of the real line, with \\(\\mu=m\\) (the Lebesgue measure), and \\(\\mathfrak{M}\\) the family of Lebesgue-measurable subsets of \\([a, b]\\). Instead of\n\\[ \\int_{X} f d m \\]\nit is customary to use the familiar notation\n\\[ \\int_{a}^{b} f d x \\]\nfor the Lebesgue integral of \\(f\\) over \\([a, b]\\). To distinguish Riemann integrals from Lebesgue integrals, we shall now denote the former by\n\\[ \\mathscr{R} \\int_{a}^{b} f d x \\]\nIf \\(f \\in \\mathscr{R}\\) on \\([a, b]\\), then \\(f \\in \\mathscr{L}\\) on \\([a, b]\\), and \\[ \\int_{a}^{b} f d x=\\mathscr{R} \\int_{a}^{b} f d x . \\]\nSuppose \\(f\\) is bounded on \\([a, b]\\). Then \\(f \\in \\mathscr{R}\\) on \\([a, b]\\) if and only if \\(f\\) is continuous almost everywhere on \\([a, b]\\). Proof Suppose \\(f\\) is bounded. By Definition \\(6.1\\) and Theorem \\(6.4\\) there is a sequence \\(\\left\\{P_{k}\\right\\}\\) of partitions of \\([a, b]\\), such that \\(P_{k+1}\\) is a refinement of \\(P_{k}\\), such that the distance between adjacent points of \\(P_{k}\\) is less than \\(1 / k\\), and such that\n\\[ \\lim _{k \\rightarrow \\infty} L\\left(P_{k}, f\\right)=\\mathscr{R} \\underline{\\int} f d x, \\quad \\lim _{k \\rightarrow \\infty} U\\left(P_{k}, f\\right)=\\mathscr{R} \\bar{\\int} f d x . \\]\n(In this proof, all integrals are taken over \\([a, b]\\).)\nIf \\(P_{k}=\\left\\{x_{0}, x_{1}, \\ldots, x_{n}\\right\\}\\), with \\(x_{0}=a, x_{n}=b\\), define\n\\[ U_{k}(a)=L_{k}(a)=f(a) ; \\]\nput \\(U_{k}(x)=M_{i}\\) and \\(L_{k}(x)=m_{i}\\) for \\(x_{i-1}\u0026lt;x \\leq x_{i}, 1 \\leq i \\leq n\\), using the notation introduced in Definition 6.1. Then\n\\[ L\\left(P_{k}, f\\right)=\\int L_{k} d x, \\quad U\\left(P_{k}, f\\right)=\\int U_{k} d x, \\]\nand\n\\[ L_{1}(x) \\leq L_{2}(x) \\leq \\cdots \\leq f(x) \\leq \\cdots \\leq U_{2}(x) \\leq U_{1}(x) \\]\nfor all \\(x \\in[a, b]\\), since \\(P_{k+1}\\) refines \\(P_{k}\\). By (90), there exist\n\\[ L(x)=\\lim _{k \\rightarrow \\infty} L_{k}(x), \\quad U(x)=\\lim _{k \\rightarrow \\infty} U_{k}(x) . \\]\nObserve that \\(L\\) and \\(U\\) are bounded measurable functions on \\([a, b]\\), that\n\\[ L(x) \\leq f(x) \\leq U(x) \\quad(a \\leq x \\leq b) \\]\nand that\n\\[ \\int L d x=\\mathscr{R} \\int f d x, \\quad \\int U d x=\\mathscr{R} \\int f d x \\]\nby \\((88),(90)\\), and the monotone convergence theorem.\nSo far, nothing has been assumed about \\(f\\) except that \\(f\\) is a bounded real function on \\([a, b]\\).\nTo complete the proof, note that \\(f \\in \\mathscr{R}\\) if and only if its upper and lower Riemann integrals are equal, hence if and only if\n\\[ \\int L d x=\\int U d x \\]\nsince \\(L \\leq U\\), (94) happens if and only if \\(L(x)=U(x)\\) for almost all \\(x \\in[a, b]\\) (Exercise 1).\nIn that case, (92) implies that\n\\[ L(x)=f(x)=U(x) \\]\nalmost everywhere on \\([a, b]\\), so that \\(f\\) is measurable, and (87) follows from (93) and (95).\nFurthermore, if \\(x\\) belongs to no \\(P_{k}\\), it is quite easy to see that \\(U(x)=\\) \\(L(x)\\) if and only if \\(f\\) is continuous at \\(x\\). Since the union of the sets \\(P_{k}\\) is countable, its measure is 0 , and we conclude that \\(f\\) is continuous almost everywhere on \\([a, b]\\) if and only if \\(L(x)=U(x)\\) almost everywhere, hence (as we saw above) if and only if \\(f \\in \\mathscr{R}\\).\nThis completes the proof.\nThe familiar connection between integration and differentiation is to a large degree carried over into the Lebesgue theory. If \\(f \\in \\mathscr{L}\\) on \\([a, b]\\), and\n\\[ F(x)=\\int_{a}^{x} f d t \\quad(a \\leq x \\leq b) \\]\nthen \\(F^{\\prime}(x)=f(x)\\) almost everywhere on \\([a, b]\\).\nConversely, if \\(F\\) is differentiable at every point of \\([a, b]\\) (“almost everywhere” is not good enough here!) and if \\(F^{\\prime} \\in \\mathscr{L}\\) on \\([a, b]\\), then\n\\[ F(x)-F(a)=\\int_{a}^{x} F^{\\prime}(t) \\quad(a \\leq x \\leq b) \\]\nFor the proofs of these two theorems, we refer the reader to any of the works on integration cited in the Bibliography.\nSuppose \\(f\\) is a complex-valued function defined on a measure space \\(X\\), and \\(f=u+i v\\), where \\(u\\) and \\(v\\) are real. We say that \\(f\\) is measurable if and only if both \\(u\\) and \\(v\\) are measurable.\nIt is easy to verify that sums and products of complex measurable functions are again measurable. Since\n\\[ |f|=\\left(u^{2}+v^{2}\\right)^{1 / 2} \\]\nTheorem \\(11.18\\) shows that \\(|f|\\) is measurable for every complex measurable \\(f\\).\nSuppose \\(\\mu\\) is a measure on \\(X, E\\) is a measurable subset of \\(X\\), and \\(f\\) is a complex function on \\(X\\). We say that \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\) provided that \\(f\\) is measurable and\n\\[ \\int_{E}|f| d \\mu\u0026lt;+\\infty \\]\nand we define\n\\[ \\int_{E} f d \\mu=\\int_{E} u d \\mu+i \\int_{E} v d \\mu \\]\nif (97) holds. Since \\(|u| \\leq|f|,|v| \\leq|f|\\), and \\(|f| \\leq|u|+|v|\\), it is clear that (97) holds if and only if \\(u \\in \\mathscr{L}(\\mu)\\) and \\(v \\in \\mathscr{L}(\\mu)\\) on \\(E\\).\nTheorems \\(11.23(a),(d),(e),(f), 11.24(b), 11.26,11.27,11.29\\), and \\(11.32\\) can now be extended to Lebesgue integrals of complex functions. The proofs are quite straightforward. That of Theorem \\(11.26\\) is the only one that offers anything of interest:\nIf \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\), there is a complex number \\(c,|c|=1\\), such that\n\\[ c \\int_{E} f d \\mu \\geq 0 . \\]\nPut \\(g=c f=u+i v, u\\) and \\(v\\) real. Then\n\\[ \\left|\\int_{E} f d \\mu\\right|=c \\int_{E} f d \\mu=\\int_{E} g d \\mu=\\int_{E} u d \\mu \\leq \\int_{E}|f| d \\mu . \\]\nThe third of the above equalities holds since the preceding ones show that \\(\\int g d \\mu\\) is real.\nAs an application of the Lebesgue theory, we shall now extend the Parseval theorem (which we proved only for Riemann-integrable functions in Chap. 8) and prove the Riesz-Fischer theorem for orthonormal sets of functions. 11.34 Definition Let \\(X\\) be a measurable space. We say that a complex function \\(f \\in \\mathscr{L}^{2}(\\mu)\\) on \\(X\\) if \\(f\\) is measurable and if\n\\[ \\int_{X}|f|^{2} d \\mu\u0026lt;+\\infty . \\]\nIf \\(\\mu\\) is Lebesgue measure, we say \\(f \\in \\mathscr{L}^{2}\\). For \\(f \\in \\mathscr{L}^{2}(\\mu)\\) (we shall omit the phrase “on \\(X\\)” from now on) we define\n\\[ \\|f\\|=\\left\\{\\int_{X}|f|^{2} d \\mu\\right\\}^{1 / 2} \\]\nand call \\(\\|f\\|\\) the \\(\\mathscr{L}^{2}(\\mu)\\) norm of \\(f\\).\n11.35 Theorem Suppose \\(f \\in \\mathscr{L}^{2}(\\mu)\\) and \\(g \\in \\mathscr{L}^{2}(\\mu)\\). Then \\(f g \\in \\mathscr{L}(\\mu)\\), and\n\\[ \\int_{x}|f g| d \\mu \\leq\\|f\\|\\|g\\| . \\]\nThis is the Schwarz inequality, which we have already encountered for series and for Riemann integrals. It follows from the inequality\n\\[ 0 \\leq \\int_{\\boldsymbol{X}}(|f|+\\lambda|g|)^{2} d \\mu=\\|f\\|^{2}+2 \\lambda \\int_{\\boldsymbol{X}}|f g| d \\mu+\\lambda^{2}\\|g\\|^{2} \\]\nwhich holds for every real \\(\\lambda\\).\n11.36 Theorem Iff \\(\\in \\mathscr{L}^{2}(\\mu)\\) and \\(g \\in \\mathscr{L}^{2}(\\mu)\\), then \\(f+g \\in \\mathscr{L}^{2}(\\mu)\\), and\n\\[ \\|f+g\\| \\leq\\|f\\|+\\|g\\| . \\]\nProof The Schwarz inequality shows that\n\\[ \\begin{aligned} \\|f+g\\|^{2} \u0026amp;=\\int|f|^{2}+\\int f \\bar{g}+\\int f g+\\int|g|^{2} \\\\ \u0026amp; \\leq\\|f\\|^{2}+2\\|f\\|\\|g\\|+\\|g\\|^{2} \\\\ \u0026amp;=(\\|f\\|+\\|g\\|)^{2} . \\end{aligned} \\]\n11.37 Remark If we define the distance between two functions \\(f\\) and \\(g\\) in \\(\\mathscr{L}^{2}(\\mu)\\) to be \\(\\|f-g\\|\\), we see that the conditions of Definition \\(2.15\\) are satisfied, except for the fact that \\(\\|f-g\\|=0\\) does not imply that \\(f(x)=g(x)\\) for all \\(x\\), but only for almost all \\(x\\). Thus, if we identify functions which differ only on a set of measure zero, \\(\\mathscr{L}^{2}(\\mu)\\) is a metric space.\nWe now consider \\(\\mathscr{L}^{2}\\) on an interval of the real line, with respect to Lebesgue measure.\n11.38 Theorem The continuous functions form a dense subset of \\(\\mathscr{L}^{2}\\) on \\([a, b]\\). More explicitly, this means that for any \\(f \\in \\mathscr{L}^{2}\\) on \\([a, b]\\), and any \\(\\varepsilon\u0026gt;0\\), there is a function \\(g\\), continuous on \\([a, b]\\), such that\n\\[ \\|f-g\\|=\\left\\{\\int_{a}^{b}|f-g|^{2} d x\\right\\}^{1 / 2}\u0026lt;\\varepsilon . \\]\nProof We shall say that \\(f\\) is approximated in \\(\\mathscr{L}^{2}\\) by a sequence \\(\\left\\{g_{n}\\right\\}\\) if \\(\\left\\|f-g_{n}\\right\\| \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\).\nLet \\(A\\) be a closed subset of \\([a, b]\\), and \\(K_{A}\\) its characteristic function. Put\n\\[ t(x)=\\inf |x-y| \\quad(y \\in A) \\]\nand\n\\[ g_{n}(x)=\\frac{1}{1+n t(x)} \\quad(n=1,2,3, \\ldots) \\]\nThen \\(g_{n}\\) is continuous on \\([a, b], g_{n}(x)=1\\) on \\(A\\), and \\(g_{n}(x) \\rightarrow 0\\) on \\(B\\), where \\(B=[a, b]-A\\). Hence\n\\[ \\left\\|g_{n}-K_{A}\\right\\|=\\left\\{\\int_{B} g_{n}^{2} d x\\right\\}^{1 / 2} \\rightarrow 0 \\]\nby Theorem 11.32. Thus characteristic functions of closed sets can be approximated in \\(\\mathscr{L}^{2}\\) by continuous functions.\nBy (39) the same is true for the characteristic function of any measurable set, and hence also for simple measurable functions.\nIf \\(f \\geq 0\\) and \\(f \\in \\mathscr{L}^{2}\\), let \\(\\left\\{s_{n}\\right\\}\\) be a monotonically increasing sequence of simple nonnegative measurable functions such that \\(s_{n}(x) \\rightarrow f(x)\\). Since \\(\\left|f-s_{n}\\right|^{2} \\leq f^{2}\\), Theorem \\(11.32\\) shows that \\(\\left\\|f-s_{n}\\right\\| \\rightarrow 0\\).\nThe general case follows.\n11.39 Definition We say that a sequence of complex functions \\(\\left\\{\\phi_{n}\\right\\}\\) is an orthonormal set of functions on a measurable space \\(X\\) if\n\\[ \\int_{X} \\phi_{n} \\Phi_{m} d \\mu= \\begin{cases}0 \u0026amp; (n \\neq m), \\\\ 1 \u0026amp; (n=m) .\\end{cases} \\]\nIn particular, we must have \\(\\phi_{n} \\in \\mathscr{L}^{2}(\\mu)\\). If \\(f \\in \\mathscr{L}^{2}(\\mu)\\) and if\n\\[ c_{n}=\\int_{X} f \\Phi_{n} d \\mu \\quad(n=1,2,3, \\ldots) \\]\nwe write\n\\[ f \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n} \\]\nas in Definition \\(8.10 .\\) The definition of a trigonometric Fourier series is extended in the same way to \\(\\mathscr{L}^{2}\\) (or even to \\(\\mathscr{L}\\) ) on \\([-\\pi, \\pi]\\). Theorems \\(8.11\\) and \\(8.12\\) (the Bessel inequality) hold for any \\(f \\in \\mathscr{L}^{2}(\\mu)\\). The proofs are the same, word for word.\nWe can now prove the Parseval theorem.\n\\[ f(x) \\sim \\sum_{-\\infty}^{\\infty} c_{n} e^{i n x}, \\]\nwhere \\(f \\in \\mathscr{L}^{2}\\) on \\([-\\pi, \\pi]\\). Let \\(s_{n}\\) be the nth partial sum of (99). Then\n\\[ \\begin{aligned} \\lim _{n \\rightarrow \\infty}\\left\\|f-s_{n}\\right\\| \u0026amp;=0, \\\\ \\sum_{-\\infty}^{\\infty}\\left|c_{n}\\right|^{2} \u0026amp;=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}|f|^{2} d x . \\end{aligned} \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. By Theorem 11.38, there is a continuous function \\(g\\) such that\n\\[ \\|f-g\\|\u0026lt;\\frac{\\varepsilon}{2} . \\]\nMoreover, it is easy to see that we can arrange it so that \\(g(\\pi)=g(-\\pi)\\). Then \\(g\\) can be extended to a periodic continuous function. By Theorem \\(8.16\\), there is a trigonometric polynomial \\(T\\), of degree \\(N\\), say, such that\n\\[ \\|g-T\\|\u0026lt;\\frac{\\varepsilon}{2} . \\]\nHence, by Theorem \\(8.11\\) (extended to \\(\\mathscr{L}^{2}\\) ), \\(n \\geq N\\) implies\n\\[ \\left\\|s_{n}-f\\right\\| \\leq\\|T-f\\|\u0026lt;\\varepsilon, \\]\nand (100) follows. Equation (101) is deduced from (100) as in the proof of Theorem \\(8.16 .\\)\nCorollary If \\(f \\in \\mathscr{L}^{2}\\) on \\([-\\pi, \\pi]\\), and if\n\\[ \\int_{-\\pi}^{\\pi} f(x) e^{-i n x} d x=0 \\quad(n=0, \\pm 1, \\pm 2, \\ldots), \\]\nthen \\(\\|f\\|=0\\).\nThus if two functions in \\(\\mathscr{L}^{2}\\) have the same Fourier series, they differ at most on a set of measure zero. 11.41 Definition Let \\(f\\) and \\(f_{n} \\in \\mathscr{L}^{2}(\\mu)(n=1,2,3, \\ldots)\\). We say that \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\) in \\(\\mathscr{L}^{2}(\\mu)\\) if \\(\\left\\|f_{n}-f\\right\\| \\rightarrow 0\\). We say that \\(\\left\\{f_{n}\\right\\}\\) is a Cauchy sequence in \\(\\mathscr{L}^{2}(\\mu)\\) if for every \\(\\varepsilon\u0026gt;0\\) there is an integer \\(N\\) such that \\(n \\geq N, m \\geq N\\) implies \\(\\left\\|f_{n}-f_{m}\\right\\| \\leq \\varepsilon\\).\n11.42 Theorem If \\(\\left\\{f_{n}\\right\\}\\) is a Cauchy sequence in \\(\\mathscr{L}^{2}(\\mu)\\), then there exists a function \\(f \\in \\mathscr{L}^{2}(\\mu)\\) such that \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\) in \\(\\mathscr{L}^{2}(\\mu)\\).\nThis says, in other words, that \\(\\mathscr{L}^{2}(\\mu)\\) is a complete metric space.\nProof Since \\(\\left\\{f_{n}\\right\\}\\) is a Cauchy sequence, we san find a sequence \\(\\left\\{n_{k}\\right\\}\\), \\(k=1,2,3, \\ldots\\), such that\n\\[ \\left\\|f_{n_{k}}-f_{n_{k+1}}\\right\\|\u0026lt;\\frac{1}{2^{k}} \\quad(k=1,2,3, \\ldots) . \\]\nChoose a function \\(g \\in \\mathscr{L}^{2}(\\mu)\\). By the Schwarz inequality,\n\\[ \\int_{X}\\left|g\\left(f_{n_{k}}-f_{n_{k+1}}\\right)\\right| d \\mu \\leq \\frac{\\|g\\|}{2^{k}} . \\]\nHence\n\\[ \\sum_{k=1}^{\\infty} \\int_{X}\\left|g\\left(f_{n_{k}}-f_{n_{k+1}}\\right)\\right| d \\mu \\leq\\|g\\| . \\]\nBy Theorem 11.30, we may interchange the summation and integration in (102). It follows that\n\\[ |g(x)| \\sum_{k=1}^{\\infty}\\left|f_{n_{k}}(x)-f_{n_{k+1}}(x)\\right|\u0026lt;+\\infty \\]\nalmost everywhere on \\(X\\). Therefore\n\\[ \\sum_{k=1}^{\\infty}\\left|f_{n_{k+1}}(x)-f_{n_{k}}(x)\\right|\u0026lt;+\\infty \\]\nalmost everywhere on \\(X\\). For if the series in (104) were divergent on a set \\(E\\) of positive measure, we could take \\(g(x)\\) to be nonzero on a subset of \\(E\\) of positive measure, thus obtaining a contradiction to (103).\nSince the \\(k\\) th partial sum of the series\n\\[ \\sum_{k=1}^{\\infty}\\left(f_{n_{k+1}}(x)-f_{n_{k}}(x)\\right) \\]\nwhich converges almost everywhere on \\(X\\), is\n\\[ f_{n_{k+1}}(x)-f_{n_{1}}(x) \\]\nwe see that the equation\n\\[ f(x)=\\lim _{k \\rightarrow \\infty} f_{n_{k}}(x) \\]\ndefines \\(f(x)\\) for almost all \\(x \\in X\\), and it does not matter how we define \\(f(x)\\) at the remaining points of \\(X\\).\nWe shall now show that this function \\(f\\) has the desired properties. Let \\(\\varepsilon\u0026gt;0\\) be given, and choose \\(N\\) as indicated in Definition 11.41. If \\(n_{k}\u0026gt;N\\), Fatou’s theorem shows that\n\\[ \\left\\|f-f_{n_{k}}\\right\\| \\leq \\liminf _{i \\rightarrow \\infty}\\left\\|f_{n_{i}}-f_{n_{k}}\\right\\| \\leq \\varepsilon . \\]\nThus \\(f-f_{n_{k}} \\in \\mathscr{L}^{2}(\\mu)\\), and since \\(f=\\left(f-f_{n_{k}}\\right)+f_{n_{k}}\\), we see that \\(f \\in \\mathscr{L}^{2}(\\mu)\\). Also, since \\(\\varepsilon\\) is arbitrary,\n\\[ \\lim _{k \\rightarrow \\infty}\\left\\|f-f_{n_{k}}\\right\\|=0 . \\]\nFinally, the inequality\n\\[ \\left\\|f-f_{n}\\right\\| \\leq\\left\\|f-f_{n_{k}}\\right\\|+\\left\\|f_{n_{k}}-f_{n}\\right\\| \\]\nshows that \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\) in \\(\\mathscr{L}^{2}(\\mu)\\); for if we take \\(n\\) and \\(n_{k}\\) large enough, each of the two terms on the right of (105) can be made arbitrarily small.\n11.43 The Riesz-Fischer theorem Let \\(\\left\\{\\phi_{n}\\right\\}\\) be orthonormal on \\(X\\). Suppose \\(\\Sigma\\left|c_{n}\\right|^{2}\\) converges, and put \\(s_{n}=c_{1} \\phi_{1}+\\cdots+c_{n} \\phi_{n}\\). Then there exists a function \\(f \\in \\mathscr{L}^{2}(\\mu)\\) such that \\(\\left\\{s_{n}\\right\\}\\) converges to \\(f\\) in \\(\\mathscr{L}^{2}(\\mu)\\), and such that\n\\[ f \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n} . \\]\nProof For \\(n\u0026gt;m\\),\n\\[ \\left\\|s_{n}-s_{m}\\right\\|^{2}=\\left|c_{m+1}\\right|^{2}+\\cdots+\\left|c_{n}\\right|^{2}, \\]\nso that \\(\\left\\{s_{n}\\right\\}\\) is a Cauchy sequence in \\(\\mathscr{L}^{2}(\\mu)\\). By Theorem \\(11.42\\), there is a function \\(f \\in \\mathscr{L}^{2}(\\mu)\\) such that\n\\[ \\lim _{n \\rightarrow \\infty}\\left\\|f-s_{n}\\right\\|=0 . \\]\nNow, for \\(n\u0026gt;k\\)\n\\[ \\int_{X} f \\Phi_{k} d \\mu-c_{k}=\\int_{X} f \\Phi_{k} d \\mu-\\int_{X} s_{n} \\bar{\\phi}_{k} d \\mu \\]\nso that\n\\[ \\left|\\int_{X} f \\phi_{k} d \\mu-c_{k}\\right| \\leq\\left\\|f-s_{n}\\right\\| \\cdot\\left\\|\\phi_{k}\\right\\|+\\left\\|f-s_{n}\\right\\| . \\]\nLetting \\(n \\rightarrow \\infty\\), we see that\n\\[ c_{k}=\\int_{X} f \\Phi_{k} d \\mu \\quad(k=1,2,3, \\ldots) \\]\nand the proof is complete.\n11.44 Definition An orthonormal set \\(\\left\\{\\phi_{n}\\right\\}\\) is said to be complete if, for \\(f \\in \\mathscr{L}^{2}(\\mu)\\), the equations\n\\[ \\int_{X} f \\phi_{n} d \\mu=0 \\quad(n=1,2,3, \\ldots) \\]\nimply that \\(\\|f\\|=0\\)\nIn the Corollary to Theorem \\(11.40\\) we deduced the completeness of the trigonometric system from the Parseval equation (101). Conversely, the Parseval equation holds for every complete orthonormal set:\n11.45 Theorem Let \\(\\left\\{\\phi_{n}\\right\\}\\) be a complete orthonormal set. If \\(f \\in \\mathscr{L}^{2}(\\mu)\\) and if\nthen\n\\[ f \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n}, \\]\n\\[ \\int_{X}|f|^{2} d \\mu=\\sum_{n=1}^{\\infty}\\left|c_{n}\\right|^{2} \\]\nProof By the Bessel inequality, \\(\\Sigma\\left|c_{n}\\right|^{2}\\) converges. Putting\n\\[ s_{n}=c_{1} \\phi_{1}+\\cdots+c_{n} \\phi_{n}, \\]\nthe Riesz-Fischer theorem shows that there is a function \\(g \\in \\mathscr{L}^{2}(\\mu)\\) such that\n\\[ g \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n}, \\]\nand such that \\(\\left\\|g-s_{n}\\right\\| \\rightarrow 0\\). Hence \\(\\left\\|s_{n}\\right\\| \\rightarrow\\|g\\|\\). Since\n\\[ \\left\\|s_{n}\\right\\|^{2}=\\left|c_{1}\\right|^{2}+\\cdots+\\left|c_{n}\\right|^{2} \\text {, } \\]\nwe have\n\\[ \\int_{X}|g|^{2} d \\mu=\\sum_{n=1}^{\\infty}\\left|c_{n}\\right|^{2} \\]\nNow (106), (108), and the completeness of \\(\\left\\{\\phi_{n}\\right\\}\\) show that \\(\\|f-g\\|=0\\), so that (109) implies (107).\nCombining Theorems \\(11.43\\) and \\(11.45\\), we arrive at the very interesting conclusion that every complete orthonormal set induces a 1-1 correspondence between the functions \\(f \\in \\mathscr{L}^{2}(\\mu)\\) (identifying those which are equal almost everywhere) on the one hand and the sequences \\(\\left\\{c_{n}\\right\\}\\) for which \\(\\Sigma\\left|c_{n}\\right|^{2}\\) converges, on the other. The representation\n\\[ f \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n}, \\]\ntogether with the Parseval equation, shows that \\(\\mathscr{L}^{2}(\\mu)\\) may be regarded as an infinite-dimensional euclidean space (the so-called “Hilbert space”), in which the point \\(f\\) has coordinates \\(c_{n}\\), and the functions \\(\\phi_{n}\\) are the coordinate vectors.\nIf \\(f \\geq 0\\) and \\(\\int_{E} f d \\mu=0\\), prove that \\(f(x)=0\\) almost everywhere on \\(E\\). Hint: Let \\(E_{n}\\) be the subset of \\(E\\) on which \\(f(x)\u0026gt;1 / n\\). Write \\(A=\\bigcup E_{n}\\). Then \\(\\mu(A)=0\\) if and only if \\(\\mu\\left(E_{n}\\right)=0\\) for every \\(n\\).\nIf \\(\\int_{A} f d \\mu=0\\) for every measurable subset \\(A\\) of a measurable set \\(E\\), then \\(f(x)=0\\) almost everywhere on \\(E\\).\nIf \\(\\left\\{f_{n}\\right\\}\\) is a sequence of measurable functions, prove that the set of points \\(x\\) at which \\(\\left\\{f_{n}(x)\\right\\}\\) converges is measurable.\nIf \\(f \\in \\mathscr{L}(\\mu)\\) on \\(E\\) and \\(g\\) is bounded and measurable on \\(E\\), then \\(f g \\in \\mathscr{L}(\\mu)\\) on \\(E\\).\nPut\n\\[ \\begin{aligned} \u0026amp; g(x)= \\begin{cases}0 \u0026amp; \\left(0 \\leq x \\leq \\frac{1}{2}\\right), \\\\ 1 \u0026amp; \\left(\\frac{1}{2}\u0026lt;x \\leq 1\\right),\\end{cases} \\\\ \u0026amp; f_{2 k}(x)=g(x) \\quad(0 \\leq x \\leq 1) \\text {, } \\\\ \u0026amp; f_{2 k+1}(x)=g(1-x) \\quad(0 \\leq x \\leq 1) \\text {. } \\end{aligned} \\]\nShow that\n\\[ \\underset{n \\rightarrow \\infty}{\\lim \\inf } f_{n}(x)=0 \\quad(0 \\leq x \\leq 1), \\]\nbut\n\\[ \\int_{0}^{1} f_{n}(x) d x=\\frac{1}{2} . \\]\n[Compare with (77).] 6. Let\n\\[ f_{n}(x)= \\begin{cases}\\frac{1}{n} \u0026amp; (|x| \\leq n) \\\\ 0 \u0026amp; (|x|\u0026gt;n)\\end{cases} \\]\nThen \\(f_{n}(x) \\rightarrow 0\\) uniformly on \\(R^{1}\\), but\n\\[ \\int_{-\\infty}^{\\infty} f_{n} d x=2 \\quad(n=1,2,3, \\ldots) . \\]\n(We write \\(\\int_{-\\infty}^{\\infty}\\) in place of \\(\\int_{R 1}\\).) Thus uniform convergence does not imply dominated convergence in the sense of Theorem 11.32. However, on sets of finite measure, uniformly convergent sequences of bounded functions do satisfy Theorem 11.32.\nFind a necessary and sufficient condition that \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\). Hint: Consider Example 11.6(b) and Theorem 11.33.\nIf \\(f \\in \\mathscr{R}\\) on \\([a, b]\\) and if \\(F(x)=\\int_{a}^{x} f(t) d t\\), prove that \\(F^{\\prime}(x)=f(x)\\) almost everywhere on \\([a, b]\\).\nProve that the function \\(F\\) given by (96) is continuous on \\([a, b]\\).\nIf \\(\\mu(X)\u0026lt;+\\infty\\) and \\(f \\in \\mathscr{L}^{2}(\\mu)\\) on \\(X\\), prove that \\(f \\in \\mathscr{L}(\\mu)\\) on \\(X\\). If\n\\[ \\mu(X)=+\\infty, \\]\nthis is false. For instance, if\n\\[ f(x)=\\frac{1}{1+|x|}, \\]\nthen \\(f \\in \\mathscr{L}^{2}\\) on \\(R^{1}\\), but \\(f \\notin \\mathscr{L}\\) on \\(R^{1}\\).\nIf \\(f, g \\in \\mathscr{L}(\\mu)\\) on \\(X\\), define the distance between \\(f\\) and \\(g\\) by \\[ \\int_{x}|f-g| d \\mu . \\]\nProve that \\(\\mathscr{L}(\\mu)\\) is a complete metric space.\nSuppose \\(|f(x, y)| \\leq 1\\) if \\(0 \\leq x \\leq 1,0 \\leq y \\leq 1\\),\nfor fixed \\(x, f(x, y)\\) is a continuous function of \\(y\\),\nfor fixed \\(y, f(x, y)\\) is a continuous function of \\(x\\).\nPut\n\\[ g(x)=\\int_{0}^{1} f(x, y) d y \\quad(0 \\leq x \\leq 1) . \\]\nIs \\(g\\) continuous?\nConsider the functions \\[ f_{n}(x)=\\sin n x \\quad(n=1,2,3, \\ldots,-\\pi \\leq x \\leq \\pi) \\]\n","date":"2022-09-23T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/11-the-lebesgue-theory/3/","section":"baby rudin","tags":null,"title":"Measure Space"},{"categories":null,"contents":"1.16 Definition A complex function \\(s\\) on a measurable space \\(X\\) whose range consists of only finitely many points will be called a simple function. Among these are the nonnegative simple functions, whose range is a finite subset of \\([0, \\infty)\\). Note that we explicitly exclude \\(\\infty\\) from the values of a simple function.\nIf \\(\\alpha_1, \\ldots, \\alpha_n\\) are the distinct values of a simple function \\(s\\), and if we set \\(A_i=\\left\\{x: s(x)=\\alpha_i\\right\\}\\), then clearly\n\\[ \\begin{equation} s=\\sum_{i=1}^n \\alpha_i \\chi_{A_i}, \\end{equation} \\]\nwhere \\(\\chi_{A_i}\\) is the characteristic function of \\(A_i\\), as defined in Sec. \\(1.9(d)\\).\nIt is also clear that \\(s\\) is measurable if and only if each of the sets \\(A_i\\) is measurable.\n1.17 Theorem Let \\(f: X \\rightarrow[0, \\infty]\\) be measurable. There exist simple measurable functions \\(s_n\\) on \\(X\\) such that\n\\(0 \\leq s_1 \\leq s_2 \\leq \\cdots \\leq f\\).\n\\(s_n(x) \\rightarrow f(x)\\) as \\(n \\rightarrow \\infty\\), for every \\(x \\in X\\).\nProof Put \\(\\delta_n=2^{-n}\\). To each positive integer \\(n\\) and each real number \\(t\\) corresponds a unique integer \\(k=k_n(t)\\) that satisfies \\(k \\delta_n \\leq t\u0026lt;(k+1) \\delta_n\\). Define\n\\[ \\begin{equation} \\varphi_n(t)= \\begin{cases}k_n(t) \\delta_n \u0026amp; \\text { if } 0 \\leq t\u0026lt;n \\\\ n \u0026amp; \\text { if } n \\leq t \\leq \\infty\\end{cases} \\end{equation} \\]\nEach \\(\\varphi_n\\) is then a Borel function on \\([0, \\infty]\\),\n\\[ \\begin{equation} t-\\delta_n\u0026lt;\\varphi_n(t) \\leq t \\quad \\text { if } 0 \\leq t \\leq n, \\end{equation} \\]\n\\(0 \\leq \\varphi_1 \\leq \\varphi_2 \\leq \\cdots \\leq t\\), and \\(\\varphi_n(t) \\rightarrow t\\) as \\(n \\rightarrow \\infty\\), for every \\(t \\in[0, \\infty]\\). It follows that the functions\n\\[ \\begin{equation} s_n=\\varphi_n \\circ f \\end{equation} \\]\nsatisfy \\((a)\\) and \\((b)\\); they are measurable, by Theorem \\(1.12(d)\\).\n","date":"2022-09-18T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/3-simple-functions/","section":"papa rudin","tags":null,"title":"3 Simple Functions"},{"categories":null,"contents":"Toward the end of the nineteenth century it became clear to many mathematicians that the Riemann integral (about which one learns in calculus courses) should be replaced by some other type of integral, more general and more flexible, better suited for dealing with limit processes. Among the attempts made in this direction, the most notable ones were due to Jordan, Borel, W. H. Young, and Lebesgue. It was Lebesgue’s construction which turned out to be the most successful.\nIn brief outline, here is the main idea: The Riemann integral of a function \\(f\\) over an interval \\([a, b]\\) can be approximated by sums of the form\n\\[ \\sum_{i=1}^n f\\left(t_i\\right) m\\left(E_i\\right) \\]\nwhere \\(E_1, \\ldots, E_n\\) are disjoint intervals whose union is \\([a, b], m\\left(E_i\\right)\\) denotes the length of \\(E_i\\), and \\(t_i \\in E_i\\) for \\(i=1, \\ldots, n\\). Lebesgue discovered that a completely satisfactory theory of integration results if the sets \\(E_i\\) in the above sum are allowed to belong to a larger class of subsets of the line, the so-called “measurable sets,” and if the class of functions under consideration is enlarged to what he called “measurable functions.” The crucial set-theoretic properties involved are the following: The union and the intersection of any countable family of measurable sets are measurable; so is the complement of every measurable set; and, most important, the notion of “length” (now called “measure”) can be extended to them in such a way that \\[ m\\left(E_1 \\cup E_2 \\cup E_3 \\cup \\cdots\\right)=m\\left(E_1\\right)+m\\left(E_2\\right)+m\\left(E_3\\right)+\\cdots \\] for every countable collection \\(\\left\\{E_i\\right\\}\\) of pairwise disjoint measurable sets. This property of \\(m\\) is called countable additivity.\nThe passage from Riemann’s theory of integration to that of Lebesgue is a process of completion (in a sense which will appear more precisely later). It is of the same fundamental importance in analysis as is the construction of the real number system from the rationals.\nThe above-mentioned measure \\(m\\) is of course intimately related to the geometry of the real line. In this chapter we shall present an abstract (axiomatic) version of the Lebesgue integral, relative to any countably additive measure on any set. (The precise definitions follow.) This abstract theory is not in any way more difficult than the special case of the real line; it shows that a large part of integration theory is independent of any geometry (or topology) of the underlying space; and, of course, it gives us a tool of much wider applicability. The existence of a large class of measures, among them that of Lebesgue, will be established in Chap. \\(2 .\\)\n","date":"2022-09-17T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/0-abstract-integration/","section":"papa rudin","tags":null,"title":"0 ABSTRACT INTEGRATION"},{"categories":null,"contents":"1.1 Some sets can be described by listing their members. Thus \\(\\left\\{x_1, \\ldots, x_n\\right\\}\\) is the set whose members are \\(x_1, \\ldots, x_n\\); and \\(\\{x\\}\\) is the set whose only member is \\(x\\). More often, sets are described by properties. We write \\[ \\{x: P\\} \\] for the set of all elements \\(x\\) which have the property \\(P\\). The symbol \\(\\varnothing\\) denotes the empty set. The words collection, family, and class will be used synonymously with set.\nWe write \\(x \\in A\\) if \\(x\\) is a member of the set \\(A\\); otherwise \\(x \\notin A\\). If \\(B\\) is a subset of \\(A\\), i.e., if \\(x \\in B\\) implies \\(x \\in A\\), we write \\(B \\subset A\\). If \\(B \\subset A\\) and \\(A \\subset B\\), then \\(A=B\\). If \\(B \\subset A\\) and \\(A \\neq B, B\\) is a proper subset of \\(A\\). Note that \\(\\varnothing \\subset A\\) for every set \\(A\\).\n\\(A \\cup B\\) and \\(A \\cap B\\) are the union and intersection of \\(A\\) and \\(B\\), respectively. If \\(\\left\\{A_\\alpha\\right\\}\\) is a collection of sets, where \\(\\alpha\\) runs through some index set \\(I\\), we write\n\\[ \\bigcup_{\\alpha \\in I} A_\\alpha \\text { and } \\bigcap_{\\alpha \\in I} A_\\alpha \\]\nfor the union and intersection of \\(\\left\\{A_\\alpha\\right\\}\\) : \\[ \\begin{aligned} \u0026amp;\\bigcup_{\\alpha \\in I} A_\\alpha=\\left\\{x: x \\in A_\\alpha \\text { for at least one } \\alpha \\in I\\right\\} \\\\ \u0026amp;\\bigcap_{\\alpha \\in I} A_\\alpha=\\left\\{x: x \\in A_\\alpha \\text { for every } \\alpha \\in I\\right\\} . \\end{aligned} \\] If \\(I\\) is the set of all positive integers, the customary notations are \\[ \\bigcup_{n=1}^{\\infty} A_n \\text { and } \\bigcap_{n=1}^{\\infty} A_n \\] If no two members of \\(\\left\\{A_\\alpha\\right\\}\\) have an element in common, then \\(\\left\\{A_\\alpha\\right\\}\\) is a disjoint collection of sets:\nWe write \\(A-B=\\{x: x \\in A, x \\notin B\\}\\), and denote the complement of \\(A\\) by \\(A^c\\) whenever it is clear from the context with respect to which larger set the complement is taken.\nThe cartesian product \\(A_1 \\times \\cdots \\times A_n\\) of the sets \\(A_1, \\ldots, A_n\\) is the set of all ordered \\(n\\)-tuples \\(\\left(a_1, \\ldots, a_n\\right)\\) where \\(a_i \\in A_i\\) for \\(i=1, \\ldots, n\\). The real line (or real number system) is \\(R^1\\), and \\[ R^k=R^1 \\times \\cdots \\times R^1 \\quad(k \\text { factors }) . \\] The extended real number system is \\(R^1\\) with two symbols, \\(\\infty\\) and \\(-\\infty\\), adjoined, and with the obvious ordering. If \\(-\\infty \\leq a \\leq b \\leq \\infty\\), the interval \\([a, b]\\) and the segment \\((a, b)\\) are defined to be \\[ [a, b]=\\{x: a \\leq x \\leq b\\}, \\quad(a, b)=\\{x: a\u0026lt;x\u0026lt;b\\} . \\] We also write \\[ [a, b)=\\{x: a \\leq x\u0026lt;b\\}, \\quad(a, b]=\\{x: a\u0026lt;x \\leq b\\} . \\] If \\(E \\subset[-\\infty, \\infty]\\) and \\(E \\neq \\varnothing\\), the least upper bound (supremum) and greatest lower bound (infimum) of \\(E\\) exist in \\([-\\infty, \\infty]\\) and are denoted by sup \\(E\\) and \\(\\inf E\\).\nSometimes (but only when sup \\(E \\in E\\) ) we write \\(\\max E\\) for sup \\(E\\). The symbol \\[ f: X \\rightarrow Y \\] If no two members of \\(\\left\\{A_\\alpha\\right\\}\\) have an element in common, then \\(\\left\\{A_\\alpha\\right\\}\\) is a disjoint collection of sets:\nWe write \\(A-B=\\{x: x \\in A, x \\notin B\\}\\), and denote the complement of \\(A\\) by \\(A^c\\) whenever it is clear from the context with respect to which larger set the complement is taken.\nThe cartesian product \\(A_1 \\times \\cdots \\times A_n\\) of the sets \\(A_1, \\ldots, A_n\\) is the set of all ordered \\(n\\)-tuples \\(\\left(a_1, \\ldots, a_n\\right)\\) where \\(a_i \\in A_i\\) for \\(i=1, \\ldots, n\\). The real line (or real number system) is \\(R^1\\), and \\[ R^k=R^1 \\times \\cdots \\times R^1 \\quad(k \\text { factors }) . \\] The extended real number system is \\(R^1\\) with two symbols, \\(\\infty\\) and \\(-\\infty\\), adjoined, and with the obvious ordering. If \\(-\\infty \\leq a \\leq b \\leq \\infty\\), the interval \\([a, b]\\) and the segment \\((a, b)\\) are defined to be \\[ [a, b]=\\{x: a \\leq x \\leq b\\}, \\quad(a, b)=\\{x: a\u0026lt;x\u0026lt;b\\} . \\] We also write \\[ [a, b)=\\{x: a \\leq x\u0026lt;b\\}, \\quad(a, b]=\\{x: a\u0026lt;x \\leq b\\} . \\] If \\(E \\subset[-\\infty, \\infty]\\) and \\(E \\neq \\varnothing\\), the least upper bound (supremum) and greatest lower bound (infimum) of \\(E\\) exist in \\([-\\infty, \\infty]\\) and are denoted by sup \\(E\\) and \\(\\inf E\\). Sometimes (but only when sup \\(E \\in E\\) ) we write \\(\\max E\\) for sup \\(E\\). The symbol \\[ f: X \\rightarrow Y \\] means that \\(f\\) is a function (or mapping or transformation) of the set \\(X\\) into the set \\(Y\\); i.e., \\(f\\) assigns to each \\(x \\in X\\) an element \\(f(x) \\in Y\\). If \\(A \\subset X\\) and \\(B \\subset Y\\), the image of \\(A\\) and the inverse image (or pre-image) of \\(B\\) are \\[ \\begin{aligned} f(A) \u0026amp;=\\{y: y=f(x) \\text { for some } x \\in A\\}, \\\\ f^{-1}(B) \u0026amp;=\\{x: f(x) \\in B\\} . \\end{aligned} \\] Note that \\(f^{-1}(B)\\) may be empty even when \\(B \\neq \\varnothing\\).\nThe domain of \\(f\\) is \\(X\\). The range of \\(f\\) is \\(f(X)\\).\nIf \\(f(X)=Y, f\\) is said to \\(\\operatorname{map} X\\) onto \\(Y\\).\nWe write \\(f^{-1}(y)\\), instead of \\(f^{-1}(\\{y\\})\\), for every \\(y \\in Y\\). If \\(f^{-1}(y)\\) consists of at most one point, for each \\(y \\in Y, f\\) is said to be one-to-one. If \\(f\\) is one-to-one, then \\(f^{-1}\\) is a function with domain \\(f(X)\\) and range \\(X\\).\nIf \\(f: X \\rightarrow[-\\infty, \\infty]\\) and \\(E \\subset X\\), it is customary to write \\(\\sup _{x \\in E} f(x)\\) rather than \\(\\sup f(E)\\).\nIf \\(f: X \\rightarrow Y\\) and \\(g: Y \\rightarrow Z\\), the composite function \\(g \\circ f: X \\rightarrow Z\\) is defined by the formula \\[ (g \\circ f)(x)=g(f(x)) \\quad(x \\in X) \\] If the range of \\(f\\) lies in the real line (or in the complex plane), then \\(f\\) is said to be a real function (or a complex function). For a complex function \\(f\\), the statement “$ f $” means that all values \\(f(x)\\) of \\(f\\) are nonnegative real numbers.\n","date":"2022-09-17T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/1-set-theoretic-notations-and-terminology/","section":"papa rudin","tags":null,"title":"1 Set-Theoretic Notations and Terminology"},{"categories":null,"contents":"The class of measurable functions plays a fundamental role in integration theory. It has some basic properties in common with another most important class of functions, namely, the continuous ones. It is helpful to keep these similarities in mind. Our presentation is therefore organized in such a way that the analogies between the concepts topological space, open set, and continuous function, on the one hand, and measurable space, measurable set, and measurable function, on the other, are strongly emphasized. It seems that the relations between these concepts emerge most clearly when the setting is quite abstract, and this (rather than a desire for mere generality) motivates our approach to the subject.\n1.2 Definition A collection \\(\\tau\\) of subsets of a set \\(X\\) is said to be a topology in \\(X\\) if \\(\\tau\\) has the following three properties: \\(\\varnothing \\in \\tau\\) and \\(X \\in \\tau\\).\nIf \\(V_i \\in \\tau\\) for \\(i=1, \\ldots, n\\), then \\(V_1 \\cap V_2 \\cap \\cdots \\cap V_n \\in \\tau\\).\nIf \\(\\left\\{V_\\alpha\\right\\}\\) is an arbitrary collection of members of \\(\\tau\\) (finite, countable, or uncountable), then \\(\\bigcup_\\alpha V_\\alpha \\in \\tau\\).\nIf \\(\\tau\\) is a topology in \\(X\\), then \\(X\\) is called a topological space, and the members of \\(\\tau\\) are called the open sets in \\(X\\).\nIf \\(X\\) and \\(Y\\) are topological spaces and if \\(f\\) is a mapping of \\(X\\) into \\(Y\\), then \\(f\\) is said to be continuous provided that \\(f^{-1}(V)\\) is an open set in \\(X\\) for every open set \\(V\\) in \\(Y\\).\n1.3 Definition A collection \\(\\mathfrak{M}\\) of subsets of a set \\(X\\) is said to be a \\(\\sigma\\)-algebra in \\(X\\) if \\(\\mathfrak{M}\\) has the following properties:\n\\(X \\in \\mathfrak{M}\\). If \\(A \\in \\mathfrak{M}\\), then \\(A^c \\in \\mathfrak{M}\\), where \\(A^c\\) is the complement of \\(A\\) relative to \\(X\\). If \\(A=\\bigcup_{n=1}^{\\infty} A_n\\) and if \\(A_n \\in \\mathfrak{M}\\) for \\(n=1,2,3, \\ldots\\), then \\(A \\in \\mathfrak{M}\\). If \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra in \\(X\\), then \\(X\\) is called a measurable space, and the members of \\(\\mathfrak{M}\\) are called the measurable sets in \\(X\\).\nIf \\(X\\) is a measurable space, \\(Y\\) is a topological space, and \\(f\\) is a mapping of \\(X\\) into \\(Y\\), then \\(f\\) is said to be measurable provided that \\(f^{-1}(V)\\) is a measurable set in \\(X\\) for every open set \\(V\\) in \\(Y\\).\nIt would perhaps be more satisfactory to apply the term “measurable space” to the ordered pair \\((X, \\mathfrak{M})\\), rather than to \\(X\\). After all, \\(X\\) is a set, and \\(X\\) has not been changed in any way by the fact that we now also have a \\(\\sigma\\)-algebra of its subsets in mind. Similarly, a topological space is an ordered pair \\((X, \\tau) .\\) But if this sort of thing were systematically done in all mathematics, the terminology would become awfully cumbersome. We shall discuss this again at somewhat greater length in Sec. 1.21.\n1.4 Comments on Definition 1.2 The most familiar topological spaces are the metric spaces. We shall assume some familiarity with metric spaces but shall give the basic definitions, for the sake of completeness.\nA metric space is a set \\(X\\) in which a distance function (or metric) \\(\\rho\\) is defined, with the following properties:\n\\(0 \\leq \\rho(x, y)\u0026lt;\\infty\\) for all \\(x\\) and \\(y \\in X\\). \\(\\rho(x, y)=0\\) if and only if \\(x=y\\). \\(\\rho(x, y)=\\rho(y, x)\\) for all \\(x\\) and \\(y \\in X\\). \\(\\rho(x, y) \\leq \\rho(x, z)+\\rho(z, y)\\) for all \\(x, y\\), and \\(z \\in X\\). Property \\((d)\\) is called the triangle inequality.\nIf \\(x \\in X\\) and \\(r \\geq 0\\), the open ball with center at \\(x\\) and radius \\(r\\) is the set \\(\\{y \\in X: \\rho(x, y)\u0026lt;r\\}\\).\nIf \\(X\\) is a metric space and if \\(\\tau\\) is the collection of all sets \\(E \\subset X\\) which are arbitrary unions of open balls, then \\(\\tau\\) is a topology in \\(X\\). This is not hard to verify; the intersection property depends on the fact that if \\(x \\in B_1 \\cap B_2\\), where \\(B_1\\) and \\(B_2\\) are open balls, then \\(x\\) is the center of an open ball \\(B \\subset B_1 \\cap B_2\\). We leave this as an exercise.\nFor instance, in the real line \\(R^1\\) a set is open if and only if it is a union of open segments \\((a, b)\\). In the plane \\(R^2\\), the open sets are those which are unions of open circular discs.\nAnother topological space, which we shall encounter frequently, is the extended real line \\([-\\infty, \\infty]\\); its topology is defined by declaring the following sets to be open: \\((a, b),[-\\infty, a),(a, \\infty]\\), and any union of segments of this type. The definition of continuity given in Sec. \\(1.2(c)\\) is a global one. Frequently it is desirable to define continuity locally: A mapping \\(f\\) of \\(X\\) into \\(Y\\) is said to be continuous at the point \\(x_0 \\in X\\) if to every neighborhood \\(V\\) of \\(f\\left(x_0\\right)\\) there corresponds a neighborhood \\(W\\) of \\(x_0\\) such that \\(f(W) \\subset V\\).\n(A neighborhood of a point \\(x\\) is, by definition, an open set which contains \\(x\\).)\nWhen \\(X\\) and \\(Y\\) are metric spaces, this local definition is of course the same as the usual epsilon-delta definition, and is equivalent to the requirement that \\(\\lim f\\left(x_n\\right)=f\\left(x_0\\right)\\) in \\(Y\\) whenever \\(\\lim x_n=x_0\\) in \\(X\\).\nThe following easy proposition relates the local and global definitions of continuity in the expected manner:\n1.5 Proposition Let \\(X\\) and \\(Y\\) be topological spaces. A mapping \\(f\\) of \\(X\\) into \\(Y\\) is continuous if and only if \\(f\\) is continuous at every point of \\(X\\).\nProof If \\(f\\) is continuous and \\(x_0 \\in X\\), then \\(f^{-1}(V)\\) is a neighborhood of \\(x_0\\), for every neighborhood \\(V\\) of \\(f\\left(x_0\\right)\\). Since \\(f\\left(f^{-1}(V)\\right) \\subset V\\), it follows that \\(f\\) is continuous at \\(x_0\\).\nIf \\(f\\) is continuous at every point of \\(X\\) and if \\(V\\) is open in \\(Y\\), every point \\(x \\in f^{-1}(V)\\) has a neighborhood \\(W_x\\) such that \\(f\\left(W_x\\right) \\subset V\\). Therefore \\(W_x \\subset\\) \\(f^{-1}(V)\\). It follows that \\(f^{-1}(V)\\) is the union of the open sets \\(W_x\\), so \\(f^{-1}(V)\\) is itself open. Thus \\(f\\) is continuous. \\(\\blacksquare\\)\n1.6 Comments on Definition 1.3 Let \\(\\mathfrak{M}\\) be a \\(\\sigma\\)-algebra in a set \\(X\\). Referring to Properties (i) to (iii) of Definition 1.3(a), we immediately derive the following facts.\nSince \\(\\varnothing=X^c\\), (i) and (ii) imply that \\(\\varnothing \\in \\mathfrak{M}\\).\nTaking \\(A_{n+1}=A_{n+2}=\\cdots=\\varnothing\\) in (iii), we see that \\(A_1 \\cup A_2 \\cup \\cdots \\cup A_n\\) \\(\\in \\mathfrak{M}\\) if \\(A_i \\in \\mathfrak{M}\\) for \\(i=1, \\ldots, n\\).\nSince \\[ \\bigcap_{n=1}^{\\infty} A_n=\\left(\\bigcup_{n=1}^{\\infty} A_n^c\\right)^c \\] \\(\\mathfrak{M}\\) is closed under the formation of countable (and also finite) intersections.\nSince \\(A-B=B^c \\cap A\\), we have \\(A-B \\in \\mathfrak{M}\\) if \\(A \\in \\mathfrak{M}\\) and \\(B \\in \\mathfrak{M}\\). The prefix \\(\\sigma\\) refers to the fact that (iii) is required to hold for all countable unions of members of \\(\\mathfrak{M}\\). If (iii) is required for finite unions only, then \\(\\mathfrak{M}\\) is called an algebra of sets.\n1.7 Theorem Let \\(Y\\) and \\(Z\\) be topological spaces, and let \\(g: Y \\rightarrow Z\\) be continuous.\nIf \\(X\\) is a topological space, if \\(f: X \\rightarrow Y\\) is continuous, and if \\(h=g \\circ f\\), then \\(h: X \\rightarrow Z\\) is continuous. If \\(X\\) is a measurable space, if \\(f: X \\rightarrow Y\\) is measurable, and if \\(h=g \\circ f\\), then \\(h: X \\rightarrow Z\\) is measurable. Stated informally, continuous functions of continuous functions are continuous; continuous functions of measurable functions are measurable.\nProof If \\(V\\) is open in \\(Z\\), then \\(g^{-1}(V)\\) is open in \\(Y\\), and \\[ h^{-1}(V)=f^{-1}\\left(g^{-1}(V)\\right) \\] If \\(f\\) is continuous, it follows that \\(h^{-1}(V)\\) is open, proving \\((a)\\). If \\(f\\) is measurable, it follows that \\(h^{-1}(V)\\) is measurable, proving \\((b)\\). \\(\\blacksquare\\)\n1.8 Theorem Let \\(u\\) and \\(v\\) be real measurable functions on a measurable space \\(X\\), let \\(\\Phi\\) be a continuous mapping of the plane into a topological space \\(Y\\), and define \\[ h(x)=\\Phi(u(x), v(x)) \\] for \\(x \\in X\\). Then \\(h: X \\rightarrow Y\\) is measurable.\nProof Put \\(f(x)=(u(x), v(x))\\). Then \\(f\\) maps \\(X\\) into the plane. Since \\(h=\\Phi \\circ f\\), Theorem \\(1.7\\) shows that it is enough to prove the measurability of \\(f\\).\nIf \\(R\\) is any open rectangle in the plane, with sides parallel to the axes, then \\(R\\) is the cartesian product of two segments \\(I_1\\) and \\(I_2\\), and \\[ f^{-1}(R)=u^{-1}\\left(I_1\\right) \\cap v^{-1}\\left(I_2\\right), \\] which is measurable, by our assumption on \\(u\\) and \\(v\\). Every open set \\(V\\) in the plane is a countable union of such rectangles \\(R_i\\), and since \\[ f^{-1}(V)=f^{-1}\\left(\\bigcup_{i=1}^{\\infty} R_i\\right)=\\bigcup_{i=1}^{\\infty} f^{-1}\\left(R_i\\right) \\] \\(f^{-1}(V)\\) is measurable. \\(\\blacksquare\\)\n1.9 Let \\(X\\) be a measurable space. The following propositions are corollaries of Theorems \\(1.7\\) and \\(1.8\\) : (a) If \\(f=u+i v\\), where \\(u\\) and \\(v\\) are real measurable functions on \\(X\\), then \\(f\\) is a complex measurable function on \\(X\\).\nThis follows from Theorem 1.8, with \\(\\Phi(z)=z\\).\nIf \\(f=u+i v\\) is a complex measurable function on \\(X\\), then \\(u, v\\), and \\(|f|\\) are real measurable functions on \\(X\\). This follows from Theorem 1.7, with \\(g(z)=\\operatorname{Re}(z), \\operatorname{Im}(z)\\), and \\(|z|\\).\nIf \\(f\\) and \\(g\\) are complex measurable functions on \\(X\\), then so are \\(f+g\\) and \\(f g\\). For real \\(f\\) and \\(g\\) this follqws from Theorem 1.8, with \\[ \\Phi(s, t)=s+t \\] and \\(\\Phi(s, t)=s t\\). The complex case then follows from \\((a)\\) and \\((b)\\).\nIf \\(E\\) is a measurable set in \\(X\\) and if \\[ \\chi_E(x)= \\begin{cases}1 \u0026amp; \\text { if } x \\in E \\\\ 0 \u0026amp; \\text { if } x \\notin E\\end{cases} \\] then \\(\\chi_E\\) is a measurable function. This is obvious. We call \\(\\chi_E\\) the characteristic function of the set \\(E\\). The letter \\(\\chi\\) will be reserved for characteristic functions throughout this book.\nIf \\(f\\) is a complex measurable function on \\(X\\), there is a complex measurable function \\(\\alpha\\) on \\(X\\) such that \\(|\\alpha|=1\\) and \\(f=\\alpha|f|\\). Proof Let \\(E=\\{x: f(x)=0\\}\\), let \\(Y\\) be the complex plane with the origin removed, define \\(\\varphi(z)=z /|z|\\) for \\(z \\in Y\\), and put \\[ \\alpha(x)=\\varphi\\left(f(x)+\\chi_E(x)\\right) \\quad(x \\in X) . \\] If \\(x \\in E, \\alpha(x)=1\\); if \\(x \\notin E, \\alpha(x)=f(x) /|f(x)|\\). Since \\(\\varphi\\) is continuous on \\(Y\\) and since \\(E\\) is measurable (why?), the measurability of \\(\\alpha\\) follows from \\((c),(d)\\), and Theorem 1.7.\nWe now show that \\(\\sigma\\)-algebras exist in great profusion.\n1.10 Theorem If \\(\\mathscr{F}\\) is any collection of subsets of \\(X\\), there exists a smallest \\(\\sigma\\)-algebra \\(\\mathfrak{M}^*\\) in \\(X\\) such that \\(\\mathscr{F} \\subset \\mathfrak{M}^*\\).\nThis \\(\\mathfrak{M}^*\\) is sometimes called the \\(\\sigma\\)-algebra generated by \\(\\mathscr{F}\\).\nProof Let \\(\\Omega\\) be the family of all \\(\\sigma\\)-algebras \\(\\mathfrak{M}\\) in \\(X\\) which contain \\(\\mathscr{F}\\). Since the collection of all subsets of \\(X\\) is such a \\(\\sigma\\)-algebra, \\(\\Omega\\) is not empty. Let \\(\\mathfrak{M}^*\\) be the intersection of all \\(\\mathfrak{M} \\in \\Omega\\). It is clear that \\(\\mathscr{F} \\subset \\mathfrak{M}^*\\) and that \\(\\mathfrak{M}^*\\) lies in every \\(\\sigma\\)-algebra in \\(X\\) which contains \\(\\mathscr{F}\\). To complete the proof, we have to show that \\(\\mathfrak{M}^*\\) is itself a \\(\\sigma\\)-algebra.\nIf \\(A_n \\in \\mathfrak{M}^*\\) for \\(n=1,2,3, \\ldots\\), and if \\(\\mathfrak{M} \\in \\Omega\\), then \\(A_n \\in \\mathfrak{M}\\), so \\(\\bigcup A_n \\in \\mathfrak{M}\\), since \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra. Since \\(\\bigcup A_n \\in \\mathfrak{M}\\) for every \\(\\mathfrak{M} \\in \\Omega\\), we conclude that \\(\\bigcup A_n \\in \\mathfrak{M}^*\\). The other two defining properties of a \\(\\sigma\\)-algebra are verified in the same manner. \\(\\blacksquare\\)\n1.11 Borel Sets Let \\(X\\) be a topological space. By Theorem 1.10, there exists a smallest \\(\\sigma\\)-algebra \\(\\mathscr{B}\\) in \\(X\\) such that every open set in \\(X\\) belongs to \\(\\mathscr{B}\\). The members of \\(\\mathscr{B}\\) are called the Borel sets of \\(X\\).\nIn particular, closed sets are Borel sets (being, by definition, the complements of open sets), and so are all countable unions of closed sets and all countable intersections of open sets. These last two are called \\(F_\\sigma\\) ’s and \\(G_\\delta\\) ’s, respectively, and play a considerable role. The notation is due to Hausdorff. The letters \\(F\\) and \\(G\\) were used for closed and open sets, respectively, and \\(\\sigma\\) refers to union (Summe), \\(\\delta\\) to intersection (Durchschnitt). For example, every half-open interval \\([a, b)\\) is a \\(G_\\delta\\) and an \\(F_\\sigma\\) in \\(R^1\\).\nSince \\(\\mathscr{B}\\) is a \\(\\sigma\\)-algebra, we may now regard \\(X\\) as a measurable space, with the Borel sets playing the role of the measurable sets; more concisely, we consider the measurable space \\((X, \\mathscr{B})\\). If \\(f: X \\rightarrow Y\\) is a continuous mapping of \\(X\\), where \\(Y\\) is any topological space, then it is evident from the definitions that \\(f^{-1}(V) \\in \\mathscr{B}\\) for every open set \\(V\\) in \\(Y\\). In other words, every continuous mapping of \\(X\\) is Borel measurable.\nBorel measurable mappings are often called Borel mappings, or Borel functions.\n1.12 Theorem Suppose \\(\\mathfrak{M}\\) is a \\(\\sigma\\)-algebra in \\(X\\), and \\(Y\\) is a topological space. Let \\(f\\) map \\(X\\) into \\(Y\\).\nIf \\(\\Omega\\) is the collection of all sets \\(E \\subset Y\\) such that \\(f^{-1}(E) \\in \\mathfrak{M}\\), then \\(\\Omega\\) is a \\(\\sigma\\)-algebra in \\(Y\\).\nIf is measurable and \\(E\\) is a Borel set in \\(Y\\), then \\(f^{-1}(E) \\in \\mathfrak{M}\\).\nIf \\(Y=[-\\infty, \\infty]\\) and \\(f^{-1}((\\alpha, \\infty]) \\in \\mathfrak{M}\\) for every real \\(\\alpha\\), then \\(f\\) is measurable.\nIf \\(f\\) is measurable, if \\(Z\\) is a topological space, if \\(g: Y \\rightarrow Z\\) is a Borel mapping, and if \\(h=g \\circ f\\), then \\(h: X \\rightarrow Z\\) is measurable.\nPart \\((c)\\) is a frequently used criterion for the measurability of real-valued functions. (See also Exercise 3.) Note that \\((d)\\) generalizes Theorem 1.7(b).\nProof (a) follows from the relations \\[ \\begin{aligned} f^{-1}(Y) \u0026amp;=X, \\\\ f^{-1}(Y-A) \u0026amp;=X-f^{-1}(A), \\\\ \\text { and } \\quad f^{-1}\\left(A_1 \\cup A_2 \\cup \\cdots\\right) \u0026amp;=f^{-1}\\left(A_1\\right) \\cup f^{-1}\\left(A_2\\right) \\cup \\cdots . \\end{aligned} \\] To prove \\((b)\\), let \\(\\Omega\\) be as in \\((a)\\); the measurability of \\(f\\) implies that \\(\\Omega\\) contains all open sets in \\(Y\\), and since \\(\\Omega\\) is a \\(\\sigma\\)-algebra, \\(\\Omega\\) contains all Borel sets in \\(Y\\).\nTo prove \\((c)\\), let \\(\\Omega\\) be the collection of all \\(E \\subset[-\\infty, \\infty]\\) such that \\(f^{-1}(E) \\in \\mathfrak{M}\\). Choose a real number \\(\\alpha\\), and choose \\(\\alpha_n\u0026lt;\\alpha\\) so that \\(\\alpha_n \\rightarrow \\alpha\\) as \\(n \\rightarrow \\infty\\). Since \\(\\left(\\alpha_n, \\infty\\right] \\in \\Omega\\) for each \\(n\\), since \\[ [-\\infty, \\alpha)=\\bigcup_{n=1}^{\\infty}\\left[-\\infty, \\alpha_n\\right]=\\bigcup_{n=1}^{\\infty}\\left(\\alpha_n, \\infty\\right]^c, \\] and since \\((a)\\) shows that \\(\\Omega\\) is a \\(\\sigma\\)-algebra, we see that \\([-\\infty, \\alpha) \\in \\Omega\\). The same is then true of \\[ (\\alpha, \\beta)=[-\\infty, \\beta) \\cap(\\alpha, \\infty] . \\] Since every open set in \\([-\\infty, \\infty]\\) is a countable union of segments of the above types, \\(\\Omega\\) contains every open set. Thus \\(f\\) is measurable.\nTo prove \\((d)\\), let \\(V \\subset Z\\) be open. Then \\(g^{-1}(V)\\) is a Borel set of \\(Y\\), and since \\[ h^{-1}(V)=f^{-1}\\left(g^{-1}(V)\\right) \\] (b) shows that \\(h^{-1}(V) \\in \\mathfrak{M}\\). \\(\\blacksquare\\)\n1.13 Definition Let \\(\\left\\{a_n\\right\\}\\) be a sequence in \\([-\\infty, \\infty]\\), and put \\[ \\begin{equation} b_k=\\sup \\left\\{a_k, a_{k+1}, a_{k+2}, \\ldots\\right\\} \\quad(k=1,2,3, \\ldots) \\end{equation} \\] and \\[ \\beta=\\inf \\left\\{b_1, b_2, b_3, \\ldots\\right\\} . \\] We call \\(\\beta\\) the upper limit of \\(\\left\\{a_n\\right\\}\\), and write \\[ \\beta=\\limsup _{n \\rightarrow \\infty} a_n . \\] The following properties are easily verified: First, \\(b_1 \\geq b_2 \\geq b_3 \\geq \\cdots\\), so that \\(b_k \\rightarrow \\beta\\) as \\(k \\rightarrow \\infty\\); secondly, there is a subsequence \\(\\left\\{a_{n_i}\\right\\}\\) of \\(\\left\\{a_n\\right\\}\\) such that \\(a_{n_i} \\rightarrow \\beta\\) as \\(i \\rightarrow \\infty\\), and \\(\\beta\\) is the largest number with this property.\nThe lower limit is defined analogously: simply interchange sup and inf in (1) and (2). Note that \\[ \\liminf _{n \\rightarrow \\infty} a_n=-\\limsup _{n \\rightarrow \\infty}\\left(-a_n\\right) \\] If \\(\\left\\{a_n\\right\\}\\) converges, then evidently \\[ \\limsup _{n \\rightarrow \\infty} a_n=\\liminf _{n \\rightarrow \\infty} a_n=\\lim _{n \\rightarrow \\infty} a_n . \\] Suppose \\(\\left\\{f_n\\right\\}\\) is a sequence of extended-real functions on a set \\(X\\). Then \\(\\sup f_n\\) and \\(\\lim \\sup f_n\\) are the functions defined on \\(X\\) by \\[ \\begin{gathered} \\left(\\sup _n f_n\\right)(x)=\\sup _n\\left(f_n(x)\\right), \\\\ \\left(\\limsup _{n \\rightarrow \\infty} f_n\\right)(x)=\\limsup _{n \\rightarrow \\infty}\\left(f_n(x)\\right) . \\end{gathered} \\] If \\[ f(x)=\\lim _{n \\rightarrow \\infty} f_n(x), \\] the limit being assumed to exist at every \\(x \\in X\\), then we call \\(f\\) the pointwise limit of the sequence \\(\\left\\{f_n\\right\\}\\).\n1.14 Theorem If \\(f_n: X \\rightarrow[-\\infty, \\infty]\\) is measurable, for \\(n=1,2,3, \\ldots\\), and \\(g=\\sup _{n \\geq 1} f_n, \\quad h=\\lim _{n \\rightarrow \\infty} \\sup _n\\), then \\(g\\) and \\(h\\) are measurable. Proof \\(g^{-1}((\\alpha, \\infty])=\\bigcup_{n=1}^{\\infty} f_n^{-1}((\\alpha, \\infty])\\). Hence Theorem 1.12(c) implies that \\(g\\) is measurable. The same result holds of course with inf in place of sup, and since \\[ h=\\inf _{k \\geq 1}\\left\\{\\sup _{i \\geq k} f_i\\right\\}, \\] it follows that \\(h\\) is measurable. \\(\\blacksquare\\)\nCorollaries (a) The limit of every pointwise convergent sequence of complex measurable functions is measurable.\nIf \\(f\\) and \\(g\\) are measurable (with range in \\([-\\infty, \\infty]\\) ), then so are \\(\\max \\{f, g\\}\\) and \\(\\min \\{f, g\\}\\). In particular, this is true of the functions \\[ f^{+}=\\max \\{f, 0\\} \\quad \\text { and } f^{-}=-\\min \\{f, 0\\} . \\] 1.15 The above functions \\(f^{+}\\)and \\(f^{-}\\)are called the positive and negative parts of \\(f\\). We have \\(|f|=f^{+}+f^{-}\\)and \\(f=f^{+}-f^{-}\\), a standard representation of \\(f\\) as a difference of two nonnegative functions, with a certain minimality property:\nProposition If \\(f=g-h, g \\geq 0\\), and \\(h \\geq 0\\), then \\(f^{+} \\leq g\\) and \\(f^{-} \\leq h\\).\nProof \\(f \\leq g\\) and \\(0 \\leq g\\) clearly implies \\(\\max \\{f, 0\\} \\leq g\\).\n","date":"2022-09-17T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-1/2-the-concept-of-measurability/","section":"papa rudin","tags":null,"title":"2 The Concept of Measurability"},{"categories":null,"contents":" youtube bilibili note pdf xopp We shall now compute the limits of some sequences which occur frequently. The proofs will all be based on the following remark: If \\(0 \\leq x_{n} \\leq s_{n}\\) for \\(n \\geq N\\), where \\(N\\) is some fixed number, and if \\(s_{n} \\rightarrow 0\\), then \\(x_{n} \\rightarrow 0\\).\n3.20 Theorem\nIf \\(p\u0026gt;0\\), then \\(\\lim _{n \\rightarrow \\infty} \\frac{1}{n^{p}}=0\\).\nIf \\(p\u0026gt;0\\), then \\(\\lim _{n \\rightarrow \\infty} \\sqrt[n]{p}=1\\).\n\\(\\lim _{n \\rightarrow \\infty} \\sqrt[n]{n}=1\\).\nIf \\(p\u0026gt;0\\) and \\(\\alpha\\) is real, then \\(\\lim _{n \\rightarrow \\infty} \\frac{n^{\\alpha}}{(1+p)^{n}}=0\\).\nIf \\(|x|\u0026lt;1\\), then \\(\\lim _{n \\rightarrow \\infty} x^{n}=0\\).\nProof (a) Take \\(n\u0026gt;(1 / \\varepsilon)^{1 / p}\\). (Note that the archimedean property of the real number system is used here.) (b) If \\(p\u0026gt;1\\), put \\(x_n=\\sqrt[n]{p}-1\\). Then \\(x_n\u0026gt;0\\), and, by the binomial theorem, \\[ 1+n x_n \\leq\\left(1+x_n\\right)^n=p, \\] so that \\[ 0\u0026lt;x_n \\leq \\frac{p-1}{n} . \\] Hence \\(x_n \\rightarrow 0\\). If \\(p=1,(b)\\) is trivial, and if \\(0\u0026lt;p\u0026lt;1\\), the result is obtained by taking reciprocals. (c) Put \\(x_n=\\sqrt[n]{n}-1\\). Then \\(x_n \\geq 0\\), and, by the binomial theorem, \\[ n=\\left(1+x_n\\right)^n \\geq \\frac{n(n-1)}{2} x_n^2 . \\] Hence \\[ 0 \\leq x_n \\leq \\sqrt{\\frac{2}{n-1}} \\quad(n \\geq 2) . \\] (d) Let \\(k\\) be an integer such that \\(k\u0026gt;\\alpha, k\u0026gt;0\\). For \\(n\u0026gt;2 k\\), \\[ (1+p)^n\u0026gt;\\left(\\begin{array}{l} n \\\\ k \\end{array}\\right) p^k=\\frac{n(n-1) \\cdots(n-k+1)}{k !} p^k\u0026gt;\\frac{n^k p^k}{2^k k !} . \\] Hence \\[ 0\u0026lt;\\frac{n^\\alpha}{(1+p)^n}\u0026lt;\\frac{2^k k !}{p^k} n^{\\alpha-k} \\quad(n\u0026gt;2 k) . \\] Since \\(\\alpha-k\u0026lt;0, n^{\\alpha-k} \\rightarrow 0\\), by \\((a)\\). (e) Take \\(\\alpha=0\\) in \\((d)\\).\n","date":"2022-09-11T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/5-some-special-sequences/","section":"baby rudin","tags":null,"title":"5 SOME SPECIAL SEQUENCES"},{"categories":null,"contents":"Following command disable suspend and hibernate function in Linux.\n# systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target ","date":"2022-09-08T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/linux-disable-suspend-and-hibernate/","section":"posts","tags":["Linux"],"title":"Linux disable suspend and hibernate"},{"categories":null,"contents":" youtube bilibili note pdf xopp 3.8 Definition A sequence \\(\\left\\{p_{n}\\right\\}\\) in a metric space \\(X\\) is said to be a Cauchy sequence if for every \\(\\varepsilon\u0026gt;0\\) there is an integer \\(N\\) such that \\(d\\left(p_{n}, p_{m}\\right)\u0026lt;\\varepsilon\\) if \\(n \\geq N\\) and \\(m \\geq N\\).\nIn our discussion of Cauchy sequences, as well as in other situations which will arise later, the following geometric concept will be useful.\n3.9 Definition Let \\(E\\) be a nonempty subset of a metric space \\(X\\), and let \\(S\\) be the set of all real numbers of the form \\(d(p, q)\\), with \\(p \\in E\\) and \\(q \\in E\\). The sup of \\(S\\) is called the diameter of \\(E\\). If \\(\\left\\{p_{n}\\right\\}\\) is a sequence in \\(X\\) and if \\(E_{N}\\) consists of the points \\(p_{N}, p_{N+1}, p_{N+2}, \\ldots\\), it is clear from the two preceding definitions that \\(\\left\\{p_{n}\\right\\}\\) is a Cauchy sequence if and only if\n\\[ \\lim _{N \\rightarrow \\infty} \\operatorname{diam} E_{N}=0 . \\]\n3.10 Theorem\nIf \\(\\bar{E}\\) is the closure of a set \\(E\\) in a metric space \\(X\\), then \\[ \\operatorname{diam} \\bar{E}=\\operatorname{diam} E . \\]\nIf \\(K_{n}\\) is a sequence of compact sets in \\(X\\) such that \\(K_{n} \\supset K_{n+1}\\) \\((n=1,2,3, \\ldots)\\) and if \\[ \\lim _{n \\rightarrow \\infty} \\operatorname{diam} K_{n}=0, \\]\nthen \\(\\bigcap_{1}^{\\infty} K_{n}\\) consists of exactly one point.\nProof\nSince \\(E \\subset \\bar{E}\\), it is clear that \\[ \\operatorname{diam} E \\leq \\operatorname{diam} \\bar{E} . \\]\nFix \\(\\varepsilon\u0026gt;0\\), and choose \\(p \\in \\bar{E}, q \\in \\bar{E}\\). By the definition of \\(\\bar{E}\\), there are points \\(p^{\\prime}, q^{\\prime}\\), in \\(E\\) such that $d(p, p^{})\u0026lt;, d(q, q^{\n\\[ \\begin{aligned} d(p, q) \u0026amp; \\leq d\\left(p, p^{\\prime}\\right)+d\\left(p^{\\prime} q^{\\prime}\\right)+d\\left(q^{\\prime}, q\\right) \\\\ \u0026amp;\u0026lt;2 \\varepsilon+d\\left(p^{\\prime}, q^{\\prime}\\right) \\leq 2 \\varepsilon+\\operatorname{diam} E \\end{aligned} \\]\nIt follows that\n\\[ \\operatorname{diam} \\bar{E} \\leq 2 \\varepsilon+\\operatorname{diam} E, \\]\nand since \\(\\varepsilon\\) was arbitrary, \\((a)\\) is proved.\nPut \\(K=\\bigcap_{1}^{\\infty} K_{n}\\). By Theorem \\(2.36, K\\) is not empty. If \\(K\\) contains more than one point, then diam \\(K\u0026gt;0\\). But for each \\(n, K_{n} \\supset K\\), so that \\(\\operatorname{diam} K_{n} \\geq \\operatorname{diam} K\\). This contradicts the assumption that diam \\(K_{n} \\rightarrow 0\\). 3.11 Theorem\nIn any metric space \\(X\\), every convergent sequence is a Cauchy sequence.\nIf \\(X\\) is a compact metric space and if \\(\\left\\{p_{n}\\right\\}\\) is a Cauchy sequence in \\(X\\), then \\(\\left\\{p_{n}\\right\\}\\) converges to some point of \\(X\\).\nIn \\(R^{k}\\), every Cauchy sequence converges.\nNote: The difference between the definition of convergence and the definition of a Cauchy sequence is that the limit is explicitly involved in the former, but not in the latter. Thus Theorem 3.11(b) may enable us to decide whether or not a given sequence converges without knowledge of the limit to which it may converge.\nThe fact (contained in Theorem 3.11) that a sequence converges in \\(R^{k}\\) if and only if it is a Cauchy sequence is usually called the Cauchy criterion for convergence.\nProof\nIf \\(p_{n} \\rightarrow p\\) and if \\(\\varepsilon\u0026gt;0\\), there is an integer \\(N\\) such that \\(d\\left(p, p_{n}\\right)\u0026lt;\\varepsilon\\) for all \\(n \\geq N\\). Hence \\[ d\\left(p_{n}, p_{m}\\right) \\leq d\\left(p_{n}, p\\right)+d\\left(p, p_{m}\\right)\u0026lt;2 \\varepsilon \\]\nas soon as \\(n \\geq N\\) and \\(m \\geq N\\). Thus \\(\\left\\{p_{n}\\right\\}\\) is a Cauchy sequence.\nLet \\(\\left\\{p_{n}\\right\\}\\) be a Cauchy sequence in the compact space \\(X\\). For \\(N=1,2,3, \\ldots\\), let \\(E_{N}\\) be the set consisting of \\(p_{N}, p_{N+1}, p_{N+2}, \\ldots\\) Then \\[ \\lim _{N \\rightarrow \\infty} \\operatorname{diam} \\bar{E}_{N}=0, \\]\nby Definition \\(3.9\\) and Theorem 3.10 \\(a\\). Being a closed subset of the compact space \\(X\\), each \\(\\bar{E}_{N}\\) is compact (Theorem 2.35). Also \\(E_{N} \\supset E_{N+1}\\), so that \\(\\bar{E}_{N} \\supset \\bar{E}_{N+1}\\).\nTheorem \\(3.10(b)\\) shows now that there is a unique \\(p \\in X\\) which lies in every \\(\\bar{E}_{N}\\).\nLet \\(\\varepsilon\u0026gt;0\\) be given. By (3) there is an integer \\(N_{0}\\) such that \\(\\operatorname{diam} \\bar{E}_{N}\u0026lt;\\varepsilon\\) if \\(N \\geq N_{0}\\). Since \\(p \\in \\bar{E}_{N}\\), it follows that \\(d(p, q)\u0026lt;\\varepsilon\\) for every \\(q \\in \\bar{E}_{N}\\), hence for every \\(q \\in E_{N}\\). In other words, \\(d\\left(p, p_{n}\\right)\u0026lt;\\varepsilon\\) if \\(n \\geq N_{0}\\). This says precisely that \\(p_{n} \\rightarrow p\\).\nLet \\(\\left\\{\\mathbf{x}_{n}\\right\\}\\) be a Cauchy sequence in \\(R^{k}\\). Define \\(E_{N}\\) as in (b), with \\(\\mathbf{x}_{i}\\) in place of \\(p_{i}\\). For some \\(N\\), \\(\\operatorname{diam} E_{N}\u0026lt;1\\). The range of \\(\\left\\{\\mathbf{x}_{n}\\right\\}\\) is the union of \\(E_{N}\\) and the finite set \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{N-1}\\right\\}\\). Hence \\(\\left\\{\\mathbf{x}_{n}\\right\\}\\) is bounded. Since every bounded subset of \\(R^{k}\\) has compact closure in \\(R^{k}\\) (Theorem 2.41), (c) follows from \\((b)\\). 3.12 Definition A metric space in which every Cauchy sequence converges is said to be complete.\nThus Theorem \\(3.11\\) says that all compact metric spaces and all Euclidean spaces are complete. Theorem \\(3.11\\) implies also that every closed subset \\(E\\) of a complete metric space \\(X\\) is complete. (Every Cauchy sequence in \\(E\\) is a Cauchy sequence in \\(X\\), hence it converges to some \\(p \\in X\\), and actually \\(p \\in E\\) since \\(E\\) is closed.) An example of a metric space which is not complete is the space of all rational numbers, with \\(d(x, y)=|x-y|\\).\nTheorem 3.2(c) and example \\((d)\\) of Definition 3.1 show that convergent sequences are bounded, but that bounded sequences in \\(R^{k}\\) need not converge. However, there is one important case in which convergence is equivalent to boundedness; this happens for monotonic sequences in \\(R^{1}\\).\n3.13 Definition A sequence \\(\\left\\{s_{n}\\right\\}\\) of real numbers is said to be\nmonotonically increasing if \\(s_{n} \\leq s_{n+1}(n=1,2,3, \\ldots)\\);\nmonotonically decreasing if \\(s_{n} \\geq s_{n+1}(n=1,2,3, \\ldots)\\).\nThe class of monotonic sequences consists of the increasing and the decreasing sequences.\n3.14 Theorem Suppose \\(\\left\\{s_{n}\\right\\}\\) is monotonic. Then \\(\\left\\{s_{n}\\right\\}\\) converges if and only if it is bounded.\nProof Suppose \\(s_{n} \\leq s_{n+1}\\) (the proof is analogous in the other case). Let \\(E\\) be the range of \\(\\left\\{s_{n}\\right\\}\\). If \\(\\left\\{s_{n}\\right\\}\\) is bounded, let \\(s\\) be the least upper bound of \\(E\\). Then\n\\[ s_{n} \\leq s \\quad(n=1,2,3, \\ldots) . \\]\nFor every \\(\\varepsilon\u0026gt;0\\), there is an integer \\(N\\) such that\n\\[ s-\\varepsilon\u0026lt;s_{N} \\leq s, \\]\nfor otherwise \\(s-\\varepsilon\\) would be an upper bound of \\(E\\). Since \\(\\left\\{s_{n}\\right\\}\\) increases, \\(n \\geq N\\) therefore implies\n\\[ s-\\varepsilon\u0026lt;s_{n} \\leq s, \\]\nwhich shows that \\(\\left\\{s_{n}\\right\\}\\) converges (to \\(s\\) ).\nThe converse follows from Theorem 3.2(c).\n","date":"2022-09-06T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/3-cauchy-sequences/","section":"baby rudin","tags":null,"title":"3 CAUCHY SEQUENCES"},{"categories":null,"contents":" youtube bilibili note pdf xopp 3.15 Definition Let \\(\\left\\{s_{n}\\right\\}\\) be a sequence of real numbers with the following property: For every real \\(M\\) there is an integer \\(N\\) such that \\(n \\geq N\\) implies \\(s_{n} \\geq M\\). We then write \\[ s_{n} \\rightarrow+\\infty . \\]\nSimilarly, if for every real \\(M\\) there is an integer \\(N\\) such that \\(n \\geq N\\) implies \\(s_{n} \\leq M\\), we write\n\\[ s_{n} \\rightarrow-\\infty \\]\nIt should be noted that we now use the symbol \\(\\rightarrow\\) (introduced in Definition 3.1) for certain types of divergent sequences, as well as for convergent sequences, but that the definitions of convergence and of limit, given in Definition 3.1, are in no way changed.\n3.16 Definition Let \\(\\left\\{s_{n}\\right\\}\\) be a sequence of real numbers. Let \\(E\\) be the set of numbers \\(x\\) (in the extended real number system) such that \\(s_{n_{k}} \\rightarrow x\\) for some subsequence \\(\\left\\{s_{n_{k}}\\right\\}\\). This set \\(E\\) contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers \\(+\\infty,-\\infty\\).\nWe now recall Definitions \\(1.8\\) and \\(1.23\\) and put\n\\[ \\begin{gathered} s^{*}=\\sup E, \\\\ s_{*}=\\inf E . \\end{gathered} \\]\nThe numbers \\(s^{*}, s_{*}\\) are called the upper and lower limits of \\(\\left\\{s_{n}\\right\\}\\); we use the notation\n\\[ \\limsup _{n \\rightarrow \\infty} s_{n}=s^{*}, \\quad \\liminf _{n \\rightarrow \\infty} s_{n}=s_{*} . \\]\n3.17 Theorem Let \\(\\left\\{s_{n}\\right\\}\\) be a sequence of real numbers. Let \\(E\\) and \\(s^{*}\\) have the same meaning as in Definition 3.16. Then \\(s^{*}\\) has the following two properties:\n\\(s^{*} \\in E\\).\nIf \\(x\u0026gt;s^{*}\\), there is an integer \\(N\\) such that \\(n \\geq N\\) implies \\(s_{n}\u0026lt;x\\).\nMoreover, \\(s^{*}\\) is the only number with the properties \\((a)\\) and \\((b)\\).\nOf course, an analogous result is true for \\(s_{*}\\).\nProof\nIf \\(s^{*}=+\\infty\\), then \\(E\\) is not bounded above; hence \\(\\left\\{s_{n}\\right\\}\\) is not bounded above, and there is a subsequence \\(\\left\\{s_{n_{k}}\\right\\}\\) such that \\(s_{n_{k}} \\rightarrow+\\infty\\). If \\(s^{*}\\) is real, then \\(E\\) is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems \\(3.7\\) and 2.28.\nIf \\(s^{*}=-\\infty\\), then \\(E\\) contains only one element, namely \\(-\\infty\\), and there is no subsequential limit. Hence, for any real \\(M, s_{n}\u0026gt;M\\) for at most a finite number of values of \\(n\\), so that \\(s_{n} \\rightarrow-\\infty\\).\nThis establishes \\((a)\\) in all cases.\nSuppose there is a number \\(x\u0026gt;s^{*}\\) such that \\(s_{n} \\geq x\\) for infinitely many values of \\(n\\). In that case, there is a number \\(y \\in E\\) such that \\(y \\geq x\u0026gt;s^{*}\\), contradicting the definition of \\(s^{*}\\). Thus \\(s^{*}\\) satisfies \\((a)\\) and \\((b)\\).\nTo show the uniqueness, suppose there are two numbers, \\(p\\) and \\(q\\), which satisfy \\((a)\\) and \\((b)\\), and suppose \\(p\u0026lt;q\\). Choose \\(x\\) such that \\(p\u0026lt;x\u0026lt;q\\). Since \\(p\\) satisfies \\((b)\\), we have \\(s_{n}\u0026lt;x\\) for \\(n \\geq N\\). But then \\(q\\) cannot satisfy \\((a)\\).\n3.18 Examples\nLet \\(\\left\\{s_{n}\\right\\}\\) be a sequence containing all rationals. Then every real number is a subsequential limit, and \\[ \\limsup _{n \\rightarrow \\infty} s_{n}=+\\infty, \\quad \\liminf _{n \\rightarrow \\infty} s_{n}=-\\infty . \\]\nLet \\(s_{n}=\\left(-1^{n}\\right) /[1+(1 / n)]\\). Then \\[ \\limsup _{n \\rightarrow \\infty} s_{n}=1, \\quad \\liminf _{n \\rightarrow \\infty} s_{n}=-1 . \\]\nFor a real-valued sequence \\(\\left\\{s_{n}\\right\\}, \\lim _{n \\rightarrow \\infty} s_{n}=s\\) if and only if \\[ \\limsup _{n \\rightarrow \\infty} s_{n}=\\liminf _{n \\rightarrow \\infty} s_{n}=s . \\]\nWe close this section with a theorem which is useful, and whose proof is quite trivial:\n3.19 Theorem If \\(s_{n} \\leq t_{n}\\) for \\(n \\geq N\\), where \\(N\\) is fixed, then \\[ \\begin{aligned} \\liminf _{n \\rightarrow \\infty} s_{n} \u0026amp; \\leq \\underset{n \\rightarrow \\infty}{\\lim \\inf } \\ t_{n}, \\\\ \\limsup _{n \\rightarrow \\infty} s_{n} \u0026amp; \\leq \\limsup _{n \\rightarrow \\infty} t_{n} . \\end{aligned} \\]\n","date":"2022-09-06T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/4-upper-and-lower-limits/","section":"baby rudin","tags":null,"title":"4 UPPER AND LOWER LIMITS"},{"categories":null,"contents":" youtube bilibili note pdf xopp 3.5 Definition Given a sequence \\(\\left\\{p_{n}\\right\\}\\), consider a sequence \\(\\left\\{n_{k}\\right\\}\\) of positive integers, such that \\(n_{1}\u0026lt;n_{2}\u0026lt;n_{3}\u0026lt;\\cdots\\). Then the sequence \\(\\left\\{p_{n i}\\right\\}\\) is called a subsequence of \\(\\left\\{p_{n}\\right\\}\\). If \\(\\left\\{p_{n_{i}}\\right\\}\\) converges, its limit is called a subsequential limit of \\(\\left\\{p_{n}\\right\\}\\).\nIt is clear that \\(\\left\\{p_{n}\\right\\}\\) converges to \\(p\\) if and only if every subsequence of \\(\\left\\{p_{n}\\right\\}\\) converges to \\(p\\). We leave the details of the proof to the reader.\n3.6 Theorem\nIf \\(\\left\\{p_{n}\\right\\}\\) is a sequence in a compact metric space \\(X\\), then some subsequence of \\(\\left\\{p_{n}\\right\\}\\) converges to a point of \\(X\\).\nEvery bounded sequence in \\(R^{k}\\) contains a convergent subsequence.\nProof\nLet \\(E\\) be the range of \\(\\left\\{p_{n}\\right\\}\\). If \\(E\\) is finite then there is a \\(p \\in E\\) and a sequence \\(\\left\\{n_{1}\\right\\}\\) with \\(n_{1}\u0026lt;n_{2}\u0026lt;n_{3}\u0026lt;\\cdots\\), such that \\[ p_{n_{1}}=p_{n_{2}}=\\cdots=p . \\]\nThe subsequence \\(\\left\\{p_{n_{1}}\\right\\}\\) so obtained converges evidently to \\(p\\).\nIf \\(E\\) is infinite, Theorem \\(2.37\\) shows that \\(E\\) has a limit point \\(p \\in X\\). Choose \\(n_{1}\\) so that \\(d\\left(p, p_{n_{1}}\\right)\u0026lt;1\\). Having chosen \\(n_{1}, \\ldots, n_{i-1}\\), we see from Theorem \\(2.20\\) that there is an integer \\(n_{i}\u0026gt;n_{i-1}\\) such that \\(d\\left(p, p_{n_{1}}\\right)\u0026lt;1 / i\\). Then \\(\\left\\{p_{n}\\right\\}\\) converges to \\(p\\).\nThis follows from (a), since Theorem \\(2.41\\) implies that every bounded subset of \\(R^{k}\\) lies in a compact subset of \\(R^{k}\\). 3.7 Theorem The subsequential limits of a sequence \\(\\left\\{p_{n}\\right\\}\\) in a metric space \\(X\\) form a closed subset of \\(X\\).\nProof Let \\(E^{*}\\) be the set of all subsequential limits of \\(\\left\\{p_{n}\\right\\}\\) and let \\(q\\) be a limit point of \\(E^{*}\\). We have to show that \\(q \\in E^{*}\\).\nChoose \\(n_{1}\\) so that \\(p_{n_{1}} \\neq q\\). (If no such \\(n_{1}\\) exists, then \\(E^{*}\\) has only one point, and there is nothing to prove.) Put \\(\\delta=d\\left(q, p_{n_{1}}\\right)\\). Suppose \\(n_{1}, \\ldots, n_{i-1}\\) are chosen. Since \\(q\\) is a limit point of \\(E^{*}\\), there is an \\(x \\in E^{*}\\) with \\(d(x, q)\u0026lt;2^{-1} \\delta\\). Since \\(x \\in E^{*}\\), there is an \\(n_{i}\u0026gt;n_{i-1}\\) such that \\(d\\left(x, p_{n_{i}}\\right)\u0026lt;2^{-i} \\delta\\). Thus\n\\[ d\\left(q, p_{n i}\\right) \\leq 2^{1-1} \\delta \\]\nfor \\(i=1,2,3, \\ldots\\). This says that \\(\\left\\{p_{n}\\right\\}\\) converges to \\(q\\). Hence \\(q \\in E^{*}\\).\n","date":"2022-09-04T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/2-subsequences/","section":"baby rudin","tags":null,"title":"2 SUBSEQUENCES"},{"categories":null,"contents":" youtube part1 part2 bilibili part1 part2 note pdf 1 xopp 1 pdf 2 xopp 2 3.1 Definition A sequence \\(\\left\\{p_{n}\\right\\}\\) in a metric space \\(X\\) is said to converge if there is a point \\(p \\in X\\) with the following property: For every \\(\\varepsilon\u0026gt;0\\) there is an integer \\(N\\) such that \\(n \\geq N\\) implies that \\(d\\left(p_{n}, p\\right)\u0026lt;\\varepsilon\\). (Here \\(d\\) denotes the distance in \\(X\\).)\nIn this case we also say that \\(\\left\\{p_{n}\\right\\}\\) converges to \\(p\\), or that \\(p\\) is the limit of \\(\\left\\{p_{n}\\right\\}\\) [see Theorem 3.2(b)], and we write \\(p_{n} \\rightarrow p\\), or\n\\[ \\lim _{n \\rightarrow \\infty} p_{n}=p . \\]\nIf \\(\\left\\{p_{n}\\right\\}\\) does not converge, it is said to diverge.\nIt might be well to point out that our definition of “convergent sequence” depends not only on \\(\\left\\{p_{n}\\right\\}\\) but also on \\(X\\); for instance, the sequence \\(\\{1 / n\\}\\) converges in \\(R^{1}\\) (to 0 ), but fails to converge in the set of all positive real numbers [with \\(d(x, y)=|x-y|\\) ]. In cases of possible ambiguity, we can be more precise and specify “convergent in \\(X\\)” rather than “convergent.”\nWe recall that the set of all points \\(p_{n}(n=1,2,3, \\ldots)\\) is the range of \\(\\left\\{p_{n}\\right\\}\\). The range of a sequence may be a finite set, or it may be infinite. The sequence \\(\\left\\{p_{n}\\right\\}\\) is said to be bounded if its range is bounded.\nAs examples, consider the following sequences of complex numbers (that is, \\(X=R^{2}\\) ):\nIf \\(s_{n}=1 / n\\), then \\(\\lim _{n \\rightarrow \\infty} s_{n}=0\\); the range is infinite, and the sequence is bounded.\nIf \\(s_{n}=n^{2}\\), the sequence \\(\\left\\{s_{n}\\right\\}\\) is unbounded, is divergent, and has infinite range.\nIf \\(s_{n}=1+\\left[(-1)^{n} / n\\right]\\), the sequence \\(\\left\\{s_{n}\\right\\}\\) converges to 1 , is bounded, and has infinite range.\nIf \\(s_{n}=i^{n}\\), the sequence \\(\\left\\{s_{n}\\right\\}\\) is divergent, is bounded, and has finite range.\nIf \\(s_{n}=1(n=1,2,3, \\ldots)\\), then \\(\\left\\{s_{n}\\right\\}\\) converges to 1 , is bounded, and has finite range.\nWe now summarize some important properties of convergent sequences in metric spaces.\n3.2 Theorem Let \\(\\left\\{p_{n}\\right\\}\\) be a sequence in a metric space \\(X\\).\n\\(\\left\\{p_{n}\\right\\}\\) converges to \\(p \\in X\\) if and only if every neighborhood of \\(p\\) contains \\(p_{n}\\) for all but finitely many \\(n\\).\nIf \\(p \\in X, p^{\\prime} \\in X\\), and if \\(\\left\\{p_{n}\\right\\}\\) converges to \\(p\\) and to \\(p^{\\prime}\\), then \\(p^{\\prime}=p\\).\nIf \\(\\left\\{p_{n}\\right\\}\\) converges, then \\(\\left\\{p_{n}\\right\\}\\) is bounded.\nIf \\(E \\subset X\\) and if \\(p\\) is a limit point of \\(E\\), then there is a sequence \\(\\left\\{p_{n}\\right\\}\\) in \\(E\\) such that \\(p=\\lim _{n \\rightarrow \\infty} p_{n}\\).\nProof (a) Suppose \\(p_{n} \\rightarrow p\\) and let \\(V\\) be a neighborhood of \\(p\\). For some \\(\\varepsilon\u0026gt;0\\), the conditions \\(d(q, p)\u0026lt;\\varepsilon, q \\in X\\) imply \\(q \\in V\\). Corresponding to this \\(\\varepsilon\\), there exists \\(N\\) such that \\(n \\geq N\\) implies \\(d\\left(p_{n}, p\\right)\u0026lt;\\varepsilon\\). Thus \\(n \\geq N\\) implies \\(p_{n} \\in V\\).\nConversely, suppose every neighborhood of \\(p\\) contains all but finitely many of the \\(p_{n}\\). Fix \\(\\varepsilon\u0026gt;0\\), and let \\(V\\) be the set of all \\(q \\in X\\) such that \\(d(p, q)\u0026lt;\\varepsilon\\). By assumption, there exists \\(N\\) (corresponding to this \\(V\\) ) such that \\(p_{n} \\in V\\) if \\(n \\geq N\\). Thus \\(d\\left(p_{n}, p\\right)\u0026lt;\\varepsilon\\) if \\(n \\geq N\\); hence \\(p_{n} \\rightarrow p\\). (b) Let \\(\\varepsilon\u0026gt;0\\) be given. There exist integers \\(N, N^{\\prime}\\) such that\n\\[ \\begin{aligned} \u0026amp;n \\geq N \\quad \\text { implies } \\quad d\\left(p_{n}, p\\right)\u0026lt;\\frac{\\varepsilon}{2}, \\\\ \u0026amp;n \\geq N^{\\prime} \\quad \\text { implies } \\quad d\\left(p_{n}, p^{\\prime}\\right)\u0026lt;\\frac{\\varepsilon}{2} . \\end{aligned} \\]\nHence if \\(n \\geq \\max \\left(N, N^{\\prime}\\right)\\), we have\n\\[ d\\left(p, p^{\\prime}\\right) \\leq d\\left(p, p_{n}\\right)+d\\left(p_{n}, p^{\\prime}\\right)\u0026lt;\\varepsilon . \\]\nSince \\(\\varepsilon\\) was arbitrary, we conclude that \\(d\\left(p, p^{\\prime}\\right)=0\\).\nSuppose \\(p_{n} \\rightarrow p\\). There is an integer \\(N\\) such that \\(n\u0026gt;N\\) implies \\(d\\left(p_{n}, p\\right)\u0026lt;1\\). Put \\[ r=\\max \\left\\{1, d\\left(p_{1}, p\\right), \\ldots, d\\left(p_{N}, p\\right)\\right\\} . \\]\nThen \\(d\\left(p_{n}, p\\right) \\leq r\\) for \\(n=1,2,3, \\ldots\\)\nFor each positive integer \\(n\\), there is a point \\(p_{n} \\in E\\) such that \\(d\\left(p_{n}, p\\right)\u0026lt;1 / n\\). Given \\(\\varepsilon\u0026gt;0\\), choose \\(N\\) so that \\(N \\varepsilon\u0026gt;1\\). If \\(n\u0026gt;N\\), it follows that \\(d\\left(p_{n}, p\\right)\u0026lt;\\varepsilon\\). Hence \\(p_{n} \\rightarrow p\\). This completes the proof.\nFor sequences in \\(R^{k}\\) we can study the relation between convergence, on the one hand, and the algebraic operations on the other. We first consider sequences of complex numbers.\n3.3 Theorem Suppose \\(\\left\\{s_{n}\\right\\},\\left\\{t_{n}\\right\\}\\) are complex sequences, and \\(\\lim _{n \\rightarrow \\infty} s_{n}=s\\), \\(\\lim _{n \\rightarrow \\infty} t_{n}=t\\). Then\n\\(\\lim _{n \\rightarrow \\infty}\\left(s_{n}+t_{n}\\right)=s+t\\)\n\\(\\lim _{n \\rightarrow \\infty} c s_{n}=c s, \\lim _{n \\rightarrow \\infty}\\left(c+s_{n}\\right)=c+s\\), for any number \\(c\\);\n\\(\\lim _{n \\rightarrow \\infty} s_{n} t_{n}=s t\\)\n\\(\\lim _{n \\rightarrow \\infty} \\frac{1}{s_{n}}=\\frac{1}{s}\\), provided \\(s_{n} \\neq 0(n=1,2,3, \\ldots)\\), and \\(s \\neq 0\\)\nProof\nGiven \\(\\varepsilon\u0026gt;0\\), there exist integers \\(N_{1}, N_{2}\\) such that \\[ \\begin{aligned} \u0026amp;n \\geq N_{1} \\text { implies }\\left|s_{n}-s\\right|\u0026lt;\\frac{\\varepsilon}{2}, \\\\ \u0026amp;n \\geq N_{2} \\text { implies }\\left|t_{n}-t\\right|\u0026lt;\\frac{\\varepsilon}{2} . \\end{aligned} \\]\nIf \\(N=\\max \\left(N_{1}, N_{2}\\right)\\), then \\(n \\geq N\\) implies\n\\[ \\left|\\left(s_{n}+t_{n}\\right)-(s+t)\\right| \\leq\\left|s_{n}-s\\right|+\\left|t_{n}-t\\right|\u0026lt;\\varepsilon . \\]\nThis proves (a). The proof of \\((b)\\) is trivial.\nWe use the identity \\[ s_{n} t_{n}-s t=\\left(s_{n}-s\\right)\\left(t_{n}-t\\right)+s\\left(t_{n}-t\\right)+t\\left(s_{n}-s\\right) . \\]\nGiven \\(\\varepsilon\u0026gt;0\\), there are integers \\(N_{1}, N_{2}\\) such that\n\\[ \\begin{array}{lll} n \\geq N_{1} \u0026amp; \\text { implies } \u0026amp; \\left|s_{n}-s\\right|\u0026lt;\\sqrt{\\varepsilon} \\\\ n \\geq N_{2} \u0026amp; \\text { implies } \u0026amp; \\left|t_{n}-t\\right|\u0026lt;\\sqrt{\\varepsilon} \\end{array} \\]\nIf we take \\(N=\\max \\left(N_{1}, N_{2}\\right), n \\geq N\\) implies\n\\[ \\left|\\left(s_{n}-s\\right)\\left(t_{n}-t\\right)\\right|\u0026lt;\\varepsilon, \\]\nso that\n\\[ \\lim _{n \\rightarrow \\infty}\\left(s_{n}-s\\right)\\left(t_{n}-t\\right)=0 . \\]\nWe now apply \\((a)\\) and \\((b)\\) to (1), and conclude that\n\\[ \\lim _{n \\rightarrow \\infty}\\left(s_{n} t_{n}-s t\\right)=0 . \\]\nChoosing \\(m\\) such that \\(\\left|s_{n}-s\\right|\u0026lt;\\frac{1}{2}|s|\\) if \\(n \\geq m\\), we see that \\[ \\left|s_{n}\\right|\u0026gt;\\frac{1}{2}|s| \\quad(n \\geq m) . \\]\nGiven \\(\\varepsilon\u0026gt;0\\), there is an integer \\(N\u0026gt;m\\) such that \\(n \\geq N\\) implies\n\\[ \\left|s_{n}-s\\right|\u0026lt;\\frac{1}{2}|s|^{2} \\varepsilon . \\]\nHence, for \\(n \\geq N\\),\n\\[ \\left|\\frac{1}{s_{n}}-\\frac{1}{s}\\right|=\\left|\\frac{s_{n}-s}{s_{n} s}\\right|\u0026lt;\\frac{2}{|s|^{2}}\\left|s_{n}-s\\right|\u0026lt;\\varepsilon . \\]\n3.4 Theorem\nSuppose \\(\\mathbf{x}_{n} \\in R^{k}(n=1,2,3, \\ldots)\\) and \\[ \\mathbf{x}_{n}=\\left(\\alpha_{1, n}, \\ldots, \\alpha_{k, n}\\right) . \\]\nThen \\(\\left\\{\\mathbf{x}_{n}\\right\\}\\) converges to \\(\\mathbf{x}=\\left(\\alpha_{1}, \\ldots, \\alpha_{k}\\right)\\) if and only if\n\\[ \\lim _{n \\rightarrow \\infty} \\alpha_{j, n}=\\alpha_{j} \\quad(1 \\leq j \\leq k) . \\]\nSuppose \\(\\left\\{\\mathbf{x}_{n}\\right\\},\\left\\{\\mathbf{y}_{n}\\right\\}\\) are sequences in \\(R^{k},\\left\\{\\beta_{n}\\right\\}\\) is a sequence of real numbers, and \\(\\mathbf{x}_{n} \\rightarrow \\mathbf{x}, \\mathbf{y}_{n} \\rightarrow \\mathbf{y}, \\beta_{n} \\rightarrow \\beta\\). Then \\[ \\lim _{n \\rightarrow \\infty}\\left(\\mathbf{x}_{n}+\\mathbf{y}_{n}\\right)=\\mathbf{x}+\\mathbf{y}, \\quad \\lim _{n \\rightarrow \\infty} \\mathbf{x}_{n} \\cdot \\mathbf{y}_{n}=\\mathbf{x} \\cdot \\mathbf{y}, \\quad \\lim _{n \\rightarrow \\infty} \\beta_{n} \\mathbf{x}_{n}=\\beta \\mathbf{x} . \\]\nIf \\(\\mathbf{x}_{n} \\rightarrow \\mathbf{x}\\), the inequalities \\[ \\left|\\alpha_{j, n}-\\alpha_{j}\\right| \\leq\\left|\\mathbf{x}_{n}-\\mathbf{x}\\right|, \\]\nwhich follow immediately from the definition of the norm in \\(R^{k}\\), show that (2) holds.\nConversely, if (2) holds, then to each \\(\\varepsilon\u0026gt;0\\) there corresponds an integer \\(N\\) such that \\(n \\geq N\\) implies\n\\[ \\left|\\alpha_{j, n}-\\alpha_{j}\\right|\u0026lt;\\frac{\\varepsilon}{\\sqrt{k}} \\quad(1 \\leq j \\leq k) . \\]\nHence \\(n \\geq N\\) implies\n\\[ \\left|\\mathbf{x}_{n}-\\mathbf{x}\\right|=\\left\\{\\sum_{j=1}^{k}\\left|\\alpha_{j, n}-\\alpha_{j}\\right|^{2}\\right\\}^{1 / 2}\u0026lt;\\varepsilon \\]\nso that \\(\\mathbf{x}_{n} \\rightarrow \\mathbf{x}\\). This proves \\((a)\\).\nPart \\((b)\\) follows from \\((a)\\) and Theorem 3.3.\n","date":"2022-09-03T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/1-convergent-sequences/","section":"baby rudin","tags":null,"title":"1 CONVERGENT SEQUENCES"},{"categories":null,"contents":"As the title indicates, this chapter will deal primarily with sequences and series of complex numbers. The basic facts about convergence, however, are just as easily explained in a more general setting. The first three sections will therefore be concerned with sequences in euclidean spaces, or even in metric spaces.\n","date":"2022-09-03T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/0-intro/","section":"baby rudin","tags":null,"title":"INTRO"},{"categories":null,"contents":" youtube bilibili note pdf xopp 2.43 Theorem Let \\(P\\) be a nonempty perfect set in \\(R^{k}\\). Then \\(P\\) is uncountable.\nProof Since \\(P\\) has limit points, \\(P\\) must be infinite. Suppose \\(P\\) is countable, and denote the points of \\(P\\) by \\(\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}, \\ldots\\) We shall construct a sequence \\(\\left\\{V_{n}\\right\\}\\) of neighborhoods, as follows.\nLet \\(V_{1}\\) be any neighborhood of \\(\\mathbf{x}_{1}\\). If \\(V_{1}\\) consists of all \\(\\mathbf{y} \\in R^{k}\\) such that \\(\\left|\\mathbf{y}-\\mathbf{x}_{1}\\right|\u0026lt;r\\), the closure \\(\\bar{V}_{1}\\) of \\(V_{1}\\) is the set of all \\(\\mathbf{y} \\in R^{k}\\) such that \\(\\left|\\mathbf{y}-\\mathbf{x}_{1}\\right| \\leq r\\).\nSuppose \\(V_{n}\\) has been constructed, so that \\(V_{n} \\cap P\\) is not empty. Since every point of \\(P\\) is a limit point of \\(P\\), there is a neighborhood \\(V_{n+1}\\) such that (i) \\(\\bar{V}_{n+1} \\subset V_{n}\\), (ii) \\(\\mathbf{x}_{n} \\notin \\bar{V}_{n+1}\\), (iii) \\(V_{n+1} \\cap P\\) is not empty. By (iii), \\(V_{n+1}\\) satisfies our induction hypothesis, and the construction can proceed.\nPut \\(K_{n}=\\bar{V}_{n} \\cap P\\). Since \\(\\bar{V}_{n}\\) is closed and bounded, \\(\\bar{V}_{n}\\) is compact. Since \\(\\mathbf{x}_{n} \\notin K_{n+1}\\), no point of \\(P\\) lies in \\(\\bigcap_{1}^{\\infty} K_{n}\\). Since \\(K_{n} \\subset P\\), this implies that \\(\\bigcap_{1}^{\\infty} K_{n}\\) is empty. But each \\(K_{n}\\) is nonempty, by (iii), and \\(K_{n} \\supset K_{n+1}\\), by (i); this contradicts the Corollary to Theorem \\(2.36\\).\nCorollary Every interval \\([a, b](a\u0026lt;b)\\) is uncountable. In particular, the set of all real numbers is uncountable.\n2.44 The Cantor set The set which we are now going to construct shows that there exist perfect sets in \\(R^{1}\\) which contain no segment.\nLet \\(E_{0}\\) be the interval \\([0,1]\\). Remove the segment \\(\\left(\\frac{1}{3}, \\frac{2}{3}\\right)\\), and let \\(E_{1}\\) be the union of the intervals\n\\[ \\left[0, \\frac{1}{3}\\right]\\left[\\frac{2}{3}, 1\\right] \\text {. } \\]\nRemove the middle thirds of these intervals, and let \\(E_{2}\\) be the union of the intervals\n\\[ \\left[0, \\frac{1}{9}\\right],\\left[\\frac{2}{9}, \\frac{3}{9}\\right],\\left[\\frac{6}{9}, \\frac{7}{9}\\right],\\left[\\frac{8}{9}, 1\\right] \\text {. } \\]\nContinuing in this way, we obtain a sequence of compact sets \\(E_{n}\\), such that\n\\(E_{1} \\supset E_{2} \\supset E_{3} \\supset \\cdots\\);\n\\(E_{n}\\) is the union of \\(2^{n}\\) intervals, each of length \\(3^{-n}\\).\nThe set\n\\[ P=\\bigcap_{n=1}^{\\infty} E_{n} \\]\nis called the Cantor set. \\(P\\) is clearly compact, and Theorem \\(2.36\\) shows that \\(P\\) is not empty. No segment of the form\n\\[ \\left(\\frac{3 k+1}{3^{m}}, \\frac{3 k+2}{3^{m}}\\right) \\]\nwhere \\(k\\) and \\(m\\) are positive integers, has a point in common with \\(P\\). Since every segment \\((\\alpha, \\beta)\\) contains a segment of the form (24), if\n\\[ 3^{-m}\u0026lt;\\frac{\\beta-\\alpha}{6}, \\]\n\\(P\\) contains no segment.\nTo show that \\(P\\) is perfect, it is enough to show that \\(P\\) contains no isolated point. Let \\(x \\in P\\), and let \\(S\\) be any segment containing \\(x\\). Let \\(I_{n}\\) be that interval of \\(E_{n}\\) which contains \\(x\\). Choose \\(n\\) large enough, so that \\(I_{n} \\subset S\\). Let \\(x_{n}\\) be an endpoint of \\(I_{n}\\), such that \\(x_{n} \\neq x\\).\nIt follows from the construction of \\(P\\) that \\(x_{n} \\in P\\). Hence \\(x\\) is a limit point of \\(P\\), and \\(P\\) is perfect.\nOne of the most interesting properties of the Cantor set is that it provides us with an example of an uncountable set of measure zero (the concept of measure will be discussed in Chap. 11).\n","date":"2022-08-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch2/4-perfect-sets/","section":"baby rudin","tags":null,"title":"4 PERFECT SETS"},{"categories":null,"contents":" youtube bilibi note pdf xopp 2.45 Definition Two subsets \\(A\\) and \\(B\\) of a metric space \\(X\\) are said to be separated if both \\(A \\cap \\bar{B}\\) and \\(\\bar{A} \\cap B\\) are empty, i.e., if no point of \\(A\\) lies in the closure of \\(B\\) and no point of \\(B\\) lies in the closure of \\(A\\).\n\\(A\\) set \\(E \\subset X\\) is said to be connected if \\(E\\) is not a union of two nonempty separated sets.\n2.46 Remark Separated sets are of course disjoint, but disjoint sets need not be separated. For example, the interval \\([0,1]\\) and the segment \\((1,2)\\) are not separated, since 1 is a limit point of \\((1,2)\\). However, the segments \\((0,1)\\) and \\((1,2)\\) are separated.\nThe connected subsets of the line have a particularly simple structure:\n2.47 Theorem A subset \\(E\\) of the real line \\(R^{1}\\) is connected if and only if \\(i\\) thas the following property: If \\(x \\in E, y \\in E\\), and \\(x\u0026lt;z\u0026lt;y\\), then \\(z \\in E\\).\nProof If there exist \\(x \\in E, y \\in E\\), and some \\(z \\in(x, y)\\) such that \\(z \\notin E\\), then \\(E=A_{z} \\cup B_{z}\\) where \\[ A_{z}=E \\cap(-\\infty, z), \\quad B_{z}=E \\cap(z, \\infty) . \\]\nSince \\(x \\in A_{z}\\) and \\(y \\in B_{z}, A\\) and \\(B\\) are nonempty. Since \\(A_{z} \\subset(-\\infty, z)\\) and \\(\\boldsymbol{B}_{z} \\subset(z, \\infty)\\), they are separated. Hence \\(E\\) is not connected.\nTo prove the converse, suppose \\(E\\) is not connected. Then there are nonempty separated sets \\(A\\) and \\(B\\) such that \\(A \\cup B=E\\). Pick \\(x \\in A, y \\in B\\), and assume (without loss of generality) that \\(x\u0026lt;y\\). Define\n\\[ z=\\sup (A \\cap[x, y]) . \\]\nBy Theorem 2.28, \\(z \\in \\bar{A}\\); hence \\(z \\notin B\\). In particular, \\(x \\leq z\u0026lt;y\\).\nIf \\(z \\notin A\\), it follows that \\(x\u0026lt;z\u0026lt;y\\) and \\(z \\notin E\\).\nIf \\(z \\in A\\), then \\(z \\notin \\bar{B}\\), hence there exists \\(z_{1}\\) such that \\(z\u0026lt;z_{1}\u0026lt;y\\) and \\(z_{1} \\notin B\\). Then \\(x\u0026lt;z_{1}\u0026lt;y\\) and \\(z_{1} \\notin E\\).\nProve that the empty set is a subset of every set.\nA complex number \\(z\\) is said to be algebraic if there are integers \\(a_{0}, \\ldots, a_{n}\\), not all zero, such that\n\\[ a_{0} z^{n}+a_{1} z^{n-1}+\\cdots+a_{n-1} z+a_{n}=0 . \\]\nProve that the set of all algebraic numbers is countable. Hint: For every positive integer \\(N\\) there are only finitely many equations with\n\\[ n+\\left|a_{0}\\right|+\\left|a_{1}\\right|+\\cdots+\\left|a_{n}\\right|=N . \\]\nProve that there exist real numbers which are not algebraic.\nIs the set of all irrational real numbers countable?\nConstruct a bounded set of real numbers with exactly three limit points.\nLet \\(E^{\\prime}\\) be the set of all limit points of a set \\(E\\). Prove that \\(E^{\\prime}\\) is closed. Prove that \\(E\\) and \\(E\\) have the same limit points. (Recall that \\(E=E \\cup E^{\\prime}\\).) Do \\(E\\) and \\(E^{\\prime}\\) always have the same limit points?\nLet \\(A_{1}, A_{2}, A_{3}, \\ldots\\) be subsets of a metric space.\nIf \\(B_{n}=\\bigcup_{i=1}^{n} A_{i}\\), prove that \\(\\bar{B}_{n}=\\bigcup_{i=1}^{n} \\bar{A}_{i}\\), for \\(n=1,2,3, \\ldots\\)\nIf \\(B=\\bigcup_{i=1}^{\\infty} A_{i}\\), prove that \\(\\bar{B} \\supset \\bigcup_{i=1}^{\\infty} \\bar{A}_{i}\\).\nShow, by an example, that this inclusion can be proper.\nIs every point of every open set \\(E \\subset R^{2}\\) a limit point of \\(E\\) ? Answer the same question for closed sets in \\(R^{2}\\).\nLet \\(E^{\\circ}\\) denote the set of all interior points of a set \\(E\\). [See Definition 2.18(e); \\(E^{\\circ}\\) is called the interior of \\(E\\).]\nProve that \\(E^{\\circ}\\) is always open.\nProve that \\(E\\) is open if and only if \\(E^{\\circ}=E\\).\nIf \\(G \\subset E\\) and \\(G\\) is open, prove that \\(G \\subset E^{\\circ}\\).\nProve that the complement of \\(E^{\\circ}\\) is the closure of the complement of \\(E\\).\nDo \\(E\\) and \\(E\\) always have the same interiors?\n( \\(f\\) ) Do \\(E\\) and \\(E^{\\circ}\\) always have the same closures? 10. Let \\(X\\) be an infinite set. For \\(p \\in X\\) and \\(q \\in X\\), define\n\\[ d(p, q)= \\begin{cases}1 \u0026amp; (\\text { if } p \\neq q) \\\\ 0 \u0026amp; (\\text { if } p=q)\\end{cases} \\]\nProve that this is a metric. Which subsets of the resulting metric space are open? Which are closed? Which are compact?\nFor \\(x \\in R^{1}\\) and \\(y \\in R^{1}\\), define \\[ \\begin{aligned} \u0026amp;d_{1}(x, y)=(x-y)^{2} \\\\ \u0026amp;d_{2}(x, y)=\\sqrt{|x-y|} \\\\ \u0026amp;d_{3}(x, y)=\\left|x^{2}-y^{2}\\right| \\\\ \u0026amp;d_{4}(x, y)=|x-2 y| \\\\ \u0026amp;d_{5}(x, y)=\\frac{|x-y|}{1+|x-y|} \\end{aligned} \\]\nDetermine, for each of these, whether it is a metric or not.\nLet \\(K \\subset R^{1}\\) consist of 0 and the numbers \\(1 / n\\), for \\(n=1,2,3, \\ldots\\) Prove that \\(K\\) is compact directly from the definition (without using the Heine-Borel theorem).\nConstruct a compact set of real numbers whose limit points form a countable set.\nGive an example of an open cover of the segment \\((0,1)\\) which has no finite subcover.\nShow that Theorem \\(2.36\\) and its Corollary become false (in \\(R^{1}\\), for example) if the word “compact”’ is replaced by “closed” or by “bounded.”\nRegard \\(Q\\), the set of all rational numbers, as a metric space, with \\(d(p, q)=|p-q|\\). Let \\(E\\) be the set of all \\(p \\in Q\\) such that \\(2\u0026lt;p^{2}\u0026lt;3\\). Show that \\(E\\) is closed and bounded in \\(Q\\), but that \\(E\\) is not compact. Is \\(E\\) open in \\(Q\\) ?\nLet \\(E\\) be the set of all \\(x \\in[0,1]\\) whose decimal expansion contains only the digits 4 and 7. Is \\(E\\) countable? Is \\(E\\) dense in \\([0,1]\\) ? Is \\(E\\) compact? Is \\(E\\) perfect?\nIs there a nonempty perfect set in \\(R^{1}\\) which contains no rational number?\nIf \\(A\\) and \\(B\\) are disjoint closed sets in some metric space \\(X\\), prove that they are separated. Prove the same for disjoint open sets.\nFix \\(p \\in X, \\delta\u0026gt;0\\), define \\(A\\) to be the set of all \\(q \\in X\\) for which \\(d(p, q)\u0026lt;\\delta\\), define \\(B\\) similarly, with \\(\u0026gt;\\) in place of \\(\u0026lt;\\). Prove that \\(A\\) and \\(B\\) are separated.\nProve that every connected metric space with at least two points is uncountable. Hint: Use \\((c)\\).\nAre closures and interiors of connected sets always connected? (Look at subsets of \\(R^{2}\\).)\nLet \\(A\\) and \\(B\\) be separated subsets of some \\(R^{k}\\), suppose \\(a \\in A, b \\in B\\), and define\n\\[ \\mathbf{p}(t)=(1-t) \\mathbf{a}+t \\mathbf{b} \\]\nfor \\(t \\in R^{1}\\). Put \\(A_{0}=p^{-1}(A), B_{0}=p^{-1}(B)\\). [Thus \\(t \\in A_{0}\\) if and only if \\(p(t) \\in A\\).] (a) Prove that \\(A_{0}\\) and \\(B_{0}\\) are separated subsets of \\(R^{1}\\).\nProve that there exists \\(t_{0} \\in(0,1)\\) such that \\(\\mathbf{p}\\left(t_{0}\\right) \\notin A \\cup B\\).\nProve that every convex subset of \\(R^{k}\\) is connected.\nA metric space is called separable if it contains a countable dense subset. Show that \\(R^{k}\\) is separable. Hint: Consider the set of points which have only rational coordinates.\nA collection \\(\\left\\{V_{\\alpha}\\right\\}\\) of open subsets of \\(X\\) is said to be a base for \\(X\\) if the following is true: For every \\(x \\in X\\) and every open set \\(G \\subset X\\) such that \\(x \\in G\\), we have \\(x \\in V_{\\alpha} \\subset G\\) for some \\(\\alpha\\). In other words, every open set in \\(X\\) is the union of a subcollection of \\(\\left\\{V_{\\alpha}\\right\\}\\).\nProve that every separable metric space has a countable base. Hint: Take all neighborhoods with rational radius and center in some countable dense subset of \\(X\\).\nLet \\(X\\) be a metric space in which every infinite subset has a limit point. Prove that \\(X\\) is separable. Hint: Fix \\(\\delta\u0026gt;0\\), and pick \\(x_{1} \\in X\\). Having chosen \\(x_{1}, \\ldots, x_{j} \\in X\\), choose \\(x_{j+1} \\in X\\), if possible, so that \\(d\\left(x_{l}, x_{j+1}\\right) \\geq \\delta\\) for \\(i=1, \\ldots, j\\). Show that this process must stop after a finite number of steps, and that \\(X\\) can therefore be covered by finitely many neighborhoods of radius \\(\\delta\\). Take \\(\\delta=1 / n(n=1,2,3, \\ldots)\\), and consider the centers of the corresponding neighborhoods.\nProve that every compact metric space \\(K\\) has a countable base, and that \\(K\\) is therefore separable. Hint: For every positive integer \\(n\\), there are finitely many neighborhoods of radius \\(1 / n\\) whose union covers \\(K\\).\nLet \\(X\\) be a metric space in which every infinite subset has a limit point. Prove that \\(X\\) is compact. Hint: By Exercises 23 and \\(24, X\\) has a countable base. It follows that every open cover of \\(X\\) has a countable subcover \\(\\left\\{G_{n}\\right\\}, n=1,2,3, \\ldots\\) If no finite subcollection of \\(\\left\\{G_{n}\\right\\}\\) covers \\(X\\), then the complement \\(F_{n}\\) of \\(G_{1} \\cup \\cdots \\cup G_{n}\\) is nonempty for each \\(n\\), but \\(\\bigcap F_{n}\\) is empty. If \\(E\\) is a set which contains a point from each \\(F_{n}\\), consider a limit point of \\(E\\), and obtain a contradiction.\nDefine a point \\(p\\) in a metric space \\(X\\) to be a condensation point of a set \\(E \\subset X\\) if every neighborhood of \\(p\\) contains uncountably many points of \\(E\\).\nSuppose \\(E \\subset R^{k}, E\\) is uncountable, and let \\(P\\) be the set of all condensation points of \\(E\\). Prove that \\(P\\) is perfect and that at most countably many points of \\(E\\) are not in \\(P\\). In other words, show that \\(P^{c} \\cap E\\) is at most countable. Hint: Let \\(\\left\\{V_{n}\\right\\}\\) be a countable base of \\(R^{k}\\), let \\(W\\) be the union of those \\(V_{n}\\) for which \\(E \\cap V_{n}\\) is at most countable, and show that \\(P=W^{c}\\).\nProve that every closed set in a separable metric space is the union of a (possibly empty) perfect set and a set which is at most countable. (Corollary: Every countable closed set in \\(R^{k}\\) has isolated points.) Hint: Use Exercise \\(27 .\\)\nProve that every open set in \\(R^{1}\\) is the union of an at most countable collection of disjoint segments. Hint: Use Exercise \\(22 .\\) 30. Imitate the proof of Theorem \\(2.43\\) to obtain the following result:\nIf \\(R^{k}=\\bigcup_{1}^{\\infty} F_{n}\\), where each \\(F_{n}\\) is a closed subset of \\(R^{k}\\), then at least one \\(F_{n}\\) has a nonempty interior.\nEquivalent statement: If \\(G_{n}\\) is a dense open subset of \\(R^{k}\\), for \\(n=1,2,3, \\ldots\\), then \\(\\bigcap_{1}^{\\infty} G_{n}\\) is not empty (in fact, it is dense in \\(R^{k}\\) ).\n(This is a special case of Baire’s theorem; see Exercise 22, Chap. 3, for the general case.)\n","date":"2022-08-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch2/5-connected-sets/","section":"baby rudin","tags":null,"title":"5 CONNECTED SETS"},{"categories":null,"contents":" If \\(S\\) is a nonempty subset of a vector space \\(X\\), prove (as asserted in Sec. 9.1) that the span of \\(S\\) is a vector space.\nProve (as asserted in \\(\\operatorname{Sec} .\\) 9.6) that \\(B A\\) is linear if \\(A\\) and \\(B\\) are linear transformations.\nProve also that \\(A^{-1}\\) is linear and invertible.\nAssume \\(A \\in L(X, Y)\\) and \\(A \\mathbf{x}=\\mathbf{0}\\) only when \\(\\mathbf{x}=0\\). Prove that \\(A\\) is then 1-1.\nProve (as asserted in Sec. 9.30) that null spaces and ranges of linear transformations are vector spaces.\nProve that to every \\(A \\in L\\left(R^{n}, R^{1}\\right)\\) corresponds a unique \\(\\mathbf{y} \\in R^{n}\\) such that \\(A \\mathbf{x}=\\mathbf{x} \\cdot \\mathbf{y}\\).\nProve also that \\(\\|A\\|=|\\mathbf{y}|\\).\nHint: Under certain conditions, equality holds in the Schwarz inequality.\nIf \\(f(0,0)=0\\) and \\[ f(x, y)=\\frac{x y}{x^{2}+y^{2}} \\quad \\text { if }(x, y) \\neq(0,0), \\]\nprove that \\(\\left(D_{1} f\\right)(x, y)\\) and \\(\\left(D_{2} f\\right)(x, y)\\) exist at every point of \\(R^{2}\\), although \\(f\\) is not continuous at \\((0,0)\\).\nSuppose that \\(f\\) is a real-valued function defined in an open set \\(E \\subset R^{n}\\), and that the partial derivatives \\(D_{1} f, \\ldots, D_{n} f\\) are bounded in \\(E\\). Prove that \\(f\\) is continuous in \\(E\\). Hint: Proceed as in the proof of Theorem 9.21.\nSuppose that \\(f\\) is a differentiable real function in an open set \\(E \\subset R^{n}\\), and that \\(f\\) has a local maximum at a point \\(\\mathbf{x} \\in E\\). Prove that \\(f^{\\prime}(\\mathbf{x})=0\\).\nIf \\(\\mathrm{f}\\) is a differentiable mapping of a connected open set \\(E \\subset R^{n}\\) into \\(R^{m}\\), and if \\(\\mathbf{f}^{\\prime}(\\mathbf{x})=0\\) for every \\(\\mathbf{x} \\in E\\), prove that \\(\\mathbf{f}\\) is constant in \\(E\\).\nIf \\(f\\) is a real function defined in a convex open set \\(E \\subset R^{n}\\), such that \\(\\left(D_{1} f\\right)(\\mathbf{x})=0\\) for every \\(\\mathbf{x} \\in E\\), prove that \\(f(\\mathbf{x})\\) depends only on \\(x_{2}, \\ldots, x_{n}\\).\nShow that the convexity of \\(E\\) can be replaced by a weaker condition, but that some condition is required. For example, if \\(n=2\\) and \\(E\\) is shaped like a horseshoe, the statement may be false.\nIf \\(f\\) and \\(g\\) are differentiable real functions in \\(R^{n}\\), prove that \\[ \\nabla(f g)=f \\nabla g+g \\nabla f \\]\nand that \\(\\nabla(1 / f)=-f^{-2} \\nabla f\\) wherever \\(f \\neq 0\\).\nFix two real numbers \\(a\\) and \\(b, 0\u0026lt;a\u0026lt;b\\). Define a mapping \\(\\mathbf{f}=\\left(f_{1}, f_{2}, f_{3}\\right)\\) of \\(R^{2}\\) into \\(R^{3}\\) by \\[ \\begin{aligned} \u0026amp;f_{1}(s, t)=(b+a \\cos s) \\cos t \\\\ \u0026amp;f_{2}(s, t)=(b+a \\cos s) \\sin t \\\\ \u0026amp;f_{3}(s, t)=a \\sin s . \\end{aligned} \\]\nDescribe the range \\(K\\) of \\(\\mathrm{f}\\). (It is a certain compact subset of \\(R^{3}\\).)\nShow that there are exactly 4 points \\(\\mathrm{p} \\in K\\) such that \\[ \\left(\\nabla f_{1}\\right)\\left(\\mathbf{f}^{-1}(\\mathbf{p})\\right)=\\mathbf{0} . \\]\nFind these points.\nDetermine the set of all \\(\\mathbf{q} \\in K\\) such that \\[ \\left(\\nabla f_{3}\\right)\\left(\\mathbf{f}^{-1}(\\mathbf{q})\\right)=\\mathbf{0} . \\]\nShow that one of the points \\(\\mathbf{p}\\) found in part (a) corresponds to a local maximum of \\(f_{1}\\), one corresponds to a local minimum, and that the other two are neither (they are so-called “saddle points”). Which of the points \\(\\mathbf{q}\\) found in part (b) correspond to maxima or minima? (d) Let \\(\\lambda\\) be an irrational real number, and define \\(\\mathbf{g}(t)=\\mathbf{f}(t, \\lambda t)\\). Prove that \\(\\mathbf{g}\\) is a 1-1 mapping of \\(R^{1}\\) onto a dense subset of \\(K\\). Prove that\n\\[ \\left|\\mathbf{g}^{\\prime}(t)\\right|^{2}=a^{2}+\\lambda^{2}(b+a \\cos t)^{2} . \\]\nSuppose \\(\\mathrm{f}\\) is a differentiable mapping of \\(R^{1}\\) into \\(R^{3}\\) such that \\(|\\mathrm{f}(t)|=1\\) for every \\(t\\). Prove that \\(\\mathbf{f}^{\\prime}(t) \\cdot \\mathbf{f}(t)=0 .\\) Interpret this result geometrically.\nDefine \\(f(0,0)=0\\) and \\[ f(x, y)=\\frac{x^{3}}{x^{2}+y^{2}} \\quad \\text { if }(x, y) \\neq(0,0) . \\]\nProve that \\(D_{1} f\\) and \\(D_{2} f\\) are bounded functions in \\(R^{2}\\). (Hence \\(f\\) is continuous.) (b) Let \\(\\mathbf{u}\\) be any unit vector in \\(R^{2}\\). Show that the directional derivative \\(\\left(D_{\\mathbf{u}} f\\right)(0,0)\\) exists, and that its absolute value is at most 1 .\nLet \\(\\gamma\\) be a differentiable mapping of \\(R^{1}\\) into \\(R^{2}\\) (in other words, \\(\\gamma\\) is a differentiable curve in \\(\\left.R^{2}\\right)\\), with \\(\\gamma(0)=(0,0)\\) and \\(\\left|\\gamma^{\\prime}(0)\\right|\u0026gt;0\\). Put \\(g(t)=f(\\gamma(t))\\) and prove that \\(g\\) is differentiable for every \\(t \\in R^{1}\\).\nIf \\(\\gamma \\in \\mathscr{C}^{\\prime}\\), prove that \\(g \\in \\mathscr{C}^{\\prime}\\).\nIn spite of this, prove that \\(f\\) is not differentiable at \\((0,0)\\). Hint: Formula (40) fails.\nDefine \\(f(0,0)=0\\), and put \\[ f(x, y)=x^{2}+y^{2}-2 x^{2} y-\\frac{4 x^{6} y^{2}}{\\left(x^{4}+y^{2}\\right)^{2}} \\]\nif \\((x, y) \\neq(0,0)\\).\nProve, for all \\((x, y) \\in R^{2}\\), that \\[ 4 x^{4} y^{2} \\leq\\left(x^{4}+y^{2}\\right)^{2} . \\]\nConclude that \\(f\\) is continuous. (b) For \\(0 \\leq \\theta \\leq 2 \\pi,-\\infty\u0026lt;t\u0026lt;\\infty\\), define\n\\[ g_{\\theta}(t)=f(t \\cos \\theta, t \\sin \\theta) . \\]\nShow that \\(g_{\\theta}(0)=0, g_{\\theta}^{\\prime}(0)=0, g_{\\theta}^{\\prime \\prime}(0)=2\\). Each \\(g_{\\theta}\\) has therefore a strict local minimum at \\(t=0\\).\nIn other words, the restriction of \\(f\\) to each line through \\((0,0)\\) has a strict local minimum at \\((0,0)\\).\nShow that \\((0,0)\\) is nevertheless not a local minimum for \\(f\\), since \\(f\\left(x, x^{2}\\right)=-x^{4}\\). Show that the continuity of \\(\\mathbf{f}^{\\prime}\\) at the point \\(\\mathbf{a}\\) is needed in the inverse function theorem, even in the case \\(n=1\\) : If \\[ f(t)=t+2 t^{2} \\sin \\left(\\frac{1}{t}\\right) \\]\nfor \\(t \\neq 0\\), and \\(f(0)=0\\), then \\(f^{\\prime}(0)=1, f^{\\prime}\\) is bounded in \\((-1,1)\\), but \\(f\\) is not one-to-one in any neighborhood of 0 .\nLet \\(\\mathbf{f}=\\left(f_{1}, f_{2}\\right)\\) be the mapping of \\(R^{2}\\) into \\(R^{2}\\) given by \\[ f_{1}(x, y)=e^{x} \\cos y, \\quad f_{2}(x, y)=e^{x} \\sin y . \\]\nWhat is the range of \\(f\\) ?\nShow that the Jacobian of \\(f\\) is not zero at any point of \\(R^{2}\\). Thus every point of \\(R^{2}\\) has a neighborhood in which \\(f\\) is one-to-one. Nevertheless, \\(f\\) is not one-toone on \\(R^{2}\\).\nPut \\(\\mathbf{a}=(0, \\pi / 3), \\mathbf{b}=f(\\mathbf{a})\\), let \\(\\mathbf{g}\\) be the continuous inverse of \\(\\mathbf{f}\\), defined in a neighborhood of \\(\\mathbf{b}\\), such that \\(\\mathbf{g}(\\mathbf{b})=\\mathbf{a}\\). Find an explicit formula for \\(\\mathbf{g}\\), compute \\(\\mathbf{f}^{\\prime}(\\mathbf{a})\\) and \\(\\mathbf{g}^{\\prime}(\\mathbf{b})\\), and verify the formula (52).\nWhat are the images under \\(f\\) of lines parallel to the coordinate axes?\nAnswer analogous questions for the mapping defined by \\[ u=x^{2}-y^{2}, \\quad v=2 x y . \\]\nShow that the system of equations \\[ \\begin{array}{r} 3 x+y-z+u^{2}=0 \\\\ x-y+2 z+u=0 \\\\ 2 x+2 y-3 z+2 u=0 \\end{array} \\]\ncan be solved for \\(x, y, u\\) in terms of \\(z\\); for \\(x, z, u\\) in terms of \\(y\\); for \\(y, z, u\\) in terms of \\(x\\); but not for \\(x, y, z\\) in terms of \\(u\\).\nTake \\(n=m=1\\) in the implicit function theorem, and interpret the theorem (as well as its proof) graphically.\nDefine \\(f\\) in \\(R^{2}\\) by\n\\[ f(x, y)=2 x^{3}-3 x^{2}+2 y^{3}+3 y^{2} . \\]\nFind the four points in \\(R^{2}\\) at which the gradient of \\(f\\) is zero. Show that \\(f\\) has exactly one local maximum and one local minimum in \\(R^{2}\\). (b) Let \\(S\\) be the set of all \\((x, y) \\in R^{2}\\) at which \\(f(x, y)=0\\). Find those points of \\(S\\) that have no neighborhoods in which the equation \\(f(x, y)=0\\) can be solved for \\(y\\) in terms of \\(x\\) (or for \\(x\\) in terms of \\(y\\) ). Describe \\(S\\) as precisely as you can. Give a similar discussion for \\[ f(x, y)=2 x^{3}+6 x y^{2}-3 x^{2}+3 y^{2} . \\]\nDefine \\(f\\) in \\(R^{3}\\) by \\[ f\\left(x, y_{1}, y_{2}\\right)=x^{2} y_{1}+e^{x}+y_{2} . \\]\nShow that \\(f(0,1,-1)=0,\\left(D_{1} f\\right)(0,1,-1) \\neq 0\\), and that there exists therefore a differentiable function \\(g\\) in some neighborhood of \\((1,-1)\\) in \\(R^{2}\\), such that \\(g(1,-1)=0\\) and\n\\[ f\\left(g\\left(y_{1}, y_{2}\\right), y_{1}, y_{2}\\right)=0 . \\]\nFind \\(\\left(D_{1} g\\right)(1,-1)\\) and \\(\\left(D_{2} g\\right)(1,-1)\\).\nFor \\((x, y) \\neq(0,0)\\), define \\(\\mathrm{f}=\\left(f_{1}, f_{2}\\right)\\) by \\[ f_{1}(x, y)=\\frac{x^{2}-y^{2}}{x^{2}+y^{2}}, \\quad f_{2}(x, y)=\\frac{x y}{x^{2}+y^{2}} . \\]\nCompute the rank of \\(\\mathrm{f}^{\\prime}(x, y)\\), and find the range of \\(\\mathrm{f}\\).\nSuppose \\(A \\in L\\left(R^{n}, R^{m}\\right)\\), let \\(r\\) be the rank of \\(A\\). Define \\(S\\) as in the proof of Theorem 9.32. Show that \\(S A\\) is a projection in \\(R^{n}\\) whose null space is \\(\\mathcal{N}(A)\\) and whose range is \\(\\mathscr{R}(S)\\). Hint: By (68), \\(S A S A=S A\\).\nUse (a) to show that\n\\[ \\operatorname{dim} \\mathscr{N}(A)+\\operatorname{dim} \\mathscr{R}(A)=n . \\]\nShow that the existence (and even the continuity) of \\(D_{12} f\\) does not imply the existence of \\(D_{1} f\\). For example, let \\(f(x, y)=g(x)\\), where \\(g\\) is nowhere differentiable.\nPut \\(f(0,0)=0\\), and\n\\[ f(x, y)=\\frac{x y\\left(x^{2}-y^{2}\\right)}{x^{2}+y^{2}} \\]\nif \\((x, y) \\neq(0,0)\\). Prove that\n\\(f, D_{1} f, D_{2} f\\) are continuous in \\(R^{2}\\);\n\\(D_{12} f\\) and \\(D_{21} f\\) exist at every point of \\(R^{2}\\), and are continuous except at \\((0,0)\\);\n\\(\\left(D_{12} f\\right)(0,0)=1\\), and \\(\\left(D_{21} f\\right)(0,0)=-1\\).\nFor \\(t \\geq 0\\), put \\[ \\varphi(x, t)= \\begin{cases}x \u0026amp; (0 \\leq x \\leq \\sqrt{t}) \\\\ -x+2 \\sqrt{t} \u0026amp; (\\sqrt{t} \\leq x \\leq 2 \\sqrt{t}) \\\\ 0 \u0026amp; \\text { (otherwise) }\\end{cases} \\]\nand put \\(\\varphi(x, t)=-\\varphi(x,|t|)\\) if \\(t\u0026lt;0\\) Show that \\(\\varphi\\) is continuous on \\(R^{2}\\), and\n\\[ \\left(D_{2} \\varphi\\right)(x, 0)=0 \\]\nfor all \\(x\\). Define\n\\[ f(t)=\\int_{-1}^{1} \\varphi(x, t) d x . \\]\nShow that \\(f(t)=t\\) if \\(|t|\u0026lt;\\frac{1}{2}\\). Hence\n\\[ f^{\\prime}(0) \\neq \\int_{-1}^{1}\\left(D_{2} \\varphi\\right)(x, 0) d x . \\]\nLet \\(E\\) be an open set in \\(R^{n}\\). The classes \\(\\mathscr{C}^{\\prime}(E)\\) and \\(\\mathscr{C}^{\\prime \\prime}(E)\\) are defined in the text. By induction, \\(\\mathscr{C}^{(k)}(E)\\) can be defined as follows, for all positive integers \\(k\\) : To say that \\(f \\in \\mathscr{C}^{(k)}(E)\\) means that the partial derivatives \\(D_{1} f, \\ldots, D_{n} f\\) belong to \\(\\mathscr{C}^{(k-1)}(E)\\). Assume \\(f \\in \\mathscr{C}^{(k)}(E)\\). and show (by repeated application of Theorem 9.41) that the \\(k\\) th-order derivative\n\\[ D_{i_{1} l_{2} \\ldots l_{k}} f=D_{l_{1}} D_{l_{2}} \\ldots D_{l_{k}} f \\]\nis unchanged if the subscripts \\(i_{1}, \\ldots, i_{k}\\) are permuted.\nFor instance, if \\(n \\geq 3\\), then\n\\[ D_{1213} f=D_{3112} f \\]\nfor every \\(f \\in \\mathscr{C}^{(4)}\\).\nLet \\(f \\in \\mathscr{C}^{(m)}(E)\\), where \\(E\\) is an open subset of \\(R^{n}\\). Fix \\(\\mathbf{a} \\in E\\), and suppose \\(\\mathbf{x} \\in R^{n}\\) is so close to 0 that the points \\[ \\mathbf{p}(t)=\\mathbf{a}+t \\mathbf{x} \\]\nlie in \\(E\\) whenever \\(0 \\leq t \\leq 1\\). Define\n\\[ h(t)=f(\\mathbf{p}(t)) \\]\nfor all \\(t \\in R^{1}\\) for which \\(\\mathrm{p}(t) \\in E\\).\nFor \\(1 \\leq k \\leq m\\), show (by repeated application of the chain rule) that \\[ h^{(k)}(t)=\\sum\\left(D_{t_{1}} \\ldots t_{k} f\\right)(\\mathbf{p}(t)) x_{l_{1}} \\ldots x_{l_{k}} . \\]\nThe sum extends over all ordered \\(k\\)-tuples \\(\\left(i_{1}, \\ldots, i_{k}\\right)\\) in which each \\(i_{J}\\) is one of the integers \\(1, \\ldots, n\\).\nBy Taylor’s theorem (5.15), \\[ h(1)=\\sum_{k=0}^{m-1} \\frac{h^{(k)}(0)}{k !}+\\frac{h^{(m)}(t)}{m !} \\]\nfor some \\(t \\in(0,1)\\). Use this to prove Taylor’s theorem in \\(n\\) variables by showing that the formula\n\\[ f(\\mathbf{a}+\\mathbf{x})=\\sum_{k=0}^{m-1} \\frac{1}{k !} \\sum\\left(D_{i_{1}} \\ldots \\iota_{k} f\\right)(\\mathbf{a}) x_{\\iota_{1}} \\ldots x_{l_{k}}+r(\\mathbf{x}) \\]\nrepresents \\(f(\\mathbf{a}+\\mathbf{x})\\) as the sum of its so-called “Taylor polynomial of degree \\(m-1\\), ,’ plus a remainder that satisfies\n\\[ \\lim _{\\mathbf{x} \\rightarrow 0} \\frac{r(\\mathbf{x})}{|\\mathbf{x}|^{m-1}}=0 . \\]\nEach of the inner sums extends over all ordered \\(k\\)-tuples \\(\\left(i_{1}, \\ldots, i_{k}\\right)\\), as in part (a); as usual, the zero-order derivative of \\(f\\) is simply \\(f\\), so that the constant term of the Taylor polynomial of \\(f\\) at a is \\(f(\\mathbf{a})\\).\nExercise 29 shows that repetition occurs in the Taylor polynomial as written in part (b). For instance, \\(D_{113}\\) occurs three times, as \\(D_{113}, D_{131}, D_{311}\\). The sum of the corresponding three terms can be written in the form \\[ 3\\left(D_{1}^{2} D_{3} f\\right)(a) x_{1}^{2} x_{3} \\text {. } \\]\nProve (by calculating how often each derivative occurs) that the Taylor polynomial in (b) can be written in the form\n\\[ \\sum \\frac{\\left(D_{1}^{s_{1}} \\cdots D_{n}^{s_{n}} f\\right)(\\mathbf{a})}{s_{1} ! \\cdots s_{n} !} x_{1}^{s_{1}} \\cdots x_{n}^{s_{n}} . \\]\nHere the summation extends over all ordered \\(n\\)-tuples \\(\\left(s_{1}, \\ldots, s_{n}\\right)\\) such that each \\(s_{1}\\) is a nonnegative integer, and \\(s_{1}+\\cdots+s_{n} \\leq m-1\\).\nSuppose \\(f \\in \\mathscr{C}^{(3)}\\) in some neighborhood of a point \\(\\mathbf{a} \\in R^{2}\\), the gradient of \\(f\\) is 0 at \\(\\mathbf{a}\\), but not all second-order derivatives of \\(f\\) are 0 at \\(\\mathbf{a}\\). Show how one can then determine from the Taylor polynomial of \\(f\\) at a (of degree 2 ) whether \\(f\\) has a local maximum, or a local minimum, or neither, at the point \\(\\mathbf{a}\\). Extend this to \\(R^{n}\\) in place of \\(R^{2}\\).\n","date":"2022-08-25T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/10-exericese/","section":"baby rudin","tags":null,"title":"10 EXERICESE"},{"categories":["Algorithm"],"contents":"Background A Fenwick tree or binary indexed tree is a data structure that can efficiently update elements and calculate prefix sums in a table of numbers.1\nSpace and Time Complexity\n\\[ \\begin{array}{lll} \\text { Algorithm } \u0026amp; \\text { Average } \u0026amp; \\text { Worst case } \\\\ \\text { Space } \u0026amp; \\mathrm{O}(n) \u0026amp; \\mathrm{O}(n) \\\\ \\text { Search } \u0026amp; \\mathrm{O}(\\log n) \u0026amp; \\mathrm{O}(\\log n) \\\\ \\text { Insert } \u0026amp; \\mathrm{O}(\\log n) \u0026amp; \\mathrm{O}(\\log n) \\\\ \\text { Delete } \u0026amp; \\mathrm{O}(\\log n) \u0026amp; \\mathrm{O}(\\log n) \\end{array} \\]\nExplaination image-20220208141125368 The picture show the data store in the binary indexed tree.\nSupporse the name of original array is arr, the name of binary indexed tree is fenw. Than we have following relationship between arr and fenw.\nfenw[0] = arr[0]; fenw[1] = arr[0] + arr[1]; fenw[2] = arr[2]; fenw[3] = arr[0] + arry[1] + arr[2] + arr[3]; Now, we need to found the pattern. First we are going to found out all index in fenw which contains arr[0]. The indexes are 0, 1, 3, 7, 15, ... . Now we see the pattern for arr[0]. First one is 0; second one is 1 which is 0 + 1 = 0 + pow(2,0); third one 3 which is 1 + pow(2,1) = 3, and so on. When we are starting from 3 for array 3 , the next element in the binary index tree contains arr[3] is 7. So how we determine this. Actually when we have 1xxxx01111, the next index is 1xxxx11111. Because of only 1...1contains previous value.\nNext problem is how we write code for these. We can use x | (x + 1) to find the next element in the binary index tree.\nSuppose we are start from 0, the next four element is 0 | 1 = 1, 1 | 2 = 3, 3 | 4 = 7 , 7 | 8 = 15. If we start from 2, the next four element is 2 | 3 = 3 then as same as 0.\nImplementation2 show detail using namespace std; template \u0026lt;typename T\u0026gt; class fenwick { public: vector\u0026lt;T\u0026gt; fenw; int n; fenwick(int _n) : n(_n) { fenw.resize(n); } void modify(int x, T v) { while (x \u0026lt; n) { fenw[x] += v; x |= (x + 1); } } T get(int x) { T v{}; while (x \u0026gt;= 0) { v += fenw[x]; x = (x \u0026amp; (x + 1)) - 1; } return v; } }; Problem set https://codeforces.com/contest/1616/problem/E https://en.wikipedia.org/wiki/Fenwick_tree↩︎\ntemplate code is from tourist↩︎\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/algorithm/binary-indexed-tree/","section":"algorithm","tags":null,"title":"Binary indexed tree (Fenwick Tree)"},{"categories":null,"contents":"Background Breadth-first search (BFS) is an algorithm for searching a tree data structure for a node that satisfies a given property. It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level. Extra memory, usually a queue, is needed to keep track of the child nodes that were encountered but not yet explored.1\nImplementation Generally, BFS is implemented using a queue. First, we are starting from the root of the tree, we put this root in the queue. Then we have a while loop, every time we get the first node in the queue and pop out the node from the queue. Next, we discover all nodes from this and put nodes into the queue, except for that node already discovered before.\nThe pseudocode is shown as follows.\nprocedure BFS(G, root) is let Q be a queue label root as explored Q.enqueue(root) while Q is not empty do v := Q.dequeue() if v is the goal then return v for all edges from v to w in G.adjacentEdges(v) do if w is not labeled as explored then label w as explored Q.enqueue(w) Problem set Problem - D - Codeforces https://en.wikipedia.org/wiki/Breadth-first_search↩︎\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/algorithm/breadth-first-search/","section":"algorithm","tags":null,"title":"Breadth-first search"},{"categories":null,"contents":"The tutorial is very clear. Here is the implementation.\n145489728\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;queue\u0026gt; using namespace std; vector\u0026lt;string\u0026gt; ans; // store the answer vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; a; // store the input // idx[i][j] is a queue, the content of the queue is the index of j in the i-th array. vector\u0026lt;map\u0026lt;int, queue\u0026lt;int\u0026gt;\u0026gt;\u0026gt; idx; // index of the number in each array // child[i] store the child of the i-th array. vector\u0026lt;set\u0026lt;int\u0026gt;\u0026gt; child; // store the child of each array // r_map[i] is a set. The content of the set is the index of the array that has the value of i. map\u0026lt;int, set\u0026lt;int\u0026gt; \u0026gt;r_map; bool dfs(int root) { if (child[root].empty()) return true; bool has_ans = true; while(child[root].size() \u0026gt; 0){ int i = *child[root].begin(); while (idx[root][i].size() \u0026gt; 0) { // remove one path from root to i and assign the answer. auto front = idx[root][i].front(); ans[root][front] = \u0026#39;L\u0026#39;; idx[root][i].pop(); if (idx[root][i].size() == 0){ // if the queue is empty, remove the child. child[root].erase(i); r_map[i].erase(root); } if (r_map[i].empty()) { return false; } // remove the inverse path from i to node. auto node = *r_map[i].begin(); front = idx[node][i].front(); ans[node][front] = \u0026#39;R\u0026#39;; idx[node][i].pop(); if (idx[node][i].size() == 0){ child[node].erase(i); r_map[i].erase(node); } // depth first search from node has_ans \u0026amp;= dfs(node); } } return has_ans; } int main() { int m; cin \u0026gt;\u0026gt; m; ans.resize(m); a.resize(m); idx.resize(m); child.resize(m); for (int i = 0; i \u0026lt; m; i++) { int n; cin \u0026gt;\u0026gt; n; ans[i].resize(n, \u0026#39;0\u0026#39;); a[i].resize(n); int tmp; for (int j = 0; j \u0026lt; n; j++) { cin \u0026gt;\u0026gt; a[i][j]; tmp = a[i][j]; idx[i][tmp].push(j); child[i].insert(tmp); r_map[tmp].insert(i); } } bool has_ans = true; for(int i=0;i\u0026lt;m;i++){ has_ans \u0026amp;= dfs(i); } if (!has_ans){ cout\u0026lt;\u0026lt;\u0026quot;NO\u0026quot;\u0026lt;\u0026lt;endl; return 0; } cout\u0026lt;\u0026lt;\u0026quot;YES\u0026quot;\u0026lt;\u0026lt;endl; for(auto i:ans) cout\u0026lt;\u0026lt;i\u0026lt;\u0026lt;endl; } ","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/codeforces/770/770_e/","section":"codeforces","tags":null,"title":"Codeforces Round 770 E"},{"categories":null,"contents":"Abstract Dynamic Programming is a very important topic in Computer Science, Operations research, Reinforcement learning, and so on.\nWhen different areas people talk about dynamic programming, they may have a different understanding of dynamic programming. This post will give a mathematical definition of dynamic programming and give some examples of programming algorithms. In the future, I will like to write some dynamic programming in control theory (optimal control) and reinforcement learning.\nDefinition Problem Set https://codeforces.com/contest/1673/problem/C https://codeforces.com/contest/1675/problem/G https://codeforces.com/contest/1670/problem/F ","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/algorithm/dynamic-programming/","section":"algorithm","tags":null,"title":"dynamic programming"},{"categories":null,"contents":"A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). 1\nThe last post is really good for understand and implement fft.\nThe part I want to explain is for following statement in the post.\nThis function works with polynomials with integer coefficients, however you can also adjust it to work with other types. Since there is some error when working with complex numbers, we need round the resulting coefficients at the end.\nFinally the function for multiplying two long numbers practically doesn’t differ from the function for multiplying polynomials. The only thing we have to do afterwards, is to normalize the number:\nint carry = 0; for (int i = 0; i \u0026lt; n; i++) result[i] += carry; carry = result[i] / 10; result[i] %= 10; } This paragraph says we can use fft to multiplying two long number.\nA number we can write as \\(A = a_0 \\times 10^0 + a_1 \\times 10^1 + ... + a_{n-1} \\times 10 ^{n-1}\\) . That’s easy to see multiply two large number is same as multiple two polynomials.\nSource code from web recursive version Recursive version will take more space.\nFFT using cd = complex\u0026lt;double\u0026gt;; const double PI = acos(-1); void fft(vector\u0026lt;cd\u0026gt; \u0026amp; a, bool invert) { int n = a.size(); if (n == 1) return; vector\u0026lt;cd\u0026gt; a0(n / 2), a1(n / 2); for (int i = 0; 2 * i \u0026lt; n; i++) { a0[i] = a[2*i]; a1[i] = a[2*i+1]; } fft(a0, invert); fft(a1, invert); double ang = 2 * PI / n * (invert ? -1 : 1); cd w(1), wn(cos(ang), sin(ang)); for (int i = 0; 2 * i \u0026lt; n; i++) { a[i] = a0[i] + w * a1[i]; a[i + n/2] = a0[i] - w * a1[i]; if (invert) { a[i] /= 2; a[i + n/2] /= 2; } w *= wn; } } multiply two polynomials vector\u0026lt;int\u0026gt; multiply(vector\u0026lt;int\u0026gt; const\u0026amp; a, vector\u0026lt;int\u0026gt; const\u0026amp; b) { vector\u0026lt;cd\u0026gt; fa(a.begin(), a.end()), fb(b.begin(), b.end()); int n = 1; while (n \u0026lt; a.size() + b.size()) n \u0026lt;\u0026lt;= 1; fa.resize(n); fb.resize(n); fft(fa, false); fft(fb, false); for (int i = 0; i \u0026lt; n; i++) fa[i] *= fb[i]; fft(fa, true); vector\u0026lt;int\u0026gt; result(n); for (int i = 0; i \u0026lt; n; i++) result[i] = round(fa[i].real()); return result; } in-place version using cd = complex\u0026lt;double\u0026gt;; const double PI = acos(-1); void fft(vector\u0026lt;cd\u0026gt; \u0026amp; a, bool invert) { int n = a.size(); for (int i = 1, j = 0; i \u0026lt; n; i++) { int bit = n \u0026gt;\u0026gt; 1; for (; j \u0026amp; bit; bit \u0026gt;\u0026gt;= 1) j ^= bit; j ^= bit; if (i \u0026lt; j) swap(a[i], a[j]); } for (int len = 2; len \u0026lt;= n; len \u0026lt;\u0026lt;= 1) { double ang = 2 * PI / len * (invert ? -1 : 1); cd wlen(cos(ang), sin(ang)); for (int i = 0; i \u0026lt; n; i += len) { cd w(1); for (int j = 0; j \u0026lt; len / 2; j++) { cd u = a[i+j], v = a[i+j+len/2] * w; a[i+j] = u + v; a[i+j+len/2] = u - v; w *= wlen; } } } if (invert) { for (cd \u0026amp; x : a) x /= n; } } Resources ​ https://zongpitt.com/posts/fourier-series/ ​ https://zongpitt.com/posts/fourier-transform/ ​ https://zongpitt.com/posts/discrete-fourier-transfrom/ ​ https://en.wikipedia.org/wiki/Fast_Fourier_transform ​ https://en.wikipedia.org/wiki/Fourier_analysis ​ https://cp-algorithms.com/algebra/fft.html\nhttps://en.wikipedia.org/wiki/Fast_Fourier_transform↩︎\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/algorithm/fast-fourier-transform/","section":"algorithm","tags":null,"title":"Fast Fourier tranform"},{"categories":null,"contents":" img How to know the heap size in RAM.\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/heap-and-stack/","section":"posts","tags":null,"title":"heap and stack"},{"categories":null,"contents":"\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/sd-card-/","section":"posts","tags":null,"title":"sdcard pin layout"},{"categories":["Algorithm"],"contents":"Background In computer science, a segment tree, also known as a statistic tree, is a tree data structure used for storing information about intervals, or segments. It allows querying which of the stored segments contain a given point. It is, in principle, a static structure; that is, it’s a structure that cannot be modified once it’s built. A similar data structure is the interval tree.1\nSpace Complexity: \\(O(n\\log n)\\)\nTime Complexity:\nbuild the tree: \\(O(n \\log n)\\) query: \\(O(\\log n)\\) node update: \\(O(\\log n)\\) A segment tree is a binary tree, each node contains information for a segment of the original array. The value in the node segment tree is some attribute of the range. i.e. sum, min, max, gcd, etc.\nAn example of a segment tree. (attribute: sum)\ngraph TD; A(10, 1-4) --\u0026gt; B(3, 1-2); A --\u0026gt; C(7, 3-4); B --\u0026gt; D(1, 1-1) ; B --\u0026gt; E(2, 2-2); C --\u0026gt; F(3, 3-3); C --\u0026gt; G(4, 4-4); The root will contain the attribution for the whole array (range 1-4). The left and right subtrees contain the range(1-2) and range(3-4), and so on. The leaves are the original value of the array.\nThe node of the tree The node of the tree contains the attribute we want to maintain and the left bound and right bound of the node (segment).\nBuild the tree I use the recursive method to build the tree. I build a segment tree from the root. The value in the root can get from the left subtree and right subtree. So before setting the value for the root node, we need to build the left and right subtree. When build left subtree and right subtree with the same idea.\nFollowing the code which builds the tree. The whole tree is stored in an array. Suppose the root is in index 1. Because a segment tree is a binary tree. So it has two subtrees. we can put the child of 1 in indexes 2 and 3. Node 2 has 2 children so we need to put it in 4 and 5. Thus the children of node 3 will be in indexes 6 and 7. We can find the pattern, the index left tree of node is node * 2 = node \u0026lt;\u0026lt; 1, the index of right tree of node is node * 2 + 1 = node \u0026lt;\u0026lt; 1 | 1. When we build the tree, we also need the assign left bound and right bound to the node. The left bound and right bound can get from the parent segment. Because we are dividing the parent segment into two-part. So the bound of the left subtree is l, mid, the bound of the right subtree is mid + 1, r where the l and r are the bound of the current node and mid = (l + r ) / 2.\nBecause we are using the recursive method to build the tree, there is a terminal condition. The terminal condition is we arrive at the leaves. The condition to judgment a leaf is the right boundary equals the left boundary ( l == r in the code).\nUpdate (node) Update a node in the segment tree. We are using the recursive method to update the value. The root node definitely will affect the new node, because it needs information from the whole array. Since two subtree does not have an intersection, so only one subtree will affect by the new node. Thus we only update one of the subtree.\nUpdate (segment with lazy) Pending\nQuery The query aims to get information from the segment tree. A query segment can be composed of many segments in the segment tree. Still using the recursive method, when the inquiry range is larger than the tree node range we directly return the value, otherwise, we go to the subtree to find the value.\nProblem Set Codeforces Round #769 Problem - D - Codeforces Codeforces Round #771 Problem - E - Codeforces Segment tree - Wikipedia↩︎\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/algorithm/segment-tree/","section":"algorithm","tags":["Algorithm"],"title":"Segment Tree"},{"categories":null,"contents":"import training speed\ncudnn_benchmark = True allow_tf32 = False torch.backends.cudnn.benchmark = cudnn_benchmark # Improves training speed. torch.backends.cuda.matmul.allow_tf32 = allow_tf32 # Allow PyTorch to internally use tf32 for matmul torch.backends.cudnn.allow_tf32 = allow_tf32 # Allow PyTorch to internally use tf32 for convolutions use nvidia custom gradient fixed convolution (not detail yet)\ntraining set keywards\nclass_name = training.dataset.ImageFolderDataset path = /home/zong/datasets/food-101-gan use_labels = False max_szie = 98000 xflip = False resolution = 256 __len__ = 6 data_loader_kwargs\npin_memory = True num_workers = 3 prefetch_factor = 2 common_kwargs\nc_dim = label_dim = 0 img_resolution = 256 img_channels = 3 G_kwargs\nclass_name = training.networks.Generator z_dim = 512 w_dim = 512 mapping_kwargs = {\u0026#39;num_layers\u0026#39;: 2} synthesis_kwargs = {\u0026#39;channels_base\u0026#39; :16384, channel_max: 512, num_fp16_res: 4, conv_clamp: 256} mapping network\nimage-20220801152802172 synthesis network\ndiscriminate network\nresume from existing pickle load network parameter and training kwargs\nprint the nwtwork setup augmentation (ada paper) augment_kwargs\nclass_name = training.augment.AugmentPipe xflip = 1 rotate90 = 1 xint = 1 scale = 1 rotate = 1 aniso = 1 xfrac = 1 brightness = 1 contrast = 1 lumaflip = 1 hue = 1 saturation = 1 Collector require_grad neet to be set before ddp ddp distributed data parallel\nDistribute model to multiple GPU\nloss loss_kwargs\nclass_name = training.loss.StyleGAN2Loss r1_gamma = 0.8192 # from train.py line 173. heuristic formula G_opt_kwargs\nclass_name = torch.optim.Adam lr = 0.0025 betas = [0, 0.99] eps = 1e-8 G_reg_interval = 4\nD_opt_kwargs\nclass_name = torch.optim.Adam lr = 0.0025 betas = [0, 0.99] eps = 1e-8 D_reg_interval = 16\nregularization mb_ratio = reg_interval / (reg_interval + 1)\ntraining process G_main\nG_reg\nD_main\nD_reg\ntraining batch sync always tru because only traiing by 1 gpu\nbatch_size and batch_gpu\nnetwork structure (detail from code) Discriminator residual connection\n","date":"2022-08-24T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/stylegan-v2-doc/","section":"posts","tags":null,"title":"stylegan v2 doc"},{"categories":null,"contents":" youtube bilibili note pdf xopp We begin this section with a definition of the function concept.\n2.1 Definition Consider two sets \\(A\\) and \\(B\\), whose elements may be any objects whatsoever, and suppose that with each element \\(x\\) of \\(A\\) there is associated, in some manner, an element of \\(B\\), which we denote by \\(f(x)\\). Then \\(f\\) is said to be a function from \\(A\\) to \\(B\\) (or a mapping of \\(A\\) into \\(B\\) ). The set \\(A\\) is called the domain of \\(f\\) (we also say \\(f\\) is defined on \\(A\\) ), and the elements \\(f(x)\\) are called the values of \\(f\\). The set of all values of \\(f\\) is called the range of \\(f\\).\n\\(2.2\\) Definition Let \\(A\\) and \\(B\\) be two sets and let \\(f\\) be a mapping of \\(A\\) into \\(B\\). If \\(E \\subset A, f(E)\\) is defined to be the set of all elements \\(f(x)\\), for \\(x \\in E\\). We call \\(f(E)\\) the image of \\(E\\) under \\(f\\). In this notation, \\(f(A)\\) is the range of \\(f\\). It is clear that \\(f(A) \\subset B\\). If \\(f(A)=B\\), we say that \\(f\\) maps \\(A\\) onto \\(B\\). (Note that, according to this usage, onto is more specific than into.)\nIf \\(E \\subset B, f^{-1}(E)\\) denotes the set of all \\(x \\in A\\) such that \\(f(x) \\in E\\). We call \\(f^{-1}(E)\\) the inverse image of \\(E\\) under \\(f\\). If \\(y \\in B, f^{-1}(y)\\) is the set of all \\(x \\in A\\) such that \\(f(x)=y\\). If, for each \\(y \\in B, f^{-1}(y)\\) consists of at most one element of \\(A\\), then \\(f\\) is said to be a 1-1 (one-to-one) mapping of \\(A\\) into \\(B\\). This may also be expressed as follows: \\(f\\) is a 1-1 mapping of \\(A\\) into \\(B\\) provided that \\(f\\left(x_{1}\\right) \\neq f\\left(x_{2}\\right)\\) whenever \\(x_{1} \\neq x_{2}, x_{1} \\in A, x_{2} \\in A\\).\n(The notation \\(x_{1} \\neq x_{2}\\) means that \\(x_{1}\\) and \\(x_{2}\\) are distinct elements; otherwise we write \\(x_{1}=x_{2}\\).)\n2.3 Definition If there exists a 1-1 mapping of \\(A\\) onto \\(B\\), we say that \\(A\\) and \\(B\\) can be put in 1-1 correspondence, or that \\(A\\) and \\(B\\) have the same cardinal number, or, briefly, that \\(A\\) and \\(B\\) are equivalent, and we write \\(A \\sim B\\). This relation clearly has the following properties:\nIt is reflexive: \\(A \\sim A\\).\nIt is symmetric: If \\(A \\sim B\\), then \\(B \\sim A\\).\nIt is transitive: If \\(A \\sim B\\) and \\(B \\sim C\\), then \\(A \\sim C\\).\nAny relation with these three properties is called an equivalence relation.\n2.4 Definition For any positive integer \\(n\\), let \\(J_{n}\\) be the set whose elements are the integers \\(1,2, \\ldots, n\\); let \\(J\\) be the set consisting of all positive integers. For any set \\(A\\), we say:\n\\(A\\) is finite if \\(A \\sim J_{n}\\) for some \\(n\\) (the empty set is also considered to be finite).\n\\(A\\) is infinite if \\(A\\) is not finite.\n\\(A\\) is countable if \\(A \\sim J\\).\n\\(A\\) is uncountable if \\(A\\) is neither finite nor countable.\n\\(A\\) is at most countable if \\(A\\) is finite or countable.\nCountable sets are sometimes called enumerable, or denumerable.\nFor two finite sets \\(A\\) and \\(B\\), we evidently have \\(A \\sim B\\) if and only if \\(A\\) and \\(B\\) contain the same number of elements. For infinite sets, however, the idea of “having the same number of elements” becomes quite vague, whereas the notion of \\(1-1\\) correspondence retains its clarity.\n2.5 Example Let \\(A\\) be the set of all integers. Then \\(A\\) is countable. For, consider the following arrangement of the sets \\(A\\) and \\(J\\) :\n\\[ \\begin{array}{ll} A: \u0026amp; 0,1,-1,2,-2,3,-3, \\ldots \\\\ J: \u0026amp; 1,2,3,4,5,6,7, \\ldots \\end{array} \\]\nWe can, in this example, even give an explicit formula for a function \\(f\\) from \\(J\\) to \\(A\\) which sets up a 1-1 correspondence:\n\\[ f(n)= \\begin{cases}\\frac{n}{2} \u0026amp; \\text { ( } n \\text { even), } \\\\ -\\frac{n-1}{2} \u0026amp; \\text { (n odd). }\\end{cases} \\]\n2.6 Remark A finite set cannot be equivalent to one of its proper subsets. That this is, however, possible for infinite sets, is shown by Example \\(2.5\\), in which \\(J\\) is a proper subset of \\(A\\).\nIn fact, we could replace Definition \\(2.4(b)\\) by the statement: \\(A\\) is infinite if \\(A\\) is equivalent to one of its proper subsets.\n2.7 Definition By a sequence, we mean a function \\(f\\) defined on the set \\(J\\) of all positive integers. If \\(f(n)=x_{n}\\), for \\(n \\in J\\), it is customary to denote the sequence \\(f\\) by the symbol \\(\\left\\{x_{n}\\right\\}\\), or sometimes by \\(x_{1}, x_{2}, x_{3}, \\ldots\\). The values of \\(f\\), that is, the elements \\(x_{n}\\), are called the terms of the sequence. If \\(A\\) is a set and if \\(x_{n} \\in A\\) for all \\(n \\in J\\), then \\(\\left\\{x_{n}\\right\\}\\) is said to be a sequence in \\(A\\), or a sequence of elements of \\(A\\).\nNote that the terms \\(x_{1}, x_{2}, x_{3}, \\ldots\\) of a sequence need not be distinct.\nSince every countable set is the range of a 1-1 function defined on \\(J\\), we may regard every countable set as the range of a sequence of distinct terms. Speaking more loosely, we may say that the elements of any countable set can be “arranged in a sequence.”\nSometimes it is convenient to replace \\(J\\) in this definition by the set of all nonnegative integers, i.e., to start with 0 rather than with 1 .\n2.8 Theorem Every infinite subset of a countable set \\(A\\) is countable.\nProof Suppose \\(E \\subset A\\), and \\(E\\) is infinite. Arrange the elements \\(x\\) of \\(A\\) in a sequence \\(\\left\\{x_{n}\\right\\}\\) of distinct elements. Construct a sequence \\(\\left\\{n_{k}\\right\\}\\) as follows:\nLet \\(n_{1}\\) be the smallest positive integer such that \\(x_{n_{1}} \\in E\\). Having chosen \\(n_{1}, \\ldots, n_{k-1}(k=2,3,4, \\ldots)\\), let \\(n_{k}\\) be the smallest integer greater than \\(n_{k-1}\\) such that \\(x_{n_{k}} \\in E\\).\nPutting \\(f(k)=x_{n_{k}}(k=1,2,3, \\ldots)\\), we obtain a 1-1 correspondence between \\(E\\) and \\(J\\).\nThe theorem shows that, roughly speaking, countable sets represent the “smallest” infinity: No uncountable set can be a subset of a countable set.\n2.9 Definition Let \\(A\\) and \\(\\Omega\\) be sets, and suppose that with each element \\(\\alpha\\) of \\(A\\) there is associated a subset of \\(\\Omega\\) which we denote by \\(E_{\\alpha}\\). The set whose elements are the sets \\(E_{\\alpha}\\) will be denoted by \\(\\left\\{E_{\\alpha}\\right\\}\\). Instead of speaking of sets of sets, we shall sometimes speak of a collection of sets, or a family of sets.\nThe union of the sets \\(E_{\\alpha}\\) is defined to be the set \\(S\\) such that \\(x \\in S\\) if and only if \\(x \\in E_{\\alpha}\\) for at least one \\(\\alpha \\in A\\). We use the notation\n\\[ S=\\bigcup_{\\alpha \\in A} E_{\\alpha} . \\]\nIf \\(A\\) consists of the integers \\(1,2, \\ldots, n\\), one usually writes\n\\[ S=\\bigcup_{m=1}^{n} E_{m} \\]\nor\n\\[ S=E_{1} \\cup E_{2} \\cup \\cdots \\cup E_{n} . \\]\nIf \\(A\\) is the set of all positive integers, the usual notation is\n\\[ S=\\bigcup_{m=1}^{\\infty} E_{m} . \\]\nThe symbol \\(\\infty\\) in (4) merely indicates that the union of a countable collection of sets is taken, and should not be confused with the symbols \\(+\\infty,-\\infty\\), introduced in Definition \\(1.23 .\\)\nThe intersection of the sets \\(E_{\\alpha}\\) is defined to be the set \\(P\\) such that \\(x \\in P\\) if and only if \\(x \\in E_{\\alpha}\\) for every \\(\\alpha \\in A\\). We use the notation\n\\[ P=\\bigcap_{\\alpha \\in A} E_{\\alpha}, \\]\nor\n\\[ P=\\bigcap_{m=1}^{n} E_{m}=E_{1} \\cap E_{2} \\cap \\cdots \\cap E_{n}, \\]\nor\n\\[ P=\\bigcap_{m=1}^{\\infty} E_{m}, \\]\nas for unions. If \\(A \\cap B\\) is not empty, we say that \\(A\\) and \\(B\\) intersect; otherwise they are disjoint.\n\\(2.10\\) Examples\nSuppose \\(E_{1}\\) consists of \\(1,2,3\\) and \\(E_{2}\\) consists of \\(2,3,4\\). Then \\(E_{1} \\cup E_{2}\\) consists of \\(1,2,3,4\\), whereas \\(E_{1} \\cap E_{2}\\) consists of 2,3 . (b) Let \\(A\\) be the set of real numbers \\(x\\) such that \\(0\u0026lt;x \\leq 1\\). For every \\(x \\in A\\), let \\(E_{x}\\) be the set of real numbers \\(y\\) such that \\(0\u0026lt;y\u0026lt;x\\). Then \\[ \\begin{gathered} E_{x} \\subset E_{z} \\text { if and only if } 0\u0026lt;x \\leq z \\leq 1 ; \\\\ \\bigcup_{\\substack{x \\in A}} E_{x}=E_{1} ; \\\\ \\bigcap_{x} \\text { is empty; } \\end{gathered} \\]\nand (ii) are clear. To prove (iii), we note that for every \\(y\u0026gt;0, y \\notin E_{x}\\) if \\(x\u0026lt;y\\). Hence \\(y \\notin \\bigcap_{x \\in A} E_{x}\\). 2.11 Remarks Many properties of unions and intersections are quite similar to those of sums and products; in fact, the words sum and product were sometimes used in this connection, and the symbols \\(\\Sigma\\) and \\(\\Pi\\) were written in place of \\(\\bigcup\\) and \\(\\bigcap\\).\nThe commutative and associative laws are trivial:\n\\[ \\begin{array}{cc} A \\cup B=B \\cup A ; \u0026amp; A \\cap B=B \\cap A . \\\\ (A \\cup B) \\cup C=A \\cup(B \\cup C) ; \u0026amp; (A \\cap B) \\cap C=A \\cap(B \\cap C) . \\end{array} \\]\nThus the omission of parentheses in (3) and (6) is justified.\nThe distributive law also holds:\n\\[ A \\cap(B \\cup C)=(A \\cap B) \\cup(A \\cap C) . \\]\nTo prove this, let the left and right members of (10) be denoted by \\(E\\) and \\(F\\), respectively.\nSuppose \\(x \\in E\\). Then \\(x \\in A\\) and \\(x \\in B \\cup C\\), that is, \\(x \\in B\\) or \\(x \\in C\\) (possibly both). Hence \\(x \\in A \\cap B\\) or \\(x \\in A \\cap C\\), so that \\(x \\in F\\). Thus \\(E \\subset F\\).\nNext, suppose \\(x \\in F\\). Then \\(x \\in A \\cap B\\) or \\(x \\in A \\cap C\\). That is, \\(x \\in A\\), and \\(x \\in B \\cup C\\). Hence \\(x \\in A \\cap(B \\cup C)\\), so that \\(F \\subset E\\).\nIt follows that \\(E=F\\).\nWe list a few more relations which are easily verified:\n\\[ \\begin{aligned} \u0026amp;A \\subset A \\cup B, \\\\ \u0026amp;A \\cap B \\subset A . \\end{aligned} \\]\nIf 0 denotes the empty set, then\n\\[ A \\cup 0=A, \\quad A \\cap 0=0 . \\]\nIf \\(A \\subset B\\), then\n\\[ A \\cup B=B, \\quad A \\cap B=A \\text {. } \\]\n2.12 Theorem Let \\(\\left\\{E_{n}\\right\\}, n=1,2,3, \\ldots\\), be a sequence of countable sets, and put\nThen \\(S\\) is countable.\n\\[ S=\\bigcup_{n=1}^{\\infty} E_{n} . \\]\nProof Let every set \\(E_{n}\\) be arranged in a sequence \\(\\left\\{x_{n k}\\right\\}, k=1,2,3, \\ldots\\), and consider the infinite array\nin which the elements of \\(E_{n}\\) form the \\(n\\)th row. The array contains all elements of \\(S\\). As indicated by the arrows, these elements can be arranged in a sequence\n\\[ x_{11} ; x_{21}, x_{12} ; x_{31}, x_{22}, x_{13} ; x_{41}, x_{32}, x_{23}, x_{14} ; \\ldots \\]\nIf any two of the sets \\(E_{n}\\) have elements in common, these will appear more than once in (17). Hence there is a subset \\(T\\) of the set of all positive integers such that \\(S \\sim T\\), which shows that \\(S\\) is at most countable (Theorem 2.8). Since \\(E_{1} \\subset S\\), and \\(E_{1}\\) is infinite, \\(S\\) is infinite, and thus countable.\nCorollary Suppose \\(A\\) is at most countable, and, for every \\(\\alpha \\in A, B_{\\alpha}\\) is at most countable. Put\n\\[ T=\\bigcup_{\\alpha \\in A} B_{\\alpha} . \\]\nThen \\(T\\) is at most countable.\nFor \\(T\\) is equivalent to a subset of \\((15)\\).\n2.13 Theorem Let \\(A\\) be a countable set, and let \\(B_{n}\\) be the set of all n-tuples \\(\\left(a_{1}, \\ldots, a_{n}\\right)\\), where \\(a_{k} \\in A(k=1, \\ldots, n)\\), and the elements \\(a_{1}, \\ldots, a_{n}\\) need not be distinct. Then \\(B_{n}\\) is countable.\nProof That \\(B_{1}\\) is countable is evident, since \\(B_{1}=A\\). Suppose \\(B_{n-1}\\) is countable \\((n=2,3,4, \\ldots)\\). The elements of \\(B_{n}\\) are of the form\n\\[ (b, a) \\quad\\left(b \\in B_{n-1}, a \\in A\\right) \\text {. } \\]\nFor every fixed \\(b\\), the set of pairs \\((b, a)\\) is equivalent to \\(A\\), and hence countable. Thus \\(B_{n}\\) is the union of a countable set of countable sets. By Theorem \\(2.12, B_{n}\\) is countable.\nThe theorem follows by induction.\nCorollary The set of all rational numbers is countable.\nProof We apply Theorem \\(2.13\\), with \\(n=2\\), noting that every rational \\(r\\) is of the form \\(b / a\\), where \\(a\\) and \\(b\\) are integers. The set of pairs \\((a, b)\\), and therefore the set of fractions \\(b / a\\), is countable.\nIn fact, even the set of all algebraic numbers is countable (see Exercise 2\\()\\)\nThat not all infinite sets are, however, countable, is shown by the next theorem.\n2.14 Theorem Let \\(A\\) be the set of all sequences whose elements are the digits 0 and 1. This set \\(A\\) is uncountable.\nThe elements of \\(A\\) are sequences like \\(1,0,0,1,0,1,1,1, \\ldots\\)\nProof Let \\(E\\) be a countable subset of \\(A\\), and let \\(E\\) consist of the sequences \\(s_{1}, s_{2}, s_{3}, \\ldots\\) We construct a sequence \\(s\\) as follows. If the \\(n\\)th digit in \\(s_{n}\\) is 1 , we let the \\(n\\)th digit of \\(s\\) be 0 , and vice versa. Then the sequence \\(s\\) differs from every member of \\(E\\) in at least one place; hence \\(s \\notin E\\). But clearly \\(s \\in A\\), so that \\(E\\) is a proper subset of \\(A\\).\nWe have shown that every countable subset of \\(A\\) is a proper subset of \\(A\\). It follows that \\(A\\) is uncountable (for otherwise \\(A\\) would be a proper subset of \\(A\\), which is absurd).\nThe idea of the above proof was first used by Cantor, and is called Cantor’s diagonal process; for, if the sequences \\(s_{1}, s_{2}, s_{3}, \\ldots\\) are placed in an array like (16), it is the elements on the diagonal which are involved in the construction of the new sequence.\nReaders who are familiar with the binary representation of the real numbers (base 2 instead of 10 ) will notice that Theorem \\(2.14\\) implies that the set of all real numbers is uncountable. We shall give a second proof of this fact in Theorem \\(2.43 .\\)\n","date":"2022-08-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch2/1-finite-countable-and-uncountable-sets/","section":"baby rudin","tags":null,"title":"1 FINITE, COUNTABLE, AND UNCOUNTABLE SETS"},{"categories":null,"contents":" youtube part1 part2 part3 bilibili part1 part2 part3 note part1 pdf xopp note part2 pdf xopp note part3 pdf oxpp 2.15 Definition A set \\(X\\), whose elements we shall call points, is said to be a metric space if with any two points \\(p\\) and \\(q\\) of \\(X\\) there is associated a real number \\(d(p, q)\\), called the distance from \\(p\\) to \\(q\\), such that\n\\(d(p, q)\u0026gt;0\\) if \\(p \\neq q ; d(p, p)=0\\);\n\\(d(p, q)=d(q, p)\\);\n\\(d(p, q) \\leq d(p, r)+d(r, q)\\), for any \\(r \\in X\\).\nAny function with these three properties is called a distance function, or a metric.\n2.16 Examples\nThe most important examples of metric spaces, from our standpoint, are the euclidean spaces \\(R^{k}\\), especially \\(R^{1}\\) (the real line) and \\(R^{2}\\) (the complex plane); the distance in \\(R^{k}\\) is defined by\n\\[ d(\\mathbf{x}, \\mathbf{y})=|\\mathbf{x}-\\mathbf{y}| \\quad\\left(\\mathbf{x}, \\mathbf{y} \\in R^{k}\\right) . \\]\nBy Theorem 1.37, the conditions of Definition \\(2.15\\) are satisfied by (19).\nIt is important to observe that every subset \\(Y\\) of a metric space \\(X\\) is a metric space in its own right, with the same distance function. For it is clear that if conditions \\((a)\\) to \\((c)\\) of Definition \\(2.15\\) hold for \\(p, q, r \\in X\\), they also hold if we restrict \\(p, q, r\\) to lie in \\(Y\\).\nThus every subset of a euclidean space is a metric space. Other examples are the spaces \\(\\mathscr{C}(K)\\) and \\(\\mathscr{L}^{2}(\\mu)\\), which are discussed in Chaps. 7 and 11 , respectively.\n2.17 Definition By the segment \\((a, b)\\) we mean the set of all real numbers \\(x\\) such that \\(a\u0026lt;x\u0026lt;b\\).\nBy the interval \\([a, b]\\) we mean the set of all real numbers \\(x\\) such that \\(a \\leq x \\leq b\\).\nOccasionally we shall also encounter “half-open intervals” \\([a, b)\\) and \\((a, b]\\); the first consists of all \\(x\\) such that \\(a \\leq x\u0026lt;b\\), the second of all \\(x\\) such that \\(a\u0026lt;x \\leq b\\)\nIf \\(a_{i}\u0026lt;b_{i}\\) for \\(i=1, \\ldots, k\\), the set of all points \\(\\mathbf{x}=\\left(x_{1}, \\ldots, x_{k}\\right)\\) in \\(R^{k}\\) whose coordinates satisfy the inequalities \\(a_{i} \\leq x_{i} \\leq b_{i}(1 \\leq i \\leq k)\\) is called a \\(k\\)-cell. Thus a 1 -cell is an interval, a 2-cell is a rectangle, etc.\nIf \\(\\mathbf{x} \\in R^{k}\\) and \\(r\u0026gt;0\\), the open (or closed) ball \\(B\\) with center at \\(\\mathbf{x}\\) and radius \\(r\\) is defined to be the set of all \\(\\mathbf{y} \\in R^{k}\\) such that \\(|\\mathbf{y}-\\mathbf{x}|\u0026lt;r(\\) or \\(|\\mathbf{y}-\\mathbf{x}| \\leq r\\) ).\nWe call a set \\(E \\subset R^{k}\\) convex if\n\\[ \\lambda \\mathbf{x}+(1-\\lambda) \\mathbf{y} \\in E \\]\nwhenever \\(\\mathbf{x} \\in E, \\mathbf{y} \\in E\\), and \\(0\u0026lt;\\lambda\u0026lt;1\\).\nFor example, balls are convex. For if \\(|\\mathbf{y}-\\mathbf{x}|\u0026lt;r,|\\mathbf{z}-\\mathbf{x}|\u0026lt;r\\), and \\(0\u0026lt;\\lambda\u0026lt;1\\), we have\n\\[ \\begin{aligned} |\\lambda \\mathbf{y}+(1-\\lambda) \\mathbf{z}-\\mathbf{x}| \u0026amp;=|\\lambda(\\mathbf{y}-\\mathbf{x})+(1-\\lambda)(\\mathbf{z}-\\mathbf{x})| \\\\ \u0026amp; \\leq \\lambda|\\mathbf{y}-\\mathbf{x}|+(1-\\lambda)|\\mathbf{z}-\\mathbf{x}|\u0026lt;\\lambda r+(1-\\lambda) r \\\\ \u0026amp;=r . \\end{aligned} \\]\nThe same proof applies to closed balls. It is also easy to see that \\(k\\)-cells are convex.\n2.18 Definition Let \\(X\\) be a metric space. All points and sets mentioned below are understood to be elements and subsets of \\(X\\).\nA neighborhood of \\(p\\) is a set \\(N_{r}(p)\\) consisting of all \\(q\\) such that \\(d(p, q)\u0026lt;r\\), for some \\(r\u0026gt;0\\). The number \\(r\\) is called the radius of \\(N_{r}(p)\\).\nA point \\(p\\) is a limit point of the set \\(E\\) if every neighborhood of \\(p\\) contains a point \\(q \\neq p\\) such that \\(q \\in E\\).\nIf \\(p \\in E\\) and \\(p\\) is not a limit point of \\(E\\), then \\(p\\) is called an isolated point of \\(E\\).\n\\(E\\) is closed if every limit point of \\(E\\) is a point of \\(E\\).\nA point \\(p\\) is an interior point of \\(E\\) if there is a neighborhood \\(N\\) of \\(p\\) such that \\(N \\subset E\\).\n\\(E\\) is open if every point of \\(E\\) is an interior point of \\(E\\).\nThe complement of \\(E\\) (denoted by \\(E^{c}\\) ) is the set of all points \\(p \\in X\\) such that \\(p \\notin E\\).\n\\(E\\) is perfect if \\(E\\) is closed and if every point of \\(E\\) is a limit point of \\(E\\).\n\\(E\\) is bounded if there is a real number \\(M\\) and a point \\(q \\in X\\) such that \\(d(p, q)\u0026lt;M\\) for all \\(p \\in E\\).\n\\(E\\) is dense in \\(X\\) if every point of \\(X\\) is a limit point of \\(E\\), or a point of \\(E\\) (or both).\nLet us note that in \\(R^{1}\\) neighborhoods are segments, whereas in \\(R^{2}\\) neighborhoods are interiors of circles.\n2.19 Theorem Every neighborhood is an open set.\nProof Consider a neighborhood \\(E=N_{r}(p)\\), and let \\(q\\) be any point of \\(E\\). Then there is a positive real number \\(h\\) such that\n\\[ d(p, q)=r-h . \\]\nFor all points \\(s\\) such that \\(d(q, s)\u0026lt;h\\), we have then\n\\[ d(p, s) \\leq d(p, q)+d(q, s)\u0026lt;r-h+h=r, \\]\nso that \\(s \\in E\\). Thus \\(q\\) is an interior point of \\(E\\).\n2.20 Theorem If \\(p\\) is a limit point of a set \\(E\\), then every neighborhood of \\(p\\) contains infinitely many points of \\(E\\).\nProof Suppose there is a neighborhood \\(N\\) of \\(p\\) which contains only a finite number of points of \\(E\\). Let \\(q_{1}, \\ldots, q_{n}\\) be those points of \\(N \\cap E\\), which are distinct from \\(p\\), and put\n\\[ r=\\min _{1 \\leq m \\leq n} d\\left(p, q_{m}\\right) \\]\n[we use this notation to denote the smallest of the numbers \\(d\\left(p, q_{1}\\right), \\ldots\\), \\(\\left.d\\left(p, q_{n}\\right)\\right]\\). The minimum of a finite set of positive numbers is clearly positive, so that \\(r\u0026gt;0\\).\nThe neighborhood \\(N_{r}(p)\\) contains no point \\(q\\) of \\(E\\) such that \\(q \\neq p\\), so that \\(p\\) is not a limit point of \\(E\\). This contradiction establishes the theorem.\n2.21 Examples Let us consider the following subsets of \\(R^{2}\\) :\nThe set of all complex \\(z\\) such that \\(|z|\u0026lt;1\\).\nThe set of all complex \\(z\\) such that \\(|z| \\leq 1\\).\nA nonempty finite set.\nThe set of all integers.\nThe set consisting of the numbers \\(1 / n(n=1,2,3, \\ldots)\\). Let us note that this set \\(E\\) has a limit point (namely, \\(z=0\\) ) but that no point of \\(E\\) is a limit point of \\(E\\); we wish to stress the difference between having a limit point and containing one.\n( \\(f\\) ) The set of all complex numbers (that is, \\(R^{2}\\) ).\nThe segment \\((a, b)\\). Let us note that \\((d),(e),(g)\\) can be regarded also as subsets of \\(R^{1}\\).\nSome properties of these sets are tabulated below:\n\\(\\begin{array}{cccc}\\text { Closed } \u0026amp; \\text { Open } \u0026amp; \\text { Perfect } \u0026amp; \\text { Bounded } \\\\ \\text { No } \u0026amp; \\text { Yes } \u0026amp; \\text { No } \u0026amp; \\text { Yes } \\\\ \\text { Yes } \u0026amp; \\text { No } \u0026amp; \\text { Yes } \u0026amp; \\text { Yes } \\\\ \\text { Yes } \u0026amp; \\text { No } \u0026amp; \\text { No } \u0026amp; \\text { Yes } \\\\ \\text { Yes } \u0026amp; \\text { No } \u0026amp; \\text { No } \u0026amp; \\text { No } \\\\ \\text { No } \u0026amp; \\text { No } \u0026amp; \\text { No } \u0026amp; \\text { Yes } \\\\ \\text { Yes } \u0026amp; \\text { Yes } \u0026amp; \\text { Yes } \u0026amp; \\text { No } \\\\ \\text { No } \u0026amp; \u0026amp; \\text { No } \u0026amp; \\text { Yes }\\end{array}\\)\nIn \\((g)\\), we left the second entry blank. The reason is that the segment \\((a, b)\\) is not open if we regard it as a subset of \\(R^{2}\\), but it is an open subset of \\(R^{1}\\).\n2.22 Theorem Let \\(\\left\\{E_{\\alpha}\\right\\}\\) be a (finite or infinite) collection of sets \\(E_{\\alpha}\\). Then\n\\[ \\left(\\bigcup_{a} E_{\\alpha}\\right)^{c}=\\bigcap_{\\alpha}\\left(E_{\\alpha}^{c}\\right) . \\]\nProof Let \\(A\\) and \\(B\\) be the left and right members of (20). If \\(x \\in A\\), then \\(x \\notin U_{\\alpha} E_{\\alpha}\\), hence \\(x \\notin E_{\\alpha}\\) for any \\(\\alpha\\), hence \\(x \\in E_{\\alpha}^{c}\\) for every \\(\\alpha\\), so that \\(x \\in \\cap E_{\\alpha}^{c}\\). Thus \\(A \\subset B\\). Conversely, if \\(x \\in B\\), then \\(x \\in E_{\\alpha}^{c}\\) for every \\(\\alpha\\), hence \\(x \\notin E_{\\alpha}\\) for any \\(\\alpha\\), hence \\(x \\notin \\cup_{\\alpha} E_{\\alpha}\\), so that \\(x \\in\\left(\\bigcup_{\\alpha} E_{\\alpha}\\right)^{c}\\). Thus \\(B \\subset A\\).\nIt follows that \\(A=B\\).\n2.23 Theorem A set \\(E\\) is open if and only if its complement is closed.\nProof First, suppose \\(E^{c}\\) is closed. Choose \\(x \\in E\\). Then \\(x \\notin E^{c}\\), and \\(x\\) is not a limit point of \\(E^{c}\\). Hence there exists a neighborhood \\(N\\) of \\(x\\) such that \\(E^{c} \\cap N\\) is empty, that is, \\(N \\subset E\\). Thus \\(x\\) is an interior point of \\(E\\), and \\(E\\) is open.\nNext, suppose \\(E\\) is open. Let \\(x\\) be a limit point of \\(E^{c}\\). Then every neighborhood of \\(x\\) contains a point of \\(E^{c}\\), so that \\(x\\) is not an interior point of \\(E\\). Since \\(E\\) is open, this means that \\(x \\in E^{c}\\). It follows that \\(E^{c}\\) is closed.\nCorollary \\(A\\) set \\(F\\) is closed if and only if its complement is open.\n2.24 Theorem\nFor any collection \\(\\left\\{G_{\\alpha}\\right\\}\\) of open sets, \\(\\bigcup_{\\alpha} G_{\\alpha}\\) is open.\nFor any collection \\(\\left\\{F_{\\alpha}\\right\\}\\) of closed sets, \\(\\bigcap_{\\alpha} F_{\\alpha}\\) is closed.\nFor any finite collection \\(G_{1}, \\ldots, G_{n}\\) of open sets, \\(\\bigcap_{i=1}^{n} G_{i}\\) is open.\nFor any finite collection \\(F_{1}, \\ldots, F_{n}\\) of closed sets, \\(\\bigcup_{i=1}^{n} F_{l}\\) is closed.\nProof Put \\(G=\\bigcup_{\\alpha} G_{\\alpha}\\). If \\(x \\in G\\), then \\(x \\in G_{\\alpha}\\) for some \\(\\alpha\\). Since \\(x\\) is an interior point of \\(G_{\\alpha}, x\\) is also an interior point of \\(G\\), and \\(G\\) is open. This proves \\((a)\\).\nBy Theorem 2.22,\n\\[ \\left(\\bigcap_{\\alpha} F_{\\alpha}\\right)^{c}=\\bigcup_{\\alpha}\\left(F_{a}^{c}\\right) \\text {, } \\]\nand \\(F_{a}^{c}\\) is open, by Theorem 2.23. Hence (a) implies that (21) is open so that \\(\\bigcap_{a} F_{\\alpha}\\) is closed.\nNext, put \\(H=\\bigcap_{i=1}^{n} G_{i}\\). For any \\(x \\in H\\), there exist neighborhoods \\(N_{l}\\) of \\(x\\), with radii \\(r_{i}\\), such that \\(N_{l} \\subset G_{i}(i=1, \\ldots, n)\\). Put\n\\[ r=\\min \\left(r_{1}, \\ldots, r_{n}\\right), \\]\nand let \\(N\\) be the neighborhood of \\(x\\) of radius \\(r\\). Then \\(N \\subset G_{i}\\) for \\(i=1\\), \\(\\ldots, n\\), so that \\(N \\subset H\\), and \\(H\\) is open.\nBy taking complements, \\((d)\\) follows from \\((c)\\) :\n\\[ \\left(\\bigcup_{l=1}^{n} F_{l}\\right)^{c}=\\bigcap_{i=1}^{n}\\left(F_{l}^{c}\\right) . \\]\n2.25 Examples In parts \\((c)\\) and \\((d)\\) of the preceding theorem, the finiteness of the collections is essential. For let \\(G_{n}\\) be the segment \\(\\left(-\\frac{1}{n}, \\frac{1}{n}\\right)(n=1,2,3, \\ldots)\\). Then \\(G_{n}\\) is an open subset of \\(R^{1}\\). Put \\(G=\\bigcap_{n=1}^{\\infty} G_{n}\\). Then \\(G\\) consists of a single point (namely, \\(x=0\\) ) and is therefore not an open subset of \\(R^{1}\\).\nThus the intersection of an infinite collection of open sets need not be open. Similarly, the union of an infinite collection of closed sets need not be closed.\n2.26 Definition If \\(X\\) is a metric space, if \\(E \\subset X\\), and if \\(E^{\\prime}\\) denotes the set of all limit points of \\(E\\) in \\(X\\), then the closure of \\(E\\) is the set \\(E=E \\cup E^{\\prime}\\).\n2.27 Theorem If \\(X\\) is a metric space and \\(E \\subset X\\), then\nE is closed,\n\\(E=E\\) if and only if \\(E\\) is closed,\n\\(\\bar{E} \\subset F\\) for every closed set \\(F \\subset X\\) such that \\(E \\subset F\\).\nBy \\((a)\\) and (c), \\(\\bar{E}\\) is the smallest closed subset of \\(X\\) that contains \\(E\\).\nProof\nIf \\(p \\in X\\) and \\(p \\notin E\\) then \\(p\\) is neither a point of \\(E\\) nor a limit point of \\(E\\). Hence \\(p\\) has a neighborhood which does not intersect \\(E\\). The complement of \\(\\bar{E}\\) is therefore open. Hence \\(E\\) is closed.\nIf \\(E=E\\), (a) implies that \\(E\\) is closed. If \\(E\\) is closed, then \\(E^{\\prime} \\subset E\\)\n[by Definitions \\(2.18(d)\\) and 2.26], hence \\(E=E\\).\nIf \\(F\\) is closed and \\(F \\supset E\\), then \\(F \\supset F^{\\prime}\\), hence \\(F \\supset E^{\\prime}\\). Thus \\(F \\supset E\\). 2.28 Theorem Let \\(E\\) be a nonempty set of real numbers which is bounded above. Let \\(y=\\sup E\\). Then \\(y \\in E\\). Hence \\(y \\in E\\) if \\(E\\) is closed.\nCompare this with the examples in Sec. 1.9.\nProof If \\(y \\in E\\) then \\(y \\in E\\). Assume \\(y \\notin E\\). For every \\(h\u0026gt;0\\) there exists then a point \\(x \\in E\\) such that \\(y-h\u0026lt;x\u0026lt;y\\), for otherwise \\(y-h\\) would be an upper bound of \\(E\\). Thus \\(y\\) is a limit point of \\(E\\). Hence \\(y \\in E\\).\n2.29 Remark Suppose \\(E \\subset Y \\subset X\\), where \\(X\\) is a metric space. To say that \\(E\\) is an open subset of \\(X\\) means that to each point \\(p \\in E\\) there is associated a positive number \\(r\\) such that the conditions \\(d(p, q)\u0026lt;r, q \\in X\\) imply that \\(q \\in E\\). But we have already observed (Sec. 2.16) that \\(Y\\) is also a metric space, so that our definitions may equally well be made within \\(Y\\). To be quite explicit, let us say that \\(E\\) is open relative to \\(Y\\) if to each \\(p \\in E\\) there is associated an \\(r\u0026gt;0\\) such that \\(q \\in E\\) whenever \\(d(p, q)\u0026lt;r\\) and \\(q \\in Y\\). Example \\(2.21(g)\\) showed that a set may be open relative to \\(Y\\) without being an open subset of \\(X\\). However, there is a simple relation between these concepts, which we now state.\n2.30 Theorem Suppose \\(Y \\subset X\\). A subset \\(E\\) of \\(Y\\) is open relative to \\(Y\\) if and only if \\(E=Y \\cap G\\) for some open subset \\(G\\) of \\(X\\).\nProof Suppose \\(E\\) is open relative to \\(Y\\). To each \\(p \\in E\\) there is a positive number \\(r_{p}\\) such that the conditions \\(d(p, q)\u0026lt;r_{p}, q \\in Y\\) imply that \\(q \\in E\\). Let \\(V_{p}\\) be the set of all \\(q \\in X\\) such that \\(d(p, q)\u0026lt;r_{p}\\), and define\n\\[ G=\\bigcup_{p \\in E} V_{p} . \\]\nThen \\(G\\) is an open subset of \\(X\\), by Theorems \\(2.19\\) and \\(2.24\\).\nSince \\(p \\in V_{p}\\) for all \\(p \\in E\\), it is clear that \\(E \\subset G \\cap Y\\).\nBy our choice of \\(V_{p}\\), we have \\(V_{p} \\cap Y \\subset E\\) for every \\(p \\in E\\), so that \\(G \\cap Y \\subset E\\). Thus \\(E=G \\cap Y\\), and one half of the theorem is proved.\nConversely, if \\(G\\) is open in \\(X\\) and \\(E=G \\cap Y\\), every \\(p \\in E\\) has a neighborhood \\(V_{p} \\subset G\\). Then \\(V_{p} \\cap Y \\subset E\\), so that \\(E\\) is open relative to \\(Y\\).\n","date":"2022-08-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch2/2-metric-spaces/","section":"baby rudin","tags":null,"title":"2 METRIC SPACES"},{"categories":null,"contents":" youtube part1 part2 bilibili part1 part2 note part 1 pdf xopp part 2pdf xopp 2.31 Definition By an open cover of a set \\(E\\) in a metric space \\(X\\) we mean a collection \\(\\left\\{G_{\\alpha}\\right\\}\\) of open subsets of \\(X\\) such that \\(E \\subset U_{\\alpha} G_{\\alpha}\\).\n2.32 Definition A subset \\(K\\) of a metric space \\(X\\) is said to be compact if every open cover of \\(K\\) contains a finite subcover.\nMore explicitly, the requirement is that if \\(\\left\\{G_{\\alpha}\\right\\}\\) is an open cover of \\(K\\), then there are finitely many indices \\(\\alpha_{1}, \\ldots, \\alpha_{n}\\) such that\n\\[ K \\subset G_{\\alpha_{1}} \\cup \\cdots \\cup G_{\\alpha_{n}} . \\]\nThe notion of compactness is of great importance in analysis, especially in connection with continuity (Chap. 4).\nIt is clear that every finite set is compact. The existence of a large class of infinite compact sets in \\(R^{k}\\) will follow from Theorem \\(2.41\\).\nWe observed earlier (in Sec. 2.29) that if \\(E \\subset Y \\subset X\\), then \\(E\\) may be open relative to \\(Y\\) without being open relative to \\(X\\). The property of being open thus depends on the space in which \\(E\\) is embedded. The same is true of the property of being closed.\nCompactness, however, behaves better, as we shall now see. To formulate the next theorem, let us say, temporarily, that \\(K\\) is compact relative to \\(X\\) if the requirements of Definition \\(2.32\\) are met.\n2.33 Theorem Suppose \\(K \\subset Y \\subset X\\). Then \\(K\\) is compact relative to \\(X\\) if and only if \\(K\\) is compact relative to \\(Y\\).\nBy virtue of this theorem we are able, in many situations, to regard compact sets as metric spaces in their own right, without paying any attention to any embedding space. In particular, although it makes little sense to talk of open spaces, or of closed spaces (every metric space \\(X\\) is an open subset of itself, and is a closed subset of itself), it does make sense to talk of compact metric spaces.\nProof Suppose \\(K\\) is compact relative to \\(X\\), and let \\(\\left\\{V_{\\alpha}\\right\\}\\) be a collection of sets, open relative to \\(Y\\), such that \\(K \\subset \\bigcup_{\\alpha} V_{\\alpha}\\). By theorem \\(2.30\\), there are sets \\(G_{\\alpha}\\), open relative to \\(X\\), such that \\(V_{\\alpha}=\\mathrm{Y} \\cap G_{\\alpha}\\), for all \\(\\alpha\\); and since \\(K\\) is compact relative to \\(X\\), we have \\[ K \\subset G_{\\alpha_{1}} \\cup \\cdots \\cup G_{\\alpha_{n}} \\]\nfor some choice of finitely many indices \\(\\alpha_{1}, \\ldots, \\alpha_{n}\\). Since \\(K \\subset Y,(22)\\) implies\n\\[ K \\subset V_{\\alpha_{1}} \\cup \\cdots \\cup V_{\\alpha_{n}} . \\]\nThis proves that \\(K\\) is compact relative to \\(Y\\).\nConversely, suppose \\(K\\) is compact relative to \\(Y\\), let \\(\\left\\{G_{\\alpha}\\right\\}\\) be a collection of open subsets of \\(X\\) which covers \\(K\\), and put \\(V_{\\alpha}=Y \\cap G_{\\alpha}\\). Then (23) will hold for some choice of \\(\\alpha_{1}, \\ldots, \\alpha_{n}\\); and since \\(V_{\\alpha} \\subset G_{\\alpha}\\), (23) implies (22).\nThis completes the proof.\n2.34 Theorem Compact subsets of metric spaces are closed.\nProof Let \\(K\\) be a compact subset of a metric space \\(X\\). We shall prove that the complement of \\(K\\) is an open subset of \\(X\\).\nSuppose \\(p \\in X, p \\notin K\\). If \\(q \\in K\\), let \\(V_{q}\\) and \\(W_{q}\\) be neighborhoods of \\(p\\) and \\(q\\), respectively, of radius less than \\(\\frac{1}{2} d(p, q)\\) [see Definition \\(2.18(a)\\) ]. Since \\(K\\) is compact, there are finitely many points \\(q_{1}, \\ldots, q_{n}\\) in \\(K\\) such that\n\\[ K \\subset W_{q_{1}} \\cup \\cdots \\cup W_{q_{n}}=W . \\]\nIf \\(V=V_{q_{1}} \\cap \\cdots \\cap V_{q_{n}}\\), then \\(V\\) is a neighborhood of \\(p\\) which does not intersect \\(W\\). Hence \\(V \\subset K^{c}\\), so that \\(p\\) is an interior point of \\(K^{c}\\). The theorem follows.\n2.35 Theorem Closed subsets of compact sets are compact.\nProof Suppose \\(F \\subset K \\subset X, F\\) is closed (relative to \\(X\\) ), and \\(K\\) is compact. Let \\(\\left\\{V_{\\alpha}\\right\\}\\) be an open cover of \\(F\\). If \\(F^{c}\\) is adjoined to \\(\\left\\{V_{\\alpha}\\right\\}\\), we obtain an open cover \\(\\Omega\\) of \\(K\\). Since \\(K\\) is compact, there is a finite subcollection \\(\\Phi\\) of \\(\\Omega\\) which covers \\(K\\), and hence \\(F\\). If \\(F^{c}\\) is a member of \\(\\Phi\\), we may remove it from \\(\\Phi\\) and still retain an open cover of \\(F\\). We have thus shown that a finite subcollection of \\(\\left\\{V_{\\alpha}\\right\\}\\) covers \\(F\\).\nCorollary If \\(F\\) is closed and \\(K\\) is compact, then \\(F \\cap K\\) is compact.\nProof Theorems \\(2.24(b)\\) and \\(2.34\\) show that \\(F \\cap K\\) is closed; since \\(F \\cap K \\subset K\\), Theorem \\(2.35\\) shows that \\(F \\cap K\\) is compact.\n2.36 Theorem If \\(\\left\\{K_{\\alpha}\\right\\}\\) is a collection of compact subsets of a metric space \\(X\\) such that the intersection of every finite subcollection of \\(\\left\\{K_{\\alpha}\\right\\}\\) is nonempty, then \\(\\cap K_{\\alpha}\\) is nonempty.\nProof Fix a member \\(K_{1}\\) of \\(\\left\\{K_{\\alpha}\\right\\}\\) and put \\(G_{\\alpha}=K_{\\alpha}^{c}\\). Assume that no point of \\(K_{1}\\) belongs to every \\(K_{\\alpha}\\). Then the sets \\(G_{\\alpha}\\) form an open cover of \\(K_{1}\\); and since \\(K_{1}\\) is compact, there are finitely many indices \\(\\alpha_{1}, \\ldots, \\alpha_{n}\\) such that \\(K_{1} \\subset G_{a_{1}} \\cup \\cdots \\cup G_{a_{n}}\\). But this means that\n\\[ K_{1} \\cap K_{\\alpha_{1}} \\cap \\cdots \\cap K_{\\alpha_{n}} \\]\nis empty, in contradiction to our hypothesis.\nCorollary If \\(\\left\\{K_{n}\\right\\}\\) is a sequence of nonempty compact sets such that \\(K_{n} \\supset K_{n+1}\\) \\((n=1,2,3, \\ldots)\\), then \\(\\bigcap_{1}^{\\infty} K_{n}\\) is not empty.\n2.37 Theorem If \\(E\\) is an infinite subset of a compact set \\(K\\), then \\(E\\) has a limit point in \\(K\\).\nProof If no point of \\(K\\) were a limit point of \\(E\\), then each \\(q \\in K\\) would have a neighborhood \\(V_{q}\\) which contains at most one point of \\(E\\) (namely, \\(q\\), if \\(q \\in E\\) ). It is clear that no finite subcollection of \\(\\left\\{V_{q}\\right\\}\\) can cover \\(E\\); and the same is true of \\(K\\), since \\(E \\subset K\\). This contradicts the compactness of \\(K\\).\n2.38 Theorem If \\(\\left\\{I_{n}\\right\\}\\) is a sequence of intervals in \\(R^{1}\\), such that \\(I_{n} \\supset I_{n+1}\\) \\((n=1,2,3, \\ldots)\\), then \\(\\bigcap_{1}^{\\infty} I_{n}\\) is not empty.\nProof If \\(I_{n}=\\left[a_{n}, b_{n}\\right]\\), let \\(E\\) be the set of all \\(a_{n}\\). Then \\(E\\) is nonempty and bounded above (by \\(b_{1}\\) ). Let \\(x\\) be the sup of \\(E\\). If \\(m\\) and \\(n\\) are positive integers, then\n\\[ a_{n} \\leq a_{m+n} \\leq b_{m+n} \\leq b_{m}, \\]\nso that \\(x \\leq b_{m}\\) for each \\(m\\). Since it is obvious that \\(a_{m} \\leq x\\), we see that \\(x \\in I_{m}\\) for \\(m=1,2,3, \\ldots\\)\n2.39 Theorem Let \\(k\\) be a positive integer. If \\(\\left\\{I_{n}\\right\\}\\) is a sequence of \\(k\\)-cells such that \\(I_{n} \\supset I_{n+1}(n=1,2,3, \\ldots)\\), then \\(\\bigcap_{1}^{\\infty} I_{n}\\) is not empty.\nProof Let \\(I_{n}\\) consist of all points \\(\\mathrm{x}=\\left(x_{1}, \\ldots, x_{k}\\right)\\) such that\n\\[ a_{n, j} \\leq x_{j} \\leq b_{n, j} \\quad(1 \\leq j \\leq k ; n=1,2,3, \\ldots) \\text {, } \\]\nand put \\(I_{n, j}=\\left[a_{n, j}, b_{n, j}\\right]\\). For each \\(j\\), the sequence \\(\\left\\{I_{n, j}\\right\\}\\) satisfies the hypotheses of Theorem \\(2.38\\). Hence there are real numbers \\(x_{j}^{*}(1 \\leq j \\leq k)\\) such that\n\\[ a_{n, j} \\leq x_{j}^{*} \\leq b_{n, j} \\quad(1 \\leq j \\leq k ; n=1,2,3, \\ldots) \\text {. } \\]\nSetting \\(\\mathbf{x}^{*}=\\left(x_{1}^{*}, \\ldots, x_{k}^{*}\\right)\\), we see that \\(\\mathrm{x}^{*} \\in I_{n}\\) for \\(n=1,2,3, \\ldots\\) The theorem follows.\n2.40 Theorem Every \\(k\\)-cell is compact.\nProof Let \\(I\\) be a \\(k\\)-cell, consisting of all points \\(\\mathrm{x}=\\left(x_{1}, \\ldots, x_{k}\\right)\\) such that \\(a_{j} \\leq x_{j} \\leq b_{j}(1 \\leq j \\leq k)\\). Put\n\\[ \\delta=\\left\\{\\sum_{1}^{k}\\left(b_{j}-a_{j}\\right)^{2}\\right\\}^{1 / 2} . \\]\nThen \\(|\\mathbf{x}-\\mathbf{y}| \\leq \\delta\\), if \\(\\mathbf{x} \\in I, \\mathbf{y} \\in I\\).\nSuppose, to get a contradiction, that there exists an open cover \\(\\left\\{G_{\\alpha}\\right\\}\\) of \\(I\\) which contains no finite subcover of \\(I\\). Put \\(c_{j}=\\left(a_{j}+b_{j}\\right) / 2\\). The intervals \\(\\left[a_{j}, c_{j}\\right]\\) and \\(\\left[c_{j}, b_{j}\\right]\\) then determine \\(2^{k} k\\)-cells \\(Q_{i}\\) whose union is \\(I\\). At least one of these sets \\(Q_{i}\\), call it \\(I_{1}\\), cannot be covered by any finite subcollection of \\(\\left\\{G_{\\alpha}\\right\\}\\) (otherwise \\(I\\) could be so covered). We next subdivide \\(I_{1}\\) and continue the process. We obtain a sequence \\(\\left\\{I_{n}\\right\\}\\) with the following properties:\n\\(I \\supset I_{1} \\supset I_{2} \\supset I_{3} \\supset \\cdots\\);\n\\(I_{n}\\) is not covered by any finite subcollection of \\(\\left\\{G_{\\alpha}\\right\\}\\);\nif \\(\\mathbf{x} \\in I_{n}\\) and \\(\\mathbf{y} \\in I_{n}\\), then \\(|\\mathbf{x}-\\mathbf{y}| \\leq 2^{-n} \\delta\\).\nBy \\((a)\\) and Theorem \\(2.39\\), there is a point \\(\\mathrm{x}^{*}\\) which lies in every \\(I_{n}\\). For some \\(\\alpha, \\mathbf{x}^{*} \\in G_{\\alpha}\\). Since \\(G_{\\alpha}\\) is open, there exists \\(r\u0026gt;0\\) such that \\(\\left|\\mathbf{y}-\\mathbf{x}^{*}\\right|\u0026lt;r\\) implies that \\(\\mathbf{y} \\in G_{\\alpha}\\). If \\(n\\) is so large that \\(2^{-n} \\delta\u0026lt;r\\) (there is such an \\(n\\), for otherwise \\(2^{n} \\leq \\delta / r\\) for all positive integers \\(n\\), which is absurd since \\(R\\) is archimedean), then (c) implies that \\(I_{n} \\subset G_{\\alpha}\\), which contradicts \\((b)\\).\nThis completes the proof.\nThe equivalence of \\((a)\\) and \\((b)\\) in the next theorem is known as the HeineBorel theorem.\n2.41 Theorem If a set \\(E\\) in \\(R^{k}\\) has one of the following three properties, then it has the other two:\nE is closed and bounded.\nE is compact.\nEvery infinite subset of \\(E\\) has a limit point in \\(E\\).\nProof If (a) holds, then \\(E \\subset I\\) for some \\(k\\)-cell \\(I\\), and \\((b)\\) follows from Theorems \\(2.40\\) and 2.35. Theorem \\(2.37\\) shows that \\((b)\\) implies \\((c)\\). It remains to be shown that \\((c)\\) implies \\((a)\\).\nIf \\(E\\) is not bounded, then \\(E\\) contains points \\(\\mathbf{x}_{n}\\) with\n\\[ \\left|\\mathbf{x}_{n}\\right|\u0026gt;n \\quad(n=1,2,3, \\ldots) \\text {. } \\]\nThe set \\(S\\) consisting of these points \\(\\mathbf{x}_{n}\\) is infinite and clearly has no limit point in \\(R^{k}\\), hence has none in \\(E\\). Thus (c) implies that \\(E\\) is bounded.\nIf \\(E\\) is not closed, then there is a point \\(\\mathbf{x}_{0} \\in R^{k}\\) which is a limit point of \\(E\\) but not a point of \\(E\\). For \\(n=1,2,3, \\ldots\\), there are points \\(\\mathbf{x}_{n} \\in E\\) such that \\(\\left|\\mathbf{x}_{n}-\\mathbf{x}_{0}\\right|\u0026lt;1 / n\\). Let \\(S\\) be the set of these points \\(\\mathbf{x}_{n}\\). Then \\(S\\) is infinite (otherwise \\(\\left|\\mathbf{x}_{n}-\\mathbf{x}_{0}\\right|\\) would have a constant positive value, for infinitely many \\(n\\) ), \\(S\\) has \\(\\mathbf{x}_{0}\\) as a limit point, and \\(S\\) has no other limit point in \\(R^{k}\\). For if \\(\\mathbf{y} \\in R^{k}, \\mathbf{y} \\neq \\mathbf{x}_{0}\\), then\n\\[ \\begin{aligned} \\left|\\mathbf{x}_{n}-\\mathbf{y}\\right| \u0026amp; \\geq\\left|\\mathbf{x}_{0}-\\mathbf{y}\\right|-\\left|\\mathbf{x}_{n}-\\mathbf{x}_{0}\\right| \\\\ \u0026amp; \\geq\\left|\\mathbf{x}_{0}-\\mathbf{y}\\right|-\\frac{1}{n} \\geq \\frac{1}{2}\\left|\\mathbf{x}_{0}-\\mathbf{y}\\right| \\end{aligned} \\]\nfor all but finitely many \\(n\\); this shows that \\(y\\) is not a limit point of \\(S\\) (Theorem 2.20).\nThus \\(S\\) has no limit point in \\(E\\); hence \\(E\\) must be closed if \\((c)\\) holds.\nWe should remark, at this point, that \\((b)\\) and \\((c)\\) are equivalent in any metric space (Exercise 26) but that \\((a)\\) does not, in general, imply \\((b)\\) and \\((c)\\). Examples are furnished by Exercise 16 and by the space \\(\\mathscr{L}^{2}\\), which is discussed in Chap. \\(11 .\\)\n2.42 Theorem (Weierstrass) Every bounded infinite subset of \\(R^{k}\\) has a limit point in \\(R^{k}\\).\nProof Being bounded, the set \\(E\\) in question is a subset of a \\(k\\)-cell \\(I \\subset R^{k}\\). By Theorem \\(2.40, I\\) is compact, and so \\(E\\) has a limit point in \\(I\\), by Theorem 2.37.\n","date":"2022-08-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch2/3-compact-sets/","section":"baby rudin","tags":null,"title":"3 COMPACT SETS"},{"categories":null,"contents":"Theorem \\(1.19\\) will be proved in this appendix by constructing \\(R\\) from \\(Q\\). We shall divide the construction into several steps.\nStep 1 The members of \\(R\\) will be certain subsets of \\(Q\\), called cuts. A cut is, by definition, any set \\(\\alpha \\subset Q\\) with the following three properties.\n\\(\\alpha\\) is not empty, and \\(\\alpha \\neq Q\\).\nIf \\(p \\in \\alpha, q \\in Q\\), and \\(q\u0026lt;p\\), then \\(q \\in \\alpha\\).\nIf \\(p \\in \\alpha\\), then \\(p\u0026lt;r\\) for some \\(r \\in \\alpha\\).\nThe letters \\(p, q, r, \\ldots\\) will always denote rational numbers, and \\(\\alpha, \\beta, \\gamma, \\ldots\\) will denote cuts.\nNote that (III) simply says that \\(\\alpha\\) has no largest member; (II) implies two facts which will be used freely:\nIf \\(p \\in \\alpha\\) and \\(q \\notin \\alpha\\) then \\(p\u0026lt;q\\).\nIf \\(r \\notin \\alpha\\) and \\(r\u0026lt;s\\) then \\(s \\notin \\alpha\\).\nStep 2 Define ” \\(\\alpha\u0026lt;\\beta\\) ” to mean: \\(\\alpha\\) is a proper subset of \\(\\beta\\).\nLet us check that this meets the requirements of Definition 1.5.\nIf \\(\\alpha\u0026lt;\\beta\\) and \\(\\beta\u0026lt;\\gamma\\) it is clear that \\(\\alpha\u0026lt;\\gamma\\). (A proper subset of a proper subset is a proper subset.) It is also clear that at most one of the three relations\n\\[ \\alpha\u0026lt;\\beta, \\quad \\alpha=\\beta, \\quad \\beta\u0026lt;\\alpha \\]\ncan hold for any pair \\(\\alpha, \\beta\\). To show that at least one holds, assume that the first two fail. Then \\(\\alpha\\) is not a subset of \\(\\beta\\). Hence there is a \\(p \\in \\alpha\\) with \\(p \\notin \\beta\\). If \\(q \\in \\beta\\), it follows that \\(q\u0026lt;p\\) (since \\(p \\notin \\beta\\) ), hence \\(q \\in \\alpha\\), by (II). Thus \\(\\beta \\subset \\alpha\\). Since \\(\\beta \\neq \\alpha\\), we conclude: \\(\\beta\u0026lt;\\alpha\\).\nThus \\(R\\) is now an ordered set.\nStep 3 The ordered set \\(R\\) has the least-upper-bound property.\nTo prove this, let \\(A\\) be a nonempty subset of \\(R\\), and assume that \\(\\beta \\in R\\) is an upper bound of \\(A\\). Define \\(\\gamma\\) to be the union of all \\(\\alpha \\in A\\). In other words, \\(p \\in \\gamma\\) if and only if \\(p \\in \\alpha\\) for some \\(\\alpha \\in A\\). We shall prove that \\(\\gamma \\in R\\) and that \\(\\gamma=\\sup A\\)\nSince \\(A\\) is not empty, there exists an \\(\\alpha_{0} \\in A\\). This \\(\\alpha_{0}\\) is not empty. Since \\(\\alpha_{0} \\subset \\gamma, \\gamma\\) is not empty. Next, \\(\\gamma \\subset \\beta\\) (since \\(\\alpha \\subset \\beta\\) for every \\(\\alpha \\in A\\) ), and therefore \\(\\gamma \\neq Q\\). Thus \\(\\gamma\\) satisfies property (I). To prove (II) and (III), pick \\(p \\in \\gamma\\). Then \\(p \\in \\alpha_{1}\\) for some \\(\\alpha_{1} \\in A\\). If \\(q\u0026lt;p\\), then \\(q \\in \\alpha_{1}\\), hence \\(q \\in \\gamma\\); this proves (II). If \\(r \\in \\alpha_{1}\\) is so chosen that \\(r\u0026gt;p\\), we see that \\(r \\in \\gamma\\) (since \\(\\left.\\alpha_{1} \\subset \\gamma\\right)\\), and therefore \\(\\gamma\\) satisfies (III).\nThus \\(\\gamma \\in R\\).\nIt is clear that \\(\\alpha \\leq \\gamma\\) for every \\(\\alpha \\in A\\).\nSuppose \\(\\delta\u0026lt;\\gamma\\). Then there is an \\(s \\in \\gamma\\) and that \\(s \\notin \\delta\\). Since \\(s \\in \\gamma, s \\in \\alpha\\) for some \\(\\alpha \\in A\\). Hence \\(\\delta\u0026lt;\\alpha\\), and \\(\\delta\\) is not an upper bound of \\(A\\).\nThis gives the desired result: \\(\\gamma=\\sup A\\).\nStep 4 If \\(\\alpha \\in R\\) and \\(\\beta \\in R\\) we define \\(\\alpha+\\beta\\) to be the set of all sums \\(r+s\\), where \\(r \\in \\alpha\\) and \\(s \\in \\beta\\).\nWe define \\(0^{*}\\) to be the set of all negative rational numbers. It is clear that \\(0^{*}\\) is a cut. We verify that the axioms for addition (see Definition 1.12) hold in \\(R\\), with \\(0^{*}\\) playing the role of 0 .\n(A1) We have to show that \\(\\alpha+\\beta\\) is a cut. It is clear that \\(\\alpha+\\beta\\) is a nonempty subset of \\(Q\\). Take \\(r^{\\prime} \\notin \\alpha, s^{\\prime} \\notin \\beta\\). Then \\(r^{\\prime}+s^{\\prime}\u0026gt;r+s\\) for all choices of \\(r \\in \\alpha, s \\in \\beta\\). Thus \\(r^{\\prime}+s^{\\prime} \\notin \\alpha+\\beta\\). It follows that \\(\\alpha+\\beta\\) has property (I).\nPick \\(p \\in \\alpha+\\beta\\). Then \\(p=r+s\\), with \\(r \\in \\alpha, s \\in \\beta\\). If \\(q\u0026lt;p\\), then \\(q-s\u0026lt;r\\), so \\(q-s \\in \\alpha\\), and \\(q=(q-s)+s \\in \\alpha+\\beta\\). Thus (II) holds. Choose \\(t \\in \\alpha\\) so that \\(t\u0026gt;r\\). Then \\(p\u0026lt;t+s\\) and \\(t+s \\in \\alpha+\\beta\\). Thus (III) holds.\n(A2) \\(\\alpha+\\beta\\) is the set of all \\(r+s\\), with \\(r \\in \\alpha, s \\in \\beta\\). By the same definition, \\(\\beta+\\alpha\\) is the set of all \\(s+r\\). Since \\(r+s=s+r\\) for all \\(r \\in Q, s \\in Q\\), we have \\(\\alpha+\\beta=\\beta+\\alpha\\).\n(A3) As above, this follows from the associative law in \\(Q\\).\n(A4) If \\(r \\in \\alpha\\) and \\(s \\in 0^{*}\\), then \\(r+s\u0026lt;r\\), hence \\(r+s \\in \\alpha\\). Thus \\(\\alpha+0^{*} \\subset \\alpha\\).\nTo obtain the opposite inclusion, pick \\(p \\in \\alpha\\), and pick \\(r \\in \\alpha, r\u0026gt;p\\). Then \\(p-r \\in 0^{*}\\), and \\(p=r+(p-r) \\in \\alpha+0^{*}\\). Thus \\(\\alpha \\subset \\alpha+0^{*}\\). We conclude that \\(\\alpha+0^{*}=\\alpha\\).\n(A5) Fix \\(\\alpha \\in R\\). Let \\(\\beta\\) be the set of all \\(p\\) with the following property:\nThere exists \\(r\u0026gt;0\\) such that \\(-p-r \\notin \\alpha\\).\nIn other words, some rational number smaller than \\(-p\\) fails to be in \\(\\alpha\\).\nWe show that \\(\\beta \\in R\\) and that \\(\\alpha+\\beta=0^{*}\\).\nIf \\(s \\notin \\alpha\\) and \\(p=-s-1\\), then \\(-p-1 \\notin \\alpha\\), hence \\(p \\in \\beta\\). So \\(\\beta\\) is not empty. If \\(q \\in \\alpha\\), then \\(-q \\notin \\beta\\). So \\(\\beta \\neq Q\\). Hence \\(\\beta\\) satisfies (I).\nPick \\(p \\in \\beta\\), and pick \\(r\u0026gt;0\\), so that \\(-p-r \\notin \\alpha\\). If \\(q\u0026lt;p\\), then \\(-q-r\u0026gt;-p-r\\), hence \\(-q-r \\notin \\alpha\\). Thus \\(q \\in \\beta\\), and (II) holds. Put \\(t=p+(r / 2)\\). Then \\(t\u0026gt;p\\), and \\(-t-(r / 2)=-p-r \\notin \\alpha\\), so that \\(t \\in \\beta\\). Hence \\(\\beta\\) satisfies (III).\nWe have proved that \\(\\beta \\in R\\).\nIf \\(r \\in \\alpha\\) and \\(s \\in \\beta\\), then \\(-s \\notin \\alpha\\), hence \\(r\u0026lt;-s, r+s\u0026lt;0\\). Thus \\(\\alpha+\\beta \\subset 0^{*}\\)\nTo prove the opposite inclusion, pick \\(v \\in 0^{*}\\), put \\(w=-v / 2\\). Then \\(w\u0026gt;0\\), and there is an integer \\(n\\) such that \\(n w \\in \\alpha\\) but \\((n+1) w \\notin \\alpha\\). (Note that this depends on the fact that \\(Q\\) has the archimedean property!) Put \\(p=-(n+2) w\\). Then \\(p \\in \\beta\\), since \\(-p-w \\notin \\alpha\\), and\n\\[ v=n w+p \\in \\alpha+\\beta \\text {. } \\]\nThus \\(0^{*} \\subset \\alpha+\\beta\\).\nWe conclude that \\(\\alpha+\\beta=0^{*}\\).\nThis \\(\\beta\\) will of course be denoted by \\(-\\alpha\\).\nStep 5 Having proved that the addition defined in Step 4 satisfies Axioms (A) of Definition 1.12, it follows that Proposition \\(1.14\\) is valid in \\(R\\), and we can prove one of the requirements of Definition 1.17:\nIf \\(\\alpha, \\beta, \\gamma \\in R\\) and \\(\\beta\u0026lt;\\gamma\\), then \\(\\alpha+\\beta\u0026lt;\\alpha+\\gamma\\).\nIndeed, it is obvious from the definition of \\(+\\) in \\(R\\) that \\(\\alpha+\\beta \\subset \\alpha+\\gamma\\); if we had \\(\\alpha+\\beta=\\alpha+\\gamma\\), the cancellation law (Proposition 1.14) would imply \\(\\beta=\\gamma\\)\nIt also follows that \\(\\alpha\u0026gt;0^{*}\\) if and only if \\(-\\alpha\u0026lt;0^{*}\\).\nStep 6 Multiplication is a little more bothersome than addition in the present context, since products of negative rationals are positive. For this reason we confine ourselves first to \\(R^{+}\\), the set of all \\(\\alpha \\in R\\) with \\(\\alpha\u0026gt;0^{*}\\).\nIf \\(\\alpha \\in R^{+}\\)and \\(\\beta \\in R^{+}\\), we define \\(\\alpha \\beta\\) to be the set of all \\(p\\) such that \\(p \\leq r s\\) for some choice of \\(r \\in \\alpha, s \\in \\beta, r\u0026gt;0, s\u0026gt;0\\).\nWe define \\(1^{*}\\) to be the set of all \\(q\u0026lt;1\\). Then the axioms \\((M)\\) and \\((D)\\) of Definition \\(1.12\\) hold, with \\(R^{+}\\)in place of \\(F\\), and with \\(1^{*}\\) in the role of 1 .\nThe proofs are so similar to the ones given in detail in Step 4 that we omit them.\nNote, in particular, that the second requirement of Definition \\(1.17\\) holds: If \\(\\alpha\u0026gt;0^{*}\\) and \\(\\beta\u0026gt;0 *\\) then \\(\\alpha \\beta\u0026gt;0^{*}\\).\nStep 7 We complete the definition of multiplication by setting \\(\\alpha 0^{*}=0^{*} \\alpha=0^{*}\\), and by setting\n\\[ \\alpha \\beta= \\begin{cases}(-\\alpha)(-\\beta) \u0026amp; \\text { if } \\alpha\u0026lt;0^{*}, \\beta\u0026lt;0^{*}, \\\\ -[(-\\alpha) \\beta] \u0026amp; \\text { if } \\alpha\u0026lt;0^{*}, \\beta\u0026gt;0^{*}, \\\\ -[\\alpha \\cdot(-\\beta)] \u0026amp; \\text { if } \\alpha\u0026gt;0^{*}, \\beta\u0026lt;0^{*}\\end{cases} \\]\nThe products on the right were defined in Step \\(6 .\\)\nHaving proved (in Step 6) that the axioms (M) hold in \\(R^{+}\\), it is now perfectly simple to prove them in \\(R\\), by repeated application of the identity \\(\\gamma=-(-\\gamma)\\) which is part of Proposition 1.14. (See Step 5.)\nThe proof of the distributive law\n\\[ \\alpha(\\beta+\\gamma)=\\alpha \\beta+\\alpha \\gamma \\]\nbreaks into cases. For instance, suppose \\(\\alpha\u0026gt;0^{*}, \\beta\u0026lt;0^{*}, \\beta+\\gamma\u0026gt;0^{*}\\). Then \\(\\gamma=(\\beta+\\gamma)+(-\\beta)\\), and (since we already know that the distributive law holds in \\(\\left.R^{+}\\right)\\)\n\\[ \\alpha \\gamma=\\alpha(\\beta+\\gamma)+\\alpha \\cdot(-\\beta) . \\]\nBut \\(\\alpha \\cdot(-\\beta)=-(\\alpha \\beta)\\). Thus\n\\[ \\alpha \\beta+\\alpha \\gamma=\\alpha(\\beta+\\gamma) . \\]\nThe other cases are handled in the same way.\nWe have now completed the proof that \\(R\\) is an ordered field with the leastupper-bound property.\nStep 8 We associate with each \\(r \\in Q\\) the set \\(r^{*}\\) which consists of all \\(p \\in Q\\) such that \\(p\u0026lt;r\\). It is clear that each \\(r^{*}\\) is a cut; that is, \\(r^{*} \\in R\\). These cuts satisfy the following relations:\n\\(r^{*}+s^{*}=(r+s)^{*}\\),\n\\(r^{*} s^{*}=(r s)^{*}\\)\n\\(r^{*}\u0026lt;s^{*}\\) if and only if \\(r\u0026lt;s\\).\nTo prove \\((a)\\), choose \\(p \\in r^{*}+s^{*}\\). Then \\(p=u+v\\), where \\(u\u0026lt;r, v\u0026lt;s\\). Hence \\(p\u0026lt;r+s\\), which says that \\(p \\in(r+s)^{*}\\). Conversely, suppose \\(p \\in(r+s)^{*}\\). Then \\(p\u0026lt;r+s\\). Choose \\(t\\) so that \\(2 t=r+s-p\\), put\n\\[ r^{\\prime}=r-t, s^{\\prime}=s-t . \\]\nThen \\(r^{\\prime} \\in r^{*}, s^{\\prime} \\in s^{*}\\), and \\(p=r^{\\prime}+s^{\\prime}\\), so that \\(p \\in r^{*}+s^{*}\\).\nThis proves \\((a)\\). The proof of \\((b)\\) is similar.\nIf \\(r\u0026lt;s\\) then \\(r \\in s^{*}\\), but \\(r \\notin r^{*}\\); hence \\(r^{*}\u0026lt;s^{*}\\).\nIf \\(r^{*}\u0026lt;s^{*}\\), then there is a \\(p \\in s^{*}\\) such that \\(p \\notin r^{*}\\). Hence \\(r \\leq p\u0026lt;s\\), so that \\(r\u0026lt;s\\).\nThis proves \\((c)\\)\nStep 9 We saw in Step 8 that the replacement of the rational numbers \\(r\\) by the corresponding “rational cuts” \\(r * \\in R\\) preserves sums, products, and order. This fact may be expressed by saying that the ordered field \\(Q\\) is isomorphic to the ordered field \\(Q^{*}\\) whose elements are the rational cuts. Of course, \\(r^{*}\\) is by no means the same as \\(r\\), but the properties we are concerned with (arithmetic and order) are the same in the two fields.\nIt is this identification of \\(Q\\) with \\(Q^{*}\\) which allows us to regard \\(Q\\) as a subfield of \\(R\\).\nThe second part of Theorem \\(1.19\\) is to be understood in terms of this identification. Note that the same phenomenon occurs when the real numbers are regarded as a subfield of the complex field, and it also occurs at a much more elementary level, when the integers are identified with a certain subset of \\(Q\\).\nIt is a fact, which we will not prove here, that any two ordered fields with the least-upper-bound property are isomorphic. The first part of Theorem \\(1.19\\) therefore characterizes the real field \\(R\\) completely.\nThe books by Landau and Thurston cited in the Bibliography are entirely devoted to number systems. Chapter 1 of Knopp’s book contains a more leisurely description of how \\(R\\) can be obtained from \\(Q\\). Another construction, in which each real number is defined to be an equivalence class of Cauchy sequences of rational numbers (see Chap. 3), is carried out in Sec. 5 of the book by Hewitt and Stromberg.\nThe cuts in \\(Q\\) which we used here were invented by Dedekind. The construction of \\(R\\) from \\(Q\\) by means of Cauchy sequences is due to Cantor. Both Cantor and Dedekind published their constructions in \\(1872 .\\)\n","date":"2022-08-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/7-appendix/","section":"baby rudin","tags":null,"title":"7 APPENDIX"},{"categories":null,"contents":"Unless the contrary is explicitly stated, all numbers that are mentioned in these exercises are understood to be real.\nIf \\(r\\) is rational \\((r \\neq 0)\\) and \\(x\\) is irrational, prove that \\(r+x\\) and \\(r x\\) are irrational. ","date":"2022-08-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/8-exercises/","section":"baby rudin","tags":null,"title":"EXERCISES"},{"categories":null,"contents":"If \\(A\\) and \\(B\\) are any two sets, we write \\(A-B\\) for the set of all elements \\(x\\) such that \\(x \\in A, x \\notin B\\). The notation \\(A-B\\) does not imply that \\(B \\subset A\\). We denote the empty set by 0 , and say that \\(A\\) and \\(B\\) are disjoint if \\(A \\cap B=0\\).\n11.1 Definition A family \\(\\mathscr{R}\\) of sets is called a ring if \\(A \\in \\mathscr{R}\\) and \\(B \\in \\mathscr{R}\\) implies}\n\\[ \\begin{equation} A \\cup B \\in \\mathscr{R}, \\quad A-B \\in \\mathscr{R} . \\end{equation} \\]\nSince \\(A \\cap B=A-(A-B)\\), we also have \\(A \\cap B \\in \\mathscr{R}\\) if \\(\\mathscr{R}\\) is a ring. A ring \\(\\mathscr{R}\\) is called a \\(\\sigma\\)-ring if\n\\[ \\begin{equation} \\bigcup_{n=1}^{\\infty} A_{n} \\in \\mathscr{R} \\end{equation} \\]\nwhenever \\(A_{n} \\in \\mathscr{R}(n=1,2,3, \\ldots)\\). Since\n\\[ \\bigcap_{n=1}^{\\infty} A_{n}=A_{1}-\\bigcup_{n=1}^{\\infty}\\left(A_{1}-A_{n}\\right), \\]\nwe also have\n\\[ \\bigcap_{n=1}^{\\infty} A_{n} \\in \\mathscr{R} \\]\nif \\(\\mathscr{R}\\) is a \\(\\sigma\\)-ring.\n11.2 Definition We say that \\(\\phi\\) is a set function defined on \\(\\mathscr{R}\\) if \\(\\phi\\) assigns to every \\(A \\in \\mathscr{R}\\) a number \\(\\phi(A)\\) of the extended real number system. \\(\\phi\\) is additive if \\(A \\cap B=0\\) implies\n\\[ \\phi(A \\cup B)=\\phi(A)+\\phi(B), \\]\nand \\(\\phi\\) is countably additive if \\(A_{i} \\cap A_{j}=0(i \\neq j)\\) implies\n\\[ \\phi\\left(\\bigcup_{n=1}^{\\infty} A_{n}\\right)=\\sum_{n=1}^{\\infty} \\phi\\left(A_{n}\\right) . \\]\nWe shall always assume that the range of \\(\\phi\\) does not contain both \\(+\\infty\\) and \\(-\\infty\\); for if it did, the right side of (3) could become meaningless. Also, we exclude set functions whose only value is \\(+\\infty\\) or \\(-\\infty\\).\nIt is interesting to note that the left side of (4) is independent of the order in which the \\(A_{n}\\) ’s are arranged. Hence the rearrangement theorem shows that the right side of (4) converges absolutely if it converges at all; if it does not converge, the partial sums tend to \\(+\\infty\\), or to \\(-\\infty\\).\nIf \\(\\phi\\) is additive, the following properties are easily verified:\n\\[ \\begin{aligned} \\phi(0) \u0026amp;=0 . \\\\ \\phi\\left(A_{1} \\cup \\cdots \\cup A_{n}\\right) \u0026amp;=\\phi\\left(A_{1}\\right)+\\cdots+\\phi\\left(A_{n}\\right) \\end{aligned} \\]\nif \\(A_{i} \\cap A_{j}=0\\) whenever \\(i \\neq j\\). (7)\n\\[ \\phi\\left(A_{1} \\cup A_{2}\\right)+\\phi\\left(A_{1} \\cap A_{2}\\right)=\\phi\\left(A_{1}\\right)+\\phi\\left(A_{2}\\right) . \\]\nIf \\(\\phi(A) \\geq 0\\) for all \\(A\\), and \\(A_{1} \\subset A_{2}\\), then\n\\[ \\phi\\left(A_{1}\\right) \\leq \\phi\\left(A_{2}\\right) . \\]\nBecause of (8), nonnegative additive set functions are often called monotonic.\n\\[ \\phi(A-B)=\\phi(A)-\\phi(B) \\]\nif \\(B \\subset A\\), and \\(|(\\phi B)|\u0026lt;+\\infty\\).\n11.3 Theorem Suppose \\(\\phi\\) is countably additive on a ring \\(\\mathscr{R}\\). Suppose \\(A_{n} \\in \\mathscr{R}\\) \\((n=1,2,3, \\ldots), A_{1} \\subset A_{2} \\subset A_{3} \\subset \\cdots, A \\in \\mathscr{R}\\), and \\[ A=\\bigcup_{n=1}^{\\infty} A_{n} . \\]\nThen, as \\(n \\rightarrow \\infty\\),\n\\[ \\phi\\left(A_{n}\\right) \\rightarrow \\phi(A) . \\]\nProof Put \\(B_{1}=A_{1}\\), and \\[ B_{n}=A_{n}-A_{n-1} \\quad(n=2,3, \\ldots) . \\]\nThen \\(B_{i} \\cap B_{j}=0\\) for \\(i \\neq j, A_{n}=B_{1} \\cup \\cdots \\cup B_{n}\\), and \\(A=\\cup B_{n}\\). Hence\n\\[ \\phi\\left(A_{n}\\right)=\\sum_{i=1}^{n} \\phi\\left(B_{i}\\right) \\]\nand\n\\[ \\phi(A)=\\sum_{i=1}^{\\infty} \\phi\\left(B_{i}\\right) \\]\n","date":"2022-08-18T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/11-the-lebesgue-theory/1-set-functions/","section":"baby rudin","tags":null,"title":"1 SET FUNCTIONS"},{"categories":null,"contents":"It is the purpose of this chapter to present the fundamental concepts of the Lebesgue theory of measure and integration and to prove some of the crucial theorems in a rather general setting, without obscuring the main lines of the development by a mass of comparatively trivial detail. Therefore proofs are only sketched in some cases, and some of the easier propositions are stated without proof. However, the reader who has become familiar with the techniques used in the preceding chapters will certainly find no difficulty in supplying the missing steps.\nThe theory of the Lebesgue integral can be developed in several distinct ways. Only one of these methods will be discussed here. For alternative procedures we refer to the more specialized treatises on integration listed in the Bibliography.\n","date":"2022-08-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/11-the-lebesgue-theory/0-intro/","section":"baby rudin","tags":null,"title":"0 INTRO"},{"categories":null,"contents":" Let \\(H\\) be a compact convex set in \\(R^{k}\\), with nonempty interior. Let \\(f \\in \\mathscr{C}(H)\\), put \\(f(\\mathbf{x})=0\\) in the complement of \\(H\\), and define \\(\\int_{H} f\\) as in Definition 10.3. Prove that \\(\\int_{H} f\\) is independent of the order in which the \\(k\\) integrations are carried out.\nHint: Approximate \\(f\\) by functions that are continuous on \\(R^{k}\\) and whose supports are in \\(H\\), as was done in Example 10.4.\nFor \\(i=1,2,3, \\ldots\\), let \\(\\varphi_{\\iota} \\in \\mathscr{C}\\left(R^{1}\\right)\\) have support in \\(\\left(2^{-1}, 2^{1-1}\\right)\\), such that \\(\\int \\varphi_{l}=1\\). Put \\[ f(x, y)=\\sum_{i=1}^{\\infty}\\left[\\varphi_{l}(x)-\\varphi_{i+1}(x)\\right] \\varphi_{l}(y) \\]\nThen \\(f\\) has compact support in \\(R^{2}, f\\) is continuous except at \\((0,0)\\), and\n\\[ \\int d y \\int f(x, y) d x=0 \\quad \\text { but } \\quad \\int d x \\int f(x, y) d y=1 . \\]\nObserve that \\(f\\) is unbounded in every neighborhood of \\((0,0)\\). 3. (a) If \\(F\\) is as in Theorem \\(10.7\\), put \\(\\mathbf{A}=\\mathbf{F}^{\\prime}(0), \\mathbf{F}_{1}(\\mathbf{x})=\\mathbf{A}^{-1} \\mathbf{F}(\\mathbf{x})\\). Then \\(\\mathbf{F}_{1}^{\\prime}(0)=I\\). Show that\n\\[ \\mathbf{F}_{1}(\\mathbf{x})=\\mathbf{G}_{n} \\circ \\mathbf{G}_{n-1} \\circ \\cdots \\circ \\mathbf{G}_{1}(\\mathbf{x}) \\]\nin some neighborhood of \\(\\mathbf{0}\\), for certain primitive mappings \\(\\mathbf{G}_{1}, \\ldots, \\mathbf{G}_{n}\\). This gives another version of Theorem 10.7:\n\\[ \\mathbf{F}(\\mathbf{x})=\\mathbf{F}^{\\prime}(\\mathbf{0}) \\mathbf{G}_{n} \\circ \\mathbf{G}_{n-1} \\circ \\cdots \\circ \\mathbf{G}_{1}(\\mathbf{x}) . \\]\nProve that the mapping \\((x, y) \\rightarrow(y, x)\\) of \\(R^{2}\\) onto \\(R^{2}\\) is not the composition of any two primitive mappings, in any neighborhood of the origin. (This shows that the flips \\(B_{1}\\) cannot be omitted from the statement of Theorem 10.7.) For \\((x, y) \\in R^{2}\\), define \\[ \\mathbf{F}(x, y)=\\left(e^{x} \\cos y-1, e^{x} \\sin y\\right) . \\]\nProve that \\(\\mathbf{F}=\\mathbf{G}_{2} \\circ \\mathbf{G}_{1}\\), where\n\\[ \\begin{aligned} \u0026amp;\\mathbf{G}_{1}(x, y)=\\left(e^{x} \\cos y-1, y\\right) \\\\ \u0026amp;\\mathbf{G}_{2}(u, v)=(u,(1+u) \\tan v) \\end{aligned} \\]\nare primitive in some neighborhood of \\((0,0)\\).\nCompute the Jacobians of \\(\\mathbf{G}_{1}, \\mathbf{G}_{2}, \\mathbf{F}\\) at \\((0,0)\\). Define\n\\[ \\mathbf{H}_{2}(x, y)=\\left(x, e^{x} \\sin y\\right) \\]\nand find\n\\[ \\mathbf{H}_{1}(u, v)=(h(u, v), v) \\]\nso that \\(\\mathbf{F}=\\mathbf{H}_{1} \\circ \\mathbf{H}_{2}\\) is some neighborhood of \\((0,0)\\).\nFormulate and prove an analogue of Theorem 10.8, in which \\(K\\) is a compact subset of an arbitrary metric space. (Replace the functions \\(\\varphi_{\\iota}\\) that occur in the proof of Theorem \\(10.8\\) by functions of the type constructed in Exercise 22 of Chap. 4.)\nStrengthen the conclusion of Theorem \\(10.8\\) by showing that the functions \\(\\psi_{t}\\) can be made differentiable, and even infinitely differentiable. (Use Exercise 1 of Chap. 8 in the construction of the auxiliary functions \\(\\varphi_{l}\\).)\nShow that the simplex \\(Q^{k}\\) is the smallest convex subset of \\(R^{k}\\) that contains \\(\\mathbf{0}, \\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\). Show that affine mappings take convex sets to convex sets. Let \\(H\\) be the parallelogram in \\(R^{2}\\) whose vertices are \\((1,1),(3,2),(4,5),(2,4)\\). Find the affine map \\(T\\) which sends \\((0,0)\\) to \\((1,1),(1,0)\\) to \\((3,2),(0,1)\\) to \\((2,4)\\). Show that \\(J_{T}=5\\). Use \\(T\\) to convert the integral \\[ \\alpha=\\int_{H} e^{x-y} d x d y \\]\nto an integral over \\(I^{2}\\) and thus compute \\(\\alpha\\). 9. Define \\((x, y)=T(r, \\theta)\\) on the rectangle\n\\[ 0 \\leq r \\leq a, \\quad 0 \\leq \\theta \\leq 2 \\pi \\]\nby the equations\n\\[ x=r \\cos \\theta, \\quad y=r \\sin \\theta . \\]\nShow that \\(T\\) maps this rectangle onto the closed disc \\(D\\) with center at \\((0,0)\\) and radius \\(a\\), that \\(T\\) is one-to-one in the interior of the rectangle, and that \\(J_{T}(r, \\theta)=r\\). If \\(f \\in \\mathscr{B}(D)\\), prove the formula for integration in polar coordinates:\n\\[ \\int_{D} f(x, y) d x d y=\\int_{0}^{a} \\int_{0}^{2 \\pi} f(T(r, \\theta)) r d r d \\theta . \\]\nHint: Let \\(D_{0}\\) be the interior of \\(D\\), minus the interval from \\((0,0)\\) to \\((0, a)\\). As it stands, Theorem \\(10.9\\) applies to continuous functions \\(f\\) whose support lies in \\(D_{0}\\). To remove this restriction, proceed as in Example 10.4.\nLet \\(a \\rightarrow \\infty\\) in Exercise 9 and prove that \\[ \\int_{\\mathbb{R}^{2}} f(x, y) d x d y=\\int_{0}^{\\infty} \\int_{0}^{2 \\pi} f(T(r, \\theta)) r d r d \\theta, \\]\nfor continuous functions \\(f\\) that decrease sufficiently rapidly as \\(|x|+|y| \\rightarrow \\infty\\). (Find a more precise formulation.) Apply this to\n\\[ f(x, y)=\\exp \\left(-x^{2}-y^{2}\\right) \\]\nto derive formula (101) of Chap. \\(8 .\\)\nDefine \\((u, v)=T(s, t)\\) on the strip \\[ 0\u0026lt;s\u0026lt;\\infty, \\quad 0\u0026lt;t\u0026lt;1 \\]\nby setting \\(u=s-s t, v=s t\\). Show that \\(T\\) is a 1-1 mapping of the strip onto the positive quadrant \\(Q\\) in \\(R^{2}\\). Show that \\(J_{T}(s, t)=s\\).\nFor \\(x\u0026gt;0, y\u0026gt;0\\), integrate\n\\[ u^{x-1} e^{-u} v^{y-1} e^{-v} \\]\nover \\(Q\\), use Theorem \\(10.9\\) to convert the integral to one over the strip, and derive formula (96) of Chap. 8 in this way.\n(For this application, Theorem \\(10.9\\) has to be extended so as to cover certain improper integrals. Provide this extension.)\nLet \\(I^{k}\\) be the set of all \\(\\mathbf{u}=\\left(u_{1}, \\ldots, u_{k}\\right) \\in R^{k}\\) with \\(0 \\leq u_{l} \\leq 1\\) for all \\(i\\); let \\(Q^{k}\\) be the set of all \\(\\mathrm{x}=\\left(x_{1}, \\ldots, x_{k}\\right) \\in R^{k}\\) with \\(x_{l} \\geq 0, \\Sigma x_{i} \\leq 1\\). ( \\(\\left(I^{k}\\right.\\) is the unit cube; \\(Q^{k}\\) is the standard simplex in \\(R^{k}\\).) Define \\(\\mathrm{x}=T(\\mathbf{u})\\) by \\[ \\begin{aligned} \u0026amp;x_{1}=u_{1} \\\\ \u0026amp;x_{2}=\\left(1-u_{1}\\right) u_{2} \\\\ \u0026amp;\\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots \\cdots . . . \\\\ \u0026amp;x_{k}=\\left(1-u_{1}\\right) \\cdots\\left(1-u_{k-1}\\right) u_{k} . \\end{aligned} \\]\nShow that\n\\[ \\sum_{i=1}^{k} x_{l}=1-\\prod_{i=1}^{k}\\left(1-u_{i}\\right) . \\]\nShow that \\(T\\) maps \\(I^{k}\\) onto \\(Q^{k}\\), that \\(T\\) is \\(1-1\\) in the interior of \\(I^{k}\\), and that its inverse \\(S\\) is defined in the interior of \\(Q^{k}\\) by \\(u_{1}=x_{1}\\) and\n\\[ u_{l}=\\frac{x_{l}}{1-x_{1}-\\cdots-x_{l-1}} \\]\nfor \\(i=2, \\ldots, k\\). Show that\n\\[ J_{T}(\\mathbf{u})=\\left(1-u_{1}\\right)^{k-1}\\left(1-u_{2}\\right)^{k-2} \\cdots\\left(1-u_{k-1}\\right), \\]\nand\n\\[ J_{S}(\\mathbf{x})=\\left[\\left(1-x_{1}\\right)\\left(1-x_{1}-x_{2}\\right) \\cdots\\left(1-x_{1}-\\cdots-x_{k-1}\\right)\\right]^{-1} . \\]\nLet \\(r_{1}, \\ldots, r_{k}\\) be nonnegative integers, and prove that \\[ \\int_{\\mathbf{Q}^{k}} x_{1}^{r_{1}} \\cdots x_{k}^{r_{k}} d x=\\frac{r_{1} ! \\cdots r_{k} !}{\\left(k+r_{1}+\\cdots+r_{k}\\right) !} \\]\nHint: Use Exercise 12, Theorems \\(10.9\\) and 8.20.\nNote that the special case \\(r_{1}=\\cdots=r_{k}=0\\) shows that the volume of \\(Q^{k}\\) is \\(1 / k !\\)\nProve formula (46).\nIf \\(\\omega\\) and \\(\\lambda\\) are \\(k\\) - and \\(m\\)-forms, respectively, prove that\n\\[ \\omega \\wedge \\lambda=(-1)^{k m} \\lambda \\wedge \\omega . \\]\nIf \\(k \\geq 2\\) and \\(\\sigma=\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{k}\\right]\\) is an oriented affine \\(k\\)-simplex, prove that \\(\\partial^{2} \\sigma=0\\), directly from the definition of the boundary operator \\(\\partial\\). Deduce from this that \\(\\partial^{2} \\Psi=0\\) for every chain \\(\\Psi\\). Hint: For orientation, do it first for \\(k=2, k=3\\). In general, if \\(i\u0026lt;j\\), let \\(\\sigma_{l \\jmath}\\) be the \\((k-2)\\)-simplex obtained by deleting \\(\\mathbf{p}_{i}\\) and \\(\\mathbf{p}_{j}\\) from \\(\\sigma\\). Show that each \\(\\sigma_{l \\jmath}\\) occurs twice in \\(\\partial^{2} \\sigma\\), with opposite sign.\nPut \\(J^{2}=\\tau_{1}+\\tau_{2}\\), where \\[ \\tau_{1}=\\left[0, \\mathbf{e}_{1}, \\mathbf{e}_{1}+\\mathbf{e}_{2}\\right], \\quad \\tau_{2}=-\\left[0, \\mathbf{e}_{2}, \\mathbf{e}_{2}+\\mathbf{e}_{1}\\right] . \\]\nExplain why it is reasonable to call \\(J^{2}\\) the positively oriented unit square in \\(R^{2}\\). Show that \\(\\partial J^{2}\\) is the sum of 4 oriented affine 1-simplexes. Find these. What is \\(\\partial\\left(\\tau_{1}-\\tau_{2}\\right) ?\\)\nConsider the oriented affine 3 -simplex \\[ \\sigma_{1}=\\left[\\mathbf{0}, \\mathbf{e}_{1}, \\mathbf{e}_{1}+\\mathbf{e}_{2}, \\mathbf{e}_{1}+\\mathbf{e}_{2}+\\mathbf{e}_{3}\\right] \\]\nin \\(R^{3}\\). Show that \\(\\sigma_{1}\\) (regarded as a linear transformation) has determinant 1 . Thus \\(\\sigma_{1}\\) is positively oriented. Let \\(\\sigma_{2}, \\ldots, \\sigma_{6}\\) be five other oriented 3-simplexes, obtained as follows: There are five permutations \\(\\left(i_{1}, i_{2}, i_{3}\\right)\\) of \\((1,2,3)\\), distinct from \\((1,2,3)\\). Associate with each \\(\\left(i_{1}, i_{2}, i_{3}\\right)\\) the simplex\n\\[ s\\left(i_{1}, i_{2}, i_{3}\\right)\\left[0, \\mathbf{e}_{l_{1}}, \\mathbf{e}_{t_{1}}+\\mathbf{e}_{l_{2}}, \\mathbf{e}_{l_{1}}+\\mathbf{e}_{l_{2}}+\\mathbf{e}_{l_{3}}\\right] \\]\nwhere \\(s\\) is the sign that occurs in the definition of the determinant. (This is how \\(\\tau_{2}\\) was obtained from \\(\\tau_{1}\\) in Exercise 17.)\nShow that \\(\\sigma_{2}, \\ldots, \\sigma_{6}\\) are positively oriented.\nPut \\(J^{3}=\\sigma_{1}+\\cdots+\\sigma_{6}\\). Then \\(J^{3}\\) may be called the positively oriented unit cube in \\(R^{3}\\).\nShow that \\(\\partial J^{3}\\) is the sum of 12 oriented affine 2 -simplexes. (These 12 triangles cover the surface of the unit cube \\(I^{3}\\).)\nShow that \\(\\mathrm{x}=\\left(x_{1}, x_{2}, x_{3}\\right)\\) is in the range of \\(\\sigma_{1}\\) if and only if \\(0 \\leq x_{3} \\leq x_{2}\\) \\(\\leq x_{1} \\leq 1\\).\nShow that the ranges of \\(\\sigma_{1}, \\ldots, \\sigma_{6}\\) have disjoint interiors, and that their union covers \\(I^{3}\\). (Compare with Exercise 13 ; note that \\(3 !=6\\).)\nLet \\(J^{2}\\) and \\(J^{3}\\) be as in Exercise 17 and 18. Define \\[ \\begin{array}{ll} B_{01}(u, v)=(0, u, v), \u0026amp; B_{11}(u, v)=(1, u, v), \\\\ B_{02}(u, v)=(u, 0, v), \u0026amp; B_{12}(u, v)=(u, 1, v), \\\\ B_{03}(u, v)=(u, v, 0), \u0026amp; B_{13}(u, v)=(u, v, 1) . \\end{array} \\]\nThese are affine, and map \\(R^{2}\\) into \\(R^{3}\\).\nPut \\(\\beta_{r i}=B_{r i}\\left(J^{2}\\right)\\), for \\(r=0,1, i=1,2,3\\). Each \\(\\beta_{r i}\\) is an affine-oriented 2-chain. (See Sec. 10.30.) Verify that\n\\[ \\partial J^{3}=\\sum_{l=1}^{3}(-1)^{t}\\left(\\beta_{0 \\imath}-\\beta_{1 i}\\right), \\]\nin agreement with Exercise \\(18 .\\)\nState conditions under which the formula \\[ \\int_{\\Phi} f d \\omega=\\int_{\\Omega_{\\Phi}} f \\omega-\\int_{\\Phi}(d f) \\wedge \\omega \\]\nis valid, and show that it generalizes the formula for integration by parts. Hint \\(: d(f \\omega)=(d f) \\wedge \\omega+f d \\omega\\).\nAs in Example 10.36, consider the 1-form \\[ \\eta=\\frac{x d y-y d x}{x^{2}+y^{2}} \\]\nin \\(R^{2}-\\{0\\}\\)\nCarry out the computation that leads to formula (113), and prove that \\(d \\eta=0\\).\nLet \\(\\gamma(t)=(r \\cos t, r \\sin t)\\), for some \\(r\u0026gt;0\\), and let \\(\\Gamma\\) be a \\(\\mathscr{C}^{\\prime \\prime}\\)-curve in \\(R^{2}-\\{0\\}\\), with parameter interval \\([0,2 \\pi]\\), with \\(\\Gamma(0)=\\Gamma(2 \\pi)\\), such that the intervals \\([\\gamma(t)\\), \\(\\Gamma(t)]\\) do not contain 0 for any \\(t \\in[0,2 \\pi]\\). Prove that\n\\[ \\int_{\\Gamma} \\eta=2 \\pi \\text {. } \\]\nHint: For \\(0 \\leq t \\leq 2 \\pi, 0 \\leq u \\leq 1\\), define\n\\[ \\Phi(t, u)=(1-u) \\Gamma(t)+u \\gamma(t) . \\]\nThen \\(\\Phi\\) is a 2 -surface in \\(R^{2}-\\{0\\}\\) whose parameter domain is the indicated rectangle. Because of cancellations (as in Example 10.32),\n\\[ \\partial \\Phi=\\Gamma-\\gamma . \\]\nUse Stokes’ theorem to deduce that\n\\[ \\int_{\\Gamma} \\eta=\\int_{\\nu} \\eta \\]\nbecause \\(d \\eta=0\\)\nTake \\(\\Gamma(t)=(a \\cos t, b \\sin t)\\) where \\(a\u0026gt;0, b\u0026gt;0\\) are fixed. Use part (b) to show that \\[ \\int_{0}^{2 \\pi} \\frac{a b}{a^{2} \\cos ^{2} t+b^{2} \\sin ^{2} t} d t=2 \\pi \\]\nShow that \\[ \\eta=d\\left(\\arctan \\frac{y}{x}\\right) \\]\nin any convex open set in which \\(x \\neq 0\\), and that\n\\[ \\eta=d\\left(-\\arctan \\frac{x}{y}\\right) \\]\nin any convex open set in which \\(y \\neq 0\\).\nExplain why this justifies the notation \\(\\eta=d \\theta\\), in spite of the fact that \\(\\eta\\) is not exact in \\(R^{2}-\\{0\\}\\).\nShow that \\((b)\\) can be derived from \\((d)\\). \\((f)\\) If \\(\\Gamma\\) is any closed \\(\\mathscr{C}^{\\prime}\\)-curve in \\(R^{2}-\\{0\\}\\), prove that\n\\[ \\frac{1}{2 \\pi} \\int_{\\Gamma} \\eta=\\operatorname{Ind}(\\Gamma) . \\]\n(See Exercise 23 of Chap. 8 for the definition of the index of a curve.) 22. As in Example 10.37, define \\(\\zeta\\) in \\(R^{3}-\\{0\\}\\) by\n\\[ \\zeta=\\frac{x d y \\wedge d z+y d z \\wedge d x+z d x \\wedge d y}{r^{3}} \\]\nwhere \\(r=\\left(x^{2}+y^{2}+z^{2}\\right)^{1 / 2}\\), let \\(D\\) be the rectangle given by \\(0 \\leq u \\leq \\pi, 0 \\leq v \\leq 2 \\pi\\), and let \\(\\Sigma\\) be the 2 -surface in \\(R^{3}\\), with parameter domain \\(D\\), given by\n\\[ x=\\sin u \\cos v, \\quad y=\\sin u \\sin v, \\quad z=\\cos u . \\]\nProve that \\(d \\zeta=0\\) in \\(R^{3}-\\{0\\}\\).\nLet \\(S\\) denote the restriction of \\(\\Sigma\\) to a parameter domain \\(E \\subset D\\). Prove that\n\\[ \\int_{s} \\zeta=\\int_{E} \\sin u d u d v=A(S), \\]\nwhere \\(A\\) denotes area, as in Sec. 10.43. Note that this contains (115) as a special case.\nSuppose \\(g, h_{1}, h_{2}, h_{3}\\), are \\(\\mathscr{C}^{\\prime \\prime}\\)-functions on \\([0,1], g\u0026gt;0\\). Let \\((x, y, z)=\\Phi(s, t)\\) define a 2-surface \\(\\Phi\\), with parameter domain \\(I^{2}\\), by \\[ x=g(t) h_{1}(s), \\quad y=g(t) h_{2}(s), \\quad z=g(t) h_{3}(s) . \\]\nProve that\n\\[ \\int_{\\Phi} \\zeta=0 \\]\ndirectly from (35).\nNote the shape of the range of \\(\\Phi\\) : For fixed \\(s, \\Phi(s, t)\\) runs over an interval on a line through 0 . The range of \\(\\Phi\\) thus lies in a “cone” with vertex at the origin.\nLet \\(E\\) be a closed rectangle in \\(D\\), with edges parallel to those of \\(D\\). Suppose \\(f \\in \\mathscr{C}^{\\prime \\prime}(D), f\u0026gt;0\\). Let \\(\\Omega\\) be the 2 -surface with parameter domain \\(E\\), defined by \\[ \\Omega(u, v)=f(u, v) \\Sigma(u, v) . \\]\nDefine \\(S\\) as in (b) and prove that\n\\[ \\int_{\\Omega} \\zeta=\\int_{S} \\zeta=A(S) . \\]\n(Since \\(S\\) is the “radial projection” of \\(\\Omega\\) into the unit sphere, this result makes it reasonable to call \\(\\int_{\\Omega} \\zeta\\) the “solid angle” subtended by the range of \\(\\Omega\\) at the origin.)\nHint: Consider the 3-surface \\(\\Psi\\) given by\n\\[ \\Psi(t, u, v)=[1-t+t f(u, v)] \\Sigma(u, v), \\]\nwhere \\((u, v) \\in E, 0 \\leq t \\leq 1\\). For fixed \\(v\\), the mapping \\((t, u) \\rightarrow \\Psi(t, u, v)\\) is a 2-sur- face \\(\\Phi\\) to which (c) can be applied to show that \\(\\int_{\\Phi} \\zeta=0\\). The same thing holds when \\(u\\) is fixed. By \\((a)\\) and Stokes’ theorem,\n\\[ \\int_{\\partial \\Psi} \\zeta=\\int_{\\Psi} d \\zeta=0 . \\]\nPut \\(\\lambda=-(z / r) \\eta\\), where \\[ \\eta=\\frac{x d y-y d x}{x^{2}+y^{2}}, \\]\nas in Exercise 21. Then \\(\\lambda\\) is a 1-form in the open set \\(V \\subset R^{3}\\) in which \\(x^{2}+y^{2}\u0026gt;0\\).\nShow that \\(\\zeta\\) is exact in \\(V\\) by showing that\n\\[ \\zeta=d \\lambda \\text {. } \\]\n\\((f)\\) Derive \\((d)\\) from \\((e)\\), without using \\((c)\\).\nHint: To begin with, assume \\(0\u0026lt;u\u0026lt;\\pi\\) on \\(E\\). By \\((e)\\),\n\\[ \\int_{\\Omega} \\zeta=\\int_{\\partial \\Omega} \\lambda \\text { and } \\int_{S} \\zeta=\\int_{\\partial S} \\lambda . \\]\nShow that the two integrals of \\(\\lambda\\) are equal, by using part \\((d)\\) of Exercise 21, and by noting that \\(z / r\\) is the same at \\(\\Sigma(u, v)\\) as at \\(\\Omega(u, v)\\).\nIs \\(\\zeta\\) exact in the complement of every line through the origin? Fix \\(n\\). Define \\(r_{k}=\\left(x_{1}^{2}+\\cdots+x_{k}^{2}\\right)^{1 / 2}\\) for \\(1 \\leq k \\leq n\\), let \\(E_{k}\\) be the set of all \\(\\mathbf{x} \\in R^{n}\\) at which \\(r_{k}\u0026gt;0\\), and let \\(\\omega_{k}\\) be the \\((k-1)\\)-form defined in \\(E_{k}\\) by \\[ \\omega_{k}=\\left(r_{k}\\right)^{-k} \\sum_{i=1}^{k}(-1)^{t-1} x_{i} d x_{1} \\wedge \\cdots \\wedge d x_{i-1} \\wedge d x_{i+1} \\wedge \\cdots \\wedge d x_{k} . \\]\nNote that \\(\\omega_{2}=\\eta, \\omega_{3}=\\zeta\\), in the terminology of Exercises 21 and 22 . Note also that\n\\[ E_{1} \\subset E_{2} \\subset \\cdots \\subset E_{n}=R^{n}-\\{0\\} . \\]\nProve that \\(d \\omega_{k}=0\\) in \\(E_{k}\\).\nFor \\(k=2, \\ldots, n\\), prove that \\(\\omega_{k}\\) is exact in \\(E_{k-1}\\), by showing that\n\\[ \\omega_{k}=d\\left(f_{k} \\omega_{k-1}\\right)=\\left(d f_{k}\\right) \\wedge \\omega_{k-1}, \\]\nwhere \\(f_{k}(\\mathbf{x})=(-1)^{k} g_{k}\\left(x_{k} / r_{k}\\right)\\) and\n\\[ g_{k}(t)=\\int_{-1}^{t}\\left(1-s^{2}\\right)^{(k-3) / 2} d s \\quad(-1\u0026lt;t\u0026lt;1) . \\]\nHint: \\(f_{k}\\) satisfies the differential equations\nand\n\\[ \\mathbf{x} \\cdot\\left(\\nabla f_{k}\\right)(\\mathbf{x})=0 \\]\n\\[ \\left(D_{k} f_{k}\\right)(\\mathbf{x})=\\frac{(-1)^{k}\\left(r_{k-1}\\right)^{k-1}}{\\left(r_{k}\\right)^{k}} . \\]\nIs \\(\\omega_{n}\\) exact in \\(E_{n}\\) ?\nNote that (b) is a generalization of part \\((e)\\) of Exercise 22. Try to extend some of the other assertions of Exercises 21 and 22 to \\(\\omega_{n}\\), for arbitrary \\(n\\).\nLet \\(\\omega=\\Sigma a_{l}(\\mathbf{x}) d x_{l}\\) be a 1 -form of class \\(\\mathscr{C}^{\\prime \\prime}\\) in a convex open set \\(E \\subset R^{n}\\). Assume \\(d \\omega=0\\) and prove that \\(\\omega\\) is exact in \\(E\\), by completing the following outline: Fix \\(\\mathbf{p} \\in E\\). Define\n\\[ f(\\mathbf{x})=\\int_{[\\mathbf{p}, \\mathbf{x}]} \\omega \\quad(\\mathbf{x} \\in E) \\text {. } \\]\nApply Stokes’ theorem to affine-oriented 2-simplexes \\([\\mathbf{p}, \\mathbf{x}, \\mathbf{y}]\\) in \\(E\\). Deduce that\n\\[ f(\\mathbf{y})-f(\\mathbf{x})=\\sum_{l=1}^{n}\\left(y_{l}-x_{l}\\right) \\int_{0}^{1} a_{l}((1-t) \\mathbf{x}+t \\mathbf{y}) d t \\]\nfor \\(\\mathbf{x} \\in E, \\mathbf{y} \\in E\\). Hence \\(\\left(D_{l} f\\right)(\\mathbf{x})=a_{l}(\\mathbf{x})\\).\nAssume that \\(\\omega\\) is a 1-form in an open set \\(E \\subset R^{n}\\) such that \\[ \\int_{\\nu} \\omega=0 \\]\nfor every closed curve \\(\\gamma\\) in \\(E\\), of class \\(\\mathscr{C}^{\\prime}\\). Prove that \\(\\omega\\) is exact in \\(E\\), by imitating part of the argument sketched in Exercise \\(24 .\\)\nAssume \\(\\omega\\) is a 1-form in \\(R^{3}-\\{0\\}\\), of class \\(\\mathscr{C}^{\\prime}\\) and \\(d \\omega=0\\). Prove that \\(\\omega\\) is exact in \\(R^{3}-\\{0\\}\\) Hint: Every closed continuously differentiable curve in \\(R^{3}-\\{0\\}\\) is the boundary of a 2-surface in \\(R^{3}-\\{0\\}\\). Apply Stokes’ theorem and Exercise \\(25 .\\)\nLet \\(E\\) be an open 3-cell in \\(R^{3}\\), with edges parallel to the coordinate axes. Suppose \\((a, b, c) \\in E, f_{i} \\in \\mathscr{C}^{\\prime}(E)\\) for \\(i=1,2,3\\), \\[ \\omega=f_{1} d y \\wedge d z+f_{2} d z \\wedge d x+f_{3} d x \\wedge d y, \\]\nand assume that \\(d \\omega=0\\) in \\(E\\). Define\n\\[ \\lambda=g_{1} d x+g_{2} d y \\]\nwhere\n\\[ \\begin{aligned} \u0026amp;g_{1}(x, y, z)=\\int_{c}^{z} f_{2}(x, y, s) d s-\\int_{b}^{y} f_{3}(x, t, c) d t \\\\ \u0026amp;g_{2}(x, y, z)=-\\int_{c}^{z} f_{1}(x, y, s) d s, \\end{aligned} \\]\nfor \\((x, y, z) \\in E\\). Prove that \\(d \\lambda=\\omega\\) in \\(E\\).\nEvaluate these integrals when \\(\\omega=\\zeta\\) and thus find the form \\(\\lambda\\) that occurs in part \\((e)\\) of Exercise \\(22 .\\) 28. Fix \\(b\u0026gt;a\u0026gt;0\\), define\n\\[ \\Phi(r, \\theta)=(r \\cos \\theta, r \\sin \\theta) \\]\nfor \\(a \\leq r \\leq b, 0 \\leq \\theta \\leq 2 \\pi\\). (The range of \\(\\Phi\\) is an annulus in \\(R^{2}\\).) Put \\(\\omega=x^{3} d y\\), and compute both\n\\[ \\int_{\\infty} d \\omega \\text { and } \\int_{\\Delta \\Phi} \\omega \\]\nto verify that they are equal.\nProve the existence of a function \\(\\alpha\\) with the properties needed in the proof of Theorem 10.38, and prove that the resulting function \\(F\\) is of class \\(\\mathscr{C}^{\\prime}\\). (Both assertions become trivial if \\(E\\) is an open cell or an open ball, since \\(\\alpha\\) can then be taken to be a constant. Refer to Theorem 9.42.)\nIf \\(\\mathbf{N}\\) is the vector given by (135), prove that\n\\[ \\operatorname{det}\\left[\\begin{array}{lll} \\alpha_{1} \u0026amp; \\beta_{1} \u0026amp; \\alpha_{2} \\beta_{3}-\\alpha_{3} \\beta_{2} \\\\ \\alpha_{2} \u0026amp; \\beta_{2} \u0026amp; \\alpha_{3} \\beta_{1}-\\alpha_{1} \\beta_{3} \\\\ \\alpha_{3} \u0026amp; \\beta_{3} \u0026amp; \\alpha_{1} \\beta_{2}-\\alpha_{2} \\beta_{1} \\end{array}\\right]=|\\mathbf{N}|^{2} \\]\nAlso, verify Eq. (137).\nLet \\(E \\subset R^{3}\\) be open, suppose \\(g \\in \\mathscr{C}^{\\prime \\prime}(E), h \\in \\mathscr{C}^{\\prime \\prime}(E)\\), and consider the vector field \\[ \\mathbf{F}=g \\nabla h . \\]\nProve that \\[ \\nabla \\cdot F=g \\nabla^{2} h+(\\nabla g) \\cdot(\\nabla h) \\]\nwhere \\(\\nabla^{2} h=\\nabla \\cdot(\\nabla h)=\\Sigma \\partial^{2} h / \\partial x_{i}^{2}\\) is the so-called “Laplacian” of \\(h\\).\nIf \\(\\Omega\\) is a closed subset of \\(E\\) with positively oriented boundary \\(\\partial \\Omega\\) (as in Theorem 10.51), prove that \\[ \\int_{\\Omega}\\left[g \\nabla^{2} h+(\\nabla g) \\cdot(\\nabla h)\\right] d V=\\int_{\\partial \\Omega} g \\frac{\\partial h}{\\partial n} d A \\]\nwhere (as is customary) we have written \\(\\partial h / \\partial n\\) in place of \\((\\nabla h) \\cdot \\mathbf{n}\\). (Thus \\(\\partial h / \\partial n\\) is the directional derivative of \\(h\\) in the direction of the outward normal to \\(\\partial \\Omega\\), the so-called normal derivative of \\(h\\).) Interchange \\(g\\) and \\(h\\), subtract the resulting formula from the first one, to obtain\n\\[ \\int_{\\Omega}\\left(g \\nabla^{2} h-h \\nabla^{2} g\\right) d V=\\int_{\\partial \\Omega}\\left(g \\frac{\\partial h}{\\partial n}-h \\frac{d g}{\\partial n}\\right) d A . \\]\nThese two formulas are usually called Green’s identities.\nAssume that \\(h\\) is harmonic in \\(E\\); this means that \\(\\nabla^{2} h=0\\). Take \\(g=1\\) and conclude that \\[ \\int_{\\partial \\Omega} \\frac{\\partial h}{\\partial n} d A=0 \\]\nTake \\(g=h\\), and conclude that \\(h=0\\) in \\(\\Omega\\) if \\(h=0\\) on \\(\\partial \\Omega\\).\nShow that Green’s identities are also valid in \\(R^{2}\\). Fix \\(\\delta, 0\u0026lt;\\delta\u0026lt;1\\). Let \\(D\\) be the set of all \\((\\theta, t) \\in R^{2}\\) such that \\(0 \\leq \\theta \\leq \\pi,-\\delta \\leq t \\leq \\delta\\). Let \\(\\Phi\\) be the 2-surface in \\(R^{3}\\), with parameter domain \\(D\\), given by \\[ \\begin{aligned} \u0026amp;x=(1-t \\sin \\theta) \\cos 2 \\theta \\\\ \u0026amp;y=(1-t \\sin \\theta) \\sin 2 \\theta \\\\ \u0026amp;z=t \\cos \\theta \\end{aligned} \\]\nwhere \\((x, y, z)=\\Phi(\\theta, t)\\). Note that \\(\\Phi(\\pi, t)=\\Phi(0,-t)\\), and that \\(\\Phi\\) is one-to-one on the rest of \\(D\\).\nThe range \\(M=\\Phi(D)\\) of \\(\\Phi\\) is known as a Möbius band. It is the simplest example of a nonorientable surface.\nProve the various assertions made in the following description: Put \\(\\mathbf{p}_{1}=(0,-\\delta), \\mathbf{p}_{2}=(\\pi,-\\delta), \\mathbf{p}_{3}=(\\pi, \\delta), \\mathbf{p}_{4}=(0, \\delta), \\mathbf{p}_{5}=\\mathbf{p}_{1}\\). Put \\(\\gamma_{i}=\\left[\\mathbf{p}_{i}, \\mathbf{p}_{i+1}\\right]\\), \\(i=1, \\ldots, 4\\), and put \\(\\Gamma_{1}=\\Phi \\circ \\gamma_{1}\\). Then\n\\[ \\partial \\Phi=\\Gamma_{1}+\\Gamma_{2}+\\Gamma_{3}+\\Gamma_{4} . \\]\nPut \\(\\mathbf{a}=(1,0,-\\delta), \\mathbf{b}=(1,0, \\delta)\\). Then\n\\[ \\Phi\\left(\\mathbf{p}_{1}\\right)=\\Phi\\left(\\mathbf{p}_{3}\\right)=\\mathbf{a}, \\quad \\Phi\\left(\\mathbf{p}_{2}\\right)=\\Phi\\left(\\mathbf{p}_{4}\\right)=\\mathbf{b}, \\]\nand \\(\\partial \\Phi\\) can be described as follows.\n\\(\\Gamma_{1}\\) spirals up from a to \\(\\mathbf{b}\\); its projection into the \\((x, y)\\)-plane has winding number \\(+1\\) around the origin. (See Exercise 23, Chap. 8.)\n\\[ \\Gamma_{2}=[\\mathbf{b}, \\mathbf{a}] \\text {. } \\]\n\\(\\Gamma_{3}\\) spirals up from a to \\(\\mathbf{b}\\); its projection into the \\((x, y)\\) plane has winding number \\(-1\\) around the origin.\n\\[ \\Gamma_{4}=[\\mathbf{b}, \\mathbf{a}] \\text {. } \\]\nThus \\(\\partial \\Phi=\\Gamma_{1}+\\Gamma_{3}+2 \\Gamma_{2}\\).\nIf we go from a to \\(\\mathbf{b}\\) along \\(\\Gamma_{1}\\) and continue along the “edge” of \\(M\\) until we return to a, the curve traced out is\n\\[ \\Gamma=\\Gamma_{1}-\\Gamma_{3}, \\]\nwhich may also be represented on the parameter interval \\([0,2 \\pi]\\) by the equations\n\\[ \\begin{aligned} \u0026amp;x=(1+\\delta \\sin \\theta) \\cos 2 \\theta \\\\ \u0026amp;y=(1+\\delta \\sin \\theta) \\sin 2 \\theta \\\\ \u0026amp;z=-\\delta \\cos \\theta \\end{aligned} \\]\nIt should be emphasized that \\(\\Gamma \\neq \\partial \\Phi\\) : Let \\(\\eta\\) be the 1-form discussed in Exercises 21 and 22. Since \\(d \\eta=0\\), Stokes’ theorem shows that\n\\[ \\int_{\\infty \\infty} \\eta=0 . \\]\nBut although \\(\\Gamma\\) is the “geometric” boundary of \\(M\\), we have\n\\[ \\int_{\\Gamma} \\eta=4 \\pi . \\]\nIn order to avoid this possible source of confusion, Stokes’ formula (Theorem \\(10.50)\\) is frequently stated only for orientable surfaces \\(\\Phi\\). 11\n","date":"2022-08-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/10-exerciese/","section":"baby rudin","tags":null,"title":"10 Exerciese.md"},{"categories":null,"contents":"11.4 Definition Let \\(R^{p}\\) denote \\(p\\)-dimensional euclidean space. By an interval in \\(R^{p}\\) we mean the set of points \\(\\mathrm{x}=\\left(x_{1}, \\ldots, x_{p}\\right)\\) such that \\[ a_{i} \\leq x_{i} \\leq b_{i} \\quad(i=1, \\ldots, p) \\]\nor the set of points which is characterized by (10) with any or all of the \\(\\leq\\) signs replaced by \\(\u0026lt;\\). The possibility that \\(a_{i}=b_{i}\\) for any value of \\(i\\) is not ruled out; in particular, the empty set is included among the intervals. If \\(A\\) is the union of a finite number of intervals, \\(A\\) is said to be an elementary set.\nIf \\(I\\) is an interval, we define\n\\[ m(I)=\\prod_{i=1}^{p}\\left(b_{i}-a_{i}\\right) \\]\nno matter whether equality is included or excluded in any of the inequalities (10).\nIf \\(A=I_{1} \\cup \\cdots \\cup I_{n}\\), and if these intervals are pairwise disjoint, we set\n\\[ m(A)=m\\left(I_{1}\\right)+\\cdots+m\\left(I_{n}\\right) . \\]\nWe let \\(\\mathscr{E}\\) denote the family of all elementary subsets of \\(R^{p}\\).\nAt this point, the following properties should be verified:\n\\(\\mathscr{E}\\) is a ring, but not a \\(\\sigma\\)-ring.\nIf \\(A \\in \\mathscr{E}\\), then \\(A\\) is the union of a finite number of disjoint intervals.\nIf \\(A \\in \\mathscr{E}, m(A)\\) is well defined by (11); that is, if two different decompositions of \\(A\\) into disjoint intervals are used, each gives rise to the same value of \\(m(A)\\).\n\\(m\\) is additive on \\(\\mathscr{E}\\).\nNote that if \\(p=1,2,3\\), then \\(m\\) is length, area, and volume, respectively.\n11.5 Definition A nonnegative additive set function \\(\\phi\\) defined on \\(\\mathscr{E}\\) is said to be regular if the following is true: To every \\(A \\in \\mathscr{E}\\) and to every \\(\\varepsilon\u0026gt;0\\) there exist sets \\(F \\in \\mathscr{E}, G \\in \\mathscr{E}\\) such that \\(F\\) is closed, \\(G\\) is open, \\(F \\subset A \\subset G\\), and \\[ \\phi(G)-\\varepsilon \\leq \\phi(A) \\leq \\phi(F)+\\varepsilon . \\]\n11.6 Examples\nThe set function \\(m\\) is regular. If \\(A\\) is an interval, it is trivial that the requirements of Definition \\(11.5\\) are satisfied. The general case follows from (13).\nTake \\(R^{p}=R^{1}\\), and let \\(\\alpha\\) be a monotonically increasing function, defined for all real \\(x\\). Put \\[ \\begin{aligned} \u0026amp;\\mu([a, b))=\\alpha(b-)-\\alpha(a-), \\\\ \u0026amp;\\mu([a, b])=\\alpha(b+)-\\alpha(a-), \\\\ \u0026amp;\\mu((a, b])=\\alpha(b+)-\\alpha(a+), \\\\ \u0026amp;\\mu((a, b))=\\alpha(b-)-\\alpha(a+) . \\end{aligned} \\]\nHere \\([a, b)\\) is the set \\(a \\leq x\u0026lt;b\\), etc.\nBecause of the possible discontinuities of \\(\\alpha\\), these cases have to be distinguished. If \\(\\mu\\) is defined for elementary sets as in (11), \\(\\mu\\) is regular on \\(\\mathscr{E}\\). The proof is just like that of \\((a)\\).\nOur next objective is to show that every regular set function on \\(\\mathscr{E}\\) can be extended to a countably additive set function on a \\(\\sigma\\)-ring which contains \\(\\mathscr{E}\\).\n11.7 Definition Let \\(\\mu\\) be additive, regular, nonnegative, and finite on \\(\\mathscr{E}\\). Consider countable coverings of any set \\(E \\subset R^{p}\\) by open elementary sets \\(A_{n}\\) : \\[ E \\subset \\bigcup_{n=1}^{\\infty} A_{n} . \\]\nDefine\n\\[ \\mu^{*}(E)=\\inf \\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right), \\]\nthe inf being taken over all countable coverings of \\(E\\) by open elementary sets. \\(\\mu^{*}(E)\\) is called the outer measure of \\(E\\), corresponding to \\(\\mu\\).\nIt is clear that \\(\\mu^{*}(E) \\geq 0\\) for all \\(E\\) and that\n\\[ \\mu^{*}\\left(E_{1}\\right) \\leq \\mu^{*}\\left(E_{2}\\right) \\]\nif \\(E_{1} \\subset E_{2}\\).\n11.8 Theorem\nFor every \\(A \\in \\mathscr{E}, \\mu^{*}(A)=\\mu(A)\\).\nIf \\(E=\\bigcup_{1}^{\\infty} E_{n}\\), then\n\\[ \\mu^{*}(E) \\leq \\sum_{n=1}^{\\infty} \\mu^{*}\\left(E_{n}\\right) . \\]\nNote that \\((a)\\) asserts that \\(\\mu^{*}\\) is an extension of \\(\\mu\\) from \\(\\mathscr{E}\\) to the family of all subsets of \\(R^{p}\\). The property (19) is called subadditivity.\nProof Choose \\(A \\in \\mathscr{E}\\) and \\(\\varepsilon\u0026gt;0\\).\nThe regularity of \\(\\mu\\) shows that \\(A\\) is contained in an open elementary set \\(G\\) such that \\(\\mu(G) \\leq \\mu(A)+\\varepsilon\\). Since \\(\\mu^{*}(A) \\leq \\mu(G)\\) and since \\(\\varepsilon\\) was arbitrary, we have\n\\[ \\mu^{*}(A) \\leq \\mu(A) . \\]\nThe definition of \\(\\mu^{*}\\) shows that there is a sequence \\(\\left\\{A_{n}\\right\\}\\) of open elementary sets whose union contains \\(A\\), such that\n\\[ \\sum_{n=1}^{\\infty} \\mu\\left(A_{n}\\right) \\leq \\mu^{*}(A)+\\varepsilon . \\]\nThe regularity of \\(\\mu\\) shows that \\(A\\) contains a closed elementary set \\(F\\) such that \\(\\mu(F) \\geq \\mu(A)-\\varepsilon\\); and since \\(F\\) is compact, we have\n\\[ F \\subset A_{1} \\cup \\cdots \\cup A_{N} \\]\nfor some \\(N\\). Hence\n\\[ \\mu(A) \\leq \\mu(F)+\\varepsilon \\leq \\mu\\left(A_{1} \\cup \\cdots \\cup A_{N}\\right)+\\varepsilon \\leq \\sum_{1}^{N} \\mu\\left(A_{n}\\right)+\\varepsilon \\leq \\mu^{*}(A)+2 \\varepsilon . \\]\nIn conjunction with \\((20)\\), this proves \\((a)\\).\nNext, suppose \\(E=\\bigcup E_{n}\\), and assume that \\(\\mu^{*}\\left(E_{n}\\right)\u0026lt;+\\infty\\) for all \\(n\\). Given \\(\\varepsilon\u0026gt;0\\), there are coverings \\(\\left\\{A_{n k}\\right\\}, k=1,2,3, \\ldots\\), of \\(E_{n}\\) by open elementary sets such that\n\\[ \\sum_{k=1}^{\\infty} \\mu\\left(A_{n k}\\right) \\leq \\mu^{*}\\left(E_{n}\\right)+2^{-n} \\varepsilon . \\]\nThen\n\\[ \\mu^{*}(E) \\leq \\sum_{n=1}^{\\infty} \\sum_{k=1}^{\\infty} \\mu\\left(A_{n k}\\right) \\leq \\sum_{n=1}^{\\infty} \\mu^{*}\\left(E_{n}\\right)+\\varepsilon, \\]\nand (19) follows. In the excluded case, i.e., if \\(\\mu^{*}\\left(E_{n}\\right)=+\\infty\\) for some \\(n\\), (19) is of course trivial.\n11.9 Definition For any \\(A \\subset R^{p}, B \\subset R^{p}\\), we define \\[ \\begin{aligned} \u0026amp;S(A, B)=(A-B) \\cup(B-A) \\\\ \u0026amp;d(A, B)=\\mu^{*}(S(A, B)) \\end{aligned} \\]\nWe write \\(A_{n} \\rightarrow A\\) if\n\\[ \\lim _{n \\rightarrow \\infty} d\\left(A, A_{n}\\right)=0 . \\]\nIf there is a sequence \\(\\left\\{A_{n}\\right\\}\\) of elementary sets such that \\(A_{n} \\rightarrow A\\), we say that \\(A\\) is finitely \\(\\mu\\)-measurable and write \\(A \\in \\mathfrak{M}_{F}(\\mu)\\).\nIf \\(A\\) is the union of a countable collection of finitely \\(\\mu\\)-measurable sets, we say that \\(A\\) is \\(\\mu\\)-measurable and write \\(A \\in \\mathfrak{M}(\\mu)\\).\n\\(S(A, B)\\) is the so-called “symmetric difference” of \\(A\\) and \\(B\\). We shall see that \\(d(A, B)\\) is essentially a distance function.\nThe following theorem will enable us to obtain the desired extension of \\(\\mu\\).\n11.10 Theorem \\(\\mathfrak{M}(\\mu)\\) is a \\(\\sigma\\)-ring, and \\(\\mu^{*}\\) is countably additive on \\(\\mathfrak{M}(\\mu)\\).\nBefore we turn to the proof of this theorem, we develop some of the properties of \\(S(A, B)\\) and \\(d(A, B)\\). We have\n\\[ \\begin{aligned} \u0026amp; S(A, B)=S(B, A), \\quad S(A, A)=0 \\text {. } \\\\ \u0026amp; S(A, B) \\subset S(A, C) \\cup S(C, B) . \\\\ \u0026amp; \\left.S\\left(A_{1} \\cup A_{2}, B_{1} \\cup B_{2}\\right)\\right) \\\\ \u0026amp; \\left.S\\left(A_{1} \\cap A_{2}, B_{1} \\cap B_{2}\\right)\\right\\} \\subset S\\left(A_{1}, B_{1}\\right) \\cup S\\left(A_{2}, B_{2}\\right) \\text {. } \\\\ \u0026amp; \\left.S\\left(A_{1}-A_{2}, B_{1}-B_{2}\\right)\\right) \\end{aligned} \\]\nis clear, and (25) follows from \\[ (A-B) \\subset(A-C) \\cup(C-B), \\quad(B-A) \\subset(C-A) \\cup(B-C) . \\]\nThe first formula of (26) is obtained from\n\\[ \\left(A_{1} \\cup A_{2}\\right)-\\left(B_{1} \\cup B_{2}\\right) \\subset\\left(A_{1}-B_{1}\\right) \\cup\\left(A_{2}-B_{2}\\right) . \\]\nNext, writing \\(E^{c}\\) for the complement of \\(E\\), we have\n\\[ \\begin{aligned} S\\left(A_{1} \\cap A_{2}, B_{1} \\cap B_{2}\\right) \u0026amp;=S\\left(A_{1}^{c} \\cup A_{2}^{c}, B_{1}^{c} \\cup B_{2}^{c}\\right) \\\\ \u0026amp; \\subset S\\left(A_{1}^{c}, B_{1}^{c}\\right) \\cup S\\left(A_{2}^{c}, B_{2}^{c}\\right)=S\\left(A_{1}, B_{1}\\right) \\cup S\\left(A_{2}, B_{2}\\right) ; \\end{aligned} \\]\nand the last formula of (26) is obtained if we note that\n\\[ A_{1}-A_{2}=A_{1} \\cap A_{2}^{c} . \\]\nBy (23), (19), and (18), these properties of \\(S(A, B)\\) imply\n\\[ \\left.\\begin{array}{c} d(A, B)=d(B, A), \\quad d(A, A)=0, \\\\ d(A, B) \\leq d(A, C)+d(C, B), \\\\ d\\left(A_{1} \\cup A_{2}, B_{1} \\cup B_{2}\\right) \\\\ d\\left(A_{1} \\cap A_{2}, B_{1} \\cap B_{2}\\right) \\\\ d\\left(A_{1}-A_{2}, B_{1}-B_{2}\\right) \\end{array}\\right\\} \\leq d\\left(A_{1}, B_{1}\\right)+d\\left(A_{2}, B_{2}\\right) . \\]\nThe relations (27) and (28) show that \\(d(A, B)\\) satisfies the requirements of Definition 2.15, except that \\(d(A, B)=0\\) does not imply \\(A=B\\). For instance, if \\(\\mu=m, A\\) is countable, and \\(B\\) is empty, we have\n\\[ d(A, B)=m^{*}(A)=0 ; \\]\nto see this, cover the \\(n\\)th point of \\(A\\) by an interval \\(I_{n}\\) such that\n\\[ m\\left(I_{n}\\right)\u0026lt;2^{-n} \\varepsilon . \\]\nBut if we define two sets \\(A\\) and \\(B\\) to be equivalent, provided\n\\[ d(A, B)=0, \\]\nwe divide the subsets of \\(R^{p}\\) into equivalence classes, and \\(d(A, B)\\) makes the set of these equivalence classes into a metric space. \\(\\mathfrak{M}_{F}(\\mu)\\) is then obtained as the closure of \\(\\mathscr{E}\\). This interpretation is not essential for the proof, but it explains the underlying idea. We need one more property of \\(d(A, B)\\), namely,\n\\[ \\left|\\mu^{*}(A)-\\mu^{*}(B)\\right| \\leq d(A, B), \\]\nif at least one of \\(\\mu^{*}(A), \\mu^{*}(B)\\) is finite. For suppose \\(0 \\leq \\mu^{*}(B) \\leq \\mu^{*}(A)\\). Then (28) shows that\n\\[ d(A, 0) \\leq d(A, B)+d(B, 0) \\]\nthat is,\n\\[ \\mu^{*}(A) \\leq d(A, B)+\\mu^{*}(B) . \\]\nSince \\(\\mu^{*}(B)\\) is finite, it follows that\n\\[ \\mu^{*}(A)-\\mu^{*}(B) \\leq d(A, B) . \\]\nProof of Theorem 11.10 Suppose \\(A \\in \\mathfrak{M}_{F}(\\mu), B \\in \\mathfrak{M}_{F}(\\mu)\\). Choose \\(\\left\\{A_{n}\\right\\}\\), \\(\\left\\{B_{n}\\right\\}\\) such that \\(A_{n} \\in \\mathscr{E} . B_{n} \\in \\mathscr{E}, A_{n} \\rightarrow A, B_{n} \\rightarrow B\\). Then (29) and (30) show that \\[ \\begin{gathered} A_{n} \\cup B_{n} \\rightarrow A \\cup B, \\\\ A_{n} \\cap B_{n} \\rightarrow A \\cap B, \\\\ A_{n}-B_{n} \\rightarrow A-B, \\\\ \\mu^{*}\\left(A_{n}\\right) \\rightarrow \\mu^{*}(A), \\end{gathered} \\]\nand \\(\\mu^{*}(A)\u0026lt;+\\infty\\) since \\(d\\left(A_{n}, A\\right) \\rightarrow 0\\). By (31) and (33), \\(\\mathfrak{M}_{F}(\\mu)\\) is a ring. By (7),\n\\[ \\mu\\left(A_{n}\\right)+\\mu\\left(B_{n}\\right)=\\mu\\left(A_{n} \\cup B_{n}\\right)+\\mu\\left(A_{n} \\cap B_{n}\\right) . \\]\nLetting \\(n \\rightarrow \\infty\\), we obtain, by (34) and Theorem \\(11.8(a)\\),\n\\[ \\mu^{*}(A)+\\mu^{*}(B)=\\mu^{*}(A \\cup B)+\\mu^{*}(A \\cap B) . \\]\nIf \\(A \\cap B=0\\), then \\(\\mu^{*}(A \\cap B)=0\\).\nIt follows that \\(\\mu^{*}\\) is additive on \\(\\mathfrak{M}_{F}(\\mu)\\).\nNow let \\(A \\in \\mathfrak{M}(\\mu)\\). Then \\(A\\) can be represented as the union of a countable collection of disjoint sets of \\(\\mathfrak{M}_{F}(\\mu)\\). For if \\(A=\\bigcup A_{n}^{\\prime}\\) with \\(A_{n}^{\\prime} \\in \\mathfrak{M}_{F}(\\mu)\\), write \\(A_{1}=A_{1}^{\\prime}\\), and\n\\[ A_{n}=\\left(A_{1}^{\\prime} \\cup \\cdots \\cup A_{n}^{\\prime}\\right)-\\left(A_{n}^{\\prime} \\cup \\cdots \\cup A_{n-1}^{\\prime}\\right) \\quad(n=2,3,4, \\ldots) \\text {. } \\]\nThen\n\\[ A=\\bigcup_{n=1}^{\\infty} A_{n} \\]\nis the required representation. By (19)\n\\[ \\mu^{*}(A) \\leq \\sum_{n=1}^{\\infty} \\mu^{*}\\left(A_{n}\\right) . \\]\nOn the other hand, \\(A \\supset A_{1} \\cup \\cdots \\cup A_{n}\\); and by the additivity of \\(\\mu^{*}\\) on \\(\\mathfrak{M}_{F}(\\mu)\\) we obtain\n\\[ \\mu^{*}(A) \\geq \\mu^{*}\\left(A_{1} \\cup \\cdots \\cup A_{n}\\right)=\\mu^{*}\\left(A_{1}\\right)+\\cdots+\\mu^{*}\\left(A_{n}\\right) . \\]\nEquations (36) and (37) imply\n\\[ \\mu^{*}(A)=\\sum_{n=1}^{\\infty} \\mu^{*}\\left(A_{n}\\right) . \\]\nSuppose \\(\\mu^{*}(A)\\) is finite. Put \\(B_{n}=A_{1} \\cup \\cdots \\cup A_{n}\\). Then (38) shows that\n\\[ d\\left(A, B_{n}\\right)=\\mu^{*}\\left(\\bigcup_{i=n+1}^{\\infty} A_{i}\\right)=\\sum_{i=n+1}^{\\infty} \\mu^{*}\\left(A_{i}\\right) \\rightarrow 0 \\]\nas \\(n \\rightarrow \\infty\\). Hence \\(B_{n} \\rightarrow A\\); and since \\(B_{n} \\in \\mathfrak{M}_{F}(\\mu)\\), it is easily seen that \\(A \\in \\mathfrak{M}_{F}(\\mu)\\).\nWe have thus shown that \\(A \\in \\mathfrak{M}_{F}(\\mu)\\) if \\(A \\in \\mathfrak{M}(\\mu)\\) and \\(\\mu^{*}(A)\u0026lt;+\\infty\\). It is now clear that \\(\\mu^{*}\\) is countably additive on \\(\\mathfrak{M}(\\mu)\\). For if\n\\[ A=\\bigcup A_{n}, \\]\nwhere \\(\\left\\{A_{n}\\right\\}\\) is a sequence of disjoint sets of \\(\\mathfrak{M}(\\mu)\\), we have shown that (38) holds if \\(\\mu^{*}\\left(A_{n}\\right)\u0026lt;+\\infty\\) for every \\(n\\), and in the other case (38) is trivial.\nFinally, we have to show that \\(\\mathfrak{M}(\\mu)\\) is a \\(\\sigma\\)-ring. If \\(A_{n} \\in \\mathfrak{M}(\\mu), n=1\\), \\(2,3, \\ldots\\), it is clear that \\(\\bigcup A_{n} \\in \\mathfrak{M}(\\mu)\\) (Theorem 2.12). Suppose \\(A \\in \\mathfrak{M}(\\mu)\\), \\(B \\in \\mathfrak{D}(\\mu)\\), and\n\\[ A=\\bigcup_{n=1}^{\\infty} A_{n}, \\quad B=\\bigcup_{n=1}^{\\infty} B_{n}, \\]\nwhere \\(A_{n}, B_{n} \\in \\mathfrak{M}_{F}(\\mu)\\). Then the identity\n\\[ A_{n} \\cap B=\\bigcup_{i=1}^{\\infty}\\left(A_{n} \\cap B_{i}\\right) \\]\nshows that \\(A_{n} \\cap B \\in \\mathfrak{M}(\\mu)\\); and since\n\\[ \\mu^{*}\\left(A_{n} \\cap B\\right) \\leq \\mu^{*}\\left(A_{n}\\right)\u0026lt;+\\infty, \\]\n\\(A_{n} \\cap B \\in \\mathfrak{M}_{F}(\\mu)\\). Hence \\(A_{n}-B \\in \\mathfrak{M}_{F}(\\mu)\\), and \\(A-B \\in \\mathfrak{M}(\\mu)\\) since \\(A-B=\\bigcup_{n=1}^{\\infty}\\left(A_{n}-B\\right)\\).\nWe now replace \\(\\mu^{*}(A)\\) by \\(\\mu(A)\\) if \\(A \\in \\mathfrak{M}(\\mu)\\). Thus \\(\\mu\\), originally only defined on \\(\\mathscr{E}\\), is extended to a countably additive set function on the \\(\\sigma\\)-ring \\(\\mathfrak{M}(\\mu)\\). This extended set function is called a measure. The special case \\(\\mu=m\\) is called the Lebesgue measure on \\(R^{p}\\).\n11.11 Remarks\nIf \\(A\\) is open, then \\(A \\in \\mathfrak{M}(\\mu)\\). For every open set in \\(R^{p}\\) is the union of a countable collection of open intervals. To see this, it is sufficient to construct a countable base whose members are open intervals. By taking complements, it follows that every closed set is in \\(\\mathfrak{M}(\\mu)\\).\nIf \\(A \\in \\mathfrak{M}(\\mu)\\) and \\(\\varepsilon\u0026gt;0\\), there exist sets \\(F\\) and \\(G\\) such that \\[ F \\subset A \\subset G \\]\n\\(F\\) is closed, \\(G\\) is open, and\n\\[ \\mu(G-A)\u0026lt;\\varepsilon, \\quad \\mu(A-F)\u0026lt;\\varepsilon . \\]\nThe first inequality holds since \\(\\mu^{*}\\) was defined by means of coverings by open elementary sets. The second inequality then follows by taking complements.\nWe say that \\(E\\) is a Borel set if \\(E\\) can be obtained by a countable number of operations, starting from open sets, each operation consisting in taking unions, intersections, or complements. The collection \\(\\mathscr{B}\\) of all Borel sets in \\(R^{p}\\) is a \\(\\sigma\\)-ring; in fact, it is the smallest \\(\\sigma\\)-ring which contains all open sets. By Remark \\((a), E \\in \\mathfrak{M}(\\mu)\\) if \\(E \\in \\mathscr{B}\\).\nIf \\(A \\in \\mathfrak{M}(\\mu)\\), there exist Borel sets \\(F\\) and \\(G\\) such that \\(F \\subset A \\subset G\\), and\n\\[ \\mu(G-A)=\\mu(A-F)=0 . \\]\nThis follows from \\((b)\\) if we take \\(\\varepsilon=1 / n\\) and let \\(n \\rightarrow \\infty\\).\nSince \\(A=F \\cup(A-F)\\), we see that every \\(A \\in \\mathfrak{M}(\\mu)\\) is the union of a Borel set and a set of measure zero.\nThe Borel sets are \\(\\mu\\)-measurable for every \\(\\mu\\). But the sets of measure zero [that is, the sets \\(E\\) for which \\(\\mu^{*}(E)=0\\) ] may be different for different \\(\\mu\\) ’s.\nFor every \\(\\mu\\), the sets of measure zero form a \\(\\sigma\\)-ring.\nIn case of the Lebesgue measure, every countable set has measure zero. But there are uncountable (in fact, perfect) sets of measure zero. The Cantor set may be taken as an example: Using the notation of Sec. \\(2.44\\), it is easily seen that\n\\[ m\\left(E_{n}\\right)=\\left(\\frac{2}{3}\\right)^{n} \\quad(n=1,2,3, \\ldots) \\]\nand since \\(P=\\bigcap E_{n}, P \\subset E_{n}\\) for every \\(n\\), so that \\(m(P)=0\\).\n","date":"2022-08-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/11-the-lebesgue-theory/2-construction-of-the-lebesgue-measure/","section":"baby rudin","tags":null,"title":"2 CONSTRUCTION OF THE LEBESGUE MEASURE"},{"categories":null,"contents":"We conclude this chapter with a few applications of the preceding material to theorems concerning vector analysis in \\(R^{3}\\). These are special cases of theorems about differential forms, but are usually stated in different terminology. We are thus faced with the job of translating from one language to another.\n10.42 Vector fields Let \\(F=F_{1} \\mathbf{e}_{1}+F_{2} \\mathbf{e}_{2}+F_{3} \\mathbf{e}_{3}\\) be a continuous mapping of an open set \\(E \\subset R^{3}\\) into \\(R^{3}\\). Since \\(\\mathbf{F}\\) associates a vector to each point of \\(E, \\mathbf{F}\\) is sometimes called a vector field, especially in physics. With every such \\(\\mathbf{F}\\) is associated a 1-form\n\\[ \\begin{equation} \\lambda_{\\mathbf{F}}=F_{1} d x+F_{2} d y+F_{3} d z \\label{eq:124} \\end{equation} \\]\nand a 2 -form\n\\[ \\begin{equation} \\omega_{\\mathrm{F}}=F_{1} d y \\wedge d z+F_{2} d z \\wedge d x+F_{3} d x \\wedge d y . \\label{eq:125} \\end{equation} \\]\nHere, and in the rest of this chapter, we use the customary notation \\((x, y, z)\\) in place of \\(\\left(x_{1}, x_{2}, x_{3}\\right)\\).\nIt is clear, conversely, that every 1 -form \\(\\lambda\\) in \\(E\\) is \\(\\lambda_{\\mathbf{F}}\\) for some vector field \\(\\mathbf{F}\\) in \\(E\\), and that every 2 -form \\(\\omega\\) is \\(\\omega_{\\mathbf{F}}\\) for some \\(\\mathbf{F}\\). In \\(R^{3}\\), the study of 1-forms and 2-forms is thus coextensive with the study of vector fields.\nIf \\(u \\in \\mathscr{C}^{\\prime}(E)\\) is a real function, then its gradient\n\\[ \\nabla u=\\left(D_{1} u\\right) \\mathbf{e}_{1}+\\left(D_{2} u\\right) \\mathbf{e}_{2}+\\left(D_{3} u\\right) \\mathbf{e}_{3} \\]\nis an example of a vector field in \\(E\\).\nSuppose now that \\(\\mathbf{F}\\) is a vector field in \\(E\\), of class \\(\\mathscr{C}^{\\prime}\\). Its \\(\\operatorname{cur} l \\nabla \\times \\mathbf{F}\\) is the vector field defined in \\(E\\) by\n\\[ \\nabla \\times \\mathbf{F}=\\left(D_{2} F_{3}-D_{3} F_{2}\\right) \\mathbf{e}_{1}+\\left(D_{3} F_{1}-D_{1} F_{3}\\right) \\mathrm{e}_{2}+\\left(D_{1} F_{2}-D_{2} F_{1}\\right) \\mathbf{e}_{3} \\]\nand its divergence is the real function \\(\\nabla \\cdot \\mathbf{F}\\) defined in \\(E\\) by\n\\[ \\nabla \\cdot \\mathbf{F}=D_{1} F_{1}+D_{2} F_{2}+D_{3} F_{3} . \\]\nThese quantities have various physical interpretations. We refer to the book by \\(\\mathbf{O}\\). D. Kellogg for more details.\nHere are some relations between gradients, curls, and divergences.\n10.43 Theorem Suppose \\(E\\) is an open set in \\(R^{3}, u \\in \\mathscr{C}^{\\prime \\prime}(E)\\), and \\(\\mathbf{G}\\) is a vector field in \\(E\\), of class \\(C^{\\prime \\prime}\\).\nIf \\(\\mathbf{F}=\\nabla u\\), then \\(\\nabla \\times \\mathbf{F}=\\mathbf{0}\\).\nIf \\(\\mathbf{F}=\\nabla \\times \\mathbf{G}\\), then \\(\\nabla \\cdot \\mathbf{F}=0\\).\nFurthermore, if \\(E\\) is \\(\\mathscr{C}^{\\prime \\prime}\\)-equivalent to a convex set, then (a) and (b) have converses, in which we assume that \\(\\mathbf{F}\\) is a vector field in \\(E\\), of class \\(\\mathscr{C}^{\\prime}\\) :\n(a’) If \\(\\nabla \\times \\mathbf{F}=\\mathbf{0}\\), then \\(\\mathbf{F}=\\nabla u\\) for some \\(u \\in \\mathscr{C}^{\\prime \\prime}(E)\\).\n(b’) If \\(\\nabla \\cdot \\mathbf{F}=0\\), then \\(\\mathbf{F}=\\nabla \\times \\mathbf{G}\\) for some vector field \\(\\mathbf{G}\\) in \\(E\\), of class \\(\\mathscr{B}^{\\prime \\prime}\\)\nProof If we compare the definitions of \\(\\nabla u, \\nabla \\times \\mathbf{F}\\), and \\(\\nabla \\cdot \\mathbf{F}\\) with the differential forms \\(\\lambda_{F}\\) and \\(\\omega_{F}\\) given by (124) and (125), we obtain the following four statements:\n\\[ \\begin{aligned} \u0026amp; \\mathbf{F}=\\nabla u \\quad \\text { if and only if } \\quad \\lambda_{\\mathbf{F}}=d u \\text {. } \\\\ \u0026amp; \\nabla \\times \\mathbf{F}=\\mathbf{0} \\quad \\text { if and only if } d \\lambda_{\\mathbf{F}}=0 \\text {. } \\\\ \u0026amp; \\mathbf{F}=\\nabla \\times \\mathbf{G} \\quad \\text { if and only if } \\omega_{\\mathbf{F}}=d \\lambda_{\\mathbf{G}} \\text {. } \\\\ \u0026amp; \\nabla \\cdot \\mathbf{F}=0 \\quad \\text { if and only if } d \\omega_{\\mathbf{F}}=0 . \\end{aligned} \\]\nNow if \\(F=\\nabla u\\), then \\(\\lambda_{F}=d u\\), hence \\(d \\lambda_{F}=d^{2} u=0\\) (Theorem 10.20), which means that \\(\\nabla \\times \\mathbf{F}=\\mathbf{0}\\). Thus (a) is proved.\nAs regards \\(\\left(a^{\\prime}\\right)\\), the hypothesis amounts to saying that \\(d \\lambda_{F}=0\\) in \\(E\\). By Theorem \\(10.40, \\lambda_{F}=d u\\) for some 0 -form \\(u\\). Hence \\(F=\\nabla u\\).\nThe proofs of \\((b)\\) and \\(\\left(b^{\\prime}\\right)\\) follow exactly the same pattern.\n10.44 Volume elements The \\(k\\)-form\n\\[ d x_{1} \\wedge \\cdots \\wedge d x_{k} \\]\nis called the volume element in \\(R^{k}\\). It is often denoted by \\(d V\\) (or by \\(d V_{k}\\) if it seems desirable to indicate the dimension explicitly), and the notation\n\\[ \\begin{equation} \\int_{\\Phi} f(\\mathbf{x}) d x_{1} \\wedge \\cdots \\wedge d x_{k}=\\int_{\\Phi} f d V \\label{eq:126} \\end{equation} \\]\nis used when \\(\\Phi\\) is a positively oriented \\(k\\)-surface in \\(R^{k}\\) and \\(f\\) is a continuous function on the range of \\(\\Phi\\).\nThe reason for using this terminology is very simple: If \\(D\\) is a parameter domain in \\(R^{k}\\), and if \\(\\Phi\\) is a 1-1 \\(\\mathscr{C}^{\\prime}\\)-mapping of \\(D\\) into \\(R^{k}\\), with positive Jacobian \\(J_{\\Phi}\\), then the left side of \\((126)\\) is\n\\[ \\int_{D} f(\\Phi(\\mathbf{u})) J_{\\Phi}(\\mathbf{u}) d \\mathbf{u}=\\int_{\\Phi(D)} f(\\mathbf{x}) d \\mathbf{x} \\]\nby (35) and Theorem \\(10.9 .\\)\nIn particular, when \\(f=1,(126)\\) defines the volume of \\(\\Phi\\). We already saw a special case of this in (36).\nThe usual notation for \\(d V_{2}\\) is \\(d A\\).\n10.45 Green’s theorem Suppose \\(E\\) is an open set in \\(R^{2}, \\alpha \\in \\mathscr{C}^{\\prime}(E), \\beta \\in \\mathscr{C}^{\\prime}(E)\\), and \\(\\Omega\\) is a closed subset of \\(E\\), with positively oriented boundary \\(\\partial \\Omega\\), as described in Sec. 10.31. Then\n\\[ \\begin{equation} \\int_{\\partial \\Omega}(\\alpha d x+\\beta d y)=\\int_{\\Omega}\\left(\\frac{\\partial \\beta}{\\partial x}-\\frac{\\partial \\alpha}{\\partial y}\\right) d A . \\label{eq:127} \\end{equation} \\]\n\\[ \\begin{aligned} \u0026amp;\\text { Proof Put } \\lambda=\\alpha d x+\\beta d y . \\text { Then } \\\\ \u0026amp;d \\lambda=\\left(D_{2} \\alpha\\right) d y \\wedge d x+\\left(D_{1} \\beta\\right) d x \\wedge d y \\\\ \u0026amp;=\\left(D_{1} \\beta-D_{2} \\alpha\\right) d A, \\end{aligned} \\]\nand (127) is the same as\n\\[ \\int_{\\partial \\Omega} \\lambda=\\int_{\\Omega} d \\lambda, \\]\nwhich is true by Theorem \\(10.33\\).\nWith \\(\\alpha(x, y)=-y\\) and \\(\\beta(x, y)=x,(127)\\) becomes\n\\[ \\begin{equation} \\frac{1}{2} \\int_{\\partial \\Omega}(x d y-y d x)=A(\\Omega), \\label{eq:128} \\end{equation} \\]\nthe area of \\(\\Omega\\).\nWith \\(\\alpha=0, \\beta=x\\), a similar formula is obtained. Example \\(10.12(b)\\) contains a special case of this.\n10.46 Area elements in \\(R^{3}\\) Let \\(\\Phi\\) be a 2-surface in \\(R^{3}\\), of class \\(\\mathscr{C}^{\\prime}\\), with parameter domain \\(D \\subset R^{2}\\). Associate with each point \\((u, v) \\in D\\) the vector\n\\[ \\begin{equation} \\mathbf{N}(u, v)=\\frac{\\partial(y, z)}{\\partial(u, v)} \\mathbf{e}_{1}+\\frac{\\partial(z, x)}{\\partial(u, v)} \\mathbf{e}_{2}+\\frac{\\partial(x, y)}{\\partial(u, v)} \\mathbf{e}_{3} . \\label{eq:129} \\end{equation} \\]\nThe Jacobians in (129) correspond to the equation\n\\[ \\begin{equation} (x, y, z)=\\Phi(u, v) \\text {. } \\label{eq:130} \\end{equation} \\]\nIf \\(f\\) is a continuous function on \\(\\Phi(D)\\), the area integral of \\(f\\) over \\(\\Phi\\) is defined to be\n\\[ \\begin{equation} \\int_{\\Phi} f d A=\\int_{D} f(\\Phi(u, v))|\\mathbf{N}(u, v)| d u d v \\label{eq:131} \\end{equation} \\]\nIn particular, when \\(f=1\\) we obtain the area of \\(\\Phi\\), namely,\n\\[ \\begin{equation} A(\\Phi)=\\int_{D}|\\mathbf{N}(u, v)| d u d v \\label{eq:132} \\end{equation} \\]\nThe following discussion will show that (131) and its special case (132) are reasonable definitions. It will also describe the geometric features of the vector \\(\\mathbf{N}\\).\nWrite \\(\\Phi=\\varphi_{1} \\mathbf{e}_{1}+\\varphi_{2} \\mathbf{e}_{2}+\\varphi_{3} \\mathbf{e}_{3}\\), fix a point \\(p_{0}=\\left(u_{0}, v_{0}\\right) \\in D\\), put \\(\\mathbf{N}=\\mathbf{N}\\left(\\mathbf{p}_{0}\\right)\\), put\n\\[ \\alpha_{i}=\\left(D_{1} \\varphi_{i}\\right)\\left(\\mathbf{p}_{0}\\right), \\quad \\beta_{i}=\\left(D_{2} \\varphi_{i}\\right)\\left(\\mathbf{p}_{0}\\right) \\quad(i=1,2,3) \\]\nand let \\(T \\in L\\left(R^{2}, R^{3}\\right)\\) be the linear transformation given by\n\\[ T(u, v)=\\sum_{i=1}^{3}\\left(\\alpha_{i} u+\\beta_{i} v\\right) \\mathbf{e}_{i} . \\]\nNote that \\(T=\\Phi^{\\prime}\\left(\\mathbf{p}_{0}\\right)\\), in accordance with Definition 9.11.\nLet us now assume that the rank of \\(T\\) is 2 . (If it is 1 or 0 , then \\(\\mathbf{N}=\\mathbf{0}\\), and the tangent plane mentioned below degenerates to a line or to a point.) The range of the affine mapping\n\\[ (u, v) \\rightarrow \\Phi\\left(\\mathbf{p}_{0}\\right)+T(u, v) \\]\nis then a plane \\(\\Pi\\), called the tangent plane to \\(\\Phi\\) at \\(\\mathbf{p}_{0}\\). [One would like to call \\(\\Pi\\) the tangent plane at \\(\\Phi\\left(p_{0}\\right)\\), rather than at \\(p_{0}\\); if \\(\\Phi\\) is not one-to-one, this runs into difficulties.]\nIf we use (133) in (129), we obtain\n\\[ \\mathbf{N}=\\left(\\alpha_{2} \\beta_{3}-\\alpha_{3} \\beta_{2}\\right) \\mathrm{e}_{1}+\\left(\\alpha_{3} \\beta_{1}-\\alpha_{1} \\beta_{3}\\right) \\mathrm{e}_{2}+\\left(\\alpha_{1} \\beta_{2}-\\alpha_{2} \\beta_{1}\\right) \\mathbf{e}_{3}, \\]\nand (134) shows that\n\\[ T \\mathrm{e}_{1}=\\sum_{i=1}^{3} \\alpha_{i} \\mathbf{e}_{i}, \\quad T \\mathrm{e}_{2}=\\sum_{i=1}^{3} \\beta_{i} \\mathbf{e}_{i} . \\]\nA straightforward computation now leads to\n\\[ \\mathbf{N} \\cdot\\left(T \\mathrm{e}_{1}\\right)=0=\\mathbf{N} \\cdot\\left(T \\mathbf{e}_{2}\\right) \\text {. } \\]\nHence \\(\\mathbf{N}\\) is perpendicular to \\(\\Pi\\). It is therefore called the normal to \\(\\Phi\\) at \\(\\mathbf{p}_{0}\\). A second property of \\(\\mathbf{N}\\), also verified by a direct computation based on (135) and (136), is that the determinant of the linear transformation of \\(R^{3}\\) that takes \\(\\left\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right\\}\\) to \\(\\left\\{\\mathbf{e}_{1}, T \\mathbf{e}_{2}, \\mathbf{N}\\right\\}\\) is \\(|\\mathbf{N}|^{2}\u0026gt;0\\) (Exercise 30). The 3-simplex\n\\[ \\left[0, T \\mathrm{e}_{1}, T \\mathrm{e}_{2}, \\mathrm{~N}\\right] \\]\nis thus positively oriented.\nThe third property of \\(\\mathbf{N}\\) that we shall use is a consequence of the first two: The above-mentioned determinant, whose value is \\(|\\mathbf{N}|^{2}\\), is the volume of the parallelepiped with edges \\(\\left[0, T e_{1}\\right],\\left[0, T e_{2}\\right],[0, N]\\). By \\((137),[0, N]\\) is perpendicular to the other two edges. The area of the parallelogram with vertices\n\\[ 0, T e_{1}, T e_{2}, T\\left(e_{1}+e_{2}\\right) \\]\nis therefore \\(|\\mathbf{N}|\\).\nThis parallelogram is the image under \\(T\\) of the unit square in \\(R^{2}\\). If \\(E\\) is any rectangle in \\(R^{2}\\), it follows (by the linearity of \\(T\\) ) that the area of the parallelogram \\(T(E)\\) is\n\\[ A(T(E))=|\\mathrm{N}| A(E)=\\int_{E}\\left|\\mathrm{~N}\\left(u_{0}, v_{0}\\right)\\right| d u d v . \\]\nWe conclude that (132) is correct when \\(\\Phi\\) is affine. To justify the definition (132) in the general case, divide \\(D\\) into small rectangles, pick a point \\(\\left(u_{0}, v_{0}\\right)\\) in each, and replace \\(\\Phi\\) in each rectangle by the corresponding tangent plane. The sum of the areas of the resulting parallelograms, obtained via (140), is then an approximation to \\(A(\\Phi)\\). Finally, one can justify (131) from (132) by approximating \\(f\\) by step functions.\n10.47 Example Let \\(0\u0026lt;a\u0026lt;b\\) be fixed. Let \\(K\\) be the 3-cell determined by \\(0 \\leq t \\leq a, \\quad 0 \\leq u \\leq 2 \\pi, \\quad 0 \\leq v \\leq 2 \\pi\\)\nThe equations\n\\[ \\begin{aligned} \u0026amp;x=t \\cos u \\\\ \u0026amp;y=(b+t \\sin u) \\cos v \\\\ \u0026amp;z=(b+t \\sin u) \\sin v \\end{aligned} \\]\ndescribe a mapping \\(\\Psi\\) of \\(R^{3}\\) into \\(R^{3}\\) which is \\(1-1\\) in the interior of \\(K\\), such that \\(\\Psi(K)\\) is a solid torus. Its Jacobian is\n\\[ J_{\\Psi}=\\frac{\\partial(x, y, z)}{\\partial(t, u, v)}=t(b+t \\sin u) \\]\nwhich is positive on \\(K\\), except on the face \\(t=0\\). If we integrate \\(J_{\\Psi}\\) over \\(K\\), we obtain\n\\[ \\operatorname{vol}(\\Psi(K))=2 \\pi^{2} a^{2} b \\]\nas the volume of our solid torus.\nNow consider the 2-chain \\(\\Phi=\\partial \\Psi\\). (See Exercise 19.) \\(\\Psi\\) maps the faces \\(u=0\\) and \\(u=2 \\pi\\) of \\(K\\) onto the same cylindrical strip, but with opposite orientations. \\(\\Psi\\) maps the faces \\(v=0\\) and \\(v=2 \\pi\\) onto the same circular disc, but with opposite orientations. \\(\\Psi\\) maps the face \\(t=0\\) onto a circle, which contributes 0 to the 2-chain \\(\\partial \\Psi\\). (The relevant Jacobians are 0.) Thus \\(\\Phi\\) is simply the 2-surface obtained by setting \\(t=a\\) in (141), with parameter domain \\(D\\) the square defined by \\(0 \\leq u \\leq 2 \\pi, 0 \\leq v \\leq 2 \\pi\\). vector\nAccording to (129) and (141), the normal to \\(\\Phi\\) at \\((u, v) \\in D\\) is thus the\n\\[ \\mathbf{N}(u, v)=a(b+a \\sin u) \\mathbf{n}(u, v) \\]\nwhere\n\\[ \\mathbf{n}(u, v)=(\\cos u) \\mathbf{e}_{1}+(\\sin u \\cos v) \\mathbf{e}_{2}+(\\sin u \\sin v) \\mathbf{e}_{3} . \\]\nSince \\(|\\mathbf{n}(u, v)|=1\\), we have \\(|\\mathbf{N}(u, v)|=a(b+a \\sin u)\\), and if we integrate this over \\(D,(131)\\) gives\n\\[ A(\\Phi)=4 \\pi^{2} a b \\]\nas the surface area of our torus.\nIf we think of \\(\\mathbf{N}=\\mathbf{N}(u, v)\\) as a directed line segment, pointing from \\(\\Phi(u, v)\\) to \\(\\Phi(u, v)+\\mathbf{N}(u, v)\\), then \\(\\mathbf{N}\\) points outward, that is to say, away from \\(\\Psi(K)\\). This is so because \\(\\mathrm{J}_{\\Psi}\u0026gt;0\\) when \\(t=a\\).\nFor example, take \\(u=v=\\pi / 2, t=a\\). This gives the largest value of \\(z\\) on \\(\\Psi(K)\\), and \\(\\mathbf{N}=a(b+a) \\mathrm{e}_{3}\\) points “upward” for this choice of \\((u, v)\\).\n10.48 Integrals of 1-forms in \\(R^{3}\\) Let \\(\\gamma\\) be a \\(\\mathscr{C}^{\\prime}\\)-curve in an open set \\(E \\subset R^{3}\\), with parameter interval \\([0,1]\\), let \\(\\mathbf{F}\\) be a vector field in \\(E\\), as in Sec. \\(10.42\\), and define \\(\\lambda_{F}\\) by (124). The integral of \\(\\lambda_{F}\\) over \\(\\gamma\\) can be rewritten in a certain way which we now describe.\nFor any \\(u \\in[0,1]\\),\n\\[ \\gamma^{\\prime}(u)=\\gamma_{1}^{\\prime}(u) \\mathbf{e}_{1}+\\gamma_{2}^{\\prime}(u) \\mathbf{e}_{2}+\\gamma_{3}^{\\prime}(u) \\mathbf{e}_{3} \\]\nis called the tangent vector to \\(\\gamma\\) at \\(u\\). We define \\(\\mathbf{t}=\\mathbf{t}(u)\\) to be the unit vector in the direction of \\(\\gamma^{\\prime}(u)\\). Thus\n\\[ \\gamma^{\\prime}(u)=\\left|\\gamma^{\\prime}(u)\\right| \\mathbf{t}(u) . \\]\n[If \\(\\gamma^{\\prime}(u)=\\mathbf{0}\\) for some \\(u\\), put \\(\\mathbf{t}(u)=\\mathbf{e}_{1}\\); any other choice would do just as well.] By (35),\n\\[ \\begin{aligned} \\int_{\\gamma} \\lambda_{\\mathbf{F}} \u0026amp;=\\sum_{i=1}^{3} \\int_{0}^{1} F_{i}(\\gamma(u)) \\gamma_{i}^{\\prime}(u) d u \\\\ \u0026amp;=\\int_{0}^{1} \\mathbf{F}(\\gamma(u)) \\cdot \\gamma^{\\prime}(u) d u \\\\ \u0026amp;=\\int_{0}^{1} \\mathbf{F}(\\gamma(u)) \\cdot \\mathbf{t}(u)\\left|\\gamma^{\\prime}(u)\\right| d u . \\end{aligned} \\]\nTheorem \\(6.27\\) makes it reasonable to call \\(\\left|\\gamma^{\\prime}(u)\\right| d u\\) the element of arc length along \\(\\gamma\\). A customary notation for it is \\(d s\\), and (142) is rewritten in the form\n\\[ \\int_{\\gamma} \\lambda_{F}=\\int_{\\gamma}(\\mathbf{F} \\cdot \\mathrm{t}) d s . \\]\nSince \\(t\\) is a unit tangent vector to \\(\\gamma, \\mathbf{F} \\cdot t\\) is called the tangential component of \\(\\mathbf{F}\\) along \\(\\gamma\\). The right side of (143) should be regarded as just an abbreviation for the last integral in (142). The point is that \\(\\mathbf{F}\\) is defined on the range of \\(\\gamma\\), but \\(\\mathbf{t}\\) is defined on \\([0,1]\\); thus \\(\\mathbf{F} \\cdot \\mathbf{t}\\) has to be properly interpreted. Of course, when \\(\\gamma\\) is one-to-one, then \\(t(u)\\) can be replaced by \\(t(\\gamma(u))\\), and this difficulty disappears.\n10.49 Integrals of 2-forms in \\(R^{3}\\) Let \\(\\Phi\\) be a 2-surface in an open set \\(E \\subset R^{3}\\), of class \\(\\mathscr{C}^{\\prime}\\), with parameter domain \\(D \\subset R^{2}\\). Let \\(\\mathbf{F}\\) be a vector field in \\(E\\), and define \\(\\omega_{\\mathbf{F}}\\) by (125). As in the preceding section, we shall obtain a different representation of the integral of \\(\\omega_{F}\\) over \\(\\Phi\\).\nBy (35) and (129),\n\\[ \\begin{aligned} \\int_{\\Phi} \\omega_{\\mathbf{F}} \u0026amp;=\\int_{\\Phi}\\left(F_{1} d y \\wedge d z+F_{2} d z \\wedge d x+F_{3} d x \\wedge d y\\right) \\\\ \u0026amp;=\\int_{D}\\left\\{\\left(F_{1} \\circ \\Phi\\right) \\frac{\\partial(y, z)}{\\partial(u, v)}+\\left(F_{2} \\circ \\Phi\\right) \\frac{\\partial(z, x)}{\\partial(u, v)}+\\left(F_{3} \\circ \\Phi\\right) \\frac{\\partial(x, y)}{\\partial(u, v)}\\right\\} d u d v \\\\ \u0026amp;=\\int_{D} \\mathbf{F}(\\Phi(u, v)) \\cdot \\mathbf{N}(u, v) d u d v \\end{aligned} \\]\nNow let \\(\\mathbf{n}=\\mathbf{n}(u, v)\\) be the unit vector in the direction of \\(\\mathbf{N}(u, v)\\). [If \\(\\mathbf{N}(u, v)=\\mathbf{0}\\) for some \\((u, v) \\in D\\), take \\(\\mathbf{n}(u, v)=\\mathbf{e}_{1}\\).] Then \\(\\mathbf{N}=|\\mathbf{N}| \\mathbf{n}\\), and therefore the last integral becomes\n\\[ \\int_{D} \\mathbf{F}(\\Phi(u, v)) \\cdot \\mathbf{n}(u, v)|\\mathbf{N}(u, v)| d u d v \\]\nBy (131), we can finally write this in the form\n\\[ \\int_{\\Phi} \\omega_{F}=\\int_{\\Phi}(\\mathbf{F} \\cdot \\mathbf{n}) d A . \\]\nWith regard to the meaning of \\(\\mathbf{F} \\cdot \\mathbf{n}\\), the remark made at the end of Sec. \\(10.48\\) applies here as well.\nWe can now state the original form of Stokes’ theorem.\n\\(10.50\\) Stokes’ formula If \\(\\mathbf{F}\\) is a vector field of class \\(\\mathscr{C}^{\\prime}\\) in an open set \\(E \\subset R^{3}\\), and if \\(\\Phi\\) is a 2-surface of class \\(\\mathscr{C}^{\\prime \\prime}\\) in \\(E\\), then\n\\[ \\int_{\\Phi}(\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{n} d A=\\int_{\\partial \\Phi}(\\mathbf{F} \\cdot \\mathbf{t}) d s . \\]\nProof Put \\(\\mathbf{H}=\\nabla \\times \\mathbf{F}\\). Then, as in the proof of Theorem 10.43, we have\n\\[ \\omega_{\\mathbf{H}}=d \\lambda_{\\mathbf{F}} . \\]\nHence\n\\[ \\begin{aligned} \\int_{\\Phi}(\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{n} d A \u0026amp;=\\int_{\\Phi}(\\mathbf{H} \\cdot \\mathbf{n}) d A=\\int_{\\Phi} \\omega_{\\mathbf{H}} \\\\ \u0026amp;=\\int_{\\Phi} d \\lambda_{F}=\\int_{\\partial \\Phi} \\lambda_{F}=\\int_{\\partial \\Phi}(\\mathbf{F} \\cdot \\mathrm{t}) d s . \\end{aligned} \\]\nHere we used the definition of \\(\\mathbf{H}\\), then (144) with \\(\\mathbf{H}\\) in place of \\(\\mathbf{F}\\), then (146), then-the main step-Theorem 10.33, and finally (143), extended in the obvious way from curves to 1-chains.\n10.51 The divergence theorem If \\(\\mathbf{F}\\) is a vector field of class \\(\\mathscr{C}^{\\prime}\\) in an open set \\(E \\subset R^{3}\\), and if \\(\\Omega\\) is a closed subset of \\(E\\) with positively oriented boundary \\(\\partial \\Omega\\) (as described in Sec. 10.31) then\n\\[ \\int_{\\Omega}(\\nabla \\cdot \\mathbf{F}) d V=\\int_{\\partial \\Omega}(\\mathbf{F} \\cdot \\mathbf{n}) d A \\]\nProof By (125),\n\\[ d \\omega_{\\mathbf{F}}=(\\nabla \\cdot \\mathbf{F}) d x \\wedge d y \\wedge d z=(\\nabla \\cdot \\mathbf{F}) d V . \\]\nHence\n\\[ \\int_{\\Omega}(\\nabla \\cdot \\mathbf{F}) d V=\\int_{\\Omega} d \\omega_{\\mathbf{F}}=\\int_{\\partial \\Omega} \\omega_{\\mathbf{F}}=\\int_{\\partial \\Omega}(\\mathbf{F} \\cdot \\mathbf{n}) d A, \\]\nby Theorem \\(10.33\\), applied to the 2 -form \\(\\omega_{F}\\), and (144).\n","date":"2022-08-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/9-vector-analysis/","section":"baby rudin","tags":null,"title":"9 VECTOR ANALYSIS"},{"categories":null,"contents":"We shall now develop some of the machinery that is needed for the \\(n\\)-dimensional version of the fundamental theorem of calculus which is usually called Stokes’ theorem. The original form of Stokes’ theorem arose in applications of vector analysis to electromagnetism and was stated in terms of the curl of a vector field. Green’s theorem and the divergence theorem are other special cases. These topics are briefly discussed at the end of the chapter.\nIt is a curious feature of Stokes’ theorem that the only thing that is difficult about it is the elaborate structure of definitions that are needed for its statement. These definitions concern differential forms, their derivatives, boundaries, and orientation. Once these concepts are understood, the statement of the theorem is very brief and succinct, and its proof presents little difficulty.\nUp to now we have considered derivatives of functions of several variables only for functions defined in open sets. This was done to avoid difficulties that can occur at boundary points. It will now be convenient, however, to discuss differentiable functions on compact sets. We therefore adopt the following convention:\nTo say that \\(\\mathbf{f}\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping (or a \\(\\mathscr{C}^{\\prime \\prime}\\)-mapping) of a compact set \\(D \\subset R^{k}\\) into \\(R^{n}\\) means that there is a \\(\\mathscr{C}^{\\prime}\\)-mapping (or a \\(\\mathscr{C}^{\\prime \\prime}\\)-mapping) \\(\\mathbf{g}\\) of an open set \\(W \\subset R^{k}\\) into \\(R^{n}\\) such that \\(D \\subset W\\) and such that \\(\\mathbf{g}(\\mathbf{x})=\\mathbf{f}(\\mathbf{x})\\) for all \\(\\mathbf{x} \\in D\\)\n10.10 Definition Suppose \\(E\\) is an open set in \\(R^{n}\\). A \\(k\\)-surface in \\(E\\) is a \\(\\mathscr{C}^{\\prime}\\) mapping \\(\\Phi\\) from a compact set \\(D \\subset R^{k}\\) into \\(E\\).\n\\(D\\) is called the parameter domain of \\(\\Phi\\). Points of \\(D\\) will be denoted by \\(\\mathbf{u}=\\left(u_{1}, \\ldots, u_{k}\\right)\\)\nWe shall confine ourselves to the simple situation in which \\(D\\) is either a \\(k\\)-cell or the \\(k\\)-simplex \\(Q^{k}\\) described in Example 10.4. The reason for this is that we shall have to integrate over \\(D\\), and we have not yet discussed integration over more complicated subsets of \\(R^{k}\\). It will be seen that this restriction on \\(D\\) (which will be tacitly made from now on) entails no significant loss of generality in the resulting theory of differential forms.\nWe stress that \\(k\\)-surfaces in \\(E\\) are defined to be mappings into \\(E\\), not subsets of \\(E\\). This agrees with our earlier definition of curves (Definition 6.26). In fact, 1-surfaces are precisely the same as continuously differentiable curves.\n10.11 Definition Suppose \\(E\\) is an open set in \\(R^{n}\\). A differential form of order \\(k \\geq 1\\) in \\(E\\) (briefly, a \\(k\\)-form in \\(E\\) ) is a function \\(\\omega\\), symbolically represented by the sum\n\\[ \\begin{equation} \\omega=\\sum a_{i_{1} \\cdots i_{k}}(\\mathbf{x}) d x_{i_{1}} \\wedge \\cdots \\wedge d x_{i_{k}} \\end{equation} \\]\n(the indices \\(i_{1}, \\ldots, i_{k}\\) range independently from 1 to \\(n\\) ), which assigns to each \\(k\\)-surface \\(\\Phi\\) in \\(E\\) a number \\(\\omega(\\Phi)=\\int_{\\Phi} \\omega\\), according to the rule\n\\[ \\begin{equation} \\int_{\\Phi} \\omega=\\int_{D} \\sum a_{i_{1}} \\cdots_{i_{k}}(\\Phi(\\mathbf{u})) \\frac{\\partial\\left(x_{i_{1}}, \\ldots, x_{i_{k}}\\right)}{\\partial\\left(u_{1}, \\ldots, u_{k}\\right)} d \\mathbf{u} \\end{equation} \\]\nwhere \\(D\\) is the parameter domain of \\(\\Phi\\).\nThe functions \\(a_{i_{1} \\cdots i_{k}}\\) are assumed to be real and continuous in \\(E\\). If \\(\\phi_{1}, \\ldots, \\phi_{n}\\) are the components of \\(\\Phi\\), the Jacobian in (2) is the one determined by the mapping\n\\[ \\left(u_{1}, \\ldots, u_{k}\\right) \\rightarrow\\left(\\phi_{i_{1}}(\\mathbf{u}), \\ldots, \\phi_{i_{k}}(\\mathbf{u})\\right) . \\]\nNote that the right side of (2) is an integral over \\(D\\), as defined in Definition \\(10.1\\) (or Example 10.4) and that (2) is the definition of the symbol \\(\\int_{\\Phi} \\omega\\). A \\(k\\)-form \\(\\omega\\) is said to be of class \\(\\mathscr{C}^{\\prime}\\) or \\(\\mathscr{C}^{\\prime \\prime}\\) if the functions \\(a_{i_{1}} \\cdots i_{k}\\) in (1) are all of class \\(\\mathscr{C}^{\\prime}\\) or \\(\\mathscr{C}^{\\prime \\prime}\\).\nA 0-form in \\(E\\) is defined to be a continuous function in \\(E\\).\n10.12 Examples\nLet \\(\\gamma\\) be a 1-surface (a curve of class \\(\\mathscr{C}^{\\prime}\\) ) in \\(R^{3}\\), with parameter domain \\([0,1]\\). Write \\((x, y, z)\\) in place of \\(\\left(x_{1}, x_{2}, x_{3}\\right)\\), and put\n\\[ \\omega=x d y+y d x \\text {. } \\]\nThen\n\\[ \\int_{\\gamma} \\omega=\\int_{0}^{1}\\left[\\gamma_{1}(t) \\gamma_{2}^{\\prime}(t)+\\gamma_{2}(t) \\gamma_{1}^{\\prime}(t)\\right] d t=\\gamma_{1}(1) \\gamma_{2}(1)-\\gamma_{1}(0) \\gamma_{2}(0) \\text {. } \\]\nNote that in this example \\(\\int_{\\gamma} \\omega\\) depends only on the initial point \\(\\gamma(0)\\) and on the end point \\(\\gamma(1)\\) of \\(\\gamma\\). In particular, \\(\\int_{\\gamma} \\omega=0\\) for every closed curve \\(\\gamma\\). (As we shall see later, this is true for every 1-form \\(\\omega\\) which is exact.)\nIntegrals of 1-forms are often called line integrals.\nFix \\(a\u0026gt;0, b\u0026gt;0\\), and define \\[ \\gamma(t)=(a \\cos t, b \\sin t) \\quad(0 \\leq t \\leq 2 \\pi), \\]\nso that \\(\\gamma\\) is a closed curve in \\(R^{2}\\). (Its range is an ellipse.) Then\n\\[ \\int_{\\gamma} x d y=\\int_{0}^{2 \\pi} a b \\cos ^{2} t d t=\\pi a b, \\]\nwhereas\n\\[ \\int_{\\gamma} y d x=-\\int_{0}^{2 \\pi} a b \\sin ^{2} t d t=-\\pi a b . \\]\nNote that \\(\\int_{\\gamma} x d y\\) is the area of the region bounded by \\(\\gamma\\). This is a special case of Green’s theorem.\nLet \\(D\\) be the 3-cell defined by \\[ 0 \\leq r \\leq 1, \\quad 0 \\leq \\theta \\leq \\pi, \\quad 0 \\leq \\varphi \\leq 2 \\pi . \\]\nDefine \\(\\Phi(r, \\theta, \\varphi)=(x, y, z)\\), where\n\\[ \\begin{aligned} \u0026amp;x=r \\sin \\theta \\cos \\varphi \\\\ \u0026amp;y=r \\sin \\theta \\sin \\varphi \\\\ \u0026amp;z=r \\cos \\theta . \\end{aligned} \\]\nThen\n\\[ J_{\\Phi}(r, \\theta, \\varphi)=\\frac{\\partial(x, y, z)}{\\partial(r, \\theta, \\varphi)}=r^{2} \\sin \\theta \\]\nHence\n\\[ \\begin{equation} \\int_{\\Phi} d x \\wedge d y \\wedge d z=\\int_{D} J_{\\Phi}=\\frac{4 \\pi}{3} \\label{eq:36} \\end{equation} \\]\nNote that \\(\\Phi\\) maps \\(D\\) onto the closed unit ball of \\(R^{3}\\), that the mapping is 1-1 in the interior of \\(D\\) (but certain boundary points are identified by \\(\\Phi)\\), and that the integral \\(\\ref{eq:36}\\) is equal to the volume of \\(\\Phi(D)\\).\n10.13 Elementary properties Let \\(\\omega, \\omega_{1}, \\omega_{2}\\) be \\(k\\)-forms in \\(E\\). We write \\(\\omega_{1}=\\omega_{2}\\) if and only if \\(\\omega_{1}(\\Phi)=\\omega_{2}(\\Phi)\\) for every \\(k\\)-surface \\(\\Phi\\) in \\(E\\). In particular, \\(\\omega=0\\) means that \\(\\omega(\\Phi)=0\\) for every \\(k\\)-surface \\(\\Phi\\) in \\(E\\). If \\(c\\) is a real number, then \\(c \\omega\\) is the \\(k\\)-form defined by\n\\[ \\begin{equation} \\int_{\\Phi} c \\omega=c \\int_{\\Phi} \\omega \\end{equation} \\label{eq:37} \\]\nand \\(\\omega=\\omega_{1}+\\omega_{2}\\) means that\n\\[ \\begin{equation} \\int_{\\Phi} \\omega=\\int_{\\Phi} \\omega_{1}+\\int_{\\Phi} \\omega_{2} \\label{eq:28} \\end{equation} \\]\nfor every \\(k\\)-surface \\(\\Phi\\) in \\(E\\). As a special case of (37), note that \\(-\\omega\\) is defined so that\n\\[ \\begin{equation} \\int_{\\Phi}(-\\omega)=-\\int_{\\Phi} d \\omega \\label{eq:39} \\end{equation} \\]\nConsider a \\(k\\)-form\n\\[ \\begin{equation} \\omega=a(\\mathbf{x}) d x_{i_{1}} \\wedge \\cdots \\wedge d x_{i_{k}} \\label{eq:40} \\end{equation} \\]\nand let \\(\\bar{\\omega}\\) be the \\(k\\)-form obtained by interchanging some pair of subscripts in (40). If (2) and (39) are combined with the fact that a determinant changes sign if two of its rows are interchanged, we see that\n\\[ \\bar{\\omega}=-\\omega . \\]\nAs a special case of this, note that the anticommutative relation\n\\[ d x_{i} \\wedge d x_{j}=-d x_{j} \\wedge d x_{i} \\]\nholds for all \\(i\\) and \\(j\\). In particular,\n\\[ d x_{i} \\wedge d x_{i}=0 \\quad(i=1, \\ldots, n) . \\]\nMore generally, let us return to (40), and assume that \\(i_{r}=i_{s}\\) for some \\(r \\neq s\\). If these two subscripts are interchanged, then \\(\\bar{\\omega}=\\omega\\), hence \\(\\omega=0\\), by (41).\nIn other words, if \\(\\omega\\) is given by (40), then \\(\\omega=0\\) unless the subscripts \\(i_{1}, \\ldots, i_{k}\\) are all distinct.\nIf \\(\\omega\\) is as in (34), the summands with repeated subscripts can therefore be omitted without changing \\(\\omega\\).\nIt follows that 0 is the only \\(k\\)-form in any open subset of \\(R^{n}\\), if \\(k\u0026gt;n\\).\nThe anticommutativity expressed by (42) is the reason for the inordinate amount of attention that has to be paid to minus signs when studying differential forms.\n10.14 Basic \\(k\\)-forms If \\(i_{1}, \\ldots, i_{k}\\) are integers such that \\(1 \\leq i_{1}\u0026lt;i_{2}\u0026lt;\\cdots\\) \\(\u0026lt;i_{k} \\leq n\\), and if \\(I\\) is the ordered \\(k\\)-tuple \\(\\left\\{i_{1}, \\ldots, i_{k}\\right\\}\\), then we call \\(I\\) an increasing \\(k\\)-index, and we use the brief notation\n\\[ d x_{I}=d x_{i_{1}} \\wedge \\cdots \\wedge d x_{i_{k}} . \\]\nThese forms \\(d x_{I}\\) are the so-called basic \\(k\\)-forms in \\(R^{n}\\).\nIt is not hard to verify that there are precisely \\(n ! / k !(n-k) !\\) basic \\(k\\)-forms in \\(R^{n}\\); we shall make no use of this, however.\nMuch more important is the fact that every \\(k\\)-form can be represented in terms of basic \\(k\\)-forms. To see this, note that every \\(k\\)-tuple \\(\\left\\{j_{1}, \\ldots, j_{k}\\right\\}\\) of distinct integers can be converted to an increasing \\(k\\)-index \\(J\\) by a finite number of interchanges of pairs; each of these amounts to a multiplication by \\(-1\\), as we saw in Sec. 10.13; hence\n\\[ d x_{j_{1}} \\wedge \\cdots \\wedge d x_{j_{k}}=\\varepsilon\\left(j_{1}, \\ldots, j_{k}\\right) d x_{J} \\]\nwhere \\(\\varepsilon\\left(j_{1}, \\ldots, j_{k}\\right)\\) is 1 or \\(-1\\), depending on the number of interchanges that are needed. In fact, it is easy to see that\n\\[ \\varepsilon\\left(j_{1}, \\ldots, j_{k}\\right)=s\\left(j_{1}, \\ldots, j_{k}\\right) \\]\nwhere \\(s\\) is as in Definition 9.33.\nFor example,\n\\[ d x_{1} \\wedge d x_{5} \\wedge d x_{3} \\wedge d x_{2}=-d x_{1} \\wedge d x_{2} \\wedge d x_{3} \\wedge d x_{5} \\]\nand\n\\[ d x_{4} \\wedge d x_{2} \\wedge d x_{3}=d x_{2} \\wedge d x_{3} \\wedge d x_{4} . \\]\nIf every \\(k\\)-tuple in (34) is converted to an increasing \\(k\\)-index, then we obtain the so-called standard presentation of \\(\\omega\\) :\n\\[ \\begin{equation} \\omega=\\sum_{I} b_{I}(\\mathbf{x}) d x_{I} . \\label{eq:47} \\end{equation} \\]\nThe summation in \\(\\ref{eq:47}\\) extends over all increasing \\(k\\)-indices \\(I\\). [Of course, every increasing \\(k\\)-index arises from many (from \\(k\\) !, to be precise) \\(k\\)-tuples. Each \\(b_{I}\\) in \\(\\ref{eq:47}\\) may thus be a sum of several of the coefficients that occur in (34).]\nFor example,\n\\[ x_{1} d x_{2} \\wedge d x_{1}-x_{2} d x_{3} \\wedge d x_{2}+x_{3} d x_{2} \\wedge d x_{3}+d x_{1} \\wedge d x_{2} \\]\nis a 2 -form in \\(R^{3}\\) whose standard presentation is\n\\[ \\left(1-x_{1}\\right) d x_{1} \\wedge d x_{2}+\\left(x_{2}+x_{3}\\right) d x_{2} \\wedge d x_{3} . \\]\nThe following uniqueness theorem is one of the main reasons for the introduction of the standard presentation of a \\(k\\)-form.\n10.15 Theorem Suppose\n\\[ \\begin{equation} \\omega=\\sum_{I} b_{I}(\\mathbf{x}) d x_{I} \\label{eq:48} \\end{equation} \\]\nis the standard presentation of a \\(k\\)-form \\(\\omega\\) in an open set \\(E \\subset R^{n} .\\) If \\(\\omega=0\\) in \\(E\\), then \\(b_{I}(\\mathbf{x})=0\\) for every increasing \\(k\\)-index \\(I\\) and for every \\(\\mathbf{x} \\in E\\).\nNote that the analogous statement would be false for sums such as \\(\\ref{eq:34}\\), since, for example,\n\\[ d x_{1} \\wedge d x_{2}+d x_{2} \\wedge d x_{1}=0 . \\]\nProof Assume, to reach a contradiction, that \\(b_{J}(\\mathbf{v})\u0026gt;0\\) for some \\(\\mathbf{v} \\in E\\) and for some increasing \\(k\\)-index \\(J=\\left\\{j_{1}, \\ldots, j_{k}\\right\\}\\). Since \\(b_{J}\\) is continuous, there exists \\(h\u0026gt;0\\) such that \\(b_{J}(\\mathbf{x})\u0026gt;0\\) for all \\(\\mathbf{x} \\in R^{n}\\) whose coordinates satisfy \\(\\left|x_{i}-v_{i}\\right| \\leq h\\). Let \\(D\\) be the \\(k\\)-cell in \\(R^{k}\\) such that \\(\\mathbf{u} \\in D\\) if and only if \\(\\left|u_{r}\\right| \\leq h\\) for \\(r=1, \\ldots, k\\). Define\n\\[ \\begin{equation} \\Phi(\\mathbf{u})=\\mathbf{v}+\\sum_{r=1}^{k} u_{r} \\mathbf{e}_{j_{r}} \\quad(\\mathbf{u} \\in D) . \\label{eq:49} \\end{equation} \\]\nThen \\(\\Phi\\) is a \\(k\\)-surface in \\(E\\), with parameter domain \\(D\\), and \\(b_{J}(\\Phi(\\mathbf{u}))\u0026gt;0\\) for every \\(\\mathbf{u} \\in D\\).\nWe claim that\n\\[ \\begin{equation} \\int_{\\Phi} \\omega=\\int_{D} b_{J}(\\Phi(\\mathbf{u})) d \\mathbf{u} \\label{eq:50} \\end{equation} \\]\nSince the right side of \\(\\ref{eq:50}\\) is positive, it follows that \\(\\omega(\\Phi) \\neq 0\\). Hence \\(\\ref{eq:50}\\) gives our contradiction.\nTo prove \\(\\ref{eq:50}\\), apply \\(\\ref{eq:35}\\) to the presentation \\(\\ref{eq:48}\\). More specifically, compute the Jacobians that occur in \\(\\ref{eq:35}\\). By \\(\\ref{eq:49}\\),\n\\[ \\frac{\\partial\\left(x_{j_{1}}, \\ldots, x_{j_{k}}\\right)}{\\partial\\left(u_{1}, \\ldots, u_{k}\\right)}=1 \\text {. } \\]\nFor any other increasing \\(k\\)-index \\(I \\neq J\\), the Jacobian is 0 , since it is the determinant of a matrix with at least one row of zeros.\n10.16 Products of basic \\(k\\)-forms Suppose\n\\[ \\begin{equation} I=\\left\\{i_{1}, \\ldots, i_{p}\\right\\}, \\quad J=\\left\\{j_{1}, \\ldots, j_{q}\\right\\} \\label{eq:51} \\end{equation} \\]\nwhere \\(1 \\leq i_{1}\u0026lt;\\cdots\u0026lt;i_{p} \\leq n\\) and \\(1 \\leq j_{1}\u0026lt;\\cdots\u0026lt;j_{q} \\leq n\\). The product of the corresponding basic forms \\(d x_{I}\\) and \\(d x_{J}\\) in \\(R^{n}\\) is a \\((p+q)\\)-form in \\(R^{n}\\), denoted by the symbol \\(d x_{I} \\wedge d x_{J}\\), and defined by\n\\[ \\begin{equation} d x_{I} \\wedge d x_{J}=d x_{i_{1}} \\wedge \\cdots \\wedge d x_{i_{p}} \\wedge d x_{j_{1}} \\wedge \\cdots \\wedge d x_{j_{q}} . \\label{eq:52} \\end{equation} \\]\nIf \\(I\\) and \\(J\\) have an element in common, then the discussion in Sec. \\(10.13\\) shows that \\(d x_{I} \\wedge d x_{J}=0\\).\nIf \\(I\\) and \\(J\\) have no element in common, let us write \\([I, J]\\) for the increasing \\((p+q)\\)-index which is obtained by arranging the members of \\(I \\cup J\\) in increasing order. Then \\(d x_{[I, J]}\\) is a basic \\((p+q)\\)-form. We claim that\n\\[ \\begin{equation} d x_{\\mathrm{I}} \\wedge d x_{J}=(-1)^{\\alpha} d x_{[I, J]} \\label{eq:53} \\end{equation} \\]\nwhere \\(\\alpha\\) is the number of differences \\(j_{t}-i_{s}\\) that are negative. (The number of positive differences is thus \\(p q-\\alpha .)\\)\nTo prove \\(\\ref{eq:53}\\), perform the following operations on the numbers\n\\[ \\begin{equation} i_{1}, \\ldots, i_{p} ; j_{1}, \\ldots, j_{q} . \\label{eq:54} \\end{equation} \\]\nMove \\(i_{p}\\) to the right, step by step, until its right neighbor is larger than \\(i_{p}\\). The number of steps is the number of subscripts \\(t\\) such that \\(i_{p}\u0026lt;j_{t}\\). (Note that 0 steps are a distinct possibility.) Then do the same for \\(i_{p-1}, \\ldots, i_{1}\\). The total number of steps taken is \\(\\alpha\\). The final arrangement reached is \\([I, J]\\). Each step, when applied to the right side of \\(\\ref{eq:52}\\), multiplies \\(d x_{I} \\wedge d x_{J}\\) by \\(-1\\). Hence \\(\\ref{eq:53}\\) holds.\nNote that the right side of \\(\\ref{eq:53}\\) is the standard presentation of \\(d x_{I} \\wedge d x_{J}\\). Next, let \\(K=\\left(k_{1}, \\ldots, k_{r}\\right)\\) be an increasing \\(r\\)-index in \\(\\{1, \\ldots, n\\}\\). We shall use \\(\\ref{eq:53}\\) to prove that\n\\[ \\begin{equation} \\left(d x_{I} \\wedge d x_{J}\\right) \\wedge d x_{K}=d x_{I} \\wedge\\left(d x_{J} \\wedge d x_{K}\\right) . \\label{eq:55} \\end{equation} \\]\nIf any two of the sets \\(I, J, K\\) have an element in common, then each side of \\(\\ref{eq:55}\\) is 0 , hence they are equal.\nSo let us assume that \\(I, J, K\\) are pairwise disjoint. Let \\([I, J, K]\\) denote the increasing \\((p+q+r)\\)-index obtained from their union. Associate \\(\\beta\\) with the ordered pair \\((J, K)\\) and \\(\\gamma\\) with the ordered pair \\((I, K)\\) in the way that \\(\\alpha\\) was associated with \\((I, J)\\) in \\(\\ref{eq:53}\\). The left side of \\(\\ref{eq:55}\\) is then\n\\[ (-1)^{\\alpha} d x_{[I, J]} \\wedge d x_{K}=(-1)^{\\alpha}(-1)^{\\beta+\\gamma} d x_{[I, J, K]} \\]\nby two applications of \\(\\ref{eq:53}\\), and the right side of \\(\\ref{eq:55}\\) is\n\\[ (-1)^{\\beta} d x_{I} \\wedge d x_{[J, K]}=(-1)^{\\beta}(-1)^{\\alpha+\\gamma} d x_{[I, J, K]} . \\]\nHence \\(\\ref{eq:55}\\) is correct.\n10.17 Multiplication Suppose \\(\\omega\\) and \\(\\lambda\\) are \\(p\\) - and \\(q\\)-forms, respectively, in some open set \\(E \\subset R^{n}\\), with standard presentations\n\\[ \\begin{equation} \\omega=\\sum_{I} b_{I}(\\mathbf{x}) d x_{I}, \\quad \\lambda=\\sum_{J} c_{J}(\\mathbf{x}) d x_{J} \\label{eq:56} \\end{equation} \\]\nwhere \\(I\\) and \\(J\\) range over all increasing \\(p\\)-indices and over all increasing \\(q\\)-indices taken from the set \\(\\{1, \\ldots, n\\}\\). Their product, denoted by the symbol \\(\\omega \\wedge \\lambda\\), is defined to be\n\\[ \\begin{equation} \\omega \\wedge \\lambda=\\sum_{I, J} b_{I}(\\mathbf{x}) c_{J}(\\mathbf{x}) d x_{I} \\wedge d x_{J} . \\label{eq:57} \\end{equation} \\]\nIn this sum, \\(I\\) and \\(J\\) range independently over their possible values, and \\(d x_{I} \\wedge d x_{J}\\) is as in Sec. 10.16. Thus \\(\\omega \\wedge \\lambda\\) is a \\((p+q)\\)-form in \\(E\\).\nIt is quite easy to see (we leave the details as an exercise) that the distributive laws\n\\[ \\left(\\omega_{1}+\\omega_{2}\\right) \\wedge \\lambda=\\left(\\omega_{1} \\wedge \\lambda\\right)+\\left(\\omega_{2} \\wedge \\lambda\\right) \\]\nand\n\\[ \\omega \\wedge\\left(\\lambda_{1}+\\lambda_{2}\\right)=\\left(\\omega \\wedge \\lambda_{1}\\right)+\\left(\\omega \\wedge \\lambda_{2}\\right) \\]\nhold, with respect to the addition defined in Sec. 10.13. If these distributive laws are combined with (55), we obtain the associative law\n\\[ \\begin{equation} (\\omega \\wedge \\lambda) \\wedge \\sigma=\\omega \\wedge(\\lambda \\wedge \\sigma) \\label{eq:58} \\end{equation} \\]\nfor arbitrary forms \\(\\omega, \\lambda, \\sigma\\) in \\(E\\).\nIn this discussion it was tacitly assumed that \\(p \\geq 1\\) and \\(q \\geq 1\\). The product of a 0 -form \\(f\\) with the \\(p\\)-form \\(\\omega\\) given by (56) is simply defined to be the \\(p\\)-form\n\\[ f \\omega=\\omega f=\\sum_{I} f(\\mathbf{x}) b_{I}(\\mathbf{x}) d x_{I} . \\]\nIt is customary to write \\(f \\omega\\), rather than \\(f \\wedge \\omega\\), when \\(f\\) is a 0 -form.\n10.18 Differentiation We shall now define a differentiation operator \\(d\\) which associates a \\((k+1)\\)-form \\(d \\omega\\) to each \\(k\\)-form \\(\\omega\\) of class \\(\\mathscr{C}^{\\prime}\\) in some open set \\(E \\subset R^{n}\\)\nA 0-form of class \\(\\mathscr{C}^{\\prime}\\) in \\(E\\) is just a real function \\(f \\in \\mathscr{C}^{\\prime}(E)\\), and we define\n\\[ \\begin{equation} d f=\\sum_{i=1}^{n}\\left(D_{i} f\\right)(\\mathbf{x}) d x_{i} . \\label{eq:59} \\end{equation} \\]\nIf \\(\\omega=\\Sigma b_{I}(\\mathbf{x}) d x_{I}\\) is the standard presentation of a \\(k\\)-form \\(\\omega\\), and \\(b_{I} \\in \\mathscr{C}^{\\prime}(E)\\) for each increasing \\(k\\)-index \\(I\\), then we define\n\\[ \\begin{equation} d \\omega=\\sum_{I}\\left(d b_{I}\\right) \\wedge d x_{I} . \\label{eq:60} \\end{equation} \\]\n10.19 Example Suppose \\(E\\) is open in \\(R^{n}, f \\in \\mathscr{C}^{\\prime}(E)\\), and \\(\\gamma\\) is a continuously differentiable curve in \\(E\\), with domain \\([0,1]\\). By \\(\\ref{eq:59}\\) and \\(\\ref{eq:35}\\),\n\\[ \\begin{equation} \\int_{\\gamma} d f=\\int_{0}^{1} \\sum_{i=1}^{n}\\left(D_{i} f\\right)(\\gamma(t)) \\gamma_{i}^{\\prime}(t) d t . \\label{eq:61} \\end{equation} \\]\nBy the chain rule, the last integrand is \\((f \\circ \\gamma)^{\\prime}(t)\\). Hence\n\\[ \\begin{equation} \\int_{\\gamma} d f=f(\\gamma(1))-f(\\gamma(0)), \\label{eq:62} \\end{equation} \\]\nand we see that \\(\\int_{\\gamma} d f\\) is the same for all \\(\\gamma\\) with the same initial point and the same end point, as in \\((a)\\) of Example \\(10.12\\).\nComparison with Example \\(10.12(b)\\) shows therefore that the 1-form \\(x d y\\) is not the derivative of any 0 -form \\(f\\). This could also be deduced from part \\((b)\\) of the following theorem, since\n\\[ d(x d y)=d x \\wedge d y \\neq 0 . \\]\n10.20 Theorem\nIf \\(\\omega\\) and \\(\\lambda\\) are \\(k\\) - and \\(m\\)-forms, respectively, of class \\(\\mathscr{C}^{\\prime}\\) in \\(E\\), then \\[ \\begin{equation} d(\\omega \\wedge \\lambda)=(d \\omega) \\wedge \\lambda+(-1)^{k} \\omega \\wedge d \\lambda . \\label{eq:63} \\end{equation} \\]\nIf \\(\\omega\\) is of class \\(\\mathscr{C}^{\\prime \\prime}\\) in \\(E\\), then \\(d^{2} \\omega=0\\). Here \\(d^{2} \\omega\\) means, of course, \\(d(d \\omega)\\).\nProof Because of \\(\\ref{eq:57}\\) and \\(\\ref{eq:60}\\),(a)$ follows if \\(\\ref{eq:63}\\) is proved for the special case\n\\[ \\begin{equation} \\omega=f d x_{I}, \\quad \\lambda=g d x_{J} \\label{eq:64} \\end{equation} \\]\nwhere \\(f, g \\in \\mathscr{C}^{\\prime}(E), d x_{I}\\) is a basic \\(k\\)-form, and \\(d x_{J}\\) is a basic \\(m\\)-form. [If \\(k\\) or \\(m\\) or both are 0 , simply omit \\(d x_{I}\\) or \\(d x_{J}\\) in (64); the proof that follows is unaffected by this.] Then\n\\[ \\omega \\wedge \\lambda=f g d x_{I} \\wedge d x_{J} . \\]\nLet us assume that \\(I\\) and \\(J\\) have no element in common. [In the other case each of the three terms in (63) is 0.] Then, using (53),\n\\[ d(\\omega \\wedge \\lambda)=d\\left(f g d x_{I} \\wedge d x_{J}\\right)=(-1)^{\\alpha} d\\left(f g d x_{[I, J]}\\right) . \\]\nBy (59), \\(d(f g)=f d g+g d f\\). Hence (60) gives\n\\[ \\begin{aligned} d(\\omega \\wedge \\lambda) \u0026amp;=(-1)^{\\alpha}(f d g+g d f) \\wedge d x_{[I, J]} \\\\ \u0026amp;=(g d f+f d g) \\wedge d x_{I} \\wedge d x_{J} . \\end{aligned} \\]\nSince \\(d g\\) is a 1-form and \\(d x_{I}\\) is a \\(k\\)-form, we have\n\\[ d g \\wedge d x_{I}=(-1)^{k} d x_{I} \\wedge d g, \\]\nby (42). Hence\n\\[ \\begin{aligned} d(\\omega \\wedge \\lambda) \u0026amp;=\\left(d f \\wedge d x_{I}\\right) \\wedge\\left(g d x_{J}\\right)+(-1)^{k}\\left(f d x_{I}\\right) \\wedge\\left(d g \\wedge d x_{J}\\right) \\\\ \u0026amp;=(d \\omega) \\wedge \\lambda+(-1)^{k} \\omega \\wedge d \\lambda \\end{aligned} \\]\nwhich proves \\((a)\\).\nNote that the associative law (58) was used freely.\nLet us prove (b) first for a 0 -form \\(f \\in \\mathscr{C}^{\\prime \\prime}\\) :\n\\[ \\begin{aligned} d^{2} f \u0026amp;=d\\left(\\sum_{j=1}^{n}\\left(D_{j} f\\right)(\\mathbf{x}) d x_{j}\\right) \\\\ \u0026amp;=\\sum_{j=1}^{n} d\\left(D_{j} f\\right) \\wedge d x_{j} \\\\ \u0026amp;=\\sum_{i, j=1}^{n}\\left(D_{i j} f\\right)(\\mathbf{x}) d x_{i} \\wedge d x_{j} . \\end{aligned} \\]\nSince \\(D_{i j} f=D_{j i} f\\) (Theorem 9.41) and \\(d x_{i} \\wedge d x_{j}=-d x_{\\jmath} \\wedge d x_{i}\\), we see that \\(d^{2} f=0\\).\nIf \\(\\omega=f d x_{I}\\), as in (64), then \\(d \\omega=(d f) \\wedge d x_{I}\\). By \\((60), d\\left(d x_{I}\\right)=0\\). Hence (63) shows that\n\\[ d^{2} \\omega=\\left(d^{2} f\\right) \\wedge d x_{I}=0 \\]\n10.21 Change of variables Suppose \\(E\\) is an open set in \\(R^{n}, T\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of \\(E\\) into an open set \\(V \\subset R^{m}\\), and \\(\\omega\\) is a \\(k\\)-form in \\(V\\), whose standard presentation is\n\\[ \\begin{equation} \\omega=\\sum_{I} b_{I}(\\mathbf{y}) d y_{I} . \\label{eq:65} \\end{equation} \\]\n(We use y for points of \\(V, \\mathbf{x}\\) for points of \\(E\\).)\nLet \\(t_{1}, \\ldots, t_{m}\\) be the components of \\(T\\) : If\n\\[ \\mathbf{y}=\\left(y_{1}, \\ldots, y_{m}\\right)=T(\\mathbf{x}) \\]\nthen \\(y_{i}=t_{i}(\\mathbf{x})\\). As in \\(\\ref{eq:59}\\),\n\\[ \\begin{equation} d t_{i}=\\sum_{j=1}^{n}\\left(D_{j} t_{i}\\right)(\\mathbf{x}) d x_{j} \\quad(1 \\leq i \\leq m) . \\label{eq:66} \\end{equation} \\]\nThus each \\(d t_{i}\\) is a 1-form in \\(E\\).\nThe mapping \\(T\\) transforms \\(\\omega\\) into a \\(k\\)-form \\(\\omega_{T}\\) in \\(E\\), whose definition is\n\\[ \\begin{equation} \\omega_{T}=\\sum_{I} b_{I}(T(\\mathbf{x})) d t_{i_{l}} \\wedge \\cdots \\wedge d t_{i_{k}} . \\label{eq:67} \\end{equation} \\]\nIn each summand of \\(\\ref{eq:67}\\), \\(I=\\left\\{i_{1}, \\ldots, i_{k}\\right\\}\\) is an increasing \\(k\\)-index.\nOur next theorem shows that addition, multiplication, and differentiation of forms are defined in such a way that they commute with changes of variables.\n10.22 Theorem With \\(E\\) and \\(T\\) as in Sec. \\(10.21\\), let \\(\\omega\\) and \\(\\lambda\\) be \\(k\\) - and \\(m\\)-forms in \\(V\\), respectively. Then\n\\((\\omega+\\lambda)_{T}=\\omega_{T}+\\lambda_{T}\\) if \\(k=m\\); \\((\\omega \\wedge \\lambda)_{T}=\\omega_{T} \\wedge \\lambda_{T}\\); \\(d\\left(\\omega_{T}\\right)=(d \\omega)_{T}\\) if \\(\\omega\\) is of class \\(\\mathscr{C}^{\\prime}\\) and \\(T\\) is of class \\(\\mathscr{C}^{\\prime \\prime}\\). Proof Part (a) follows immediately from the definitions. Part \\((b)\\) is almost as obvious, once we realize that\n\\[ \\left(d y_{i_{1}} \\wedge \\cdots \\wedge d y_{i_{r}}\\right)_{T}=d t_{i_{1}} \\wedge \\cdots \\wedge d t_{i_{r}} \\]\nregardless of whether \\(\\left\\{i_{1}, \\ldots, i_{r}\\right\\}\\) is increasing or not; (68) holds because the same number of minus signs are needed on each side of \\((68)\\) to produce increasing rearrangements.\nWe turn to the proof of \\((c)\\). If \\(f\\) is a 0 -form of class \\(\\mathscr{C}^{\\prime}\\) in \\(V\\), then\n\\[ f_{T}(\\mathbf{x})=f(T(\\mathbf{x})), \\quad d f=\\sum_{i}\\left(D_{i} f\\right)(\\mathbf{y}) d y_{i} . \\]\nBy the chain rule, it follows that\n\\[ \\begin{aligned} d\\left(f_{T}\\right) \u0026amp;=\\sum_{j}\\left(D_{j} f_{T}\\right)(\\mathbf{x}) d x_{j} \\\\ \u0026amp;=\\sum_{j} \\sum_{i}\\left(D_{i} f\\right)(T(\\mathbf{x}))\\left(D_{j} t_{i}\\right)(\\mathbf{x}) d x_{j} \\\\ \u0026amp;=\\sum_{i}\\left(D_{i} f\\right)(T(\\mathbf{x})) d t_{i} \\\\ \u0026amp;=(d f)_{T} \\end{aligned} \\]\nIf \\(d y_{I}=d y_{i_{1}} \\wedge \\cdots \\wedge d y_{i_{k}}\\), then \\(\\left(d y_{I}\\right)_{T}=d t_{i_{1}} \\wedge \\cdots \\wedge d t_{i_{k}}\\), and Theorem \\(10.20\\) shows that\n\\[ d\\left(\\left(d y_{I}\\right)_{T}\\right)=0 . \\]\n(This is where the assumption \\(T \\in \\mathscr{C}^{\\prime \\prime}\\) is used.)\nAssume now that \\(\\omega=f d y_{I}\\). Then\n\\[ \\omega_{T}=f_{T}(\\mathbf{x})\\left(d y_{I}\\right)_{T} \\]\nand the preceding calculations lead to\n\\[ \\begin{aligned} d\\left(\\omega_{T}\\right) \u0026amp;=d\\left(f_{T}\\right) \\wedge\\left(d y_{I}\\right)_{T}=(d f)_{T} \\wedge\\left(d y_{I}\\right)_{T} \\\\ \u0026amp;=\\left((d f) \\wedge d y_{I}\\right)_{T}=(d \\omega)_{T} \\end{aligned} \\]\nThe first equality holds by (63) and (70), the second by (69), the third by part (b), and the last by the definition of \\(d \\omega\\).\nThe general case of \\((c)\\) follows from the special case just proved, if we apply \\((a)\\). This completes the proof.\nOur next objective is Theorem 10.25. This will follow directly from two other important transformation properties of differential forms, which we state first.\n10.23 Theorem Suppose \\(T\\) is \\(a \\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into an open set \\(V \\subset R^{m}, S\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of \\(V\\) into an open set \\(W \\subset R^{p}\\), and \\(\\omega\\) is a \\(k\\)-form in \\(W\\), so that \\(\\omega_{S}\\) is a \\(k\\)-form in \\(V\\) and both \\(\\left(\\omega_{S}\\right)_{T}\\) and \\(\\omega_{S T}\\) are \\(k\\)-forms in \\(E\\), where \\(S T\\) is defined by \\((S T)(\\mathbf{x})=S(T(\\mathbf{x}))\\). Then\n\\[ \\begin{equation} \\left(\\omega_{S}\\right)_{T}=\\omega_{S T} . \\label{eq:71} \\end{equation} \\]\nProof If \\(\\omega\\) and \\(\\lambda\\) are forms in \\(W\\), Theorem \\(10.22\\) shows that\n\\[ \\left((\\omega \\wedge \\lambda)_{S}\\right)_{T}=\\left(\\omega_{S} \\wedge \\lambda_{S}\\right)_{T}=\\left(\\omega_{S}\\right)_{T} \\wedge\\left(\\lambda_{S}\\right)_{T} \\]\nand\n\\[ (\\omega \\wedge \\lambda)_{S T}=\\omega_{S T} \\wedge \\lambda_{S T} \\text {. } \\]\nThus if (71) holds for \\(\\omega\\) and for \\(\\lambda\\), it follows that (71) also holds for \\(\\omega \\wedge \\lambda\\). Since every form can be built up from 0-forms and 1-forms by addition and multiplication, and since (71) is trivial for 0 -forms, it is enough to prove (71) in the case \\(\\omega=d z_{q}, q=1, \\ldots, p\\). (We denote the points of \\(E, V, W\\) by \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\), respectively.)\nLet \\(t_{1}, \\ldots, t_{m}\\) be the components of \\(T\\), let \\(s_{1}, \\ldots, s_{p}\\) be the components of \\(S\\), and let \\(r_{1}, \\ldots, r_{p}\\) be the components of \\(S T\\). If \\(\\omega=d z_{q}\\), then\n\\[ \\omega_{S}=d s_{q}=\\sum_{j}\\left(D_{j} s_{q}\\right)(\\mathbf{y}) d y_{j}, \\]\nso that the chain rule implies\n\\[ \\begin{aligned} \\left(\\omega_{S}\\right)_{T} \u0026amp;=\\sum_{j}\\left(D_{j} s_{q}\\right)(T(\\mathbf{x})) d t_{j} \\\\ \u0026amp;=\\sum_{j}\\left(D_{j} s_{q}\\right)(T(\\mathbf{x})) \\sum_{i}\\left(D_{i} t_{j}\\right)(\\mathbf{x}) d x_{i} \\\\ \u0026amp;=\\sum_{i}\\left(D_{i} r_{q}\\right)(\\mathbf{x}) d x_{i}=d r_{q}=\\omega_{S T} \\end{aligned} \\]\n10.24 Theorem Suppose \\(\\omega\\) is a \\(k\\)-form in an open set \\(E \\subset R^{n}, \\Phi\\) is a \\(k\\)-surface in \\(E\\), with parameter domain \\(D \\subset R^{k}\\), and \\(\\Delta\\) is the \\(k\\)-surface in \\(R^{k}\\), with parameter domain \\(D\\), defined by \\(\\Delta(\\mathbf{u})=\\mathbf{u}(\\mathbf{u} \\in D)\\). Then\n\\[ \\int_{\\Phi} \\omega=\\int_{\\Delta} \\omega_{\\Phi} . \\]\nProof We need only consider the case\n\\[ \\omega=a(\\mathbf{x}) d x_{i_{1}} \\wedge \\cdots \\wedge d x_{i_{k}} . \\]\nIf \\(\\phi_{1}, \\ldots, \\phi_{n}\\) are the components of \\(\\Phi\\), then\n\\[ \\omega_{\\Phi}=a(\\Phi(\\mathbf{u})) d \\phi_{i_{1}} \\wedge \\cdots \\wedge d \\phi_{i_{k}} . \\]\nThe theorem will follow if we can show that\n\\[ d \\phi_{i_{1}} \\wedge \\cdots \\wedge d \\phi_{i_{k}}=J(\\mathbf{u}) d u_{1} \\wedge \\cdots \\wedge d u_{k}, \\]\nwhere\n\\[ J(\\mathbf{u})=\\frac{\\partial\\left(x_{i_{1}}, \\ldots, x_{i_{k}}\\right)}{\\partial\\left(u_{1}, \\ldots, u_{k}\\right)} \\]\nsince (72) implies\n\\[ \\begin{aligned} \\int_{\\Phi} \\omega \u0026amp;=\\int_{D} a(\\Phi(\\mathbf{u})) J(\\mathbf{u}) d \\mathbf{u} \\\\ \u0026amp;=\\int_{\\Delta} a(\\Phi(\\mathbf{u})) J(\\mathbf{u}) d u_{1} \\wedge \\cdots \\wedge d u_{k}=\\int_{\\Delta} \\omega_{\\Phi} . \\end{aligned} \\]\nLet \\([A]\\) be the \\(k\\) by \\(k\\) matrix with entries\n\\[ \\alpha(p, q)=\\left(D_{q} \\phi_{i_{p}}\\right)(\\mathbf{u}) \\quad(p, q=1, \\ldots, k) . \\]\nThen\n\\[ d \\phi_{i_{p}}=\\sum_{q} \\alpha(p, q) d u_{q} \\]\nso that\n\\[ d \\phi_{i_{1}} \\wedge \\cdots \\wedge d \\phi_{i_{k}}=\\sum \\alpha\\left(1, q_{1}\\right) \\cdots \\alpha\\left(k, q_{k}\\right) d u_{q_{1}} \\wedge \\cdots \\wedge d u_{q_{k}} \\text {. } \\]\nIn this last sum, \\(q_{1}, \\ldots, q_{k}\\) range independently over \\(1, \\ldots, k\\). The anticommutative relation (42) implies that\n\\[ d u_{q_{1}} \\wedge \\cdots \\wedge d u_{q_{k}}=s\\left(q_{1}, \\ldots, q_{k}\\right) d u_{1} \\wedge \\cdots \\wedge d u_{k}, \\]\nwhere \\(s\\) is as in Definition 9.33; applying this definition, we see that\n\\[ d \\phi_{i_{1}} \\wedge \\cdots \\wedge d \\phi_{i_{k}}=\\operatorname{det}[A] d u_{1} \\wedge \\cdots \\wedge d u_{k} \\]\nand since \\(J(\\mathbf{u})=\\operatorname{det}[A],(72)\\) is proved.\nThe final result of this section combines the two preceding theorems.\n10.25 Theorem Suppose \\(T\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into an open set \\(V \\subset R^{m}, \\Phi\\) is a \\(k\\)-surface in \\(E\\), and \\(\\omega\\) is a \\(k\\)-form in \\(V\\).\nThen\n\\[ \\int_{T \\Phi} \\omega=\\int_{\\Phi} \\omega_{T} \\]\nProof Let \\(D\\) be the parameter domain of \\(\\Phi\\) (hence also of \\(T \\Phi\\) ) and define \\(\\Delta\\) as in Theorem \\(10.24\\).\nThen\n\\[ \\int_{T \\Phi} \\omega=\\int_{\\Delta} \\omega_{T \\Phi}=\\int_{\\Delta}\\left(\\omega_{T}\\right)_{\\Phi}=\\int_{\\Phi} \\omega_{T} . \\]\nThe first of these equalities is Theorem \\(10.24\\), applied to \\(T \\Phi\\) in place of \\(\\Phi\\). The second follows from Theorem 10.23. The third is Theorem 10.24, with \\(\\omega_{T}\\) in place of \\(\\omega\\).\n","date":"2022-08-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/5-differential-forms/","section":"baby rudin","tags":null,"title":"5 DIFFERENTIAL FORMS"},{"categories":null,"contents":"10.26 Affine simplexes A mapping \\(\\mathrm{f}\\) that carries a vector space \\(X\\) into a vector space \\(Y\\) is said to be affine if \\(\\mathbf{f}-\\mathbf{f}(0)\\) is linear. In other words, the requirement is that\n\\[ \\begin{equation} \\mathbf{f}(\\mathbf{x})=\\mathbf{f}(\\mathbf{0})+A \\mathbf{x} \\label{eq:73} \\end{equation} \\]\nfor some \\(A \\in L(X, Y)\\).\nAn affine mapping of \\(R^{k}\\) into \\(R^{n}\\) is thus determined if we know \\(\\mathbf{f}(0)\\) and \\(\\mathbf{f}\\left(\\mathbf{e}_{i}\\right)\\) for \\(1 \\leq i \\leq k\\); as usual, \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\right\\}\\) is the standard basis of \\(R^{k}\\).\nWe define the standard simplex \\(Q^{k}\\) to be the set of all \\(\\mathbf{u} \\in R^{k}\\) of the form\n\\[ \\begin{equation} \\mathbf{u}=\\sum_{i=1}^{k} \\alpha_{i} \\mathbf{e}_{i} \\label{eq:74} \\end{equation} \\]\nsuch that \\(\\alpha_{i} \\geq 0\\) for \\(i=1, \\ldots, k\\) and \\(\\Sigma \\alpha_{i} \\leq 1\\).\nAssume now that \\(\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{k}\\) are points of \\(R^{n}\\). The oriented affine \\(k\\)-simplex\n\\[ \\begin{equation} \\sigma=\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{k}\\right] \\label{eq:75} \\end{equation} \\]\nis defined to be the \\(k\\)-surface in \\(R^{n}\\) with parameter domain \\(Q^{k}\\) which is given by the affine mapping\n\\[ \\begin{equation} \\sigma\\left(\\alpha_{1} \\mathbf{e}_{1}+\\cdots+\\alpha_{k} \\mathbf{e}_{k}\\right)=\\mathbf{p}_{0}+\\sum_{i=1}^{k} \\alpha_{i}\\left(\\mathbf{p}_{i}-\\mathbf{p}_{0}\\right) \\text {. } \\label{eq:76} \\end{equation} \\]\nNote that \\(\\sigma\\) is characterized by\n\\[ \\begin{equation} \\sigma(0)=\\mathbf{p}_{0}, \\quad \\sigma\\left(\\mathbf{e}_{i}\\right)=\\mathbf{p}_{i} \\quad \\text { (for } 1 \\leq i \\leq k \\text { ), } \\label{eq:77} \\end{equation} \\]\nand that\n\\[ \\begin{equation} \\sigma(\\mathbf{u})=\\mathbf{p}_{0}+A \\mathbf{u} \\quad\\left(\\mathbf{u} \\in Q^{k}\\right) \\label{eq:78} \\end{equation} \\]\nwhere \\(A \\in L\\left(R^{k}, R^{n}\\right)\\) and \\(A \\mathbf{e}_{i}=\\mathbf{p}_{i}-\\mathbf{p}_{0}\\) for \\(1 \\leq i \\leq k\\).\nWe call \\(\\sigma\\) oriented to emphasize that the ordering of the vertices \\(\\mathbf{p}_{0}, \\ldots, \\mathbf{p}_{k}\\) is taken into account. If\n\\[ \\begin{equation} \\bar{\\sigma}=\\left[p_{i_{0}}, p_{i_{1}}, \\ldots, p_{i_{k}}\\right] \\label{eq:79} \\end{equation} \\]\nwhere \\(\\left\\{i_{0}, i_{1}, \\ldots, i_{k}\\right\\}\\) is a permutation of the ordered set \\(\\{0,1, \\ldots, k\\}\\), we adopt the notation\n\\[ \\begin{equation} \\bar{\\sigma}=s\\left(i_{0}, i_{1}, \\ldots, i_{k}\\right) \\sigma, \\] \\end{equation}\nwhere \\(s\\) is the function defined in Definition 9.33. Thus \\(\\bar{\\sigma}=\\pm \\sigma\\), depending on whether \\(s=1\\) or \\(s=-1\\). Strictly speaking, having adopted (75) and (76) as the definition of \\(\\sigma\\), we should not write \\(\\bar{\\sigma}=\\sigma\\) unless \\(i_{0}=0, \\ldots, i_{k}=k\\), even if \\(s\\left(i_{0}, \\ldots, i_{k}\\right)=1\\); what we have here is an equivalence relation, not an equality. However, for our purposes the notation is justified by Theorem 10.27.\nIf \\(\\bar{\\sigma}=\\varepsilon \\sigma\\) (using the above convention) and if \\(\\varepsilon=1\\), we say that \\(\\bar{\\sigma}\\) and \\(\\sigma\\) have the same orientation; if \\(\\varepsilon=-1, \\bar{\\sigma}\\) and \\(\\sigma\\) are said to have opposite orientations. Note that we have not defined what we mean by the “orientation of a simplex.” What we have defined is a relation between pairs of simplexes having the same set of vertices, the relation being that of “having the same orientation.”\nThere is, however, one situation where the orientation of a simplex can be defined in a natural way. This happens when \\(n=k\\) and when the vectors \\(\\mathbf{p}_{i}-\\mathbf{p}_{0}(1 \\leq i \\leq k)\\) are independent. In that case, the linear transformation \\(A\\) that appears in (78) is invertible, and its determinant (which is the same as the Jacobian of \\(\\sigma\\) ) is not 0 . Then \\(\\sigma\\) is said to be positively (or negatively) oriented if \\(\\operatorname{det} A\\) is positive (or negative). In particular, the simplex \\(\\left[0, \\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\right]\\) in \\(R^{k}\\), given by the identity mapping, has positive orientation.\nSo far we have assumed that \\(k \\geq 1\\). An oriented 0 -simplex is defined to be a point with a sign attached. We write \\(\\sigma=+\\mathbf{p}_{0}\\) or \\(\\sigma=-\\mathbf{p}_{0}\\). If \\(\\sigma=\\varepsilon \\mathbf{p}_{0}\\) \\((\\varepsilon=\\pm 1)\\) and if \\(f\\) is a 0 -form (i.e., a real function), we define\n\\[ \\int_{\\sigma} f=\\varepsilon f\\left(p_{0}\\right) \\]\n10.27 Theorem If \\(\\sigma\\) is an oriented rectilinear \\(k\\)-simplex in an open set \\(E \\subset R^{n}\\) and if \\(\\bar{\\sigma}=\\varepsilon \\sigma\\) then\n\\[ \\begin{equation} \\int_{\\bar{\\sigma}} \\omega=\\varepsilon \\int_{\\sigma} \\omega \\label{eq:81} \\end{equation} \\]\nfor every \\(k\\)-form \\(\\omega\\) in \\(E\\).\nProof For \\(k=0\\), (81) follows from the preceding definition. So we assume \\(k \\geq 1\\) and assume that \\(\\sigma\\) is given by (75). Suppose \\(1 \\leq j \\leq k\\), and suppose \\(\\bar{\\sigma}\\) is obtained from \\(\\sigma\\) by interchanging \\(\\mathbf{p}_{0}\\) and \\(\\mathbf{p}_{j}\\). Then \\(\\varepsilon=-1\\), and\n\\[ \\bar{\\sigma}(\\mathbf{u})=\\mathbf{p}_{j}+B \\mathbf{u} \\quad\\left(\\mathbf{u} \\in Q^{k}\\right), \\]\nwhere \\(B\\) is the linear mapping of \\(R^{k}\\) into \\(R^{n}\\) defined by \\(B e_{j}=\\mathbf{p}_{0}-\\mathbf{p}_{j}\\), \\(B \\mathbf{e}_{i}=\\mathbf{p}_{i}-\\mathbf{p}_{j}\\) if \\(i \\neq j\\). If we write \\(A \\mathbf{e}_{i}=\\mathbf{x}_{i}(1 \\leq i \\leq k)\\), where \\(A\\) is given by (78), the column vectors of \\(B\\) (that is, the vectors \\(B e_{i}\\) ) are\n\\[ \\mathbf{x}_{1}-\\mathbf{x}_{j}, \\ldots, \\mathbf{x}_{j-1}-\\mathbf{x}_{j},-\\mathbf{x}_{j}, \\mathbf{x}_{j+1}-\\mathbf{x}_{j}, \\ldots, \\mathbf{x}_{k}-\\mathbf{x}_{j} \\text {. } \\]\nIf we subtract the \\(j\\) th column from each of the others, none of the determinants in (35) are affected, and we obtain columns \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{j-1},-\\mathbf{x}_{j}\\), \\(\\mathbf{x}_{j+1}, \\ldots, \\mathbf{x}_{k}\\). These differ from those of \\(A\\) only in the sign of the \\(j\\) th column. Hence (81) holds for this case.\nSuppose next that \\(0\u0026lt;i\u0026lt;j \\leq k\\) and that \\(\\bar{\\sigma}\\) is obtained from \\(\\sigma\\) by interchanging \\(\\mathbf{p}_{i}\\) and \\(\\mathbf{p}_{j}\\). Then \\(\\bar{\\sigma}(\\mathbf{u})=\\mathbf{p}_{0}+C \\mathbf{u}\\), where \\(C\\) has the same columns as \\(A\\), except that the \\(i\\) th and \\(j\\) th columns have been interchanged. This again implies that (81) holds, since \\(\\varepsilon=-1\\).\nThe general case follows, since every permutation of \\(\\{0,1, \\ldots, k\\}\\) is a composition of the special cases we have just dealt with.\n10.28 Affine chains An affine \\(k\\)-chain \\(\\Gamma\\) in an open set \\(E \\subset R^{n}\\) is a collection of finitely many oriented affine \\(k\\)-simplexes \\(\\sigma_{1}, \\ldots, \\sigma_{r}\\) in \\(E\\). These need not be distinct; a simplex may thus occur in \\(\\Gamma\\) with a certain multiplicity.\nIf \\(\\Gamma\\) is as above, and if \\(\\omega\\) is a \\(k\\)-form in \\(E\\), we define\n\\[ \\begin{equation} \\int_{\\Gamma} \\omega=\\sum_{i=1}^{r} \\int_{\\sigma_{i}} \\omega . \\label{eq:82} \\end{equation} \\]\nWe may view a \\(k\\)-surface \\(\\Phi\\) in \\(E\\) as a function whose domain is the collection of all \\(k\\)-forms in \\(E\\) and which assigns the number \\(\\int_{\\Phi} \\omega\\) to \\(\\omega\\). Since realvalued functions can be added (as in Definition 4.3), this suggests the use of the notation\n\\[ \\begin{equation} \\Gamma=\\sigma_{1}+\\cdots+\\sigma_{r} \\label{eq:83} \\end{equation} \\]\nor, more compactly,\n\\[ \\begin{equation} \\Gamma=\\sum_{i=1}^{r} \\sigma_{i} \\label{eq:84} \\end{equation} \\]\nto state the fact that (82) holds for every \\(k\\)-form \\(\\omega\\) in \\(E\\).\nTo avoid misunderstanding, we point out explicitly that the notations introduced by (83) and (80) have to be handled with care. The point is that every oriented affine \\(k\\)-simplex \\(\\sigma\\) in \\(R^{n}\\) is a function in two ways, with different domains and different ranges, and that therefore two entirely different operations of addition are possible. Originally, \\(\\sigma\\) was defined as an \\(R^{n}\\)-valued function with domain \\(Q^{k}\\); accordingly, \\(\\sigma_{1}+\\sigma_{2}\\) could be interpreted to be the function \\(\\sigma\\) that assigns the vector \\(\\sigma_{1}(\\mathbf{u})+\\sigma_{2}(\\mathbf{u})\\) to every \\(\\mathbf{u} \\in Q^{k}\\); note that \\(\\sigma\\) is then again an oriented affine \\(k\\)-simplex in \\(R^{n} !\\) This is not what is meant by (83).\nFor example, if \\(\\sigma_{2}=-\\sigma_{1}\\) as in (80) (that is to say, if \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) have the same set of vertices but are oppositely oriented) and if \\(\\Gamma=\\sigma_{1}+\\sigma_{2}\\), then \\(\\int_{\\Gamma} \\omega=0\\) for all \\(\\omega\\), and we may express this by writing \\(\\Gamma=0\\) or \\(\\sigma_{1}+\\sigma_{2}=0\\). This does not mean that \\(\\sigma_{1}(\\mathbf{u})+\\sigma_{2}(\\mathbf{u})\\) is the null vector of \\(R^{n}\\).\n10.29 Boundaries For \\(k \\geq 1\\), the boundary of the oriented affine \\(k\\)-simplex\n\\[ \\sigma=\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\ldots, \\mathbf{p}_{k}\\right] \\]\nis defined to be the affine \\((k-1)\\)-chain\n\\[ \\partial \\sigma=\\sum_{j=0}^{k}(-1)^{j}\\left[\\mathbf{p}_{0}, \\ldots, \\mathbf{p}_{j-1}, \\mathbf{p}_{j+1}, \\ldots, \\mathbf{p}_{k}\\right] . \\]\nFor example, if \\(\\sigma=\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}, \\mathbf{p}_{2}\\right]\\), then\n\\[ \\partial \\sigma=\\left[\\mathbf{p}_{1}, \\mathbf{p}_{2}\\right]-\\left[\\mathbf{p}_{0}, \\mathbf{p}_{2}\\right]+\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}\\right]=\\left[\\mathbf{p}_{0}, \\mathbf{p}_{1}\\right]+\\left[\\mathbf{p}_{1}, \\mathbf{p}_{2}\\right]+\\left[\\mathbf{p}_{2}, \\mathbf{p}_{0}\\right] \\]\nwhich coincides with the usual notion of the oriented boundary of a triangle.\nFor \\(1 \\leq j \\leq k\\), observe that the simplex \\(\\sigma_{j}=\\left[\\mathbf{p}_{0}, \\ldots, \\mathbf{p}_{j-1}, \\mathbf{p}_{j+1}, \\ldots, \\mathbf{p}_{k}\\right]\\) which occurs in (85) has \\(Q^{k-1}\\) as its parameter domain and that it is defined by\n\\[ \\sigma_{j}(\\mathbf{u})=\\mathbf{p}_{0}+B \\mathbf{u} \\quad\\left(\\mathbf{u} \\in Q^{k-1}\\right), \\]\nwhere \\(B\\) is the linear mapping from \\(R^{k-1}\\) to \\(R^{n}\\) determined by\n\\[ \\begin{array}{ll} \\left.B \\mathbf{e}_{i}=\\mathbf{p}_{i}-\\mathbf{p}_{0} \\quad \\text { (if } \\quad 1 \\leq i \\leq j-1\\right \\left.B \\mathbf{e}_{i}=\\mathbf{p}_{i+1}-\\mathbf{p}_{0} \\quad \\text { (if } j \\leq i \\leq k-1\\right) . \\end{array} \\]\nThe simplex\n\\[ \\sigma_{0}=\\left[\\mathbf{p}_{1}, \\mathbf{p}_{2}, \\ldots, \\mathbf{p}_{k}\\right] \\text {, } \\]\nwhich also occurs in (85), is given by the mapping\n\\[ \\sigma_{0}(\\mathbf{u})=\\mathbf{p}_{1}+B \\mathbf{u}, \\]\nwhere \\(B \\mathbf{e}_{i}=\\mathbf{p}_{i+1}-\\mathbf{p}_{1}\\) for \\(1 \\leq i \\leq k-1\\).\n10.30 Differentiable simplexes and chains Let \\(T\\) be a \\(\\mathscr{C}^{\\prime \\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into an open set \\(V \\subset R^{m} ; T\\) need not be one-to-one. If \\(\\sigma\\) is an oriented affine \\(k\\)-simplex in \\(E\\), then the composite mapping \\(\\Phi=T \\circ \\sigma\\) (which we shall sometimes write in the simpler form \\(T \\sigma\\) ) is a \\(k\\)-surface in \\(V\\), with parameter domain \\(Q^{k}\\). We call \\(\\Phi\\) an oriented \\(k\\)-simplex of class \\(\\mathscr{C}^{\\prime \\prime}\\).\nA finite collection \\(\\Psi\\) of oriented \\(k\\)-simplexes \\(\\Phi_{1}, \\ldots, \\Phi_{r}\\) of class \\(\\mathscr{C}^{\\prime \\prime}\\) in \\(V\\) is called a \\(k\\)-chain of class \\(\\mathscr{G}^{\\prime \\prime}\\) in \\(V\\). If \\(\\omega\\) is a \\(k\\)-form in \\(V\\), we define\n\\[ \\begin{equation} \\int_{\\Psi} \\omega=\\sum_{i=1}^{r} \\int_{\\Phi i} \\omega \\label{eq:87} \\\\end{equation} \\]\nand use the corresponding notation \\(\\Psi=\\Sigma \\Phi_{i}\\).\nIf \\(\\Gamma=\\Sigma \\sigma_{i}\\) is an affine chain and if \\(\\Phi_{i}=T \\circ \\sigma_{i}\\), we also write \\(\\Psi=T \\circ \\Gamma\\), or\n\\[ \\begin{equation} T\\left(\\sum \\sigma_{i}\\right)=\\sum T \\sigma_{i} . \\label{eq:88} \\end{equation} \\]\nThe boundary \\(\\partial \\Phi\\) of the oriented \\(k\\)-simplex \\(\\Phi=T \\circ \\sigma\\) is defined to be the \\((k-1)\\) chain\n\\[ \\begin{equation} \\partial \\Phi=T(\\partial \\sigma) . \\label{eq:89} \\end{equation} \\]\nIn justification of (89), observe that if \\(T\\) is affine, then \\(\\Phi=T \\circ \\sigma\\) is an oriented affine \\(k\\)-simplex, in which case (89) is not a matter of definition, but is seen to be a consequence of (85). Thus (89) generalizes this special case.\nIt is immediate that \\(\\partial \\Phi\\) is of class \\(\\mathscr{C}^{\\prime \\prime}\\) if this is true of \\(\\Phi\\).\nFinally, we define the boundary \\(\\partial \\Psi\\) of the \\(k\\)-chain \\(\\Psi=\\Sigma \\Phi_{i}\\) to be the \\((k-1)\\) chain\n\\[ \\begin{equation} \\partial \\Psi=\\sum \\partial \\Phi_{i} \\label{eq:90} \\end{equation} \\]\n10.31 Positively oriented boundaries So far we have associated boundaries to chains, not to subsets of \\(R^{n}\\). This notion of boundary is exactly the one that is most suitable for the statement and proof of Stokes’ theorem. However, in applications, especially in \\(R^{2}\\) or \\(R^{3}\\), it is customary and convenient to talk about “oriented boundaries” of certain sets as well. We shall now describe this briefly.\nLet \\(Q^{n}\\) be the standard simplex in \\(R^{n}\\), let \\(\\sigma_{0}\\) be the identity mapping with domain \\(Q^{n}\\). As we saw in Sec. 10.26, \\(\\sigma_{0}\\) may be regarded as a positively oriented \\(n\\)-simplex in \\(R^{n}\\). Its boundary \\(\\partial \\sigma_{0}\\) is an affine \\((n-1)\\)-chain. This chain is called the positively oriented boundary of the set \\(Q^{n}\\).\nFor example, the positively oriented boundary of \\(Q^{3}\\) is\n\\[ \\left[\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right]-\\left[0, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\right]+\\left[0, \\mathbf{e}_{1}, \\mathbf{e}_{3}\\right]-\\left[0, \\mathbf{e}_{1}, \\mathbf{e}_{2}\\right] \\text {. } \\]\nNow let \\(T\\) be a 1-1 mapping of \\(Q^{n}\\) into \\(R^{n}\\), of class \\(\\mathscr{C}^{\\prime \\prime}\\), whose Jacobian is positive (at least in the interior of \\(Q^{n}\\) ). Let \\(E=T\\left(Q^{n}\\right)\\). By the inverse function theorem, \\(E\\) is the closure of an open subset of \\(R^{n}\\). We define the positively oriented boundary of the set \\(E\\) to be the \\((n-1)\\)-chain\n\\[ \\partial T=T\\left(\\partial \\sigma_{0}\\right), \\]\nand we may denote this \\((n-1)\\)-chain by \\(\\partial E\\). An obvious question occurs here: If \\(E=T_{1}\\left(Q^{n}\\right)=T_{2}\\left(Q^{n}\\right)\\), and if both \\(T_{1}\\) and \\(T_{2}\\) have positive Jacobians, is it true that \\(\\partial T_{1}=\\partial T_{2}\\) ? That is to say, does the equality\n\\[ \\int_{\\partial T_{1}} \\omega=\\int_{\\partial T_{2}} \\omega \\]\nhold for every \\((n-1)\\)-form \\(\\omega\\) ? The answer is yes, but we shall omit the proof. (To see an example, compare the end of this section with Exercise 17.)\nOne can go further. Let\n\\[ \\Omega=E_{1} \\cup \\cdots \\cup E_{r}, \\]\nwhere \\(E_{i}=T_{i}\\left(Q^{n}\\right)\\), each \\(T_{i}\\) has the properties that \\(T\\) had above, and the interiors of the sets \\(E_{i}\\) are pairwise disjoint. Then the \\((n-1)\\)-chain\n\\[ \\partial T_{1}+\\cdots+\\partial T_{r}=\\partial \\Omega \\]\nis called the positively oriented boundary of \\(\\Omega\\). where\nFor example, the unit square \\(I^{2}\\) in \\(R^{2}\\) is the union of \\(\\sigma_{1}\\left(Q^{2}\\right)\\) and \\(\\sigma_{2}\\left(Q^{2}\\right)\\),\n\\[ \\sigma_{1}(\\mathbf{u})=\\mathbf{u}, \\quad \\sigma_{2}(\\mathbf{u})=\\mathbf{e}_{1}+\\mathbf{e}_{2}-\\mathbf{u} . \\]\nBoth \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\) have Jacobian \\(1\u0026gt;0\\). Since\n\\[ \\sigma_{1}=\\left[\\mathbf{0}, \\mathbf{e}_{1}, \\mathbf{e}_{2}\\right], \\quad \\sigma_{2}=\\left[\\mathbf{e}_{1}+\\mathbf{e}_{2}, \\mathbf{e}_{2}, \\mathbf{e}_{1}\\right] \\]\nwe have\n\\[ \\begin{aligned} \u0026amp;\\partial \\sigma_{1}=\\left[\\mathbf{e}_{1}, \\mathbf{e}_{2}\\right]-\\left[\\mathbf{0}, \\mathbf{e}_{2}\\right]+\\left[\\mathbf{0}, \\mathbf{e}_{1}\\right] \\\\ \u0026amp;\\partial \\sigma_{2}=\\left[\\mathbf{e}_{2}, \\mathbf{e}_{1}\\right]-\\left[\\mathbf{e}_{1}+\\mathbf{e}_{2}, \\mathbf{e}_{1}\\right]+\\left[\\mathbf{e}_{1}+\\mathbf{e}_{2}, \\mathbf{e}_{2}\\right] \\end{aligned} \\]\nThe sum of these two boundaries is\n\\[ \\partial I^{2}=\\left[\\mathbf{0}, \\mathbf{e}_{1}\\right]+\\left[\\mathbf{e}_{1}, \\mathbf{e}_{1}+\\mathbf{e}_{2}\\right]+\\left[\\mathbf{e}_{1}+\\mathbf{e}_{2}, \\mathbf{e}_{2}\\right]+\\left[\\mathbf{e}_{2}, 0\\right] \\]\nthe positively oriented boundary of \\(I^{2}\\). Note that \\(\\left[\\mathbf{e}_{1}, \\mathbf{e}_{2}\\right]\\) canceled \\(\\left[\\mathbf{e}_{2}, \\mathbf{e}_{1}\\right]\\).\nIf \\(\\Phi\\) is a 2-surface in \\(R^{m}\\), with parameter domain \\(I^{2}\\), then \\(\\Phi\\) (regarded as a function on 2-forms) is the same as the 2-chain\n\\[ \\Phi \\circ \\sigma_{1}+\\Phi \\circ \\sigma_{2} . \\]\nThus\n\\[ \\begin{aligned} \\partial \\Phi \u0026amp;=\\partial\\left(\\Phi \\circ \\sigma_{1}\\right)+\\partial\\left(\\Phi \\circ \\sigma_{2}\\right) \\\\ \u0026amp;=\\Phi\\left(\\partial \\sigma_{1}\\right)+\\Phi\\left(\\partial \\sigma_{2}\\right)=\\Phi\\left(\\partial I^{2}\\right) \\end{aligned} \\]\nIn other words, if the parameter domain of \\(\\Phi\\) is the square \\(I^{2}\\), we need not refer back to the simplex \\(Q^{2}\\), but can obtain \\(\\partial \\Phi\\) directly from \\(\\partial I^{2}\\).\nOther examples may be found in Exercises 17 to \\(19 .\\)\n10.32 Example For \\(0 \\leq u \\leq \\pi, 0 \\leq v \\leq 2 \\pi\\), define\n\\[ \\Sigma(u, v)=(\\sin u \\cos v, \\sin u \\sin v, \\cos u) . \\]\nThen \\(\\Sigma\\) is a 2-surface in \\(R^{3}\\), whose parameter domain is a rectangle \\(D \\subset R^{2}\\), and whose range is the unit sphere in \\(R^{3}\\). Its boundary is\n\\[ \\partial \\Sigma=\\Sigma(\\partial D)=\\gamma_{1}+\\gamma_{2}+\\gamma_{3}+\\gamma_{4} \\]\nwhere\n\\[ \\begin{aligned} \u0026amp;\\gamma_{1}(u)=\\Sigma(u, 0)=(\\sin u, 0, \\cos u), \\\\ \u0026amp;\\gamma_{2}(v)=\\Sigma(\\pi, v)=(0,0,-1), \\\\ \u0026amp;\\gamma_{3}(u)=\\Sigma(\\pi-u, 2 \\pi)=(\\sin u, 0,-\\cos u), \\\\ \u0026amp;\\gamma_{4}(v)=\\Sigma(0,2 \\pi-v)=(0,0,1), \\end{aligned} \\]\nwith \\([0, \\pi]\\) and \\([0,2 \\pi]\\) as parameter intervals for \\(u\\) and \\(v\\), respectively.\nSince \\(\\gamma_{2}\\) and \\(\\gamma_{4}\\) are constant, their derivatives are 0 , hence the integral of any 1-form over \\(\\gamma_{2}\\) or \\(\\gamma_{4}\\) is 0 . [See Example 1.12(a).]\nSince \\(\\gamma_{3}(u)=\\gamma_{1}(\\pi-u)\\), direct application of (35) shows that\n\\[ \\int_{\\gamma_{3}} \\omega=-\\int_{\\gamma_{1}} \\omega \\]\nfor every 1-form \\(\\omega\\). Thus \\(\\int_{\\partial \\Sigma} \\omega=0\\), and we conclude that \\(\\partial \\Sigma=0\\).\n(In geographic terminology, \\(\\partial \\Sigma\\) starts at the north pole \\(N\\), runs to the south pole \\(S\\) along a meridian, pauses at \\(S\\), returns to \\(N\\) along the same meridian, and finally pauses at \\(N\\). The two passages along the meridian are in opposite directions. The corresponding two line integrals therefore cancel each other. In Exercise 32 there is also one curve which occurs twice in the boundary, but without cancellation.)\n","date":"2022-08-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/6-simplexes-and-chains/","section":"baby rudin","tags":null,"title":"6 SIMPLEXEs AND CHAINS"},{"categories":null,"contents":"10.33 Theorem If \\(\\Psi\\) is a \\(k\\)-chain of class \\(\\mathscr{C}^{\\prime \\prime}\\) in an open set \\(V \\subset R^{m}\\) and if \\(\\omega\\) is a \\((k-1)\\)-form of class \\(\\mathscr{C}^{\\prime}\\) in \\(V\\), then\n\\[ \\int_{\\Psi} d \\omega=\\int_{\\partial \\Psi} \\omega . \\]\nThe case \\(k=m=1\\) is nothing but the fundamental theorem of calculus (with an additional differentiability assumption). The case \\(k=m=2\\) is Green’s theorem, and \\(k=m=3\\) gives the so-called “divergence theorem’” of Gauss. The case \\(k=2, m=3\\) is the one originally discovered by Stokes. (Spivak’s book describes some of the historical background.) These special cases will be discussed further at the end of the present chapter.\nProof It is enough to prove that\n\\[ \\int_{\\Phi} d \\omega=\\int_{\\partial \\Phi} \\omega \\]\nfor every oriented \\(k\\)-simplex \\(\\Phi\\) of class \\(\\mathscr{C}^{\\prime \\prime}\\) in \\(V\\). For if (92) is proved and if \\(\\Psi=\\Sigma \\Phi_{i}\\), then (87) and (89) imply (91).\nFix such a \\(\\Phi\\) and put\n\\[ \\sigma=\\left[\\mathbf{0}, \\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\right] . \\]\nThus \\(\\sigma\\) is the oriented affine \\(k\\)-simplex with parameter domain \\(Q^{k}\\) which is defined by the identity mapping. Since \\(\\Phi\\) is also defined on \\(Q^{k}\\) (see Definition 10.30) and \\(\\Phi \\in \\mathscr{C}^{\\prime \\prime}\\), there is an open set \\(E \\subset R^{k}\\) which contains \\(Q^{k}\\), and there is a \\(\\mathscr{C}^{\\prime \\prime}\\)-mapping \\(T\\) of \\(E\\) into \\(V\\) such that \\(\\Phi=T \\circ \\sigma\\). By Theorems \\(10.25\\) and \\(10.22(c)\\), the left side of \\((92)\\) is equal to\n\\[ \\int_{T \\sigma} d \\omega=\\int_{\\sigma}(d \\omega)_{T}=\\int_{\\sigma} d\\left(\\omega_{T}\\right) . \\]\nAnother application of Theorem \\(10.25\\) shows, by (89), that the right side of \\((92)\\) is\n\\[ \\int_{\\partial(T \\sigma)} \\omega=\\int_{T(\\partial \\sigma)} \\omega=\\int_{\\partial \\sigma} \\omega_{T} . \\]\nSince \\(\\omega_{T}\\) is a \\((k-1)\\)-form in \\(E\\), we see that in order to prove (92) we merely have to show that\n\\[ \\int_{\\sigma} d \\lambda=\\int_{\\partial \\sigma} \\lambda \\]\nfor the special simplex (93) and for every \\((k-1)\\)-form \\(\\lambda\\) of class \\(\\mathscr{C}^{\\prime}\\) in \\(E\\).\nIf \\(k=1\\), the definition of an oriented 0 -simplex shows that (94) merely asserts that\n\\[ \\int_{0}^{1} f^{\\prime}(u) d u=f(1)-f(0) \\]\nfor every continuously differentiable function \\(f\\) on \\([0,1]\\), which is true by the fundamental theorem of calculus.\nFrom now on we assume that \\(k\u0026gt;1\\), fix an integer \\(r(1 \\leq r \\leq k)\\), and choose \\(f \\in \\mathscr{C}^{\\prime}(E)\\). It is then enough to prove (94) for the case\n\\[ \\lambda=f(\\mathbf{x}) d x_{1} \\wedge \\cdots \\wedge d x_{r-1} \\wedge d x_{r+1} \\wedge \\cdots \\wedge d x_{k} \\]\nsince every \\((k-1)\\)-form is a sum of these special ones, for \\(r=1, \\ldots, k\\). By (85), the boundary of the simplex (93) is\n\\[ \\partial \\sigma=\\left[\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\right]+\\sum_{i=1}^{k}(-1)^{i} \\tau_{i} \\]\nwhere\n\\[ \\tau_{i}=\\left[0, \\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{i-1}, \\mathbf{e}_{i+1}, \\ldots, \\mathbf{e}_{k}\\right] \\]\nfor \\(i=1, \\ldots, k\\). Put\n\\[ \\tau_{0}=\\left[\\mathbf{e}_{r}, \\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{r-1}, \\mathbf{e}_{r+1}, \\ldots, \\mathbf{e}_{k}\\right] . \\]\nNote that \\(\\tau_{0}\\) is obtained from \\(\\left[\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{k}\\right]\\) by \\(r-1\\) successive interchanges of \\(\\mathbf{e}_{r}\\) and its left neighbors. Thus\n\\[ \\partial \\sigma=(-1)^{r-1} \\tau_{0}+\\sum_{i=1}^{k}(-1)^{i} \\tau_{i} . \\]\nEach \\(\\tau_{i}\\) has \\(Q^{k-1}\\) as parameter domain.\nIf \\(\\mathbf{x}=\\tau_{0}(\\mathbf{u})\\) and \\(\\mathbf{u} \\in Q^{k-1}\\), then\n\\[ x_{j}= \\begin{cases}u_{j} \u0026amp; (1 \\leq j\u0026lt;r) \\\\ 1-\\left(u_{1}+\\cdots+u_{k-1}\\right) \u0026amp; (j=r) \\\\ u_{j-1} \u0026amp; (r\u0026lt;j \\leq k)\\end{cases} \\]\nIf \\(1 \\leq i \\leq k, \\mathbf{u} \\in Q^{k-1}\\), and \\(\\mathbf{x}=\\tau_{i}(\\mathbf{u})\\), then\n\\[ x_{j}= \\begin{cases}u_{j} \u0026amp; (1 \\leq j\u0026lt;i) \\\\ 0 \u0026amp; (j=i), \\\\ u_{j-1} \u0026amp; (i\u0026lt;j \\leq k)\\end{cases} \\]\nFor \\(0 \\leq i \\leq k\\), let \\(J_{i}\\) be the Jacobian of the mapping\n\\[ \\left(u_{1}, \\ldots, u_{k-1}\\right) \\rightarrow\\left(x_{1}, \\ldots, x_{r-1}, x_{r+1}, \\ldots, x_{k}\\right) \\]\ninduced by \\(\\tau_{i}\\). When \\(i=0\\) and when \\(i=r,(98)\\) and (99) show that (100) is the identity mapping. Thus \\(J_{0}=1, J_{r}=1\\). For other \\(i\\), the fact that \\(x_{i}=0\\) in (99) shows that \\(J_{i}\\) has a row of zeros, hence \\(J_{i}=0\\). Thus\n\\[ \\int_{\\tau_{i}} \\lambda=0 \\quad(i \\neq 0, i \\neq r), \\]\nby (35) and (96). Consequently, (97) gives\n\\[ \\begin{aligned} \\int_{\\partial \\sigma} \\lambda \u0026amp;=(-1)^{r-1} \\int_{\\tau_{0}} \\lambda+(-1)^{r} \\int_{\\tau_{r}} \\lambda \\\\ \u0026amp;=(-1)^{r-1} \\int_{0}\\left[f\\left(\\tau_{0}(\\mathbf{u})\\right)-f\\left(\\tau_{r}(\\mathbf{u})\\right)\\right] d \\mathbf{u} . \\end{aligned} \\]\nOn the other hand,\n\\[ \\begin{aligned} d \\lambda \u0026amp;=\\left(D_{r} f\\right)(\\mathbf{x}) d x_{r} \\wedge d x_{1} \\wedge \\cdots \\wedge d x_{r-1} \\wedge d x_{r+1} \\wedge \\cdots \\wedge d x_{k} \\\\ \u0026amp;=(-1)^{r-1}\\left(D_{r} f\\right)(\\mathbf{x}) d x_{1} \\wedge \\cdots \\wedge d x_{k} \\end{aligned} \\]\nso that\n\\[ \\int_{0} d \\lambda=(-1)^{r-1} \\int_{Q^{k}}\\left(D_{r} f\\right)(\\mathbf{x}) d \\mathbf{x} . \\]\nWe evaluate (103) by first integrating with respect to \\(x_{r}\\), over the interval\n\\[ \\left[0,1-\\left(x_{1}+\\cdots+x_{r-1}+x_{r+1}+\\cdots+x_{k}\\right)\\right] \\text {, } \\]\nput \\(\\left(x_{1}, \\ldots, x_{r-1}, x_{r+1}, \\ldots, x_{k}\\right)=\\left(u_{1}, \\ldots, u_{k-1}\\right)\\), and see with the aid of (98) that the integral over \\(Q^{k}\\) in (103) is equal to the integral over \\(Q^{k-1}\\) in (102). Thus (94) holds, and the proof is complete.\n","date":"2022-08-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/7-stokes-theorem/","section":"baby rudin","tags":null,"title":"7 STOKES' THEOREM"},{"categories":null,"contents":"10.34 Definition Let \\(\\omega\\) be a \\(k\\)-form in an open set \\(E \\subset R^{n}\\). If there is a \\((k-1)\\) form \\(\\lambda\\) in \\(E\\) such that \\(\\omega=d \\lambda\\), then \\(\\omega\\) is said to be exact in \\(E\\).\nIf \\(\\omega\\) is of class \\(\\mathscr{C}^{\\prime}\\) and \\(d \\omega=0\\), then \\(\\omega\\) is said to be closed.\nTheorem \\(10.20(b)\\) shows that every exact form of class \\(\\mathscr{C}^{\\prime}\\) is closed.\nIn certain sets \\(E\\), for example in convex ones, the converse is true; this is the content of Theorem \\(10.39\\) (usually known as Poincarés lemma) and\nTheorem 10.40. However, Examples \\(10.36\\) and \\(10.37\\) will exhibit closed forms that are not exact.\n10.35 Remarks\nWhether a given \\(k\\)-form \\(\\omega\\) is or is not closed can be verified by simply differentiating the coefficients in the standard presentation of \\(\\omega\\). For example, a 1-form \\[ \\begin{equation} \\omega=\\sum_{i=1}^{n} f_{i}(\\mathbf{x}) d x_{i}, \\label{eq:104} \\end{equation} \\]\nwith \\(f_{i} \\in \\mathscr{C}^{\\prime}(E)\\) for some open set \\(E \\subset R^{n}\\), is closed if and only if the equations\n\\[ \\begin{equation} \\left(D_{j} f_{i}\\right)(\\mathbf{x})=\\left(D_{i} f_{j}\\right)(\\mathbf{x}) \\label{eq:105} \\end{equation} \\]\nhold for all \\(i, j\\) in \\(\\{1, \\ldots, n\\}\\) and for all \\(\\mathbf{x} \\in E\\). Note that (105) is a “pointwise” condition; it does not involve any global properties that depend on the shape of \\(E\\).\nOn the other hand, to show that \\(\\omega\\) is exact in \\(E\\), one has to prove the existence of a form \\(\\lambda\\), defined in \\(E\\), such that \\(d \\lambda=\\omega\\). This amounts to solving a system of partial differential equations, not just locally, but in all of \\(E\\). For example, to show that (104) is exact in a set \\(E\\), one has to find a function (or 0-form) \\(g \\in \\mathscr{B}^{\\prime}(E)\\) such that\n\\[ \\begin{equation} \\left(D_{i} g\\right)(\\mathbf{x})=f_{i}(\\mathbf{x}) \\quad(\\mathbf{x} \\in E, 1 \\leq i \\leq n) . \\label{eq:106} \\end{equation} \\]\nOf course, (105) is a necessary condition for the solvability of (106).\nLet \\(\\omega\\) be an exact \\(k\\)-form in \\(E\\). Then there is a \\((k-1)\\)-form \\(\\lambda\\) in \\(E\\) with \\(d \\lambda=\\omega\\), and Stokes’ theorem asserts that \\[ \\begin{equation} \\int_{\\Psi} \\omega=\\int_{\\Psi} d \\lambda=\\int_{\\partial \\Psi} \\lambda \\label{eq:107} \\end{equation} \\]\nfor every \\(k\\)-chain \\(\\Psi\\) of class \\(\\mathscr{C}^{\\prime \\prime}\\) in \\(E\\).\nIf \\(\\Psi_{1}\\) and \\(\\Psi_{2}\\) are such chains, and if they have the same boundaries, it follows that\n\\[ \\int_{\\Psi_{1}} \\omega=\\int_{\\Psi_{2}} \\omega . \\]\nIn particular, the integral of an exact \\(k\\)-form in \\(E\\) is 0 over every \\(k\\)-chain in \\(E\\) whose boundary is 0 .\nAs an important special case of this, note that integrals of exact 1-forms in \\(E\\) are 0 over closed (differentiable) curves in \\(E\\).\nLet \\(\\omega\\) be a closed \\(k\\)-form in \\(E\\). Then \\(d \\omega=0\\), and Stokes’ theorem asserts that \\[ \\begin{equation} \\int_{\\partial \\Psi} \\omega=\\int_{\\Psi} d \\omega=0 \\label{eq:108} \\end{equation} \\]\nfor every \\((k+1)\\)-chain \\(\\Psi\\) of class \\(\\mathscr{B}^{\\prime \\prime}\\) in \\(E\\).\nIn other words, integrals of closed \\(k\\)-forms in \\(E\\) are 0 over \\(k\\)-chains that are boundaries of \\((k+1)\\)-chains in \\(E\\).\nLet \\(\\Psi\\) be a \\((k+1)\\)-chain in \\(E\\) and let \\(\\lambda\\) be a \\((k-1)\\)-form in \\(E\\), both of class \\(\\mathscr{C}^{\\prime \\prime}\\). Since \\(d^{2} \\lambda=0\\), two applications of Stokes’ theorem show that \\[ \\begin{equation} \\int_{\\partial \\partial \\Psi} \\lambda=\\int_{\\partial \\Psi} d \\lambda=\\int_{\\Psi} d^{2} \\lambda=0 . \\label{eq:109} \\end{equation} \\]\nWe conclude that \\(\\partial^{2} \\Psi=0\\). In other words, the boundary of a boundary is 0 .\nSee Exercise 16 for a more direct proof of this.\n10.36 Example Let \\(E=R^{2}-\\{0\\}\\), the plane with the origin removed. The 1-form\n\\[ \\begin{equation} \\eta=\\frac{x d y-y d x}{x^{2}+y^{2}} \\label{eq:110} \\end{equation} \\]\nis closed in \\(R^{2}-\\{\\mathbf{0}\\}\\). This is easily verified by differentiation. Fix \\(r\u0026gt;0\\), and define\n\\[ \\begin{equation} \\gamma(t)=(r \\cos t, r \\sin t) \\quad(0 \\leq t \\leq 2 \\pi) . \\label{eq:111} \\end{equation} \\]\nThen \\(\\gamma\\) is a curve (an “oriented 1-simplex”) in \\(R^{2}-\\{0\\}\\). Since \\(\\gamma(0)=\\gamma(2 \\pi)\\), we have\n\\[ \\begin{equation} \\partial \\gamma=0 \\text {. } \\label{eq:112} \\end{equation} \\]\nDirect computation shows that\n\\[ \\begin{equation} \\int_{\\gamma} \\eta=2 \\pi \\neq 0 . \\label{eq:113} \\end{equation} \\]\nThe discussion in Remarks \\(10.35(b)\\) and \\((c)\\) shows that we can draw two conclusions from (113):\nFirst, \\(\\eta\\) is not exact in \\(R^{2}-\\{0\\}\\), for otherwise (112) would force the integral (113) to be \\(0 .\\)\nSecondly, \\(\\gamma\\) is not the boundary of any 2 -chain in \\(R^{2}-\\{0\\}\\) (of class \\(\\mathscr{C}^{\\prime \\prime}\\) ), for otherwise the fact that \\(\\eta\\) is closed would force the integral (113) to be 0 .\n10.37 Example Let \\(E=R^{3}-\\{0\\}, 3\\)-space with the origin removed. Define\n\\[ \\begin{equation} \\zeta=\\frac{x d y \\wedge d z+y d z \\wedge d x+z d x \\wedge d y}{\\left(x^{2}+y^{2}+z^{2}\\right)^{3 / 2}} \\label{eq:114} \\end{equation} \\]\nwhere we have written \\((x, y, z)\\) in place of \\(\\left(x_{1}, x_{2}, x_{3}\\right)\\). Differentiation shows that \\(d \\zeta=0\\), so that \\(\\zeta\\) is a closed 2-form in \\(R^{3}-\\{\\boldsymbol{0}\\}\\).\nLet \\(\\Sigma\\) be the 2-chain in \\(R^{3}-\\{0\\}\\) that was constructed in Example 10.32; recall that \\(\\Sigma\\) is a parametrization of the unit sphere in \\(R^{3}\\). Using the rectangle \\(D\\) of Example \\(10.32\\) as parameter domain, it is easy to compute that\n\\[ \\begin{equation} \\int_{\\Sigma} \\zeta=\\int_{D} \\sin u d u d v=4 \\pi \\neq 0 . \\label{eq:115} \\end{equation} \\]\nAs in the preceding example, we can now conclude that \\(\\zeta\\) is not exact in \\(R^{3}-\\{\\boldsymbol{0}\\}\\) (since \\(\\partial \\Sigma=0\\), as was shown in Example 10.32) and that the sphere \\(\\Sigma\\) is not the boundary of any 3 -chain in \\(R^{3}-\\{0\\}\\) (of class \\(\\mathscr{C}^{\\prime \\prime}\\) ), although \\(\\partial \\Sigma=0\\).\nThe following result will be used in the proof of Theorem 10.39.\n10.38 Theorem Suppose \\(E\\) is a convex open set in \\(R^{n}, f \\in \\mathscr{C}^{\\prime}(E), p\\) is an integer, \\(1 \\leq p \\leq n\\), and\n\\[ \\begin{equation} \\left(D_{j} f\\right)(\\mathbf{x})=0 \\quad(p\u0026lt;j \\leq n, \\mathbf{x} \\in E) . \\label{eq:116} \\end{equation} \\]\nThen there exists an \\(F \\in \\mathscr{C}^{\\prime}(E)\\) such that\n\\[ \\begin{equation} \\left(D_{p} F\\right)(\\mathbf{x})=f(\\mathbf{x}), \\quad\\left(D_{j} F\\right)(\\mathbf{x})=0 \\quad(p\u0026lt;j \\leq n, \\mathbf{x} \\in E) . \\label{eq:117} \\end{equation} \\]\nProof Write \\(\\mathbf{x}=\\left(\\mathbf{x}^{\\prime}, x_{p}, \\mathbf{x}^{\\prime \\prime}\\right)\\), where\n\\[ \\mathbf{x}^{\\prime}=\\left(x_{1}, \\ldots, x_{p-1}\\right), \\mathbf{x}^{\\prime \\prime}=\\left(x_{p+1}, \\ldots, x_{n}\\right) \\text {. } \\]\n(When \\(p=1, \\mathbf{x}^{\\prime}\\) is absent; when \\(p=n, \\mathbf{x}^{\\prime \\prime}\\) is absent.) Let \\(V\\) be the set of all \\(\\left(\\mathbf{x}^{\\prime}, x_{p}\\right) \\in R^{p}\\) such that \\(\\left(\\mathbf{x}^{\\prime}, x_{p}, \\mathbf{x}^{\\prime \\prime}\\right) \\in E\\) for some \\(\\mathbf{x}^{\\prime \\prime}\\). Being a projection of \\(E, V\\) is a convex open set in \\(R^{p}\\). Since \\(E\\) is convex and (116) holds, \\(f(\\mathbf{x})\\) does not depend on \\(\\mathbf{x}^{\\prime \\prime}\\). Hence there is a function \\(\\varphi\\), with domain \\(V\\), such that\n\\[ f(\\mathbf{x})=\\varphi\\left(\\mathbf{x}^{\\prime}, x_{p}\\right) \\]\nfor all \\(\\mathbf{x} \\in E\\).\nIf \\(p=1, V\\) is a segment in \\(R^{1}\\) (possibly unbounded). Pick \\(c \\in V\\) and define\n\\[ F(\\mathbf{x})=\\int_{c}^{x_{1}} \\varphi(t) d t \\quad(\\mathbf{x} \\in E) . \\]\nIf \\(p\u0026gt;1\\), let \\(U\\) be the set of all \\(\\mathbf{x}^{\\prime} \\in R^{p-1}\\) such that \\(\\left(\\mathbf{x}^{\\prime}, x_{p}\\right) \\in V\\) for some \\(x_{p}\\). Then \\(U\\) is a convex open set in \\(R^{p-1}\\), and there is a function \\(\\alpha \\in \\mathscr{C}^{\\prime}(U)\\) such that \\(\\left(\\mathbf{x}^{\\prime}, \\alpha\\left(\\mathbf{x}^{\\prime}\\right)\\right) \\in V\\) for every \\(\\mathbf{x}^{\\prime} \\in U\\); in other words, the graph of \\(\\alpha\\) lies in \\(V\\) (Exercise 29). Define\n\\[ F(\\mathbf{x})=\\int_{\\alpha\\left(\\mathbf{x}^{\\prime}\\right)}^{x_{p}} \\varphi\\left(\\mathbf{x}^{\\prime}, t\\right) d t \\quad(\\mathbf{x} \\in E) . \\]\nIn either case, \\(F\\) satisfies (117).\n(Note: Recall the usual convention that \\(\\int_{a}^{b}\\) means \\(-\\int_{b}^{a}\\) if \\(b\u0026lt;a\\).)\n10.39 Theorem If \\(E \\subset R^{n}\\) is convex and open, if \\(k \\geq 1\\), if \\(\\omega\\) is a \\(k\\)-form of class \\(\\mathscr{C}^{\\prime}\\) in \\(E\\), and if \\(d \\omega=0\\), then there is \\(a(k-1)\\)-form \\(\\lambda\\) in \\(E\\) such that \\(\\omega=d \\lambda\\).\nBriefly, closed forms are exact in convex sets.\nProof For \\(p=1, \\ldots, n\\), let \\(Y_{p}\\) denote the set of all \\(k\\)-forms \\(\\omega\\), of class \\(\\mathscr{C}^{\\prime}\\) in \\(E\\), whose standard presentation\n\\[ \\omega=\\sum_{I} f_{I}(\\mathbf{x}) d x_{I} \\]\ndoes not involve \\(d x_{p+1}, \\ldots, d x_{n}\\). In other words, \\(I \\subset\\{1, \\ldots, p\\}\\) if \\(f_{I}(\\mathbf{x}) \\neq 0\\) for some \\(\\mathbf{x} \\in E\\). We shall proceed by induction on \\(p\\).\nAssume first that \\(\\omega \\in Y_{1}\\). Then \\(\\omega=f(\\mathbf{x}) d x_{1}\\). Since \\(d \\omega=0\\), \\(\\left(D_{j} f\\right)(\\mathbf{x})=0\\) for \\(1\u0026lt;j \\leq n, \\mathbf{x} \\in E\\). By Theorem \\(10.38\\) there is an \\(F \\in \\mathscr{C}^{\\prime}(E)\\) such that \\(D_{1} F=f\\) and \\(D_{j} F=0\\) for \\(1\u0026lt;j \\leq n\\). Thus\n\\[ d F=\\left(D_{1} F\\right)(\\mathbf{x}) d x_{1}=f(\\mathbf{x}) d x_{1}=\\omega . \\]\nNow we take \\(p\u0026gt;1\\) and make the following induction hypothesis: Every closed \\(k\\)-form that belongs to \\(Y_{p-1}\\) is exact in \\(E\\).\nChoose \\(\\omega \\in Y_{p}\\) so that \\(d \\omega=0\\). By (118),\n\\[ \\sum_{I} \\sum_{j=1}^{n}\\left(D_{j} f_{I}\\right)(\\mathbf{x}) d x_{j} \\wedge d x_{I}=d \\omega=0 . \\]\nConsider a fixed \\(j\\), with \\(p\u0026lt;j \\leq n\\). Each \\(I\\) that occurs in (118) lies in \\(\\{1, \\ldots, p\\}\\). If \\(I_{1}, I_{2}\\) are two of these \\(k\\)-indices, and if \\(I_{1} \\neq I_{2}\\), then the \\((k+1)\\)-indices \\(\\left(I_{1}, j\\right),\\left(I_{2}, j\\right)\\) are distinct. Thus there is no cancellation, and we conclude from (119) that every coefficient in (ii8) satisfies\n\\[ \\left(D_{j} f_{I}\\right)(\\mathbf{x})=0 \\quad(\\mathbf{x} \\in E, p\u0026lt;j \\leq n) \\text {. } \\]\nWe now gather those terms in (118) that contain \\(d x_{p}\\) and rewrite \\(\\omega\\) in the form\n\\[ \\omega=\\alpha+\\sum_{I_{0}} f_{I}(\\mathbf{x}) d x_{I_{0}} \\wedge d x_{p}, \\]\nwhere \\(\\alpha \\in Y_{p-1}\\), each \\(I_{0}\\) is an increasing \\((k-1)\\)-index in \\(\\{1, \\ldots, p-1\\}\\), and \\(I=\\left(I_{0}, p\\right)\\). By \\((120)\\), Theorem \\(10.38\\) furnishes functions \\(F_{I} \\in \\mathscr{C}^{\\prime}(E)\\) such that\n\\[ D_{p} F_{I}=f_{I}, \\quad D_{j} F_{I}=0 \\quad(p\u0026lt;j \\leq n) . \\]\nPut\n\\[ \\beta=\\sum_{I_{0}} F_{I}(\\mathbf{x}) d x_{I_{0}} \\]\nand define \\(\\gamma=\\omega-(-1)^{k-1} d \\beta\\). Since \\(\\beta\\) is a \\((k-1)\\)-form, it follows that\n\\[ \\begin{aligned} \\gamma \u0026amp;=\\omega-\\sum_{I_{0}} \\sum_{j=1}^{p}\\left(D_{j} F_{I}\\right)(\\mathbf{x}) d x_{I_{0}} \\wedge d x_{j} \\\\ \u0026amp;=\\alpha-\\sum_{I_{0}}^{p-1} \\sum_{j=1}^{p-1}\\left(D_{j} F_{I}\\right)(\\mathbf{x}) d x_{I_{0}} \\wedge d x_{j} \\end{aligned} \\]\nwhich is clearly in \\(Y_{p-1}\\). Since \\(d \\omega=0\\) and \\(d^{2} \\beta=0\\), we have \\(d \\gamma=0\\). Our induction hypothesis shows therefore that \\(\\gamma=d \\mu\\) for some \\((k-1)\\)-form \\(\\mu\\) in \\(E\\). If \\(\\lambda=\\mu+(-1)^{k-1} \\beta\\), we conclude that \\(\\omega=d \\lambda\\),\nBy induction, this completes the proof.\n10.40 Theorem Fix \\(k, 1 \\leq k \\leq n\\). Let \\(E \\subset R^{n}\\) be an open set in which every closed \\(k\\)-form is exact. Let \\(T\\) be a 1-1 \\(\\mathscr{C}^{\\prime \\prime}\\)-mapping of \\(E\\) onto an open set \\(U \\subset R^{n}\\) whose inverse \\(S\\) is also of class \\(\\mathscr{C}^{\\prime \\prime}\\).\nThen every closed \\(k\\)-form in \\(U\\) is exact in \\(U\\).\nNote that every convex open set \\(E\\) satisfies the present hypothesis, by Theorem 10.39. The relation between \\(E\\) and \\(U\\) may be expressed by saying that they are \\(\\mathscr{C}^{\\prime \\prime}\\)-equivalent.\nThus every closed form is exact in any set which is \\(\\mathscr{C}^{\\prime \\prime}\\)-equivalent to a convex open set.\nProof Let \\(\\omega\\) be a \\(k\\)-form in \\(U\\), with \\(d \\omega=0\\). By Theorem \\(10.22(c)\\), \\(\\omega_{T}\\) is a \\(k\\)-form in \\(E\\) for which \\(d\\left(\\omega_{T}\\right)=0\\). Hence \\(\\omega_{T}=d \\lambda\\) for some \\((k-1)\\)-form \\(\\lambda\\) in \\(E\\). By Theorem 10.23, and another application of Theorem \\(10.22(c)\\),\n\\[ \\omega=\\left(\\omega_{T}\\right)_{S}=(d \\lambda)_{S}=d\\left(\\lambda_{S}\\right) . \\]\nSince \\(\\lambda_{S}\\) is a \\((k-1)\\)-form in \\(U, \\omega\\) is exact in \\(U\\).\n10.41 Remark In applications, cells (see Definition 2.17) are often more convenient parameter domains than simplexes. If our whole development had been based on cells rather than simplexes, the computation that occurs in the proof of Stokes’ theorem would be even simpler. (It is done that way in Spivak’s book.) The reason for preferring simplexes is that the definition of the boundary of an oriented simplex seems easier and more natural than is the case for a cell. (See Exercise 19.) Also, the partitioning of sets into simplexes (called “triangulation’) plays an important role in topology, and there are strong connections between certain aspects of topology, on the one hand, and differential forms, on the other. These are hinted at in Sec. 10.35. The book by Singer and Thorpe contains a good introduction to this topic.\nSince every cell can be triangulated, we may regard it as a chain. For dimension 2, this was done in Example 10.32; for dimension 3, see Exercise \\(18 .\\)\nPoincaré’s lemma (Theorem 10.39) can be proved in several ways. See, for example, page 94 in Spivak’s book, or page 280 in Fleming’s. Two simple proofs for certain special cases are indicated in Exercises 24 and \\(27 .\\)\n","date":"2022-08-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/8-closed-forms-and-exact-forms/","section":"baby rudin","tags":null,"title":"8 CLOSED FORMS AND EXACT FORMS"},{"categories":null,"contents":"10.1 Definition Suppose \\(I^{k}\\) is a \\(k\\)-cell in \\(R^{k}\\), consisting of all\n\\[ \\mathbf{x}=\\left(x_{1}, \\ldots, x_{k}\\right) \\]\nsuch that\n\\[ a_{i} \\leq x_{i} \\leq b_{i} \\quad(i=1, \\ldots, k), \\]\n\\(I^{j}\\) is the \\(j\\)-cell in \\(R^{j}\\) defined by the first \\(j\\) inequalities (1), and \\(f\\) is a real continuous function on \\(I^{k}\\).\nPut \\(f=f_{k}\\), and define \\(f_{k-1}\\) on \\(I^{k-1}\\) by\n\\[ f_{k-1}\\left(x_{1}, \\ldots, x_{k-1}\\right)=\\int_{a_{k}}^{b_{k}} f_{k}\\left(x_{1}, \\ldots, x_{k-1}, x_{k}\\right) d x_{k} . \\]\nThe uniform continuity of \\(f_{k}\\) on \\(I^{k}\\) shows that \\(f_{k-1}\\) is continuous on \\(I^{k-1}\\). Hence we can repeat this process and obtain functions \\(f_{j}\\), continuous on \\(I^{j}\\), such that \\(f_{j-1}\\) is the integral of \\(f_{j}\\), with respect to \\(x_{j}\\), over \\(\\left[a_{j}, b_{j}\\right]\\). After \\(k\\) steps we arrive at a number \\(f_{0}\\), which we call the integral of \\(f\\) over \\(I^{k}\\); we write it in the form\n\\[ \\int_{I^{k}} f(\\mathbf{x}) d \\mathbf{x} \\quad \\text { or } \\quad \\int_{I^{k}} f . \\]\nA priori, this definition of the integral depends on the order in which the \\(k\\) integrations are carried out. However, this dependence is only apparent. To prove this, let us introduce the temporary notation \\(L(f)\\) for the integral (2) and \\(L^{\\prime}(f)\\) for the result obtained by carrying out the \\(k\\) integrations in some other order.\n10.2 Theorem For every \\(f \\in \\mathscr{C}\\left(I^{k}\\right), L(f)=L^{\\prime}(f)\\).\nProof If \\(h(\\mathbf{x})=h_{1}\\left(x_{1}\\right) \\cdots h_{k}\\left(x_{k}\\right)\\), where \\(h_{j} \\in \\mathscr{C}\\left(\\left[a_{j}, b_{j}\\right]\\right)\\), then\n\\[ L(h)=\\prod_{i=1}^{k} \\int_{a_{i}}^{b_{i}} h_{i}\\left(x_{i}\\right) d x_{i}=L^{\\prime}(h) . \\]\nIf \\(\\mathscr{A}\\) is the set of all finite sums of such functions \\(h\\), it follows that \\(L(g)=\\) \\(L^{\\prime}(g)\\) for all \\(g \\in \\mathscr{A}\\). Also, \\(\\mathscr{A}\\) is an algebra of functions on \\(I^{k}\\) to which the Stone-Weierstrass theorem applies.\nPut \\(V=\\prod_{1}^{k}\\left(b_{i}-a_{i}\\right)\\). If \\(f \\in \\mathscr{C}\\left(I^{k}\\right)\\) and \\(\\varepsilon\u0026gt;0\\), there exists \\(g \\in \\mathscr{A}\\) such that \\(\\|f-g\\|\u0026lt;\\varepsilon / V\\), where \\(\\|f\\|\\) is defined as \\(\\max |f(\\mathbf{x})|\\left(\\mathbf{x} \\in I^{k}\\right)\\). Then \\(|L(f-g)|\u0026lt;\\varepsilon,\\left|L^{\\prime}(f-g)\\right|\u0026lt;\\varepsilon\\), and since\n\\[ L(f)-L^{\\prime}(f)=L(f-g)+L^{\\prime}(g-f), \\]\nwe conclude that \\(\\left|L(f)-L^{\\prime}(f)\\right|\u0026lt;2 \\varepsilon\\).\nIn this connection, Exercise 2 is relevant.\n10.3 Definition The support of a (real or complex) function \\(f\\) on \\(R^{k}\\) is the closure of the set of all points \\(\\mathbf{x} \\in R^{k}\\) at which \\(f(\\mathbf{x}) \\neq 0\\). If \\(f\\) is a continuous function with compact support, let \\(I^{k}\\) be any \\(k\\)-cell which contains the support of \\(f\\), and define\n\\[ \\int_{\\boldsymbol{R}^{\\boldsymbol{k}}} f=\\int_{I^{k}} f . \\]\nThe integral so defined is evidently independent of the choice of \\(I^{k}\\), provided only that \\(I^{k}\\) contains the support of \\(f\\).\nIt is now tempting to extend the definition of the integral over \\(R^{k}\\) to functions which are limits (in some sense) of continuous functions with compact support. We do not want to discuss the conditions under which this can be done; the proper setting for this question is the Lebesgue integral. We shall merely describe one very simple example which will be used in the proof of Stokes’ theorem.\n10.4 Example Let \\(Q^{k}\\) be the \\(k\\)-simplex which consists of all points \\(\\mathbf{x}=\\) \\(\\left(x_{1}, \\ldots, x_{k}\\right)\\) in \\(R^{k}\\) for which \\(x_{1}+\\cdots+x_{k} \\leq 1\\) and \\(x_{i} \\geq 0\\) foi \\(i=1, \\ldots, k\\). If \\(k=3\\), for example, \\(Q^{k}\\) is a tetrahedron, with vertices at \\(\\mathbf{0}, \\mathbf{e}_{1}, \\mathbf{e}_{2}, \\mathbf{e}_{3}\\). If \\(f \\in \\mathscr{C}\\left(Q^{k}\\right)\\), extend \\(f\\) to a function on \\(I^{k}\\) by setting \\(f(\\mathbf{x})=0\\) off \\(Q^{k}\\), and define\n\\[ \\int_{\\mathbf{Q}^{k}} f=\\int_{I^{k}} f . \\]\nHere \\(I^{k}\\) is the “unit cube” defined by\n\\[ 0 \\leq x_{i} \\leq 1 \\quad(1 \\leq i \\leq k) . \\]\nSince \\(f\\) may be discontinuous on \\(I^{k}\\), the existence of the integral on the right of (4) needs proof. We also wish to show that this integral is independent of the order in which the \\(k\\) single integrations are carried out.\nTo do this, suppose \\(0\u0026lt;\\delta\u0026lt;1\\), put\n\\[ \\varphi(t)= \\begin{cases}1 \u0026amp; (t \\leq 1-\\delta) \\\\ \\frac{1-t)}{\\delta} \u0026amp; (1-\\delta\u0026lt;t \\leq 1) \\\\ 0 \u0026amp; (1\u0026lt;t),\\end{cases} \\]\nand define\n\\[ F(\\mathbf{x})=\\varphi\\left(x_{1}+\\cdots+x_{k}\\right) f(\\mathbf{x}) \\quad\\left(\\mathbf{x} \\in I^{k}\\right) \\]\nThen \\(F \\in \\mathscr{C}\\left(I^{k}\\right)\\).\nPut \\(\\mathbf{y}=\\left(x_{1}, \\ldots, x_{k-1}\\right), \\mathbf{x}=\\left(\\mathbf{y}, x_{k}\\right)\\). For each \\(\\mathbf{y} \\in I^{k-1}\\), the set of all \\(x_{k}\\) such that \\(F\\left(\\mathbf{y}, x_{k}\\right) \\neq f\\left(\\mathbf{y} ; x_{k}\\right)\\) is either empty or is a segment whose length does not exceed \\(\\delta\\). Since \\(0 \\leq \\varphi \\leq 1\\), it follows that\n\\[ \\left|F_{k-1}(\\mathbf{y})-f_{k-1}(\\mathbf{y})\\right| \\leq \\delta\\|f\\| \\quad\\left(\\mathbf{y} \\in I^{k-1}\\right), \\]\nwhere \\(\\|f\\|\\) has the same meaning as in the proof of Theorem 10.2, and \\(F_{k-1}\\), \\(f_{k-1}\\) are as in Definition \\(10.1\\).\nAs \\(\\delta \\rightarrow 0\\), (7) exhibits \\(f_{k-1}\\) as a uniform limit of a sequence of continuous functions. Thus \\(f_{k-1} \\in \\mathscr{C}\\left(I^{k-1}\\right)\\), and the further integrations present no problem.\nThis proves the existence of the integral (4). Moreover, (7) shows that\n\\[ \\left|\\int_{I^{k}} F(\\mathbf{x}) d \\mathbf{x}-\\int_{I^{k}} f(\\mathbf{x}) d \\mathbf{x}\\right| \\leq \\delta\\|f\\| . \\]\nNote that (8) is true, regardless of the order in which the \\(k\\) single integrations are carried out. Since \\(F \\in \\mathscr{C}\\left(I^{k}\\right), \\int F\\) is unaffected by any change in this order. Hence (8) shows that the same is true of \\(\\int f\\).\nThis completes the proof.\nOur next goal is the change of variables formula stated in Theorem \\(10.9 .\\) To facilitate its proof, we first discuss so-called primitive mappings, and partitions of unity. Primitive mappings will enable us to get a clearer picture of the local action of a \\(\\mathscr{C}^{\\prime}\\)-mapping with invertible derivative, and partitions of unity are a very useful device that makes it possible to use local information in a global setting.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/1-integration/","section":"baby rudin","tags":null,"title":"1 INTEGRATION"},{"categories":null,"contents":"10.5 Definition If \\(\\mathbf{G}\\) maps an open set \\(E \\subset R^{n}\\) into \\(R^{n}\\), and if there is an integer \\(m\\) and a real function \\(g\\) with domain \\(E\\) such that\n\\[ \\mathbf{G}(\\mathbf{x})=\\sum_{i \\neq m} x_{i} \\mathbf{e}_{i}+g(\\mathbf{x}) \\mathbf{e}_{m} \\quad(\\mathbf{x} \\in E), \\]\nthen we call G primitive. A primitive mapping is thus one that changes at most one coordinate. Note that (9) can also be written in the form\n\\[ \\mathbf{G}(\\mathbf{x})=\\mathbf{x}+\\left[g(\\mathbf{x})-x_{m}\\right] \\mathbf{e}_{m} . \\]\nIf \\(g\\) is differentiable at some point \\(\\mathbf{a} \\in E\\), so is \\(\\mathbf{G}\\). The matrix \\(\\left[\\alpha_{i j}\\right]\\) of the operator \\(\\mathbf{G}^{\\prime}(\\mathbf{a})\\) has\n\\[ \\left(D_{1} g\\right)(\\mathbf{a}), \\ldots,\\left(D_{m} g\\right)(\\mathbf{a}), \\ldots,\\left(D_{n} g\\right)(\\mathbf{a}) \\]\nas its \\(m\\) th row. For \\(j \\neq m\\), we have \\(\\alpha_{j j}=1\\) and \\(\\alpha_{i j}=0\\) if \\(i \\neq j\\). The Jacobian of \\(\\mathbf{G}\\) at \\(\\mathbf{a}\\) is thus given by\n\\[ J_{\\mathbf{G}}(\\mathbf{a})=\\operatorname{det}\\left[\\mathbf{G}^{\\prime}(\\mathbf{a})\\right]=\\left(D_{m} g\\right)(\\mathbf{a}), \\]\nand we see (by Theorem 9.36) that \\(\\mathbf{G}^{\\prime}(\\mathbf{a})\\) is invertible if and only if \\(\\left(D_{m} g\\right)(\\mathbf{a}) \\neq 0\\).\n10.6 Definition A linear operator \\(B\\) on \\(R^{n}\\) that interchanges some pair of members of the standard basis and leaves the others fixed will be called a flip.\nFor example, the flip \\(B\\) on \\(R^{4}\\) that interchanges \\(\\mathbf{e}_{2}\\) and \\(\\mathbf{e}_{4}\\) has the form\n\\[ B\\left(x_{1} \\mathbf{e}_{1}+x_{2} \\mathbf{e}_{2}+x_{3} \\mathbf{e}_{3}+x_{4} \\mathbf{e}_{4}\\right)=x_{1} \\mathbf{e}_{1}+x_{2} \\mathbf{e}_{4}+x_{3} \\mathbf{e}_{3}+x_{4} \\mathbf{e}_{2} \\]\nor, equivalently,\n\\[ B\\left(x_{1} \\mathbf{e}_{1}+x_{2} \\mathbf{e}_{2}+x_{3} \\mathbf{e}_{3}+x_{4} \\mathbf{e}_{4}\\right)=x_{1} \\mathbf{e}_{1}+x_{4} \\mathbf{e}_{2}+x_{3} \\mathbf{e}_{3}+x_{2} \\mathbf{e}_{4} . \\]\nHence \\(B\\) can also be thought of as interchanging two of the coordinates, rather than two basis vectors.\nIn the proof that follows, we shall use the projections \\(P_{0}, \\ldots, P_{n}\\) in \\(R^{n}\\), defined by \\(P_{0} \\mathbf{x}=\\mathbf{0}\\) and\n\\[ P_{m} \\mathbf{x}=x_{1} \\mathbf{e}_{1}+\\cdots+x_{m} \\mathbf{e}_{m} \\]\nfor \\(1 \\leq m \\leq n\\). Thus \\(P_{m}\\) is the projection whose range and null space are spanned by \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{m}\\right\\}\\) and \\(\\left\\{\\mathbf{e}_{m+1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\), respectively.\n10.7 Theorem Suppose \\(\\mathbf{F}\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into \\(R^{n}, \\mathbf{0} \\in E\\), \\(\\mathbf{F}(\\mathbf{0})=\\mathbf{0}\\), and \\(\\mathbf{F}^{\\prime}(\\mathbf{0})\\) is invertible.\nThen there is a neighborhood of 0 in \\(R^{n}\\) in which a representation\n\\[ \\mathbf{F}(\\mathbf{x})=B_{1} \\cdots B_{n-1} \\mathbf{G}_{n} \\circ \\cdots \\circ \\mathbf{G}_{1}(\\mathbf{x}) \\]\nis valid.\nIn (16), each \\(\\mathbf{G}_{i}\\) is a primitive \\(\\mathscr{C}^{\\prime}\\)-mapping in some neighborhood of \\(\\mathbf{0}\\); \\(\\mathbf{G}_{i}(\\mathbf{0})=\\mathbf{0}, \\mathbf{G}_{i}^{\\prime}(\\mathbf{0})\\) is invertible, and each \\(B_{i}\\) is either a flip or the identity operator.\nBriefly, (16) represents \\(\\mathbf{F}\\) locally as a composition of primitive mappings and flips.\nProof Put \\(\\mathbf{F}=\\mathbf{F}_{1}\\). Assume \\(1 \\leq m \\leq n-1\\), and make the following induction hypothesis (which evidently holds for \\(m=1\\) ):\n\\(V_{m}\\) is a neighborhood of \\(\\mathbf{0}, \\mathbf{F}_{m} \\in \\mathscr{C}^{\\prime}\\left(V_{m}\\right), \\mathbf{F}_{m}(\\mathbf{0})=\\mathbf{0}, \\mathbf{F}_{m}^{\\prime}(\\mathbf{0})\\) is invertible, and\n\\[ P_{m-1} \\mathbf{F}_{m}(\\mathbf{x})=P_{m-1} \\mathbf{x} \\quad\\left(\\mathbf{x} \\in V_{m}\\right) . \\]\nBy (17), we have\n\\[ \\mathbf{F}_{m}(\\mathbf{x})=P_{m-1} \\mathbf{x}+\\sum_{i=m}^{n} \\alpha_{i}(\\mathbf{x}) \\mathbf{e}_{i}, \\]\nwhere \\(\\alpha_{m}, \\ldots, \\alpha_{n}\\) are real \\(\\mathscr{C}^{\\prime}\\)-functions in \\(V_{m}\\). Hence\n\\[ \\mathbf{F}_{m}^{\\prime}(\\mathbf{0}) \\mathbf{e}_{m}=\\sum_{i=m}^{n}\\left(D_{m} \\alpha_{i}\\right)(\\mathbf{0}) \\mathbf{e}_{i} . \\]\nSince \\(\\mathbf{F}_{m}^{\\prime}(\\mathbf{0})\\) is invertible, the left side of (19) is not \\(\\mathbf{0}\\), and therefore there is a \\(k\\) such that \\(m \\leq k \\leq n\\) and \\(\\left(D_{m} \\alpha_{k}\\right)(0) \\neq 0\\).\nLet \\(B_{m}\\) be the flip that interchanges \\(m\\) and this \\(k\\) (if \\(k=m, B_{m}\\) is the identity) and define\n\\[ \\mathbf{G}_{m}(\\mathbf{x})=\\mathbf{x}+\\left[\\alpha_{k}(\\mathbf{x})-x_{m}\\right] \\mathbf{e}_{m} \\quad\\left(\\mathbf{x} \\in V_{m}\\right) \\]\nThen \\(\\mathbf{G}_{m} \\in \\mathscr{C}^{\\prime}\\left(V_{m}\\right), \\mathbf{G}_{m}\\) is primitive, and \\(\\mathbf{G}_{m}^{\\prime}(0)\\) is invertible, since \\(\\left(D_{m} \\alpha_{k}\\right)(0) \\neq 0\\).\nThe inverse function theorem shows therefore that there is an open set \\(U_{m}\\), with \\(\\mathbf{0} \\in U_{m} \\subset V_{m}\\), such that \\(\\mathbf{G}_{m}\\) is a 1-1 mapping of \\(U_{m}\\) onto a neighborhood \\(V_{m+1}\\) of 0 , in which \\(\\mathbf{G}_{m}^{-1}\\) is continuously differentiable. Define \\(\\mathbf{F}_{m+1}\\) by\n\\[ \\mathbf{F}_{m+1}(\\mathbf{y})=B_{m} \\mathbf{F}_{m} \\circ \\mathbf{G}_{m}^{-1}(\\mathbf{y}) \\quad\\left(\\mathbf{y} \\in V_{m+1}\\right) \\text {. } \\]\nThen \\(\\mathbf{F}_{m+1} \\in \\mathscr{C}^{\\prime}\\left(V_{m+1}\\right), \\mathbf{F}_{m+1}(\\mathbf{0})=\\mathbf{0}\\), and \\(\\mathbf{F}_{m+1}^{\\prime}(\\mathbf{0})\\) is invertible (by the chain rule). Also, for \\(\\mathbf{x} \\in U_{m}\\),\n\\[ \\begin{aligned} P_{m} \\mathbf{F}_{m+1}\\left(\\mathbf{G}_{m}(\\mathbf{x})\\right) \u0026amp;=P_{m} B_{m} \\mathbf{F}_{m}(\\mathbf{x}) \\\\ \u0026amp;=P_{m}\\left[P_{m-1} \\mathbf{x}+\\alpha_{k}(\\mathbf{x}) \\mathbf{e}_{m}+\\cdots\\right] \\\\ \u0026amp;=P_{m-1} \\mathbf{x}+\\alpha_{k}(\\mathbf{x}) \\mathbf{e}_{m} \\\\ \u0026amp;=P_{m} \\mathbf{G}_{m}(\\mathbf{x}) \\end{aligned} \\]\nso that\n\\[ P_{m} \\mathbf{F}_{m+1}(\\mathbf{y})=P_{m} \\mathbf{y} \\quad\\left(\\mathbf{y} \\in V_{m+1}\\right) . \\]\nOur induction hypothesis holds therefore with \\(m+1\\) in place of \\(m\\).\n[In (22), we first used (21), then (18) and the definition of \\(B_{m}\\), then the definition of \\(P_{m}\\), and finally (20).]\nSince \\(B_{m} B_{m}=I,(21)\\), with \\(\\mathbf{y}=\\mathbf{G}_{m}(\\mathbf{x})\\), is equivalent to\n\\[ \\mathbf{F}_{m}(\\mathbf{x})=B_{m} \\mathbf{F}_{m+1}\\left(\\mathbf{G}_{m}(\\mathbf{x})\\right) \\quad\\left(\\mathbf{x} \\in U_{m}\\right) \\]\nIf we apply this with \\(m=1, \\ldots, n-1\\), we successively obtain\n\\[ \\begin{aligned} \\mathbf{F}=\\mathbf{F}_{1} \u0026amp;=B_{1} \\mathbf{F}_{2} \\circ \\mathbf{G}_{1} \\\\ \u0026amp;=B_{1} B_{2} \\mathbf{F}_{3} \\circ \\mathbf{G}_{2} \\circ \\mathbf{G}_{1}=\\cdots \\\\ \u0026amp;=B_{1} \\cdots B_{n-1} \\mathbf{F}_{n} \\circ \\mathbf{G}_{n-1} \\circ \\cdots \\circ \\mathbf{G}_{1} \\end{aligned} \\]\nin some neighborhood of \\(\\mathbf{0}\\). By (17), \\(\\mathbf{F}_{n}\\) is primitive. This completes the proof.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/2-primitive-mappings/","section":"baby rudin","tags":null,"title":"2 PRIMITIVE MAPPINGS"},{"categories":null,"contents":"10.8 Theorem Suppose \\(K\\) is a compact subset of \\(R^{n}\\), and \\(\\left\\{\\mathrm{V}_{\\alpha}\\right\\}\\) is an open cover of \\(K\\). Then there exist functions \\(\\psi_{1}, \\ldots, \\psi_{s} \\in \\mathscr{C}\\left(R^{n}\\right)\\) such that\n\\(0 \\leq \\psi_{i} \\leq 1\\) for \\(1 \\leq i \\leq s\\); each \\(\\psi_{i}\\) has its support in some \\(V_{\\alpha}\\), and \\(\\psi_{1}(\\mathbf{x})+\\cdots+\\psi_{s}(\\mathbf{x})=1\\) for every \\(\\mathbf{x} \\in K\\). Because of \\((c),\\left\\{\\psi_{i}\\right\\}\\) is called a partition of unity, and \\((b)\\) is sometimes expressed by saying that \\(\\left\\{\\psi_{i}\\right\\}\\) is subordinate to the cover \\(\\left\\{V_{\\alpha}\\right\\}\\).\nCorollary If \\(f \\in \\mathscr{C}\\left(R^{n}\\right)\\) and the support of \\(f\\) lies in \\(K\\), then\n\\[ f=\\sum_{i=1}^{s} \\psi_{i} f . \\]\nEach \\(\\psi_{i} f\\) has its support in some \\(V_{\\alpha}\\).\nThe point of (25) is that it furnishes a representation of \\(f\\) as a sum of continuous functions \\(\\psi_{i} f\\) with “small” supports.\nProof Associate with each \\(\\mathbf{x} \\in K\\) an index \\(\\alpha(\\mathbf{x})\\) so that \\(\\mathbf{x} \\in V_{\\alpha(\\mathbf{x})}\\). Then there are open balls \\(B(\\mathbf{x})\\) and \\(W(\\mathbf{x})\\), centered at \\(\\mathbf{x}\\), with\n\\[ \\overline{B(\\mathbf{x})} \\subset W(\\mathbf{x}) \\subset \\overline{W(\\mathbf{x})} \\subset V_{\\alpha(\\mathbf{x})} . \\]\nSince \\(K\\) is compact, there are points \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{s}\\) in \\(K\\) such that\n\\[ K \\subset B\\left(\\mathbf{x}_{1}\\right) \\cup \\cdots \\cup B\\left(\\mathbf{x}_{\\mathbf{s}}\\right) . \\]\nBy (26), there are functions \\(\\varphi_{1}, \\ldots, \\varphi_{s} \\in \\mathscr{C}\\left(R^{n}\\right)\\), such that \\(\\varphi_{i}(\\mathbf{x})=1\\) on \\(B\\left(\\mathbf{x}_{i}\\right), \\varphi_{i}(\\mathbf{x})=0\\) outside \\(W\\left(\\mathbf{x}_{i}\\right)\\), and \\(0 \\leq \\varphi_{i}(\\mathbf{x}) \\leq 1\\) on \\(R^{n}\\). Define \\(\\psi_{1}=\\varphi_{1}\\) and\n\\[ \\psi_{i+1}=\\left(1-\\varphi_{1}\\right) \\cdots\\left(1-\\varphi_{i}\\right) \\varphi_{i+1} \\]\nfor \\(i=1, \\ldots, s-1\\).\nProperties \\((a)\\) and \\((b)\\) are clear. The relation\n\\[ \\psi_{1}+\\cdots+\\psi_{i}=1-\\left(1-\\varphi_{1}\\right) \\cdots\\left(1-\\varphi_{i}\\right) \\]\nis trivial for \\(i=1\\). If (29) holds for some \\(i\u0026lt;s\\), addition of (28) and (29) yields (29) with \\(i+1\\) in place of \\(i\\). It follows that\n\\[ \\sum_{i=1}^{s} \\psi_{i}(\\mathbf{x})=1-\\prod_{i=1}^{s}\\left[1-\\varphi_{i}(\\mathbf{x})\\right] \\quad\\left(\\mathbf{x} \\in R^{n}\\right) \\text {. } \\]\nIf \\(\\mathbf{x} \\in K\\), then \\(\\mathbf{x} \\in B\\left(\\mathbf{x}_{i}\\right)\\) for some \\(i\\), hence \\(\\varphi_{i}(\\mathbf{x})=1\\), and the product in (30) is 0 . This proves \\((c)\\).\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/3-partitions-of-unity/","section":"baby rudin","tags":null,"title":"3 PARTITIONS OF UNITY"},{"categories":null,"contents":"We can now describe the effect of a change of variables on a multiple integral. For simplicity, we confine ourselves here to continuous functions with compact support, although this is too restrictive for many applications. This is illustrated by Exercises 9 to 13 .\n10.9 Theorem Suppose \\(T\\) is a 1-1 \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{k}\\) into \\(R^{k}\\) such that \\(J_{T}(\\mathbf{x}) \\neq 0\\) for all \\(\\mathbf{x} \\in E\\). Iff is a continuous function on \\(R^{k}\\) whose support is compact and lies in \\(T(E)\\), then\n\\[ \\int_{R^{\\mathbf{k}}} f(\\mathbf{y}) d \\mathbf{y}=\\int_{R^{\\mathbf{k}}} f(T(\\mathbf{x}))\\left|J_{T}(\\mathbf{x})\\right| d \\mathbf{x} . \\]\nWe recall that \\(J_{T}\\) is the Jacobian of \\(T\\). The assumption \\(J_{T}(\\mathbf{x}) \\neq 0\\) implies, by the inverse function theorem, that \\(T^{-1}\\) is continuous on \\(T(E)\\), and this ensures that the integrand on the right of (31) has compact support in \\(E\\) (Theorem 4.14).\nThe appearance of the absolute value of \\(J_{T}(\\mathbf{x})\\) in (31) may call for a comment. Take the case \\(k=1\\), and suppose \\(T\\) is a \\(1-1 \\mathscr{C}^{\\prime}\\)-mapping of \\(R^{1}\\) onto \\(R^{1}\\). Then \\(J_{T}(x)=T^{\\prime}(x)\\); and if \\(T\\) is increasing, we have\n\\[ \\int_{R^{1}} f(y) d y=\\int_{R^{1}} f(T(x)) T^{\\prime}(x) d x, \\]\nby Theorems \\(6.19\\) and \\(6.17\\), for all continuous \\(f\\) with compact support. But if \\(T\\) decreases, then \\(T^{\\prime}(x)\u0026lt;0\\); and if \\(f\\) is positive in the interior of its support, the left side of (32) is positive and the right side is negative. A correct equation is obtained if \\(T^{\\prime}\\) is replaced by \\(\\left|T^{\\prime}\\right|\\) in (32).\nThe point is that the integrals we are now considering are integrals of functions over subsets of \\(R^{k}\\), and we associate no direction or orientation with these subsets. We shall adopt a different point of view when we come to integration of differential forms over surfaces.\nProof It follows from the remarks just made that (31) is true if \\(T\\) is a primitive \\(\\mathscr{C}^{\\prime}\\)-mapping (see Definition 10.5), and Theorem \\(10.2\\) shows that (31) is true if \\(T\\) is a linear mapping which merely interchanges two coordinates.\nIf the theorem is true for transformations \\(P, Q\\), and if \\(S(\\mathbf{x})=P(Q(\\mathbf{x}))\\), then\n\\[ \\begin{aligned} \\int f(\\mathbf{z}) d \\mathbf{z} \u0026amp;=\\int f(P(\\mathbf{y}))\\left|J_{P}(\\mathbf{y})\\right| d \\mathbf{y} \\\\ \u0026amp;=\\int f(P(Q(\\mathbf{x})))\\left|J_{P}(Q(\\mathbf{x}))\\right|\\left|J_{Q}(\\mathbf{x})\\right| d \\mathbf{x} \\\\ \u0026amp;=\\int f(S(\\mathbf{x}))\\left|J_{S}(\\mathbf{x})\\right| d \\mathbf{x} \\end{aligned} \\]\nsince\n\\[ \\begin{aligned} J_{P}(Q(\\mathbf{x})) J_{Q}(\\mathbf{x}) \u0026amp;=\\operatorname{det} P^{\\prime}(Q(\\mathbf{x})) \\operatorname{det} Q^{\\prime}(\\mathbf{x}) \\\\ \u0026amp;=\\operatorname{det} P^{\\prime}(Q(\\mathbf{x})) Q^{\\prime}(\\mathbf{x})=\\operatorname{det} S^{\\prime}(\\mathbf{x})=J_{S}(\\mathbf{x}) \\end{aligned} \\]\nby the multiplication theorem for determinants and the chain.rule. Thus the theorem is also true for \\(S\\).\nEach point \\(\\mathbf{a} \\in E\\) has a neighborhood \\(U \\subset E\\) in which\n\\[ T(\\mathbf{x})=T(\\mathbf{a})+B_{1} \\cdots B_{k-1} \\mathbf{G}_{k} \\circ \\mathbf{G}_{k-1} \\circ \\cdots \\circ \\mathbf{G}_{1}(\\mathbf{x}-\\mathbf{a}), \\]\nwhere \\(\\mathbf{G}_{i}\\) and \\(B_{i}\\) are as in Theorem 10.7. Setting \\(V=T(U)\\), it follows that (31) holds if the support of \\(f\\) lies in \\(V\\). Thus:\nEach point \\(\\mathbf{y} \\in T(E)\\) lies in an open set \\(V_{\\mathbf{y}} \\subset T(E)\\) such that (31) holds for all continuous functions whose support lies in \\(V_{\\mathbf{y}}\\).\nNow let \\(f\\) be a continuous function with compact support \\(K \\subset T(E)\\). Since \\(\\left\\{V_{\\mathbf{y}}\\right\\}\\) covers \\(K\\), the Corollary to Theorem \\(10.8\\) shcrvs that \\(f=\\Sigma \\psi_{i} f\\), where each \\(\\psi_{i}\\) is continuous, and each \\(\\psi_{i}\\) has its support in some \\(\\mathbf{V}_{\\mathbf{y}}\\). Thus (31) holds for each \\(\\psi_{i} f\\), and hence also for their sum \\(f\\).\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/4-change-of-variables/","section":"baby rudin","tags":null,"title":"4 CHANGE OF VARIABLES"},{"categories":null,"contents":" youtube bilibili note pdf xopp 1.23 Definition The extended real number system consists of the real field \\(R\\) and two symbols, \\(+\\infty\\) and \\(-\\infty\\). We preserve the original order in \\(R\\), and define\nfor every \\(x \\in R\\).\n\\[ -\\infty\u0026lt;x\u0026lt;+\\infty \\]\nIt is then clear that \\(+\\infty\\) is an upper bound of every subset of the extended real number system, and that every nonempty subset has a least upper bound. If, for example, \\(E\\) is a nonempty set of real numbers which is not bounded above in \\(R\\), then \\(\\sup E=+\\infty\\) in the extended real number system.\nExactly the same remarks apply to lower bounds.\nThe extended real number system does not form a field, but it is customary to make the following conventions:\nIf \\(x\\) is real then \\[ x+\\infty=+\\infty, \\quad x-\\infty=-\\infty, \\quad \\frac{x}{+\\infty}=\\frac{x}{-\\infty}=0 . \\]\nIf \\(x\u0026gt;0\\) then \\(x \\cdot(+\\infty)=+\\infty, x \\cdot(-\\infty)=-\\infty\\).\nIf \\(x\u0026lt;0\\) then \\(x \\cdot(+\\infty)=-\\infty, x \\cdot(-\\infty)=+\\infty\\).\nWhen it is desired to make the distinction between real numbers on the one hand and the symbols \\(+\\infty\\) and \\(-\\infty\\) on the other quite explicit, the former are called finite.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/4-the-extended-real-number-system/","section":"baby rudin","tags":null,"title":"4 THE EXTENDED REAL NUMBER SYSTEM"},{"categories":null,"contents":" youtube bilibili note pdf xopp 1.24 Definition A complex number is an ordered pair \\((a, b)\\) of real numbers. “Ordered” means that \\((a, b)\\) and \\((b, a)\\) are regarded as distinct if \\(a \\neq b\\).\nLet \\(x=(a, b), y=(c, d)\\) be two complex numbers. We write \\(x=y\\) if and only if \\(a=c\\) and \\(b=d\\). (Note that this definition is not entirely superfluous; think of equality of rational numbers, represented as quotients of integers.) We define\n\\[ \\begin{aligned} x+y \u0026amp;=(a+c, b+d), \\\\ x y \u0026amp;=(a c-b d, a d+b c) . \\end{aligned} \\]\n\\(1.25\\) Theorem These definitions of addition and multiplication turn the set of all complex numbers into a field, with \\((0,0)\\) and \\((1,0)\\) in the role of 0 and \\(1 .\\)\nProof We simply verify the field axioms, as listed in Definition 1.12. (Of course, we use the field structure of \\(R\\).)\nLet \\(x=(a, b), y=(c, d), z=(e, f)\\).\n(A1) is clear.\n(A2) \\(x+y=(a+c, b+d)=(c+a, d+b)=y+x\\). (A3) \\((x+y)+z=(a+c, b+d)+(e, f)\\)\n\\[ =(a+c+e, b+d+f) \\]\n\\[ \\begin{aligned} \u0026amp; =(a, b)+(c+e, d+f)=x+(y+z) \\text {. } \\end{aligned} \\]\n(A4) \\(x+0=(a, b)+(0,0)=(a, b)=x\\).\n(A5) Put \\(-x=(-a,-b)\\). Then \\(x+(-x)=(0,0)=0\\).\n(M1) is clear.\n(M2) \\(x y=(a c-b d, a d+b c)=(c a-d b, d a+c b)=y x\\).\n(M3) \\((x y) z=(a c-b d, a d+b c)(e, f)\\)\n\\[ =(a c e-b d e-a d f-b c f, a c f-b d f+a d e+b c e) \\]\n\\[ \\begin{aligned} \u0026amp; =(a, b)(c e-d f, c f+d e)=x(y z) \\text {. } \\end{aligned} \\]\n(M4) \\(1 x=(1,0)(a, b)=(a, b)=x\\).\n(M5) If \\(x \\neq 0\\) then \\((a, b) \\neq(0,0)\\), which means that at least one of the real numbers \\(a, b\\) is different from 0 . Hence \\(a^{2}+b^{2}\u0026gt;0\\), by Proposition \\(1.18(d)\\), and we can define\n\\[ \\frac{1}{x}=\\left(\\frac{a}{a^{2}+b^{2}}, \\frac{-b}{a^{2}+b^{2}}\\right) . \\]\nThen\n\\[ x \\cdot \\frac{1}{x}=(a, b)\\left(\\frac{a}{a^{2}+b^{2}}, \\frac{-b}{a^{2}+b^{2}}\\right)=(1,0)=1 . \\]\n\\(x(y+z)=(a, b)(c+e, d+f)\\) \\[ \\begin{aligned} \u0026amp;=(a c+a e-b d-b f, a d+a f+b c+b e) \\\\ \u0026amp;=(a c-b d, a d+b c)+(a e-b f, a f+b e) \\\\ \u0026amp;=x y+x z . \\end{aligned} \\]\n\\(1.26\\) Theorem For any real numbers \\(a\\) and \\(b\\) we have\n\\[ (a, 0)+(b, 0)=(a+b, 0), \\quad(a, 0)(b, 0)=(a b, 0) \\text {. } \\]\nThe proof is trivial.\nTheorem \\(1.26\\) shows that the complex numbers of the form \\((a, 0)\\) have the same arithmetic properties as the corresponding real numbers \\(a\\). We can therefore identify \\((a, 0)\\) with \\(a\\). This identification gives us the real field as a subfield of the complex field.\nThe reader may have noticed that we have defined the complex numbers without any reference to the mysterious square root of \\(-1\\). We now show that the notation \\((a, b)\\) is equivalent to the more customary \\(a+b i\\).\n1.27 Definition \\(i=(0,1)\\).\n\\(1.28\\) Theorem \\(i^{2}=-1\\).\nProof \\(i^{2}=(0,1)(0,1)=(-1,0)=-1\\).\n1.29 Theorem If \\(a\\) and \\(b\\) are real, then \\((a, b)=a+b i\\).\nProof\n\\[ \\begin{aligned} a+b i \u0026amp;=(a, 0)+(b, 0)(0,1) \\\\ \u0026amp;=(a, 0)+(0, b)=(a, b) . \\end{aligned} \\]\n1.30 Definition If \\(a, b\\) are real and \\(z=a+b i\\), then the complex number \\(\\bar{z}=a-b i\\) is called the conjugate of \\(z\\). The numbers \\(a\\) and \\(b\\) are the real part and the imaginary part of \\(z\\), respectively.\nWe shall occasionally write\n\\[ a=\\operatorname{Re}(z), \\quad b=\\operatorname{Im}(z) . \\]\n1.31 Theorem If \\(z\\) and \\(w\\) are complex, then} (a) \\(\\overline{z+w}=\\bar{z}+\\bar{w}\\) (b) \\(\\overline{z w}=\\bar{z} \\cdot \\bar{w}\\) (c) \\(z+\\bar{z}=2 \\operatorname{Re}(z), z-\\bar{z}=2 i \\operatorname{Im}(z)\\), (d) \\(z \\bar{z}\\) is real and positive (except when \\(z=0\\) ).\nProof \\((a),(b)\\), and \\((c)\\) are quite trivial. To prove \\((d)\\), write \\(z=a+b i\\), and note that \\(z \\bar{z}=a^{2}+b^{2}\\).\n1.32 Definition If \\(z\\) is a complex number, its absolute value \\(|z|\\) is the nonnegative square root of \\(z \\bar{z}\\); that is, \\(|z|=(z \\bar{z})^{1 / 2}\\).\nThe existence (and uniqueness) of \\(|z|\\) follows from Theorem \\(1.21\\) and part \\((d)\\) of Theorem \\(1.31\\).\nNote that when \\(x\\) is real, then \\(\\bar{x}=x\\), hence \\(|x|=\\sqrt{x^{2}}\\). Thus \\(|x|=x\\) if \\(x \\geq 0,|x|=-x\\) if \\(x\u0026lt;0\\).\n\\(1.33\\) Theorem Let \\(z\\) and \\(w\\) be complex numbers. Then (a) \\(|z|\u0026gt;0\\) unless \\(z=0,|0|=0\\), (b) \\(|\\bar{z}|=|z|\\), (c) \\(|z w|=|z||w|\\), (d) \\(|\\operatorname{Re} z| \\leq|z|\\) (e) \\(|z+w| \\leq|z|+|w|\\). Proof \\((a)\\) and \\((b)\\) are trivial. Put \\(z=a+b i, w=c+d i\\), with \\(a, b, c, d\\) real. Then\n\\[ |z w|^{2}=(a c-b d)^{2}+(a d+b c)^{2}=\\left(a^{2}+b^{2}\\right)\\left(c^{2}+d^{2}\\right)=|z|^{2}|w|^{2} \\]\nor \\(|z w|^{2}=(|z||w|)^{2}\\). Now (c) follows from the uniqueness assertion of Theorem \\(1.21\\).\nTo prove \\((d)\\), note that \\(a^{2} \\leq a^{2}+b^{2}\\), hence\n\\[ |a|=\\sqrt{a^{2}} \\leq \\sqrt{a^{2}+b^{2}} . \\]\nTo prove \\((e)\\), note that \\(\\bar{z} w\\) is the conjugate of \\(z \\bar{w}\\), so that \\(z \\bar{w}+\\bar{z} w=\\) \\(2 \\operatorname{Re}(z \\bar{w})\\). Hence\n\\[ \\begin{aligned} |z+w|^{2} \u0026amp;=(z+w)(\\bar{z}+\\bar{w})=z \\bar{z}+z \\bar{w}+\\bar{z} w+w \\bar{w} \\\\ \u0026amp;=|z|^{2}+2 \\operatorname{Re}(z \\bar{w})+|w|^{2} \\\\ \u0026amp; \\leq|z|^{2}+2|z \\bar{w}|+|w|^{2} \\\\ \u0026amp;=|z|^{2}+2|z||w|+|w|^{2}=(|z|+|w|)^{2} \\end{aligned} \\]\nNow \\((e)\\) follows by taking square roots.\n\\(1.34\\) Notation If \\(x_{1}, \\ldots, x_{n}\\) are complex numbers, we write\n\\[ x_{1}+x_{2}+\\cdots+x_{n}=\\sum_{j=1}^{n} x_{j} . \\]\nWe conclude this section with an important inequality, usually known as the Schwarz inequality.\n\\(1.35\\) Theorem If \\(a_{1}, \\ldots, a_{n}\\) and \\(b_{1}, \\ldots, b_{n}\\) are complex numbers, then\n\\[ \\left|\\sum_{j=1}^{n} a_{j} \\bar{b}_{j}\\right|^{2} \\leq \\sum_{j=1}^{n}\\left|a_{j}\\right|^{2} \\sum_{j=1}^{n}\\left|b_{j}\\right|^{2} . \\]\nProof Put \\(A=\\Sigma\\left|a_{j}\\right|^{2}, B=\\Sigma\\left|b_{j}\\right|^{2}, C=\\Sigma a_{j} b_{j}\\) (in all sums in this proof, \\(j\\) runs over the values \\(1, \\ldots, n\\) ). If \\(B=0\\), then \\(b_{1}=\\cdots=b_{n}=0\\), and the conclusion is trivial. Assume therefore that \\(B\u0026gt;0\\). By Theorem \\(1.31\\) we have\n\\[ \\begin{aligned} \\sum\\left|B a_{j}-C b_{j}\\right|^{2} \u0026amp;=\\sum\\left(B a_{j}-C b_{j}\\right)\\left(B \\bar{a}_{j}-\\overline{C b_{j}}\\right) \\\\ \u0026amp;=B^{2} \\sum\\left|a_{j}\\right|^{2}-B \\bar{C} \\sum a_{j} \\bar{b}_{j}-B C \\sum \\bar{a}_{j} b_{j}+|C|^{2} \\sum\\left|b_{j}\\right|^{2} \\\\ \u0026amp;=B^{2} A-B|C|^{2} \\\\ \u0026amp;=B\\left(A B-|C|^{2}\\right) . \\end{aligned} \\]\nSince each term in the first sum is nonnegative, we see that\n\\[ B\\left(A B-|C|^{2}\\right) \\geq 0 \\text {. } \\]\nSince \\(B\u0026gt;0\\), it follows that \\(A B-|C|^{2} \\geq 0\\). This is the desired inequality.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/5-the-complex-field/","section":"baby rudin","tags":null,"title":"5 THE COMPLEX FIELD"},{"categories":null,"contents":" youtube bilibili note pdf xopp 1.36 Definitions For each positive integer \\(k\\), let \\(R^{k}\\) be the set of all ordered \\(k\\)-tuples\n\\[ \\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right), \\]\nwhere \\(x_{1}, \\ldots, x_{k}\\) are real numbers, called the coordinates of \\(\\mathbf{x}\\). The elements of \\(R^{k}\\) are called points, or vectors, especially when \\(k\u0026gt;1\\). We shall denote vectors by boldfaced letters. If \\(\\mathbf{y}=\\left(y_{1}, \\ldots, y_{k}\\right)\\) and if \\(\\alpha\\) is a real number, put\n\\[ \\begin{aligned} \\mathbf{x}+\\mathbf{y} \u0026amp;=\\left(x_{1}+y_{1}, \\ldots, x_{k}+y_{k}\\right) \\\\ \\alpha \\mathbf{x} \u0026amp;=\\left(\\alpha x_{1}, \\ldots, \\alpha x_{k}\\right) \\end{aligned} \\]\nso that \\(\\mathbf{x}+\\mathbf{y} \\in R^{k}\\) and \\(\\alpha \\mathbf{x} \\in R^{k}\\). This defines addition of vectors, as well as multiplication of a vector by a real number (a scalar). These two operations satisfy the commutative, associative, and distributive laws (the proof is trivial, in view of the analogous laws for the real numbers) and make \\(R^{k}\\) into a vector space over the real field. The zero element of \\(R^{k}\\) (sometimes called the origin or the null vector) is the point 0 , all of whose coordinates are 0 .\nWe also define the so-called “inner product” (or scalar product) of \\(\\mathbf{x}\\) and y by\n\\[ \\mathbf{x} \\cdot \\mathbf{y}=\\sum_{i=1}^{k} x_{i} y_{i} \\]\nand the norm of \\(\\mathbf{x}\\) by\n\\[ |x|=(x \\cdot x)^{1 / 2}=\\left(\\sum_{1}^{k} x_{i}^{2}\\right)^{1 / 2} . \\]\nThe structure now defined (the vector space \\(R^{k}\\) with the above inner product and norm) is called euclidean \\(k\\)-space.\n1.37 Theorem Suppose \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in R^{k}\\), and \\(\\alpha\\) is real. Then (a) \\(|\\mathbf{x}| \\geq 0\\) (b) \\(|\\mathbf{x}|=0\\) if and only if \\(\\mathbf{x}=\\mathbf{0}\\) (c) \\(\\quad|\\alpha \\mathbf{x}|=|\\alpha||\\mathbf{x}|\\) (d) \\(|\\mathbf{x} \\cdot \\mathbf{y}| \\leq|\\mathbf{x}||\\mathbf{y}|\\); (e) \\(|\\mathbf{x}+\\mathbf{y}| \\leq|\\mathbf{x}|+|\\mathbf{y}|\\) (f) \\(|\\mathbf{x}-\\mathbf{z}| \\leq|\\mathbf{x}-\\mathbf{y}|+|\\mathbf{y}-\\mathbf{z}|\\). Proof \\((a),(b)\\), and \\((c)\\) are obvious, and \\((d)\\) is an immediate consequence of the Schwarz inequality. By \\((d)\\) we have\n\\[ \\begin{aligned} |x+y|^{2} \u0026amp;=(x+y) \\cdot(x+y) \\\\ \u0026amp;=\\mathbf{x} \\cdot \\mathbf{x}+2 \\mathbf{x} \\cdot \\mathbf{y}+\\mathbf{y} \\cdot \\mathbf{y} \\\\ \u0026amp; \\leq|\\mathbf{x}|^{2}+2|\\mathbf{x}||\\mathbf{y}|+|\\mathbf{y}|^{2} \\\\ \u0026amp;=(|\\mathbf{x}|+|\\mathbf{y}|)^{2} \\end{aligned} \\]\nso that \\((e)\\) is proved. Finally, \\((f)\\) follows from \\((e)\\) if we replace \\(x\\) by \\(\\mathbf{x}-\\mathbf{y}\\) and \\(\\mathbf{y}\\) by \\(\\mathbf{y}-\\mathbf{z}\\).\n\\(1.38\\) Remarks Theorem \\(1.37(a),(b)\\), and \\((f)\\) will allow us (see Chap. 2) to regard \\(R^{k}\\) as a metric space.\n\\(R^{1}\\) (the set of all real numbers) is usually called the line, or the real line. Likewise, \\(R^{2}\\) is called the plane, or the complex plane (compare Definitions \\(1.24\\) and 1.36). In these two cases the norm is just the absolute value of the corresponding real or complex number.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/6-euclidean-spaces/","section":"baby rudin","tags":null,"title":"6 EUCLIDEAN SPACES"},{"categories":null,"contents":"9.39 Definition Suppose \\(f\\) is a real function defined in an open set \\(E \\subset R^{n}\\), with partial derivatives \\(D_{1} f, \\ldots, D_{n} f\\). If the functions \\(D_{j} f\\) are themselves differentiable, then the second-order partial derivatives of \\(f\\) are defined by\n\\[ D_{i j} f=D_{i} D_{j} f \\quad(i, j=1, \\ldots, n) . \\]\nIf all these functions \\(D_{i j} f\\) are continuous in \\(E\\), we say that \\(f\\) is of class \\(\\mathscr{B}^{\\prime \\prime}\\) in \\(E\\), or that \\(f \\in \\mathscr{C}^{\\prime \\prime}(E)\\).\nA mapping \\(f\\) of \\(E\\) into \\(R^{m}\\) is said to be of class \\(\\mathscr{E}^{\\prime \\prime}\\) if each component of \\(f\\) is of class \\(\\mathscr{B}^{\\prime \\prime}\\).\nIt can happen that \\(D_{i j} f \\neq D_{j i} f\\) at some point, although both derivatives exist (see Exercise 27). However, we shall see below that \\(D_{i j} f=D_{j i} f\\) whenever these derivatives are continuous.\nFor simplicity (and without loss of generality) we state our next two theorems for real functions of two variables. The first one is a mean value theorem.\n9.40 Theorem Suppose \\(f\\) is defined in an open set \\(E \\subset R^{2}\\), and \\(D_{1} f\\) and \\(D_{21} f\\) exist at every point of \\(E\\). Suppose \\(Q \\subset E\\) is a closed rectangle with sides parallel to the coordinate axes, having \\((a, b)\\) and \\((a+h, b+k)\\) as opposite vertices \\((h \\neq 0, k \\neq 0)\\). Put\n\\[ \\Delta(f, Q)=f(a+h, b+k)-f(a+h, b)-f(a, b+k)+f(a, b) . \\]\nThen there is a point \\((x, y)\\) in the interior of \\(Q\\) such that\n\\[ \\Delta(f, Q)=h k\\left(D_{21} f\\right)(x, y) . \\]\nNote the analogy between (95) and Theorem 5.10; the area of \\(Q\\) is \\(h k\\).\nProof Put \\(u(t)=f(t, b+k)-f(t, b)\\). Two applications of Theorem 5.10 show that there is an \\(x\\) between \\(a\\) and \\(a+h\\), and that there is a \\(y\\) between \\(b\\) and \\(b+k\\), such that\n\\[ \\begin{aligned} \\Delta(f, Q) \u0026amp;=u(a+h)-u(a) \\\\ \u0026amp;=h u^{\\prime}(x) \\\\ \u0026amp;=h\\left[\\left(D_{1} f\\right)(x, b+k)-\\left(D_{1} f\\right)(x, b)\\right] \\\\ \u0026amp;=h k\\left(D_{21} f\\right)(x, y) \\end{aligned} \\]\n9.41 Theorem Suppose \\(f\\) is defined in an open set \\(E \\subset R^{2}\\), suppose that \\(D_{1} f\\), \\(D_{21} f\\), and \\(D_{2} f\\) exist at every point of \\(E\\), and \\(D_{21} f\\) is continuous at some point \\((a, b) \\in E\\) Then \\(D_{12} f\\) exists at \\((a, b)\\) and\n\\[ \\left(D_{12} f\\right)(a, b)=\\left(D_{21} f\\right)(a, b) . \\]\nCorollary \\(D_{21} f=D_{12} f\\) if \\(f \\in \\mathscr{C}^{\\prime \\prime}(E)\\).\nProof Put \\(A=\\left(D_{21} f\\right)(a, b)\\). Choose \\(\\varepsilon\u0026gt;0\\). If \\(Q\\) is a rectangle as in Theorem 9.40, and if \\(h\\) and \\(k\\) are sufficiently small, we have\n\\[ \\left|A-\\left(D_{21} f\\right)(x, y)\\right|\u0026lt;\\varepsilon \\]\nfor all \\((x, y) \\in Q\\). Thus\n\\[ \\left|\\frac{\\Delta(f, Q)}{h k}-A\\right|\u0026lt;\\varepsilon \\]\nby (95). Fix \\(h\\), and let \\(k \\rightarrow 0\\). Since \\(D_{2} f\\) exists in \\(E\\), the last inequality implies that\n\\[ \\left|\\frac{\\left(D_{2} f\\right)(a+h, b)-\\left(D_{2} f\\right)(a, b)}{h}-A\\right| \\leq \\varepsilon . \\]\nSince \\(\\varepsilon\\) was arbitrary, and since (97) holds for all sufficiently small \\(h \\neq 0\\), it follows that \\(\\left(D_{12} f\\right)(a, b)=A\\). This gives (96).\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/8-derivatives-of-higher-order/","section":"baby rudin","tags":null,"title":"8 DERIVATIVES OF HIGHER ORDER"},{"categories":null,"contents":"Suppose \\(\\varphi\\) is a function of two variables which can be integrated with respect to one and which can be differentiated with respect to the other. Under what conditions will the result be the same if these two limit processes are carried out in the opposite order? To state the question more precisely: Under what conditions on \\(\\varphi\\) can one prove that the equation\n\\[ \\frac{d}{d t} \\int_{a}^{b} \\varphi(x, t) d x=\\int_{a}^{b} \\frac{\\partial \\varphi}{\\partial t}(x, t) d x \\]\nis true? (A counter example is furnished by Exercise 28.)\nIt will be convenient to use the notation\n\\[ \\varphi^{t}(x)=\\varphi(x, t) . \\]\nThus \\(\\varphi^{t}\\) is, for each \\(t\\), a function of one variable.\n\\(\\varphi(x, t)\\) is defined for \\(a \\leq x \\leq b, c \\leq t \\leq d\\);\n\\(\\alpha\\) is an increasing function on \\([a, b]\\); (c) \\(\\varphi^{t} \\in \\mathscr{R}(\\alpha)\\) for every \\(t \\in[c, d]\\);\n\\(c\u0026lt;s\u0026lt;d\\), and to every \\(\\varepsilon\u0026gt;0\\) corresponds a \\(\\delta\u0026gt;0\\) such that\n\\[ \\left|\\left(D_{2} \\varphi\\right)(x, t)-\\left(D_{2} \\varphi\\right)(x, s)\\right|\u0026lt;\\varepsilon \\]\nfor all \\(x \\in[a, b]\\) and for all \\(t \\in(s-\\delta, s+\\delta)\\).\n\\[ f(t)=\\int_{a}^{b} \\varphi(x, t) d \\alpha(x) \\quad(c \\leq t \\leq d) . \\]\nThen \\(\\left(D_{2} \\varphi\\right)^{s} \\in \\mathscr{R}(\\alpha), f^{\\prime}(s)\\) exists, and\n\\[ f^{\\prime}(s)=\\int_{a}^{b}\\left(D_{2} \\varphi\\right)(x, s) d \\alpha(x) . \\]\nNote that (c) simply asserts the existence of the integrals (100) for all \\(t \\in[c, d]\\). Note also that \\((d)\\) certainly holds whenever \\(D_{2} \\varphi\\) is continuous on the rectangle on which \\(\\varphi\\) is defined.\nProof Consider the difference quotients\n\\[ \\psi(x, t)=\\frac{\\varphi(x, t)-\\varphi(x, s)}{t-s} \\]\nfor \\(0\u0026lt;|t-s|\u0026lt;\\delta\\). By Theorem \\(5.10\\) there corresponds to each \\((x, t)\\) a number \\(u\\) between \\(s\\) and \\(t\\) such that\n\\[ \\psi(x, t)=\\left(D_{2} \\varphi\\right)(x, u) \\]\nHence \\((d)\\) implies that\n\\[ \\left|\\psi(x, t)-\\left(D_{2} \\varphi\\right)(x, s)\\right|\u0026lt;\\varepsilon \\quad(a \\leq x \\leq b, \\quad 0\u0026lt;|t-s|\u0026lt;\\delta) . \\]\nNote that\n\\[ \\frac{f(t)-f(s)}{t-s}=\\int_{a}^{b} \\psi(x, t) d \\alpha(x) . \\]\nBy (102), \\(\\psi^{t} \\rightarrow\\left(D_{2} \\varphi\\right)^{s}\\), uniformly on \\([a, b]\\), as \\(t \\rightarrow s\\). Since each \\(\\psi^{t} \\in \\mathscr{R}(\\alpha)\\), the desired conclusion follows from (103) and Theorem 7.16.\n9.43 Example One can of course prove analogues of Theorem \\(9.42\\) with \\((-\\infty, \\infty)\\) in place of \\([a, b]\\). Instead of doing this, let us simply look at an example. Define\n\\[ f(t)=\\int_{-\\infty}^{\\infty} e^{-x^{2}} \\cos (x t) d x \\]\nand\n\\[ g(t)=-\\int_{-\\infty}^{\\infty} x e^{-x^{2}} \\sin (x t) d x \\]\nfor \\(-\\infty\u0026lt;t\u0026lt;\\infty\\). Both integrals exist (they converge absolutely) since the absolute values of the integrands are at most \\(\\exp \\left(-x^{2}\\right)\\) and \\(|x| \\exp \\left(-x^{2}\\right)\\), respectively.\nNote that \\(g\\) is obtained from \\(f\\) by differentiating the integrand with respect to \\(t\\). We claim that \\(f\\) is differentiable and that\n\\[ f^{\\prime}(t)=g(t) \\quad(-\\infty\u0026lt;t\u0026lt;\\infty) . \\]\nTo prove this, let us first examine the difference quotients of the cosine: if \\(\\beta\u0026gt;0\\), then\n\\[ \\frac{\\cos (\\alpha+\\beta)-\\cos \\alpha}{\\beta}+\\sin \\alpha=\\frac{1}{\\beta} \\int_{\\alpha}^{\\alpha+\\beta}(\\sin \\alpha-\\sin t) d t . \\]\nSince \\(|\\sin \\alpha-\\sin t| \\leq|t-\\alpha|\\), the right side of (107) is at most \\(\\beta / 2\\) in absolute value; the case \\(\\beta\u0026lt;0\\) is handled similarly. Thus\n\\[ \\left|\\frac{\\cos (\\alpha+\\beta)-\\cos \\alpha}{\\beta}+\\sin \\alpha\\right| \\leq|\\beta| \\]\nfor all \\(\\beta\\) (if the left side is interpreted to be 0 when \\(\\beta=0\\) ).\nNow fix \\(t\\), and fix \\(h \\neq 0\\). Apply (108) with \\(\\alpha=x t, \\beta=x h\\); it follows from (104) and (105) that\n\\[ \\left|\\frac{f(t+h)-f(t)}{h}-g(t)\\right| \\leq|h| \\int_{-\\infty}^{\\infty} x^{2} e^{-x^{2}} d x . \\]\nWhen \\(h \\rightarrow 0\\), we thus obtain (106).\nLet us go a step further: An integration by parts, applied to (104), shows that\n\\[ f(t)=2 \\int_{-\\infty}^{\\infty} x e^{-x^{2}} \\frac{\\sin (x t)}{t} d x . \\]\nThus \\(t f(t)=-2 g(t)\\), and (106) implies now that \\(f\\) satisfies the differential equation\n\\[ 2 f^{\\prime}(t)+t f(t)=0 . \\]\nIf we solve this differential equation and use the fact that \\(f(0)=\\sqrt{\\pi}\\) (see Sec. \\(8.21\\) ), we find that\n\\[ f(t)=\\sqrt{\\pi} \\exp \\left(-\\frac{t^{2}}{4}\\right) . \\]\nThe integral (104) is thus explicitly determined.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/9-differentiation-of-integrals/","section":"baby rudin","tags":null,"title":"9 DIFFERENTIATION OF INTEGRALS.md"},{"categories":null,"contents":"Integration can be studied on many levels. In Chap. 6, the theory was developed for reasonably well-behaved functions on subintervals of the real line. In Chap. 11 we shall encounter a very highly developed theory of integration that can be applied to much larger classes of functions, whose domains are more or less arbitrary sets, not necessarily subsets of \\(R^{n}\\). The present chapter is devoted to those aspects of integration theory that are closely related to the geometry of euclidean spaces, such as the change of variables formula, line integrals, and the machinery of differential forms that is used in the statement and proof of the \\(n\\)-dimensional analogue of the fundamental theorem of calculus, namely Stokes’ theorem.\n","date":"2022-08-15T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/10-integration-of-differential-forms/0-intro/","section":"baby rudin","tags":null,"title":"INTRO"},{"categories":null,"contents":"We now interrupt our discussion of differentiation to insert a fixed point theorem that is valid in arbitrary complete metric spaces. It will be used in the proof of the inverse function theorem.\n9.22 Definition Let \\(X\\) be a metric space, with metric \\(d\\). If \\(\\varphi\\) maps \\(X\\) into \\(X\\) and if there is a number \\(c\u0026lt;1\\) such that\n\\[ d(\\varphi(x), \\varphi(y)) \\leq c d(x, y) \\]\nfor all \\(x, y \\in X\\), then \\(\\varphi\\) is said to be a contraction of \\(X\\) into \\(X\\).\n9.23 Theorem If \\(X\\) is a complete metric space, and if \\(\\varphi\\) is a contraction of \\(X\\) into \\(X\\), then there exists one and only one \\(x \\in X\\) such that \\(\\varphi(x)=x\\).\nIn other words, \\(\\varphi\\) has a unique fixed point. The uniqueness is a triviality, for if \\(\\varphi(x)=x\\) and \\(\\varphi(y)=y\\), then (43) gives \\(d(x, y) \\leq c d(x, y)\\), which can only happen when \\(d(x, y)=0\\).\nThe existence of a fixed point of \\(\\varphi\\) is the essential part of the theorem. The proof actually furnishes a constructive method for locating the fixed point.\nProof Pick \\(x_{0} \\in X\\) arbitrarily, and define \\(\\left\\{x_{n}\\right\\}\\) recursively, by setting\n\\[ x_{n+1}=\\varphi\\left(x_{n}\\right) \\quad(n=0,1,2, \\ldots) \\text {. } \\]\nChoose \\(c\u0026lt;1\\) so that (43) holds. For \\(n \\geq 1\\) we then have\n\\[ d\\left(x_{n+1}, x_{n}\\right)=d\\left(\\varphi\\left(x_{n}\\right), \\varphi\\left(x_{n-1}\\right)\\right) \\leq c d\\left(x_{n}, x_{n-1}\\right) . \\]\nHence induction gives\n\\[ d\\left(x_{n+1}, x_{n}\\right) \\leq c^{n} d\\left(x_{1}, x_{0}\\right) \\quad(n=0,1,2, \\ldots) . \\]\nIf \\(n\u0026lt;m\\), it follows that\n\\[ \\begin{aligned} d\\left(x_{n}, x_{m}\\right) \u0026amp; \\leq \\sum_{i=n+1}^{m} d\\left(x_{i}, x_{i-1}\\right) \\\\ \u0026amp; \\leq\\left(c^{n}+c^{n+1}+\\cdots+c^{m-1}\\right) d\\left(x_{1}, x_{0}\\right) \\\\ \u0026amp; \\leq\\left[(1-c)^{-1} d\\left(x_{1}, x_{0}\\right)\\right] c^{n} \\end{aligned} \\]\nThus \\(\\left\\{x_{n}\\right\\}\\) is a Cauchy sequence. Since \\(X\\) is complete, \\(\\lim x_{n}=x\\) for some \\(x \\in X\\).\nSince \\(\\varphi\\) is a contraction, \\(\\varphi\\) is continuous (in fact, uniformly continuous) on \\(X\\). Hence\n\\[ \\varphi(x)=\\lim _{n \\rightarrow \\infty} \\varphi\\left(x_{n}\\right)=\\lim _{n \\rightarrow \\infty} x_{n+1}=x . \\]\n","date":"2022-08-13T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/3-the-contraction-principle/","section":"baby rudin","tags":null,"title":"3 THE CONTRACTION PRINCIPLE"},{"categories":null,"contents":"The inverse function theorem states, roughly speaking, that a continuously differentiable mapping \\(f\\) is invertible in a neighborhood of any point \\(\\mathbf{x}\\) at which the linear transformation \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) is invertible:\n9.24 Theorem Suppose \\(\\mathrm{f}\\) is \\(a \\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into \\(R^{n}\\), \\(\\mathbf{f}^{\\prime}(\\mathrm{a})\\) is invertible for some \\(\\mathbf{a} \\in E\\), and \\(\\mathbf{b}=\\mathbf{f}(\\mathbf{a})\\). Then\nthere exist open sets \\(U\\) and \\(V\\) in \\(R^{n}\\) such that \\(\\mathbf{a} \\in U, \\mathbf{b} \\in V\\), \\(\\mathbf{f}\\) is one-toone on \\(U\\), and \\(\\mathbf{f}(U)=V\\);\nif \\(\\mathbf{g}\\) is the inverse of \\(\\mathrm{f}\\) [which exists, by \\((a)\\]\\), defined in \\(V\\) by\n\\[ \\mathbf{g}(\\mathbf{f}(\\mathbf{x}))=\\mathbf{x} \\quad(\\mathbf{x} \\in U), \\]\nthen \\(\\mathbf{g} \\in \\mathscr{C}^{\\prime}(V)\\).\nWriting the equation \\(\\mathbf{y}=\\mathbf{f}(\\mathbf{x})\\) in component form, we arrive at the following interpretation of the conclusion of the theorem: The system of \\(n\\) equations\n\\[ y_{i}=f_{i}\\left(x_{1}, \\ldots, x_{n}\\right) \\quad(1 \\leq i \\leq n) \\]\ncan be solved for \\(x_{1}, \\ldots, x_{n}\\) in terms of \\(y_{1}, \\ldots, y_{n}\\), if we restrict \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) to small enough neighborhoods of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\); the solutions are unique and continuously differentiable.\nProof\nPut \\(\\mathbf{f}^{\\prime}(\\mathrm{a})=A\\), and choose \\(\\lambda\\) so that \\[ 2 \\lambda\\left\\|A^{-1}\\right\\|=1 . \\]\nSince \\(\\mathbf{f}^{\\prime}\\) is continuous at \\(\\mathbf{a}\\), there is an open ball \\(U \\subset E\\), with center at \\(\\mathbf{a}\\), such that\n\\[ \\left\\|\\mathbf{f}^{\\prime}(\\mathbf{x})-A\\right\\|\u0026lt;\\lambda \\quad(\\mathbf{x} \\in U) . \\]\nWe associate to each \\(y \\in R^{n}\\) a function \\(\\varphi\\), defined by\n\\[ \\varphi(\\mathbf{x})=\\mathbf{x}+A^{-1}(\\mathbf{y}-\\mathbf{f}(\\mathbf{x})) \\quad(\\mathbf{x} \\in E) . \\]\nNote that \\(\\mathbf{f}(\\mathbf{x})=\\mathbf{y}\\) if and only if \\(\\mathbf{x}\\) is a fixed point of \\(\\varphi\\).\nSince \\(\\varphi^{\\prime}(\\mathbf{x})=I-A^{-1} \\mathbf{f}^{\\prime}(\\mathbf{x})=A^{-1}\\left(A-\\mathbf{f}^{\\prime}(\\mathbf{x})\\right)\\), (46) and (47) imply that\n\\[ \\left\\|\\varphi^{\\prime}(\\mathbf{x})\\right\\|\u0026lt;\\frac{1}{2} \\quad(\\mathbf{x} \\in U) \\]\nHence\n\\[ \\left|\\varphi\\left(\\mathbf{x}_{1}\\right)-\\varphi\\left(\\mathbf{x}_{2}\\right)\\right| \\leq \\frac{1}{2}\\left|\\mathbf{x}_{1}-\\mathbf{x}_{2}\\right| \\quad\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2} \\in U\\right) \\text {, } \\]\nby Theorem 9.19. It follows that \\(\\varphi\\) has at most one fixed point in \\(U\\), so that \\(\\mathbf{f}(\\mathbf{x})=\\mathbf{y}\\) for at most one \\(\\mathbf{x} \\in U\\).\nThus \\(\\mathrm{f}\\) is \\(1-1\\) in \\(U\\).\nNext, put \\(V=\\mathbf{f}(U)\\), and pick \\(\\mathbf{y}_{0} \\in V\\). Then \\(\\mathbf{y}_{0}=\\mathbf{f}\\left(\\mathbf{x}_{0}\\right)\\) for some \\(\\mathbf{x}_{0} \\in U\\). Let \\(B\\) be an open ball with center at \\(\\mathbf{x}_{0}\\) and radius \\(r\u0026gt;0\\), so small that its closure \\(\\bar{B}\\) lies in \\(U\\). We will show that \\(\\mathrm{y} \\in V\\) whenever \\(\\left|\\mathbf{y}-\\mathbf{y}_{0}\\right|\u0026lt;\\lambda r\\). This proves, of course, that \\(V\\) is open.\nFix y, \\(\\left|\\mathbf{y}-\\mathbf{y}_{0}\\right|\u0026lt;\\lambda r\\). With \\(\\varphi\\) as in (48),\n\\[ \\left|\\varphi\\left(\\mathbf{x}_{0}\\right)-\\mathbf{x}_{0}\\right|=\\left|A^{-1}\\left(\\mathbf{y}-\\mathbf{y}_{0}\\right)\\right|\u0026lt;\\left\\|A^{-1}\\right\\| \\lambda r=\\frac{r}{2} . \\]\nIf \\(\\mathbf{x} \\in \\bar{B}\\), it therefore follows from (50) that\n\\[ \\begin{aligned} \\left|\\varphi(\\mathbf{x})-\\mathbf{x}_{0}\\right| \u0026amp; \\leq\\left|\\varphi(\\mathbf{x})-\\varphi\\left(\\mathbf{x}_{0}\\right)\\right|+\\left|\\varphi\\left(\\mathbf{x}_{0}\\right)-\\mathbf{x}_{0}\\right| \\\\ \u0026amp;\u0026lt;\\frac{1}{2}\\left|\\mathbf{x}-\\mathbf{x}_{0}\\right|+\\frac{r}{2} \\leq r \\end{aligned} \\]\nhence \\(\\varphi(\\mathbf{x}) \\in B\\). Note that (50) holds if \\(\\mathbf{x}_{1} \\in \\bar{B}, \\mathbf{x}_{2} \\in \\bar{B}\\).\nThus \\(\\varphi\\) is a contraction of \\(\\bar{B}\\) into \\(\\bar{B}\\). Being a closed subset of \\(R^{n}\\), \\(\\bar{B}\\) is complete. Theorem \\(9.23\\) implies therefore that \\(\\varphi\\) has a fixed point \\(\\mathbf{x} \\in \\bar{B}\\). For this \\(\\mathbf{x}, f(\\mathbf{x})=\\mathbf{y}\\). Thus \\(\\mathbf{y} \\in \\mathbf{f}(\\bar{B}) \\subset \\mathbf{f}(U)=V\\).\nThis proves part \\((a)\\) of the theorem.\nPick \\(\\mathbf{y} \\in V, \\mathbf{y}+\\mathbf{k} \\in V\\). Then there exist \\(\\mathbf{x} \\in U, \\mathbf{x}+\\mathbf{h} \\in U\\), so that \\(\\mathbf{y}=\\mathbf{f}(\\mathbf{x}), \\mathbf{y}+\\mathbf{k}=\\mathbf{f}(\\mathbf{x}+\\mathbf{h})\\). With \\(\\varphi\\) as in (48), \\[ \\varphi(\\mathbf{x}+\\mathbf{h})-\\varphi(\\mathbf{x})=\\mathbf{h}+A^{-1}[\\mathbf{f}(\\mathbf{x})-\\mathbf{f}(\\mathbf{x}+\\mathbf{h})]=\\mathbf{h}-A^{-1} \\mathbf{k} \\text {. } \\]\nBy (50), \\(\\left|\\mathbf{h}-A^{-1} \\mathbf{k}\\right| \\leq \\frac{1}{2}|\\mathbf{h}|\\). Hence \\(\\left|A^{-1} \\mathbf{k}\\right| \\geq \\frac{1}{2}|\\mathbf{h}|\\), and\n\\[ |\\mathbf{h}| \\leq 2\\left\\|A^{-1}\\right\\||\\mathbf{k}|=\\lambda^{-1}|\\mathbf{k}| \\text {. } \\]\nBy (46), (47), and Theorem \\(9.8, \\mathbf{f}^{\\prime}(\\mathbf{x})\\) has an inverse, say \\(T\\). Since\n\\[ g(y+k)-g(y)-T k=h-T k=-T\\left[f(x+h)-f(x)-f^{\\prime}(\\mathbf{x}) \\mathbf{h}\\right] \\]\nimplies \\[ \\frac{|\\mathbf{g}(\\mathbf{y}+\\mathbf{k})-\\mathbf{g}(\\mathbf{y})-T \\mathbf{k}|}{|\\mathbf{k}|} \\leq \\frac{\\|T\\|}{\\lambda} \\cdot \\frac{\\left|\\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})-\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{h}\\right|}{|\\mathbf{h}|} . \\]\nAs \\(\\mathbf{k} \\rightarrow \\mathbf{0}\\), (51) shows that \\(\\mathbf{h} \\rightarrow \\mathbf{0}\\). The right side of the last inequality thus tends to 0 . Hence the same is true of the left. We have thus proved that \\(\\mathbf{g}^{\\prime}(\\mathbf{y})=T\\). But \\(T\\) was chosen to be the inverse of \\(\\mathbf{f}^{\\prime}(\\mathbf{x})=\\mathbf{f}^{\\prime}(\\mathbf{g}(\\mathbf{y}))\\). Thus\n\\[ \\mathbf{g}^{\\prime}(\\mathbf{y})=\\left\\{\\mathbf{f}^{\\prime}(\\mathbf{g}(\\mathbf{y}))\\right\\}^{-1} \\quad(\\mathbf{y} \\in V) . \\]\nFinally, note that \\(\\mathbf{g}\\) is a continuous mapping of \\(V\\) onto \\(U\\) (since \\(\\mathbf{g}\\) is differentiable), that \\(f^{\\prime}\\) is a continuous mapping of \\(U\\) into the set \\(\\Omega\\) of all invertible elements of \\(L\\left(R^{n}\\right)\\), and that inversion is a continuous mapping of \\(\\Omega\\) onto \\(\\Omega\\), by Theorem 9.8. If we combine these facts with (52), we see that \\(\\mathbf{g} \\in \\mathscr{C}^{\\prime}(V)\\).\nThis completes the proof.\nRemark. The full force of the assumption that \\(\\mathbf{f} \\in \\mathscr{C}^{\\prime}(E)\\) was only used in the last paragraph of the preceding proof. Everything else, down to Eq. (52), was derived from the existence of \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) for \\(\\mathbf{x} \\in E\\), the invertibility of \\(\\mathbf{f}^{\\prime}(\\mathbf{a})\\), and the continuity of \\(\\mathbf{f}^{\\prime}\\) at just the point a. In this connection, we refer to the article by A. Nijenhuis in Amer. Math. Monthly, vol. 81, 1974, pp. 969-980.\nThe following is an immediate consequence of part \\((a)\\) of the inverse function theorem.\n9.25 Theorem If \\(\\mathrm{f}\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into \\(R^{n}\\) and if \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) is invertible for every \\(\\mathbf{x} \\in E\\), then \\(\\mathbf{f}(W)\\) is an open subset of \\(R^{n}\\) for every open set \\(W \\subset E\\)\nIn other words, \\(\\mathrm{f}\\) is an open mapping of \\(E\\) into \\(R^{n}\\).\nThe hypotheses made in this theorem ensure that each point \\(\\mathbf{x} \\in E\\) has a neighborhood in which \\(f\\) is 1-1. This may be expressed by saying that \\(f\\) is locally one-to-one in \\(E\\). But \\(\\mathrm{f}\\) need not be 1-1 in \\(E\\) under these circumstances. For an example, see Exercise \\(17 .\\)\n","date":"2022-08-13T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/4-the-inverse-function-theorem/","section":"baby rudin","tags":null,"title":"4 THE INVERSE FUNCTION THEOREM"},{"categories":null,"contents":"If \\(f\\) is a continuously differentiable real function in the plane, then the equation \\(f(x, y)=0\\) can be solved for \\(y\\) in terms of \\(x\\) in a neighborhood of any point \\((a, b)\\) at which \\(f(a, b)=0\\) and \\(\\partial f / \\partial y \\neq 0\\). Likewise, one can solve for \\(x\\) in terms of \\(y\\) near \\((a, b)\\) if \\(\\partial f / \\partial x \\neq 0\\) at \\((a, b)\\). For a simple example which illustrates the need for assuming \\(\\partial f / \\partial y \\neq 0\\), consider \\(f(x, y)=x^{2}+y^{2}-1\\).\nThe preceding very informal statement is the simplest case (the case \\(m=n=1\\) of Theorem 9.28) of the so-called “implicit function theorem.” Its proof makes stronguse of the fact that continuously differentiable transformations behave locally very much like their derivatives. Accordingly, we first prove Theorem 9.27, the linear version of Theorem 9.28.\n9.26 Notation If \\(\\mathbf{x}=\\left(x_{1}, \\ldots, x_{n}\\right) \\in R^{n}\\) and \\(\\mathbf{y}=\\left(y_{1}, \\ldots, y_{m}\\right) \\in R^{m}\\), let us write \\((\\mathbf{x}, \\mathbf{y})\\) for the point (or vector)\n\\[ \\left(x_{1}, \\ldots, x_{n}, y_{1}, \\ldots, y_{m}\\right) \\in R^{n+m} \\text {. } \\]\nIn what follows, the first entry in \\((\\mathbf{x}, \\mathbf{y})\\) or in a similar symbol will always be a vector in \\(R^{n}\\), the second will be a vector in \\(R^{m}\\).\nEvery \\(A \\in L\\left(R^{n+m}, R^{n}\\right)\\) can be split into two linear transformations \\(A_{x}\\) and \\(A_{y}\\), defined by\n\\[ A_{x} \\mathbf{h}=A(\\mathbf{h}, \\mathbf{0}), \\quad A_{y} \\mathbf{k}=A(\\mathbf{0}, \\mathbf{k}) \\]\nfor any \\(\\mathbf{h} \\in R^{n}, \\mathbf{k} \\in R^{m}\\). Then \\(A_{x} \\in L\\left(R^{n}\\right), A_{y} \\in L\\left(R^{m}, R^{n}\\right)\\), and\n\\[ A(\\mathbf{h}, \\mathbf{k})=A_{x} \\mathbf{h}+A_{y} \\mathbf{k} . \\]\nThe linear version of the implicit function theorem is now almost obvious.\n9.27 Theorem If \\(A \\in L\\left(R^{n+m}, R^{n}\\right)\\) and if \\(A_{x}\\) is invertible, then there corresponds to every \\(\\mathbf{k} \\in R^{m}\\) a unique \\(\\mathbf{h} \\in R^{n}\\) such that \\(A(\\mathbf{h}, \\mathbf{k})=\\mathbf{0}\\).\nThis \\(\\mathbf{h}\\) can be computed from \\(\\mathbf{k}\\) by the formula\n\\[ \\mathbf{h}=-\\left(A_{x}\\right)^{-1} A_{y} \\mathbf{k} . \\]\nProof By (54), \\(A(\\mathbf{h}, \\mathbf{k})=\\mathbf{0}\\) if and only if\n\\[ A_{x} \\mathbf{h}+A_{y} \\mathbf{k}=\\mathbf{0}, \\]\nwhich is the same as (55) when \\(A_{x}\\) is invertible.\nThe conclusion of Theorem \\(9.27\\) is, in other words, that the equation \\(A(\\mathrm{~h}, \\mathrm{k})=\\mathbf{0}\\) can be solved (uniquely) for \\(\\mathrm{h}\\) if \\(\\mathbf{k}\\) is given, and that the solution \\(\\mathrm{h}\\) is a linear function of \\(\\mathbf{k}\\). Those who have some acquaintance with linear algebra will recognize this as a very familiar statement about systems of linear equations.\n9.28 Theorem Let \\(\\mathrm{f}\\) be a \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n+m}\\) into \\(R^{n}\\), such that \\(\\mathbf{f}(\\mathbf{a}, \\mathbf{b})=0\\) for some point \\((\\mathbf{a}, \\mathbf{b}) \\in E\\).\nPut \\(A=\\mathbf{f}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\) and assume that \\(A_{x}\\) is invertible. Then there exist open sets \\(U \\subset R^{n+m}\\) and \\(W \\subset R^{m}\\), with \\((\\mathbf{a}, \\mathbf{b}) \\in U\\) and \\(\\mathbf{b} \\in W\\), having the following property:\nTo every \\(\\mathbf{y} \\in W\\) corresponds a unique \\(\\mathbf{x}\\) such that\n\\[ (\\mathbf{x}, \\mathbf{y}) \\in U \\text { and } \\mathbf{f}(\\mathbf{x}, \\mathbf{y})=0 \\text {. } \\]\nIf this \\(\\mathbf{x}\\) is defined to be \\(\\mathbf{g}(\\mathbf{y})\\), then \\(\\mathbf{g}\\) is \\(a \\mathscr{C}^{\\prime}\\)-mapping of \\(W\\) into \\(R^{n}, \\mathbf{g}(\\mathbf{b})=\\mathbf{a}\\),\n\\[ \\mathbf{f}(\\mathbf{g}(\\mathbf{y}), \\mathbf{y})=\\mathbf{0} \\quad(\\mathbf{y} \\in W), \\]\nand\n\\[ \\mathbf{g}^{\\prime}(\\mathbf{b})=-\\left(A_{x}\\right)^{-1} A_{y} . \\]\nThe function \\(g\\) is “implicitly” defined by (57). Hence the name of the theorem.\nThe equation \\(\\mathbf{f}(\\mathbf{x}, \\mathbf{y})=\\mathbf{0}\\) can be written as a system of \\(n\\) equations in \\(n+m\\) variables:\n\\[ \\begin{aligned} \u0026amp;f_{1}\\left(x_{1}, \\ldots, x_{n}, y_{1}, \\ldots, y_{m}\\right)=0 \\\\ \u0026amp;\\left.\\cdots \\ldots \\ldots, \\ldots, x_{n}, y_{1}, \\ldots, y_{m}\\right)=0 . \\end{aligned} \\]\nThe assumption that \\(A_{x}\\) is invertible means that the \\(n\\) by \\(n\\) matrix\n\\[ \\left[\\begin{array}{ccc} D_{1} f_{1} \u0026amp; \\cdots \u0026amp; D_{n} f_{1} \\\\ \\cdots \\cdots \u0026amp; \\cdots \u0026amp; \\cdots \\\\ D_{1} f_{n} \u0026amp; \\cdots \u0026amp; D_{n} f_{n} \\end{array}\\right] \\]\nevaluated at \\((\\mathbf{a}, \\mathbf{b})\\) defines an invertible linear operator in \\(R^{n}\\); in other words, its column vectors should be independent, or, equivalently, its determinant should be \\(\\neq 0\\). (See Theorem 9.36.) If, furthermore, (59) holds when \\(\\mathbf{x}=\\mathbf{a}\\) and \\(\\mathbf{y}=\\mathbf{b}\\), then the conclusion of the theorem is that (59) can be solved for \\(x_{1}, \\ldots, x_{n}\\) in terms of \\(y_{1}, \\ldots, y_{m}\\), for every \\(\\mathbf{y}\\) near \\(\\mathbf{b}\\), and that these solutions are continuously differentiable functions of \\(\\mathbf{y}\\).\nProof Define \\(\\mathbf{F}\\) by\n\\[ \\mathbf{F}(\\mathbf{x}, \\mathbf{y})=(\\mathbf{f}(\\mathbf{x}, \\mathbf{y}), \\mathbf{y}) \\quad((\\mathbf{x}, \\mathbf{y}) \\in E) . \\]\nThen \\(\\mathbf{F}\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of \\(E\\) into \\(R^{n+m}\\). We claim that \\(\\mathbf{F}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\) is an invertible element of \\(L\\left(R^{n+m}\\right)\\) :\nSince \\(\\mathbf{f}(\\mathbf{a}, \\mathbf{b})=\\mathbf{0}\\), we have\n\\[ \\mathbf{f}(\\mathbf{a}+\\mathbf{h}, \\mathbf{b}+\\mathbf{k})=A(\\mathbf{h}, \\mathbf{k})+\\mathbf{r}(\\mathbf{h}, \\mathbf{k}) \\]\nwhere \\(\\mathbf{r}\\) is the remainder that occurs in the definition of \\(\\mathbf{f}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\). Since\n\\[ \\begin{aligned} \\mathbf{F}(\\mathbf{a}+\\mathbf{h}, \\mathbf{b}+\\mathbf{k})-\\mathbf{F}(\\mathbf{a}, \\mathbf{b}) \u0026amp;=(\\mathbf{f}(\\mathbf{a}+\\mathbf{h}, \\mathbf{b}+\\mathbf{k}), \\mathbf{k}) \\\\ \u0026amp;=(\\mathbf{A}(\\mathbf{h}, \\mathbf{k}), \\mathbf{k})+(\\mathbf{r}(\\mathbf{h}, \\mathbf{k}), \\mathbf{0}) \\end{aligned} \\]\nit follows that \\(\\mathbf{F}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\) is the linear operator on \\(R^{n+m}\\) that maps (h, k) to ( \\(A(\\mathbf{h}, \\mathbf{k}), \\mathbf{k})\\). If this image vector is \\(\\mathbf{0}\\), then \\(A(\\mathbf{h}, \\mathbf{k})=\\mathbf{0}\\) and \\(\\mathbf{k}=\\mathbf{0}\\), hence \\(A(\\mathbf{h}, \\mathbf{0})=\\mathbf{0}\\), and Theorem \\(9.27\\) implies that \\(\\mathbf{h}=\\mathbf{0}\\). It follows that \\(\\mathbf{F}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\) is \\(1-1\\); hence it is invertible (Theorem 9.5).\nThe inverse function theorem can therefore be applied to \\(\\mathbf{F}\\). It shows that there exist open sets \\(U\\) and \\(V\\) in \\(R^{n+m}\\), with \\((\\mathbf{a}, \\mathbf{b}) \\in U,(\\mathbf{0}, \\mathbf{b}) \\in V\\), such that \\(\\mathbf{F}\\) is a 1-1 mapping of \\(U\\) onto \\(V\\).\nWe let \\(W\\) be the set of all \\(\\mathbf{y} \\in R^{m}\\) such that \\((0, y) \\in V\\). Note that \\(\\mathbf{b} \\in W\\)\nIt is clear that \\(W\\) is open since \\(V\\) is open.\nIf \\(\\mathbf{y} \\in W\\), then \\((\\mathbf{0}, \\mathbf{y})=\\mathbf{F}(\\mathbf{x}, \\mathbf{y})\\) for some \\((\\mathbf{x}, \\mathbf{y}) \\in U . B y(60), \\mathbf{f}(\\mathbf{x}, \\mathbf{y})=\\mathbf{0}\\) for this \\(\\mathbf{x}\\).\nSuppose, with the same \\(\\mathbf{y}\\), that \\(\\left(\\mathbf{x}^{\\prime}, \\mathbf{y}\\right) \\in U\\) and \\(\\mathbf{f}\\left(\\mathbf{x}^{\\prime}, \\mathbf{y}\\right)=\\mathbf{0}\\). Then\n\\[ \\mathbf{F}\\left(\\mathbf{x}^{\\prime}, \\mathbf{y}\\right)=\\left(\\mathbf{f}\\left(\\mathbf{x}^{\\prime}, \\mathbf{y}\\right), \\mathbf{y}\\right)=(\\mathbf{f}(\\mathbf{x}, \\mathbf{y}), \\mathbf{y})=\\mathbf{F}(\\mathbf{x}, \\mathbf{y}) . \\]\nSince \\(\\mathbf{F}\\) is \\(1-1\\) in \\(U\\), it follows that \\(\\mathbf{x}^{\\prime}=\\mathbf{x}\\).\nThis proves the first part of the theorem.\nFor the second part, define \\(\\mathbf{g}(\\mathbf{y})\\), for \\(\\mathbf{y} \\in W\\), so that \\((\\mathbf{g}(\\mathbf{y}), \\mathbf{y}) \\in U\\) and (57) holds. Then\n\\[ \\mathbf{F}(\\mathbf{g}(\\mathbf{y}), \\mathbf{y})=(\\mathbf{0}, \\mathbf{y}) \\quad(\\mathbf{y} \\in W) \\text {. } \\]\nIf \\(\\mathbf{G}\\) is the mapping of \\(V\\) onto \\(U\\) that inverts \\(\\mathbf{F}\\), then \\(\\mathbf{G} \\in \\mathscr{C}^{\\prime}\\), by the inverse function theorem, and (61) gives\n\\[ (\\mathbf{g}(\\mathbf{y}), \\mathbf{y})=\\mathbf{G}(\\mathbf{0}, \\mathbf{y}) \\quad(\\mathbf{y} \\in W) \\text {. } \\]\nSince \\(\\mathbf{G} \\in \\mathscr{C}^{\\prime}\\), (62) shows that \\(\\mathbf{g} \\in \\mathscr{C}^{\\prime}\\).\nFinally, to compute \\(\\mathbf{g}^{\\prime}(\\mathbf{b})\\), put \\((\\mathbf{g}(\\mathbf{y}), \\mathbf{y})=\\Phi(\\mathbf{y})\\). Then\n\\[ \\Phi^{\\prime}(\\mathbf{y}) \\mathbf{k}=\\left(\\mathbf{g}^{\\prime}(\\mathbf{y}) \\mathbf{k}, \\mathbf{k}\\right) \\quad\\left(\\mathbf{y} \\in W, \\mathbf{k} \\in R^{m}\\right) \\]\nBy \\((57), \\mathbf{f}(\\Phi(\\mathbf{y}))=\\mathbf{0}\\) in \\(W\\). The chain rule shows therefore that\n\\[ \\mathbf{f}^{\\prime}(\\Phi(\\mathbf{y})) \\Phi^{\\prime}(\\mathbf{y})=0 . \\]\nWhen \\(\\mathbf{y}=\\mathbf{b}\\), then \\(\\Phi(\\mathbf{y})=(\\mathbf{a}, \\mathbf{b})\\), and \\(\\mathbf{f}^{\\prime}(\\Phi(\\mathbf{y}))=A\\). Thus\n\\[ A \\Phi^{\\prime}(\\mathbf{b})=0 \\]\nIt now follows from (64), (63), and (54), that\n\\[ A_{x} \\mathbf{g}^{\\prime}(\\mathbf{b}) \\mathbf{k}+A_{y} \\mathbf{k}=A\\left(\\mathbf{g}^{\\prime}(\\mathbf{b}) \\mathbf{k}, \\mathbf{k}\\right)=A \\Phi^{\\prime}(\\mathbf{b}) \\mathbf{k}=\\mathbf{0} \\]\nfor every \\(\\mathbf{k} \\in R^{m}\\). Thus\n\\[ A_{x} \\mathbf{g}^{\\prime}(\\mathbf{b})+A_{y}=0 \\]\nThis is equivalent to (58), and completes the proof.\nNote. In terms of the components of \\(f\\) and \\(g,(65)\\) becomes\n\\[ \\sum_{j=1}^{n}\\left(D_{j} f_{i}\\right)(\\mathbf{a}, \\mathbf{b})\\left(D_{k} g_{j}\\right)(\\mathbf{b})=-\\left(D_{n+k} f_{i}\\right)(\\mathbf{a}, \\mathbf{b}) \\]\nor\n\\[ \\sum_{j=1}^{n}\\left(\\frac{\\partial f_{i}}{\\partial x_{j}}\\right)\\left(\\frac{\\partial g_{j}}{\\partial y_{k}}\\right)=-\\left(\\frac{\\partial f_{i}}{\\partial y_{k}}\\right) \\]\nwhere \\(1 \\leq i \\leq n, 1 \\leq k \\leq m\\)\nFor each \\(k\\), this is a system of \\(n\\) linear equations in which the derivatives \\(\\partial g_{j} / \\partial y_{k}(1 \\leq j \\leq n)\\) are the unknowns.\n9.29 Example Take \\(n=2, m=3\\), and consider the mapping \\(\\mathbf{f}=\\left(f_{1}, f_{2}\\right)\\) of \\(R^{5}\\) into \\(R^{2}\\) given by\n\\[ \\begin{aligned} \u0026amp;f_{1}\\left(x_{1}, x_{2}, y_{1}, y_{2}, y_{3}\\right)=2 e^{x_{1}}+x_{2} y_{1}-4 y_{2}+3 \\\\ \u0026amp;f_{2}\\left(x_{1}, x_{2}, y_{1}, y_{2}, y_{3}\\right)=x_{2} \\cos x_{1}-6 x_{1}+2 y_{1}-y_{3} . \\end{aligned} \\]\nIf \\(\\mathbf{a}=(0,1)\\) and \\(\\mathbf{b}=(3,2,7)\\), then \\(\\mathbf{f}(\\mathbf{a}, \\mathbf{b})=0\\).\nWith respect to the standard bases, the matrix of the transformation \\(A=\\mathbf{f}^{\\prime}(\\mathbf{a}, \\mathbf{b})\\) is\n\\[ [A]=\\left[\\begin{array}{rrrrr} 2 \u0026amp; 3 \u0026amp; 1 \u0026amp; -4 \u0026amp; 0 \\\\ -6 \u0026amp; 1 \u0026amp; 2 \u0026amp; 0 \u0026amp; -1 \\end{array}\\right] \\]\nHence\n\\[ \\left[A_{x}\\right]=\\left[\\begin{array}{rr} 2 \u0026amp; 3 \\\\ -6 \u0026amp; 1 \\end{array}\\right], \\quad\\left[A_{y}\\right]=\\left[\\begin{array}{rrr} 1 \u0026amp; -4 \u0026amp; 0 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\end{array}\\right] \\]\nWe see that the column vectors of \\(\\left[A_{x}\\right]\\) are independent. Hence \\(A_{x}\\) is invertible and the implicit function theorem asserts the existence of a \\(\\mathscr{C}^{\\prime}\\)-mapping \\(\\mathbf{g}\\), defined in a neighborhood of \\((3,2,7)\\), such that \\(\\mathbf{g}(3,2,7)=(0,1)\\) and \\(\\mathbf{f}(\\mathbf{g}(\\mathbf{y}), \\mathbf{y})=\\mathbf{0}\\).\nWe can use (58) to compute \\(\\mathbf{g}^{\\prime}(3,2,7)\\) : Since\n\\[ \\left[\\left(A_{x}\\right)^{-1}\\right]=\\left[A_{x}\\right]^{-1}=\\frac{1}{20}\\left[\\begin{array}{rr} 1 \u0026amp; -3 \\\\ 6 \u0026amp; 2 \\end{array}\\right] \\]\ngives \\[ \\left[g^{\\prime}(3,2,7)\\right]=-\\frac{1}{20}\\left[\\begin{array}{rr} 1 \u0026amp; -3 \\\\ 6 \u0026amp; 2 \\end{array}\\right]\\left[\\begin{array}{rrr} 1 \u0026amp; -4 \u0026amp; 0 \\\\ 2 \u0026amp; 0 \u0026amp; -1 \\end{array}\\right]=\\left[\\begin{array}{rrr} \\frac{1}{4} \u0026amp; \\frac{1}{5} \u0026amp; -\\frac{3}{20} \\\\ -\\frac{1}{2} \u0026amp; \\frac{6}{3} \u0026amp; \\frac{1}{10} \\end{array}\\right] \\]\nIn terms of partial derivatives, the conclusion is that\n\\[ \\begin{aligned} \u0026amp; D_{1} g_{1}=\\frac{1}{4}, \\quad D_{2} g_{1}=\\frac{1}{3} \\quad D_{3} g_{1}=-\\frac{3}{20} \\\\ \u0026amp; D_{1} g_{2}=-\\frac{1}{2} \\quad D_{2} g_{2}=\\frac{6}{5} \\quad D_{3} g_{2}=\\frac{1}{10} \\end{aligned} \\]\nat the point \\((3,2,7)\\).\n","date":"2022-08-13T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/5-the-implicit-function-theorem/","section":"baby rudin","tags":null,"title":"5 THE IMPLICIT FUNCTION THEOREM"},{"categories":null,"contents":"Although this theorem is not as important as the inverse function theorem or the implicit function theorem, we include it as another interesting illustration of the general principle that the local behavior of a continuously differentiable mapping \\(\\mathbf{F}\\) near a point \\(\\mathbf{x}\\) is similar to that of the linear transformation \\(\\mathbf{F}^{\\prime}(\\mathbf{x})\\).\nBefore stating it, we need a few more facts about linear transformations.\n9.30 Definitions Suppose \\(X\\) and \\(Y\\) are vector spaces, and \\(A \\in L(X, Y)\\), as in Definition 9.6. The null space of \\(A, \\mathscr{N}(A)\\), is the set of all \\(\\mathbf{x} \\in X\\) at which \\(A \\mathbf{x}=\\mathbf{0}\\). It is clear that \\(\\mathcal{N}(A)\\) is a vector space in \\(X\\).\nLikewise, the range of \\(A, \\mathscr{R}(A)\\), is a vector space in \\(Y\\).\nThe \\(\\operatorname{rank}\\) of \\(A\\) is defined to be the dimension of \\(\\mathscr{R}(A)\\).\nFor example, the invertible elements of \\(L\\left(R^{n}\\right)\\) are precisely those whose rank is \\(n\\). This follows from Theorem 9.5.\nIf \\(A \\in L(X, Y)\\) and \\(A\\) has \\(\\operatorname{rank} 0\\), then \\(A \\mathbf{x}=0\\) for all \\(x \\in A\\), hence \\(\\mathscr{N}(A)=X\\). In this connection, see Exercise \\(25 .\\)\n9.31 Projections Let \\(X\\) be a vector space. An operator \\(P \\in L(X)\\) is said to be a projection in \\(X\\) if \\(P^{2}=P\\).\nMore explicitly, the requirement is that \\(P(P \\mathbf{x})=P \\mathbf{x}\\) for every \\(\\mathbf{x} \\in X\\). In other words, \\(P\\) fixes every vector in its range \\(\\mathscr{R}(P)\\).\nHere are some elementary properties of projections:\nIf \\(P\\) is a projection in \\(X\\), then every \\(\\mathbf{x} \\in X\\) has a unique representation of the form \\[ \\mathbf{x}=\\mathbf{x}_{1}+\\mathbf{x}_{2} \\]\nwhere \\(\\mathbf{x}_{1} \\in \\mathscr{R}(P), \\mathbf{x}_{2} \\in \\mathscr{N}(P)\\).\nTo obtain the representation, put \\(\\mathbf{x}_{1}=P \\mathbf{x}, \\mathbf{x}_{2}=\\mathbf{x}-\\mathbf{x}_{1}\\). Then \\(P \\mathbf{x}_{2}=P \\mathbf{x}-P \\mathbf{x}_{1}=P \\mathbf{x}-P^{2} \\mathbf{x}=0\\). As regards the uniqueness, apply \\(P\\) to the equation \\(\\mathbf{x}=\\mathbf{x}_{1}+\\mathbf{x}_{2}\\). Since \\(\\mathbf{x}_{1} \\in \\mathscr{R}(P), P \\mathbf{x}_{1}=\\mathbf{x}_{1}\\); since \\(P \\mathbf{x}_{2}=\\mathbf{0}\\), it follows that \\(\\mathbf{x}_{1}=P \\mathbf{x}\\).\nIf \\(X\\) is a finite-dimensional vector space and if \\(X_{1}\\) is a vector space in \\(X\\), then there is a projection \\(P\\) in \\(X\\) with \\(\\mathscr{R}(P)=X_{1}\\). If \\(X_{1}\\) contains only 0 , this is trivial: put \\(P \\mathbf{x}=\\mathbf{0}\\) for all \\(\\mathbf{x} \\in X\\). Assume \\(\\operatorname{dim} X_{1}=k\u0026gt;0\\). By Theorem 9.3, \\(X\\) has then a basis \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}\\right\\}\\) such that \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{k}\\right\\}\\) is a basis of \\(X_{1}\\). Define\n\\[ P\\left(c_{1} \\mathbf{u}_{1}+\\cdots+c_{n} \\mathbf{u}_{n}\\right)=c_{1} \\mathbf{u}_{1}+\\cdots+c_{k} \\mathbf{u}_{k} \\]\nfor arbitrary scalars \\(c_{1}, \\ldots, c_{n}\\).\nThen \\(P \\mathbf{x}=\\mathbf{x}\\) for every \\(\\mathbf{x} \\in X_{1}\\), and \\(X_{1}=\\mathscr{R}(P)\\).\nNote that \\(\\left\\{\\mathbf{u}_{k+1}, \\ldots, \\mathbf{u}_{n}\\right\\}\\) is a basis of \\(\\mathcal{N}(P)\\). Note also that there are infinitely many projections in \\(X\\), with range \\(X_{1}\\), if \\(0\u0026lt;\\operatorname{dim} X_{1}\u0026lt;\\operatorname{dim} X\\).\n9.32 Theorem Suppose \\(m, n, r\\) are nonnegative integers, \\(m \\geq r, n \\geq r, \\mathbf{F}\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\), and \\(\\mathbf{F}^{\\prime}(\\mathbf{x})\\) has rank \\(r\\) for every \\(\\mathbf{x} \\in E\\).\nFix \\(\\mathbf{a} \\in E\\), put \\(A=\\mathbf{F}^{\\prime}(\\mathbf{a})\\), let \\(Y_{1}\\) be the range of \\(A\\), and let \\(P\\) be a projection in \\(R^{m}\\) whose range is \\(Y_{1}\\). Let \\(Y_{2}\\) be the null space of \\(P\\).\nThen there are open sets \\(U\\) and \\(V\\) in \\(R^{n}\\), with \\(\\mathbf{a} \\in U, U \\subset E\\), and there is a 1-1 \\(\\mathscr{C}^{\\prime}\\)-mapping \\(\\mathbf{H}\\) of \\(V\\) onto \\(U\\) (whose inverse is also of class \\(\\mathscr{C}^{\\prime}\\) ) such that\n\\[ \\mathbf{F}(\\mathbf{H}(\\mathbf{x}))=A \\mathbf{x}+\\varphi(A \\mathbf{x}) \\quad(\\mathbf{x} \\in V) \\]\nwhere \\(\\varphi\\) is \\(a \\mathscr{C}^{\\prime}\\)-mapping of the open set \\(A(V) \\subset Y_{1}\\) into \\(Y_{2}\\).\nAfter the proof we shall give a more geometric description of the information that (66) contains.\nProof If \\(r=0\\), Theorem \\(9.19\\) shows that \\(\\mathbf{F}(\\mathbf{x})\\) is constant in a neighborhood \\(U\\) of a, and (66) holds trivially, with \\(V=U, \\mathbf{H}(\\mathbf{x})=\\mathbf{x}, \\varphi(0)=\\mathbf{F}(\\mathbf{a})\\).\nFrom now on we assume \\(r\u0026gt;0\\). Since \\(\\operatorname{dim} Y_{1}=r, Y_{1}\\) has a basis \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}\\right\\}\\). Choose \\(\\mathbf{z}_{i} \\in R^{n}\\) so that \\(A \\mathbf{z}_{i}=\\mathbf{y}_{i}(1 \\leq i \\leq r)\\), and define a linear mapping \\(S\\) of \\(Y_{1}\\) into \\(R^{n}\\) by setting\n\\[ S\\left(c_{1} \\mathbf{y}_{1}+\\cdots+c_{r} \\mathbf{y}_{r}\\right)=c_{1} \\mathbf{z}_{1}+\\cdots+c_{r} \\mathbf{z}_{r} \\]\nfor all scalars \\(c_{1}, \\ldots, c_{r}\\).\nThen \\(A S \\mathbf{y}_{i}=A \\mathbf{z}_{i}=\\mathbf{y}_{i}\\) for \\(1 \\leq i \\leq r\\). Thus\n\\[ A S \\mathbf{y}=\\mathrm{y} \\quad\\left(\\mathrm{y} \\in Y_{1}\\right) \\text {. } \\]\nDefine a mapping \\(\\mathbf{G}\\) of \\(E\\) into \\(R^{n}\\) by setting\n\\[ \\mathbf{G}(\\mathbf{x})=\\mathbf{x}+S P[\\mathbf{F}(\\mathbf{x})-A \\mathbf{x}] \\quad(\\mathbf{x} \\in E) . \\]\nSince \\(\\mathbf{F}^{\\prime}(\\mathbf{a})=A\\), differentiation of (69) shows that \\(\\mathbf{G}^{\\prime}(\\mathbf{a})=I\\), the identity operator on \\(R^{n}\\). By the inverse function theorem, there are open sets \\(U\\) and \\(V\\) in \\(R^{n}\\), with \\(\\mathbf{a} \\in U\\), such that \\(\\mathbf{G}\\) is a 1-1 mapping of \\(U\\) onto \\(V\\) whose inverse \\(\\mathbf{H}\\) is also of class \\(\\mathscr{C}^{\\prime}\\). Moreover, by shrinking \\(U\\) and \\(V\\), if necessary, we can arrange it so that \\(V\\) is convex and \\(\\mathbf{H}^{\\prime}(\\mathbf{x})\\) is invertible for every \\(\\mathbf{x} \\in V\\). Note that \\(A S P A=A\\), since \\(P A=A\\) and (68) holds. Therefore (69) gives\n\\[ A \\mathbf{G}(\\mathbf{x})=P \\mathbf{F}(\\mathbf{x}) \\quad(\\mathbf{x} \\in E) . \\]\nIn particular, (70) holds for \\(\\mathbf{x} \\in U\\). If we replace \\(\\mathbf{x}\\) by \\(\\mathbf{H}(\\mathbf{x})\\), we obtain\n\\[ P \\mathbf{F}(\\mathbf{H}(\\mathbf{x}))=A \\mathbf{x} \\quad(\\mathrm{x} \\in V) . \\]\nDefine\n\\[ \\psi(\\mathbf{x})=\\mathbf{F}(\\mathbf{H}(\\mathbf{x}))-A \\mathbf{x} \\quad(\\mathbf{x} \\in V) \\]\nSince \\(P A=A\\), (71) implies that \\(P \\psi(\\mathbf{x})=0\\) for all \\(\\mathbf{x} \\in V\\). Thus \\(\\psi\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping of \\(V\\) into \\(Y_{2}\\).\nSince \\(V\\) is open, it is clear that \\(A(V)\\) is an open subset of its range \\(\\mathscr{R}(A)=Y_{1}\\)\nTo complete the proof, i.e., to go from (72) to (66), we have to show that there is a \\(\\mathscr{C}^{\\prime}\\)-mapping \\(\\varphi\\) of \\(A(V)\\) into \\(Y_{2}\\) which satisfies\n\\[ \\varphi(A \\mathbf{x})=\\psi(\\mathbf{x}) \\quad(\\mathbf{x} \\in V) . \\]\nAs a step toward (73), we will first prove that\n\\[ \\psi\\left(\\mathbf{x}_{1}\\right)=\\psi\\left(\\mathbf{x}_{2}\\right) \\]\nif \\(\\mathbf{x}_{1} \\in V, \\mathbf{x}_{2} \\in V, A \\mathbf{x}_{1}=A \\mathbf{x}_{2}\\).\nPut \\(\\Phi(\\mathbf{x})=\\mathbf{F}(\\mathbf{H}(\\mathbf{x}))\\), for \\(\\mathbf{x} \\in V\\). Since \\(\\mathbf{H}^{\\prime}(\\mathbf{x})\\) has rank \\(n\\) for every \\(\\mathbf{x} \\in V\\), and \\(\\mathbf{F}^{\\prime}(\\mathbf{x})\\) has rank \\(r\\) for every \\(\\mathbf{x} \\in U\\), it follows that\n\\[ \\operatorname{rank} \\Phi^{\\prime}(\\mathbf{x})=\\operatorname{rank} \\mathbf{F}^{\\prime}(\\mathbf{H}(\\mathbf{x})) \\mathbf{H}^{\\prime}(\\mathbf{x})=r \\quad(\\mathbf{x} \\in V) . \\]\nFix \\(\\mathbf{x} \\in V\\). Let \\(M\\) be the range of \\(\\Phi^{\\prime}(\\mathbf{x})\\). Then \\(M \\subset R^{m}, \\operatorname{dim} M=r\\). By (71),\n\\[ P \\Phi^{\\prime}(\\mathbf{x})=A . \\]\nThus \\(P\\) maps \\(M\\) onto \\(\\mathscr{R}(A)=Y_{1}\\). Since \\(M\\) and \\(Y_{1}\\) have the same dimension, it follows that \\(P\\) (restricted to \\(M\\) ) is 1-1.\nSuppose now that \\(A \\mathbf{h}=\\mathbf{0}\\). Then \\(P \\Phi^{\\prime}(\\mathbf{x}) \\mathbf{h}=\\mathbf{0}\\), by (76). But \\(\\Phi^{\\prime}(\\mathbf{x}) \\mathbf{h} \\in M\\), and \\(P\\) is 1-1 on \\(M\\). Hence \\(\\Phi^{\\prime}(\\mathbf{x}) \\mathbf{h}=\\mathbf{0}\\). A look at (72) shows now that we have proved the following:\nIf \\(\\mathbf{x} \\in V\\) and \\(A \\mathbf{h}=\\mathbf{0}\\), then \\(\\psi^{\\prime}(\\mathbf{x}) \\mathbf{h}=\\mathbf{0}\\).\nWe can now prove (74). Suppose \\(\\mathbf{x}_{1} \\in V, \\mathbf{x}_{2} \\in V, A \\mathbf{x}_{1}=A \\mathbf{x}_{2}\\). Put \\(\\mathbf{h}=\\mathbf{x}_{2}-\\mathbf{x}_{1}\\) and define\n\\[ \\mathbf{g}(t)=\\psi\\left(\\mathbf{x}_{1}+t \\mathbf{h}\\right) \\quad(0 \\leq t \\leq 1) \\]\nThe convexity of \\(V\\) shows that \\(\\mathbf{x}_{1}+t \\mathbf{h} \\in V\\) for these \\(t\\). Hence\n\\[ \\mathbf{g}^{\\prime}(t)=\\psi^{\\prime}\\left(\\mathbf{x}_{1}+t \\mathbf{h}\\right) h=0 \\quad(0 \\leq t \\leq 1), \\]\nso that \\(\\mathbf{g}(1)=\\mathbf{g}(0)\\). But \\(\\mathbf{g}(1)=\\psi\\left(\\mathbf{x}_{2}\\right)\\) and \\(\\mathbf{g}(0)=\\psi\\left(\\mathbf{x}_{1}\\right)\\). This proves (74).\nBy (74), \\(\\psi(\\mathbf{x})\\) depends only on \\(A \\mathbf{x}\\), for \\(\\mathbf{x} \\in V\\). Hence (73) defines \\(\\varphi\\) unambiguously in \\(A(V)\\). It only remains to be proved that \\(\\varphi \\in \\mathscr{C}^{\\prime}\\).\nFix \\(\\mathbf{y}_{0} \\in A(V)\\), fix \\(\\mathbf{x}_{0} \\in V\\) so that \\(A \\mathbf{x}_{0}=\\mathbf{y}_{0}\\). Since \\(V\\) is open, \\(\\mathbf{y}_{0}\\) has a neighborhood \\(W\\) in \\(Y_{1}\\) such that the vector\n\\[ \\mathbf{x}=\\mathbf{x}_{0}+S\\left(\\mathbf{y}-\\mathbf{y}_{0}\\right) \\]\nlies in \\(V\\) for all \\(\\mathbf{y} \\in W\\). By (68),\n\\[ A \\mathrm{x}=A \\mathrm{x}_{0}+\\mathrm{y}-\\mathrm{y}_{0}=\\mathrm{y} \\]\nThus (73) and (79) give\n\\[ \\varphi(\\mathbf{y})=\\psi\\left(\\mathbf{x}_{0}-S \\mathbf{y}_{0}+S \\mathbf{y}\\right) \\quad(\\mathbf{y} \\in W) . \\]\nThis formula shows that \\(\\varphi \\in \\mathscr{C}^{\\prime}\\) in \\(W\\), hence in \\(A(V)\\), since \\(\\mathbf{y}_{0}\\) was chosen arbitrarily in \\(A(V)\\).\nThe proof is now complete.\nHere is what the theorem tells us about the geometry of the mapping \\(\\mathbf{F}\\).\nIf \\(\\mathbf{y} \\in \\mathbf{F}(U)\\) then \\(\\mathbf{y}=\\mathbf{F}(\\mathbf{H}(\\mathbf{x}))\\) for some \\(\\mathbf{x} \\in V\\), and (66) shows that \\(P \\mathbf{y}=A \\mathbf{x}\\). Therefore\n\\[ \\mathbf{y}=P \\mathbf{y}+\\varphi(P \\mathbf{y}) \\quad(\\mathbf{y} \\in \\mathbf{F}(U)) \\]\nThis shows that \\(y\\) is determined by its projection \\(P \\mathbf{y}\\), and that \\(P\\), restricted to \\(\\mathbf{F}(U)\\), is a 1-1 mapping of \\(\\mathbf{F}(U)\\) onto \\(A(V)\\). Thus \\(\\mathbf{F}(U)\\) is an ” \\(r\\)-dimensional surface” with precisely one point “over” each point of \\(A(V)\\). We may also regard \\(F(U)\\) as the graph of \\(\\varphi\\).\nIf \\(\\Phi(\\mathbf{x})=\\mathbf{F}(\\mathbf{H}(\\mathbf{x}))\\), as in the proof, then (66) shows that the level sets of \\(\\Phi\\) (these are the sets on which \\(\\Phi\\) attains a given value) are precisely the level sets of \\(A\\) in \\(V\\). These are “flat” since they are intersections with \\(V\\) of translates of the vector space \\(\\mathcal{N}(A)\\). Note that \\(\\operatorname{dim} \\mathcal{N}(A)=n-r\\) (Exercise 25).\nThe level sets of \\(\\mathbf{F}\\) in \\(U\\) are the images under \\(\\mathbf{H}\\) of the flat level sets of \\(\\Phi\\) in \\(V\\). They are thus ” \\((n-r)\\)-dimensional surfaces” in \\(U\\).\n","date":"2022-08-13T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/6-the-rank-theorem/","section":"baby rudin","tags":null,"title":"6 THE RANK THEOREM"},{"categories":null,"contents":"Determinants are numbers associated to square matrices, and hence to the operators represented by such matrices. They are 0 if and only if the corresponding operator fails to be invertible. They can therefore be used to decide whether the hypotheses of some of the preceding theorems are satisfied. They will play an even more important role in Chap. \\(10 .\\)\n9.33 Definition If \\(\\left(j_{1}, \\ldots, j_{n}\\right)\\) is an ordered \\(n\\)-tuple of integers, define\n\\[ s\\left(j_{1}, \\ldots, j_{n}\\right)=\\prod_{p\u0026lt;q} \\operatorname{sgn}\\left(j_{q}-j_{p}\\right), \\]\nwhere \\(\\operatorname{sgn} x=1\\) if \\(x\u0026gt;0, \\operatorname{sgn} x=-1\\) if \\(x\u0026lt;0, \\operatorname{sgn} x=0\\) if \\(x=0\\). Then \\(s\\left(j_{1}, \\ldots, j_{n}\\right)=1,-1\\), or 0 , and it changes sign if any two of the \\(j\\) ’s are interchanged.\nLet \\([A]\\) be the matrix of a linear operator \\(A\\) on \\(R^{n}\\), relative to the standard basis \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\), with entries \\(a(i, j)\\) in the \\(i\\) th row and jth column. The determinant of \\([A]\\) is defined to be the number\n\\[ \\operatorname{det}[A]=\\sum s\\left(j_{1}, \\ldots, j_{n}\\right) a\\left(1, j_{1}\\right) a\\left(2, j_{2}\\right) \\cdots a\\left(n, j_{n}\\right) . \\]\nThe sum in (83) extends over all ordered \\(n\\)-tuples of integers \\(\\left(j_{1}, \\ldots, j_{n}\\right)\\) with \\(1 \\leq j_{r} \\leq n\\).\nThe column vectors \\(\\mathbf{x}_{j}\\) of \\([A]\\) are\n\\[ \\mathbf{x}_{j}=\\sum_{i=1}^{n} a(i, j) \\mathbf{e}_{i} \\quad(1 \\leq j \\leq n) . \\]\nIt will be convenient to think of det \\([A]\\) as a function of the column vectors of \\([A]\\). If we write\n\\[ \\operatorname{det}\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right)=\\operatorname{det}[A] \\text {, } \\]\ndet is now a real function on the set of all ordered \\(n\\)-tuples of vectors in \\(R^{n}\\).\n9.34 Theorem\nIf I is the identity operator on \\(R^{n}\\), then \\[ \\operatorname{det}[I]=\\operatorname{det}\\left(\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right)=1 . \\]\ndet is a linear function of each of the column vectors \\(\\mathbf{x}_{j}\\), if the others are held fixed.\nIf \\([A]_{1}\\) is obtained from \\([A]\\) by interchanging two columns, then \\(\\operatorname{det}[A]_{1}=-\\operatorname{det}[A]\\).\nIf \\([A]\\) has two equal columns, then \\(\\operatorname{det}[A]=0\\).\nProof If \\(A=I\\), then \\(a(i, i)=1\\) and \\(a(i, j)=0\\) for \\(i \\neq j\\). Hence\n\\[ \\operatorname{det}[I]=s(1,2, \\ldots, n)=1 \\text {, } \\]\nwhich proves \\((a)\\). By (82), \\(s\\left(j_{1}, \\ldots, j_{n}\\right)=0\\) if any two of the \\(j\\) ’s are equal. Each of the remaining \\(n\\) ! products in (83) contains exactly one factor from each column. This proves \\((b)\\). Part \\((c)\\) is an immediate consequence of the fact that \\(s\\left(j_{1}, \\ldots, j_{n}\\right)\\) changes sign if any two of the j’s are interchanged, and \\((d)\\) is a corollary of \\((c)\\).\n9.35 Theorem If \\([A]\\) and \\([B]\\) are \\(n\\) by \\(n\\) matrices, then\n\\[ \\operatorname{det}([B][A])=\\operatorname{det}[B] \\operatorname{det}[A] . \\]\nProof If \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\) are the columns of \\([A]\\), define\n\\[ \\Delta_{B}\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right)=\\Delta_{B}[A]=\\operatorname{det}([B][A]) . \\]\nThe columns of \\([B][A]\\) are the vectors \\(B \\mathbf{x}_{1}, \\ldots, B \\mathbf{x}_{n}\\). Thus\n\\[ \\Delta_{B}\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right)=\\operatorname{det}\\left(B \\mathbf{x}_{1}, \\ldots, B \\mathbf{x}_{n}\\right) \\text {. } \\]\nBy (86) and Theorem \\(9.34, \\Delta_{B}\\) also has properties \\(9.34(b)\\) to \\((d)\\). By \\((b)\\) and (84),\n\\[ \\Delta_{B}[A]=\\Delta_{B}\\left(\\sum_{i} a(i, 1) \\mathbf{e}_{i}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}\\right)=\\sum_{i} a(i, 1) \\Delta_{B}\\left(\\mathbf{e}_{i}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}\\right) . \\]\nRepeating this process with \\(\\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}\\), we obtain\n\\[ \\Delta_{B}[A]=\\sum a\\left(i_{1}, 1\\right) a\\left(i_{2}, 2\\right) \\cdots a\\left(i_{n}, n\\right) \\Delta_{B}\\left(\\mathbf{e}_{i_{1}}, \\ldots, \\mathbf{e}_{i_{n}}\\right) \\text {, } \\]\nthe sum being extended over all ordered \\(n\\)-tuples \\(\\left(i_{1}, \\ldots, i_{n}\\right)\\) with \\(1 \\leq i_{r} \\leq n .\\) By \\((c)\\) and \\((d)\\)\n\\[ \\Delta_{B}\\left(\\mathbf{e}_{i_{1}}, \\ldots, \\mathbf{e}_{i_{n}}\\right)=t\\left(i_{1}, \\ldots, i_{n}\\right) \\Delta_{B}\\left(\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right), \\]\nwhere \\(t=1,0\\), or \\(-1\\), and since \\([B][I]=[B],(85)\\) shows that\n\\[ \\Delta_{B}\\left(\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right)=\\operatorname{det}[B] \\text {. } \\]\nSubstituting (89) and (88) into (87), we obtain\n\\[ \\operatorname{det}([B][A])=\\left\\{\\sum a\\left(i_{1}, 1\\right) \\cdots a\\left(i_{n}, n\\right) t\\left(i_{1}, \\ldots, i_{n}\\right)\\right\\} \\operatorname{det}[B] \\text {, } \\]\nfor all \\(n\\) by \\(n\\) matrices \\([A]\\) and \\([B]\\). Taking \\(B=I\\), we see that the above sum in braces is \\(\\operatorname{det}[A]\\). This proves the theorem.\n9.36 Theorem \\(A\\) linear operator \\(A\\) on \\(R^{n}\\) is invertible if and only if \\(\\operatorname{det}[A] \\neq 0\\).\nProof If \\(A\\) is invertible, Theorem \\(9.35\\) shows that\n\\[ \\operatorname{det}[A] \\operatorname{det}\\left[A^{-1}\\right]=\\operatorname{det}\\left[A A^{-1}\\right]=\\operatorname{det}[I]=1 \\text {, } \\]\nso that \\(\\operatorname{det}[A] \\neq 0\\).\nIf \\(A\\) is not invertible, the columns \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\) of \\([A]\\) are dependent (Theorem 9.5); hence there is one, say, \\(\\mathbf{x}_{k}\\), such that\n\\[ \\mathbf{x}_{k}+\\sum_{j \\neq k} c_{j} \\mathbf{x}_{j}=0 \\]\nfor certain scalars \\(c_{j}\\). By \\(9.34(b)\\) and \\((d), \\mathbf{x}_{k}\\) can be replaced by \\(\\mathbf{x}_{k}+c_{j} \\mathbf{x}_{j}\\) without altering the determinant, if \\(j \\neq k\\). Repeating, we see that \\(\\mathbf{x}_{k}\\) can be replaced by the left side of (90), i.e., by 0 , without altering the determinant. But a matrix which has 0 for one column has determinant 0 . Hence \\(\\operatorname{det}[A]=0\\).\n9.37 Remark Suppose \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}\\right\\}\\) are bases in \\(R^{n}\\). Every linear operator \\(A\\) on \\(R^{n}\\) determines matrices \\([A]\\) and \\([A]_{U}\\), with entries \\(a_{i j}\\) and \\(\\alpha_{i j}\\), given by\n\\[ A \\mathbf{e}_{j}=\\sum_{\\imath} a_{i j} \\mathbf{e}_{i}, \\quad A \\mathbf{u}_{j}=\\sum_{i} \\alpha_{i j} \\mathbf{u}_{i} . \\]\nIf \\(\\mathbf{u}_{j}=B \\mathbf{e}_{j}=\\Sigma b_{i j} \\mathbf{e}_{i}\\), then \\(A \\mathbf{u}_{j}\\) is equal to\n\\[ \\sum_{k} \\alpha_{k j} B \\mathbf{e}_{k}=\\sum_{k} \\alpha_{k j} \\sum_{i} b_{i k} \\mathbf{e}_{i}=\\sum_{i}\\left(\\sum_{k} b_{i k} \\alpha_{k j}\\right) \\mathbf{e}_{i}, \\]\nand also to\n\\[ A B \\mathbf{e}_{j}=A \\sum_{k} b_{k j} \\mathbf{e}_{k}=\\sum_{i}\\left(\\sum_{k} a_{i k} b_{k j}\\right) \\mathbf{e}_{i} . \\]\nThus \\(\\Sigma b_{i k} \\alpha_{k j}=\\Sigma a_{i k} b_{k j}\\), or\n\\[ [B][A]_{U}=[A][B] \\text {. } \\]\nSince \\(B\\) is invertible, \\(\\operatorname{det}[B] \\neq 0\\). Hence \\((91)\\), combined with Theorem \\(9.35\\), shows that\n\\[ \\operatorname{det}[A]_{U}=\\operatorname{det}[A] \\text {. } \\]\nThe determinant of the matrix of a linear operator does therefore not depend on the basis which is used to construct the matrix. It is thus meaningful to speak of the determinant of a linear operator, without having any basis in mind.\n9.38 Jacobians If \\(\\mathrm{f}\\) maps an open set \\(E \\subset R^{n}\\) into \\(R^{n}\\), and if \\(\\mathrm{f}\\) is differentiable at a point \\(\\mathbf{x} \\in E\\), the determinant of the linear operator \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) is called the Jacobian of \\(\\mathbf{f}\\) at \\(\\mathbf{x}\\). In symbols,\n\\[ J_{\\mathbf{f}}(\\mathbf{x})=\\operatorname{det} \\mathbf{f}^{\\prime}(\\mathbf{x}) . \\]\nWe shall also use the notation\n\\[ \\frac{\\partial\\left(y_{1}, \\ldots, y_{n}\\right)}{\\partial\\left(x_{1}, \\ldots, x_{n}\\right)} \\]\nfor \\(J_{\\mathbf{f}}(\\mathbf{x})\\), if \\(\\left(y_{1}, \\ldots, y_{n}\\right)=\\mathbf{f}\\left(x_{1}, \\ldots, x_{n}\\right)\\).\nIn terms of Jacobians, the crucial hypothesis in the inverse function theorem is that \\(J_{\\mathbf{f}}(\\mathbf{a}) \\neq 0\\) (compare Theorem 9.36). If the implicit function theorem is stated in terms of the functions (59), the assumption made there on \\(A\\) amounts to\n\\[ \\frac{\\partial\\left(f_{1}, \\ldots, f_{n}\\right)}{\\partial\\left(x_{1}, \\ldots, x_{n}\\right)} \\neq 0 \\]\n","date":"2022-08-13T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/7-determinants/","section":"baby rudin","tags":null,"title":"7 DETERMINANTS"},{"categories":null,"contents":"9.10 Preliminaries In order to arrive at a definition of the derivative of a function whose domain is \\(R^{n}\\) (or an open subset of \\(R^{n}\\) ), let us take another look at the familiar case \\(n=1\\), and let us see how to interpret the derivative in that case in a way which will naturally extend to \\(n\u0026gt;1\\).\nIf \\(f\\) is a real function with domain \\((a, b) \\subset R^{1}\\) and if \\(x \\in(a, b)\\), then \\(f^{\\prime}(x)\\) is usually defined to be the real number \\[ \\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h} \\] provided, of course, that this limit exists. Thus \\[ f(x+h)-f(x)=f^{\\prime}(x) h+r(h) \\] where the “remainder” \\(r(h)\\) is small, in the sense that \\[ \\lim _{h \\rightarrow 0} \\frac{r(h)}{h}=0 \\]\nNote that (8) expresses the difference \\(f(x+h)-f(x)\\) as the sum of the linear function that takes \\(h\\) to \\(f^{\\prime}(x) h\\), plus a small remainder.\nWe can therefore regard the derivative of \\(f\\) at \\(x\\), not as a real number, but as the linear operator on \\(R^{1}\\) that takes \\(h\\) to \\(f^{\\prime}(x) h\\).\n[Observe that every real number \\(\\alpha\\) gives rise to a linear operator on \\(R^{1}\\); the operator in question is simply multiplication by \\(\\alpha\\). Conversely, every linear function that carries \\(R^{1}\\) to \\(R^{1}\\) is multiplication by some real number. It is this natural 1-1 correspondence between \\(R^{1}\\) and \\(L\\left(R^{1}\\right)\\) which motivates the preceding statements.]\nLet us next consider a function \\(\\mathrm{f}\\) that maps \\((a, b) \\subset R^{1}\\) into \\(R^{m}\\). In that case, \\(\\mathrm{f}^{\\prime}(x)\\) was defined to be that vector \\(\\mathrm{y} \\in R^{m}\\) (if there is one) for which\n\\[ \\lim _{h \\rightarrow 0}\\left\\{\\frac{\\mathbf{f}(x+h)-\\mathbf{f}(x)}{h}-\\mathbf{y}\\right\\}=\\mathbf{0} . \\]\nWe can again rewrite this in the form\n\\[ \\mathbf{f}(x+h)-\\mathbf{f}(x)=h \\mathbf{y}+\\mathbf{r}(h), \\]\nwhere \\(\\mathbf{r}(h) / h \\rightarrow \\mathbf{0}\\) as \\(h \\rightarrow 0\\). The main term on the right side of (11) is again a linear function of \\(h\\). Every \\(\\mathbf{y} \\in R^{m}\\) induces a linear transformation of \\(R^{1}\\) into \\(R^{m}\\), by associating to each \\(h \\in R^{1}\\) the vector \\(h y \\in R^{m}\\). This identification of \\(R^{m}\\) with \\(L\\left(R^{1}, R^{m}\\right)\\) allows us to regard \\(f^{\\prime}(x)\\) as a member of \\(L\\left(R^{1}, R^{m}\\right)\\).\nThus, if \\(\\mathrm{f}\\) is a differentiable mapping of \\((a, b) \\subset R^{1}\\) into \\(R^{m}\\), and if \\(x \\in(a, b)\\), then \\(\\mathbf{f}^{\\prime}(x)\\) is the linear transformation of \\(R^{1}\\) into \\(R^{m}\\) that satisfies\n\\[ \\lim _{h \\rightarrow 0} \\frac{\\mathbf{f}(x+h)-\\mathbf{f}(x)-\\mathbf{f}^{\\prime}(x) h}{h}=\\mathbf{0}, \\]\nor, equivalently,\n\\[ \\lim _{h \\rightarrow 0} \\frac{\\left|\\mathbf{f}(x+h)-\\mathbf{f}(x)-\\mathbf{f}^{\\prime}(x) h\\right|}{|h|}=0 . \\]\nWe are now ready for the case \\(n\u0026gt;1\\).\n9.11 Definition Suppose \\(E\\) is an open set in \\(R^{n}, f\\) maps \\(E\\) into \\(R^{m}\\), and \\(\\mathbf{x} \\in E\\). If there exists a linear transformation \\(A\\) of \\(R^{n}\\) into \\(R^{m}\\) such that \\[ \\lim _{\\mathbf{h} \\rightarrow 0} \\frac{|\\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})-A \\mathbf{h}|}{|\\mathbf{h}|}=0, \\] then we say that \\(\\mathbf{f}\\) is differentiable at \\(\\mathbf{x}\\), and we write \\[ \\mathbf{f}^{\\prime}(\\mathbf{x})=A . \\] If \\(\\mathbf{f}\\) is differentiable at every \\(\\mathbf{x} \\in E\\), we say that \\(\\mathrm{f}\\) is differentiable in \\(E\\).\nIt is of course understood in (14) that \\(\\mathbf{h} \\in R^{n}\\). If \\(|\\mathbf{h}|\\) is small enough, then \\(\\mathbf{x}+\\mathbf{h} \\in E\\), since \\(E\\) is open. Thus \\(\\mathbf{f}(\\mathbf{x}+\\mathbf{h})\\) is defined, \\(\\mathbf{f}(\\mathbf{x}+\\mathbf{h}) \\in R^{m}\\), and since \\(A \\in L\\left(R^{n}, R^{m}\\right), A \\mathbf{h} \\in R^{m}\\). Thus\n\\[ \\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})-A \\mathbf{h} \\in R^{m} . \\]\nThe norm in the numerator of (14) is that of \\(R^{m}\\). In the denominator we have the \\(R^{n}\\)-norm of \\(\\mathbf{h}\\).\nThere is an obvious uniqueness problem which has to be settled before we go any further.\n9.12 Theorem Suppose \\(E\\) and \\(\\mathbf{f}\\) are as in Definition 9.11, \\(\\mathbf{x} \\in E\\), and (14) holds with \\(A=A_{1}\\) and with \\(A=A_{2}\\). Then \\(A_{1}=A_{2}\\).\nProof If \\(B=A_{1}-A_{2}\\), the inequality\n\\[ |B \\mathbf{h}| \\leq\\left|\\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})-A_{1} \\mathbf{h}\\right|+\\left|\\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})-A_{2} \\mathbf{h}\\right| \\]\nshows that \\(|B \\mathbf{h}| /|\\mathbf{h}| \\rightarrow 0\\) as \\(\\mathbf{h} \\rightarrow \\mathbf{0}\\). For fixed \\(\\mathbf{h} \\neq \\mathbf{0}\\), it follows that\n\\[ \\frac{\\mid B(t \\mathbf{h})}{|t h|} \\rightarrow 0 \\quad \\text { as } \\quad t \\rightarrow 0 . \\]\nThe linearity of \\(B\\) shows that the left side of (16) is independent of \\(t\\). Thus \\(B \\mathbf{h}=0\\) for every \\(\\mathbf{h} \\in R^{n}\\). Hence \\(B=0\\).\n9.13 Remarks\nThe relation (14) can be rewritten in the form \\[ \\mathbf{f}(\\mathbf{x}+\\mathbf{h})-\\mathbf{f}(\\mathbf{x})=\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{h}+\\mathbf{r}(\\mathbf{h}) \\]\nwhere the remainder \\(\\mathbf{r}(\\mathbf{h})\\) satisfies\n\\[ \\lim _{\\mathbf{h} \\rightarrow \\mathbf{0}} \\frac{|\\mathbf{r}(\\mathbf{h})|}{|\\mathbf{h}|}=0 . \\]\nWe may interpret (17), as in Sec. 9.10, by saying that for fixed \\(\\mathbf{x}\\) and small \\(\\mathbf{h}\\), the left side of (17) is approximately equal to \\(\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{h}\\), that is, to the value of a linear transformation applied to \\(h\\).\nSuppose \\(f\\) and \\(E\\) are as in Definition 9.11, and \\(f\\) is differentiable in \\(E\\). For every \\(\\mathbf{x} \\in E, \\mathbf{f}^{\\prime}(\\mathbf{x})\\) is then a function, namely, a linear transformation of \\(R^{n}\\) into \\(R^{m}\\). But \\(\\mathrm{f}^{\\prime}\\) is also a function: \\(\\mathbf{f}^{\\prime}\\) maps \\(E\\) into \\(L\\left(R^{n}, R^{m}\\right)\\).\nA glance at (17) shows that \\(f\\) is continuous at any point at which \\(f\\) is differentiable.\nThe derivative defined by (14) or (17) is often called the differential of \\(f\\) at \\(\\mathbf{x}\\), or the total derivative of \\(f\\) at \\(\\mathbf{x}\\), to distinguish it from the partial derivatives that will occur later.\n9.14 Example We have defined derivatives of functions carrying \\(R^{n}\\) to \\(R^{m}\\) to be linear transformations of \\(R^{n}\\) into \\(R^{m}\\). What is the derivative of such a linear transformation? The answer is very simple.\nIf \\(A \\in L\\left(R^{n}, R^{m}\\right)\\) and if \\(\\mathbf{x} \\in R^{n}\\), then\n\\[ A^{\\prime}(\\mathbf{x})=A \\text {. } \\]\nNote that \\(\\mathbf{x}\\) appears on the left side of (19), but not on the right. Both sides of (19) are members of \\(L\\left(R^{n}, R^{m}\\right)\\), whereas \\(A \\mathbf{x} \\in R^{m}\\).\nThe proof of (19) is a triviality, since\n\\[ A(\\mathbf{x}+\\mathbf{h})-A \\mathbf{x}=A \\mathbf{h} \\]\nby the linearity of \\(A\\). With \\(\\mathbf{f}(\\mathbf{x})=A \\mathbf{x}\\), the numerator in (14) is thus 0 for every \\(\\mathbf{h} \\in R^{n} .\\) In (17), \\(\\mathbf{r}(\\mathbf{h})=\\mathbf{0}\\).\nWe now extend the chain rule (Theorem 5.5) to the present situation.\n9.15 Theorem Suppose \\(E\\) is an open set in \\(R^{n}, \\mathrm{f}\\) maps \\(E\\) into \\(R^{m}\\), f is differentiable at \\(\\mathbf{x}_{0} \\in E, g\\) maps an open set containing \\(\\mathbf{f}(E)\\) into \\(R^{k}\\), and \\(g\\) is differentiable at \\(\\mathbf{f}\\left(\\mathbf{x}_{0}\\right)\\). Then the mapping \\(\\mathbf{F}\\) of \\(E\\) into \\(R^{k}\\) defined by\n\\[ \\mathbf{F}(\\mathbf{x})=\\mathbf{g}(\\mathbf{f}(\\mathbf{x})) \\]\nis differentiable at \\(\\mathbf{x}_{0}\\), and\n\\[ \\mathbf{F}^{\\prime}\\left(\\mathbf{x}_{0}\\right)=\\mathbf{g}^{\\prime}\\left(\\mathbf{f}\\left(\\mathbf{x}_{0}\\right)\\right) \\mathbf{f}^{\\prime}\\left(\\mathbf{x}_{0}\\right) . \\]\nOn the right side of \\((21)\\), we have the product of two linear transformations, as defined in Sec. 9.6.\nProof Put \\(\\mathbf{y}_{0}=\\mathbf{f}\\left(\\mathbf{x}_{0}\\right), A=\\mathbf{f}^{\\prime}\\left(\\mathbf{x}_{0}\\right), B=\\mathbf{g}^{\\prime}\\left(\\mathbf{y}_{0}\\right)\\), and define\n\\[ \\begin{aligned} \u0026amp;\\mathbf{u}(\\mathbf{h})=\\mathbf{f}\\left(\\mathbf{x}_{0}+\\mathbf{h}\\right)-\\mathbf{f}\\left(\\mathbf{x}_{0}\\right)-A \\mathbf{h}, \\\\ \u0026amp;\\mathbf{v}(\\mathbf{k})=\\mathbf{g}\\left(\\mathbf{y}_{0}+\\mathbf{k}\\right)-\\mathbf{g}\\left(\\mathbf{y}_{0}\\right)-B \\mathbf{k}, \\end{aligned} \\]\nfor all \\(\\mathbf{h} \\in R^{n}\\) and \\(\\mathbf{k} \\in R^{m}\\) for which \\(\\mathbf{f}\\left(\\mathbf{x}_{0}+\\mathbf{h}\\right)\\) and \\(\\mathbf{g}\\left(\\mathbf{y}_{0}+\\mathbf{k}\\right)\\) are defined. Then\n\\[ |\\mathbf{u}(\\mathbf{h})|=\\varepsilon(\\mathbf{h})|\\mathbf{h}|, \\quad|\\mathbf{v}(\\mathbf{k})|=\\eta(\\mathbf{k})|\\mathbf{k}|, \\]\nwhere \\(\\varepsilon(\\mathbf{h}) \\rightarrow 0\\) as \\(\\mathbf{h} \\rightarrow \\mathbf{0}\\) and \\(\\eta(\\mathbf{k}) \\rightarrow 0\\) as \\(\\mathbf{k} \\rightarrow \\mathbf{0}\\).\nGiven h, put \\(\\mathbf{k}=\\mathbf{f}\\left(\\mathbf{x}_{0}+\\mathbf{h}\\right)-\\mathbf{f}\\left(\\mathbf{x}_{0}\\right)\\). Then\n\\[ |\\mathbf{k}|=|A \\mathbf{h}+\\mathbf{u}(\\mathbf{h})| \\leq[\\|A\\|+\\varepsilon(\\mathbf{h})]|\\mathbf{h}| \\text {, } \\]\nand\n\\[ \\begin{aligned} \\mathbf{F}\\left(\\mathbf{x}_{0}+\\mathbf{h}\\right)-\\mathbf{F}\\left(\\mathbf{x}_{0}\\right)-B A \\mathbf{h} \u0026amp;=\\mathbf{g}\\left(\\mathbf{y}_{0}+\\mathbf{k}\\right)-\\mathbf{g}\\left(\\mathbf{y}_{0}\\right)-B A \\mathbf{h} \\\\ \u0026amp;=B(\\mathbf{k}-A \\mathbf{h})+\\mathbf{v}(\\mathbf{k}) \\\\ \u0026amp;=B \\mathbf{u}(\\mathbf{h})+\\mathbf{v}(\\mathbf{k}) \\end{aligned} \\]\nHence (22) and (23) imply, for \\(h \\neq 0\\), that\n\\[ \\frac{\\left|\\mathbf{F}\\left(\\mathbf{x}_{0}+\\mathbf{h}\\right)-\\mathbf{F}\\left(\\mathbf{x}_{0}\\right)-B A \\mathbf{h}\\right|}{|\\mathbf{h}|} \\leq\\|B\\| \\varepsilon(\\mathbf{h})+[\\|A\\|+\\varepsilon(\\mathbf{h})] \\eta(\\mathbf{k}) . \\]\nLet \\(\\mathbf{h} \\rightarrow \\mathbf{0}\\). Then \\(\\varepsilon(\\mathbf{h}) \\rightarrow 0\\). Also, \\(\\mathbf{k} \\rightarrow \\mathbf{0}\\), by (23), so that \\(\\eta(\\mathbf{k}) \\rightarrow 0\\). It follows that \\(\\mathbf{F}^{\\prime}\\left(\\mathbf{x}_{0}\\right)=B A\\), which is what (21) asserts.\n9.16 Partial derivatives We again consider a function \\(f\\) that maps an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\). Let \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{m}\\right\\}\\) be the standard bases of \\(R^{n}\\) and \\(R^{m}\\). The components of \\(\\mathrm{f}\\) are the real functions \\(f_{1}, \\ldots, f_{m}\\) defined by\n\\[ \\mathbf{f}(\\mathbf{x})=\\sum_{i=1}^{m} f_{i}(\\mathbf{x}) \\mathbf{u}_{i} \\quad(\\mathbf{x} \\in E) \\text {, } \\]\nor, equivalently, by \\(f_{i}(\\mathbf{x})=\\mathbf{f}(\\mathbf{x}) \\cdot \\mathbf{u}_{i}, 1 \\leq i \\leq m\\).\nFor \\(\\mathrm{x} \\in E, 1 \\leq i \\leq m, 1 \\leq j \\leq n\\), we define\n\\[ \\left(D_{j} f_{i}\\right)(\\mathbf{x})=\\lim _{t \\rightarrow 0} \\frac{f_{i}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-f_{i}(\\mathbf{x})}{t}, \\]\nprovided the limit exists. Writing \\(f_{i}\\left(x_{1}, \\ldots, x_{n}\\right)\\) in place of \\(f_{l}(\\mathbf{x})\\), we see that \\(D_{j} f_{i}\\) is the derivative of \\(f_{i}\\) with respect to \\(x_{j}\\), keeping the other variables fixed. The notation\n\\[ \\frac{\\partial f_{i}}{\\partial x_{j}} \\]\nis therefore often used in place of \\(D_{j} f_{i}\\), and \\(D_{j} f_{i}\\) is called a partial derivative.\nIn many cases where the existence of a derivative is sufficient when dealing with functions of one variable, continuity or at least boundedness of the partial derivatives is needed for functions of several variables. For example, the functions \\(f\\) and \\(g\\) described in Exercise 7, Chap. 4, are not continuous, although their partial derivatives exist at every point of \\(R^{2}\\). Even for continuous functions. the existence of all partial derivatives does not imply differentiability in the sense of Definition 9.11; see Exercises 6 and 14, and Theorem 9.21.\nHowever, if \\(\\mathbf{f}\\) is known to be differentiable at a point \\(\\mathbf{x}\\), then its partial derivatives exist at \\(\\mathbf{x}\\), and they determine the linear transformation \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) completely:\n9.17 Theorem Suppose \\(\\mathrm{f}\\) maps an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\), and \\(\\mathrm{f}\\) is differentiable at a point \\(\\mathbf{x} \\in E\\). Then the partial derivatives \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) exist, and\n\\[ \\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j}=\\sum_{i=1}^{m}\\left(D_{j} f_{i}\\right)(\\mathbf{x}) \\mathbf{u}_{i} \\quad(1 \\leq j \\leq n) \\]\nHere, as in Sec. \\(9.16,\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{m}\\right\\}\\) are the standard bases of \\(R^{n}\\) and \\(R^{m}\\)\nProof Fix \\(j\\). Since \\(f\\) is differentiable at \\(\\mathbf{x}\\),\n\\[ \\mathbf{f}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-\\mathbf{f}(\\mathbf{x})=\\mathbf{f}^{\\prime}(\\mathbf{x})\\left(t \\mathrm{e}_{j}\\right)+\\mathbf{r}\\left(t \\mathrm{e}_{j}\\right) \\]\nwhere \\(\\left|\\mathbf{r}\\left(t \\mathbf{e}_{j}\\right)\\right| / t \\rightarrow 0\\) as \\(t \\rightarrow 0\\). The linearity of \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) shows therefore that\n\\[ \\lim _{t \\rightarrow 0} \\frac{\\mathbf{f}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-\\mathbf{f}(\\mathbf{x})}{t}=\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j} \\]\nIf we now represent \\(f\\) in terms of its components, as in (24), then (28) becomes\n\\[ \\lim _{t \\rightarrow 0} \\sum_{i=1}^{m} \\frac{f_{i}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-f_{i}(\\mathbf{x})}{t} \\mathbf{u}_{i}=\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j} \\]\nIt follows that each quotient in this sum has a limit, as \\(t \\rightarrow 0\\) (see Theorem 4.10), so that each \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) exists, and then (27) follows from (29).\nHere are some consequences of Theorem 9.17:\nLet \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\) be the matrix that represents \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) with respect to our standard bases, as in Sec. 9.9.\nThen \\(\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j}\\) is the \\(j\\) th column vector of \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\), and (27) shows therefore that the number \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) occupies the spot in the \\(i\\) th row and \\(j\\) th column of \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\). Thus\n\\[ \\left[f^{\\prime}(\\mathbf{x})\\right]=\\left[\\begin{array}{lll} \\left(D_{1} f_{1}\\right)(\\mathbf{x}) \u0026amp; \\cdots \u0026amp; \\left(D_{n} f_{1}\\right)(\\mathbf{x}) \\\\ \\cdots \\cdots \\cdots \\cdots \u0026amp; \\cdots \u0026amp; \\cdots \\\\ \\left(D_{1} f_{m}\\right)(\\mathbf{x}) \u0026amp; \\cdots \u0026amp; \\left(D_{n} f_{m}\\right)(\\mathbf{x}) \\end{array}\\right] \\]\nIf \\(\\mathbf{h}=\\Sigma h_{j} \\mathbf{e}_{j}\\) is any vector in \\(R^{n}\\), then (27) implies that\n\\[ \\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{h}=\\sum_{i=1}^{m}\\left\\{\\sum_{j=1}^{n}\\left(D_{j} f_{i}\\right)(\\mathbf{x}) h_{j}\\right\\} \\mathbf{u}_{i} . \\]\n9.17 Theorem Suppose \\(\\mathrm{f}\\) maps an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\), and \\(\\mathrm{f}\\) is differentiable at a point \\(\\mathbf{x} \\in E\\). Then the partial derivatives \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) exist, and\n\\[ \\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j}=\\sum_{i=1}^{m}\\left(D_{j} f_{i}\\right)(\\mathbf{x}) \\mathbf{u}_{i} \\quad(1 \\leq j \\leq n) \\]\nHere, as in Sec. \\(9.16,\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{m}\\right\\}\\) are the standard bases of \\(R^{n}\\) and \\(R^{m}\\)\nProof Fix \\(j\\). Since \\(f\\) is differentiable at \\(\\mathbf{x}\\),\n\\[ \\mathbf{f}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-\\mathbf{f}(\\mathbf{x})=\\mathbf{f}^{\\prime}(\\mathbf{x})\\left(t \\mathrm{e}_{j}\\right)+\\mathbf{r}\\left(t \\mathrm{e}_{j}\\right) \\]\nwhere \\(\\left|\\mathbf{r}\\left(t \\mathbf{e}_{j}\\right)\\right| / t \\rightarrow 0\\) as \\(t \\rightarrow 0\\). The linearity of \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) shows therefore that\n\\[ \\lim _{t \\rightarrow 0} \\frac{\\mathbf{f}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-\\mathbf{f}(\\mathbf{x})}{t}=\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j} \\]\nIf we now represent \\(f\\) in terms of its components, as in (24), then (28) becomes\n\\[ \\lim _{t \\rightarrow 0} \\sum_{i=1}^{m} \\frac{f_{i}\\left(\\mathbf{x}+t \\mathbf{e}_{j}\\right)-f_{i}(\\mathbf{x})}{t} \\mathbf{u}_{i}=\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j} \\]\nIt follows that each quotient in this sum has a limit, as \\(t \\rightarrow 0\\) (see Theorem 4.10), so that each \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) exists, and then (27) follows from (29).\nHere are some consequences of Theorem 9.17:\nLet \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\) be the matrix that represents \\(\\mathbf{f}^{\\prime}(\\mathbf{x})\\) with respect to our standard bases, as in Sec. 9.9.\nThen \\(\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j}\\) is the \\(j\\) th column vector of \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\), and (27) shows therefore that the number \\(\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\) occupies the spot in the \\(i\\) th row and \\(j\\) th column of \\(\\left[\\mathbf{f}^{\\prime}(\\mathbf{x})\\right]\\). Thus\n\\[ \\left[f^{\\prime}(\\mathbf{x})\\right]=\\left[\\begin{array}{lll} \\left(D_{1} f_{1}\\right)(\\mathbf{x}) \u0026amp; \\cdots \u0026amp; \\left(D_{n} f_{1}\\right)(\\mathbf{x}) \\\\ \\cdots \\cdots \\cdots \\cdots \u0026amp; \\cdots \u0026amp; \\cdots \\\\ \\left(D_{1} f_{m}\\right)(\\mathbf{x}) \u0026amp; \\cdots \u0026amp; \\left(D_{n} f_{m}\\right)(\\mathbf{x}) \\end{array}\\right] \\]\nIf \\(\\mathbf{h}=\\Sigma h_{j} \\mathbf{e}_{j}\\) is any vector in \\(R^{n}\\), then (27) implies that\n\\[ \\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{h}=\\sum_{i=1}^{m}\\left\\{\\sum_{j=1}^{n}\\left(D_{j} f_{i}\\right)(\\mathbf{x}) h_{j}\\right\\} \\mathbf{u}_{i} . \\]\n9.18 Example Let \\(\\gamma\\) be a differentiable mapping of the segment \\((a, b) \\subset R^{1}\\) into an open set \\(E \\subset R^{n}\\), in other words, \\(\\gamma\\) is a differentiable curve in \\(E\\). Let \\(f\\) be a real-valued differentiable function with domain \\(E\\). Thus \\(f\\) is a differentiable mapping of \\(E\\) into \\(R^{1}\\). Define\n\\[ g(t)=f(\\gamma(t)) \\quad(a\u0026lt;t\u0026lt;b) . \\]\nThe chain rule asserts then that\n\\[ g^{\\prime}(t)=f^{\\prime}(\\gamma(t)) \\gamma^{\\prime}(t) \\quad(a\u0026lt;t\u0026lt;b) . \\]\nSince \\(\\gamma^{\\prime}(t) \\in L\\left(R^{1}, R^{n}\\right)\\) and \\(f^{\\prime}(\\gamma(t)) \\in L\\left(R^{n}, R^{1}\\right)\\), (32) defines \\(g^{\\prime}(t)\\) as a linear operator on \\(R^{1}\\). This agrees with the fact that \\(g\\) maps \\((a, b)\\) into \\(R^{1}\\). However, \\(g^{\\prime}(t)\\) can also be regarded as a real number. (This was discussed in Sec. 9.10.) This number can be computed in terms of the partial derivatives of \\(f\\) and the derivatives of the components of \\(\\gamma\\), as we shall now see.\nWith respect to the standard basis \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) of \\(R^{n},\\left[\\gamma^{\\prime}(t)\\right]\\) is the \\(n\\) by 1 matrix (a “column matrix”) which has \\(\\gamma_{i}^{\\prime}(t)\\) in the ith row, where \\(\\gamma_{1}, \\ldots, \\gamma_{n}\\) are the components of \\(\\gamma\\). For every \\(\\mathbf{x} \\in E,\\left[f^{\\prime}(\\mathbf{x})\\right]\\) is the 1 by \\(n\\) matrix (a “row matrix”) which has \\(\\left(D_{j} f\\right)(\\mathbf{x})\\) in the \\(j\\) th column. Hence \\(\\left[g^{\\prime}(t)\\right]\\) is the 1 by 1 matrix whose only entry is the real number\n\\[ g^{\\prime}(t)=\\sum_{i=1}^{n}\\left(D_{i} f\\right)(\\gamma(t)) \\gamma_{i}^{\\prime}(t) \\]\nThis is a frequently encountered special case of the chain rule. It can be rephrased in the following manner.\nAssociate with each \\(\\mathbf{x} \\in E\\) a vector, the so-called “grarient” of \\(f\\) at \\(\\mathbf{x}\\), defined by\n\\[ (\\nabla f)(\\mathbf{x})=\\sum_{i=1}^{n}\\left(D_{i} f\\right)(\\mathbf{x}) \\mathrm{e}_{i} \\]\nSince\n\\[ \\gamma^{\\prime}(t)=\\sum_{i=1}^{n} \\gamma_{i}^{\\prime}(t) \\mathbf{e}_{i}, \\]\ncan be written in the form \\[ g^{\\prime}(t)=(\\nabla f)(\\gamma(t)) \\cdot \\gamma^{\\prime}(t), \\]\nthe scalar product of the vectors \\((\\nabla f)(\\gamma(t))\\) and \\(\\gamma^{\\prime}(t)\\).\nLet us now fix an \\(\\mathbf{x} \\in E\\), let \\(\\mathbf{u} \\in R^{n}\\) be a unit vector (that is, \\(|\\mathbf{u}|=1\\) ), and specialize \\(\\gamma\\) so that\n\\[ \\gamma(t)=\\mathbf{x}+t \\mathbf{u} \\quad(-\\infty\u0026lt;t\u0026lt;\\infty) . \\]\nThen \\(\\gamma^{\\prime}(t)=\\mathbf{u}\\) for every \\(t\\). Hence (36) shows that\n\\[ g^{\\prime}(0)=(\\nabla f)(\\mathbf{x}) \\cdot \\mathbf{u} . \\]\nOn the other hand, (37) shows that\n\\[ g(t)-g(0)=f(\\mathbf{x}+t \\mathbf{u})-f(\\mathbf{x}) . \\]\nHence (38) gives\n\\[ \\lim _{t \\rightarrow 0} \\frac{f(\\mathbf{x}+t \\mathbf{u})-f(\\mathbf{x})}{t}=(\\nabla f)(\\mathbf{x}) \\cdot \\mathbf{u} \\]\nThe limit in (39) is usually called the directional derivative of \\(f\\) at \\(\\mathbf{x}\\), in the direction of the unit vector \\(\\mathbf{u}\\), and may be denoted by \\(\\left(D_{\\mathbf{n}} f\\right)(\\mathbf{x})\\).\nIf \\(f\\) and \\(\\mathbf{x}\\) are fixed, but \\(\\mathbf{u}\\) varies, then (39) shows that \\(\\left(D_{\\mathbf{u}} f\\right)(\\mathbf{x})\\) attains its maximum when \\(\\mathbf{u}\\) is a positive scalar multiple of \\((\\nabla f)(\\mathbf{x})\\). [The case \\((\\nabla f)(\\mathbf{x})=\\mathbf{0}\\) should be excluded here.]\nIf \\(\\mathbf{u}=\\Sigma u_{i} \\mathbf{e}_{i}\\), then (39) shows that \\(\\left(D_{\\mathbf{n}} f\\right)(\\mathbf{x})\\) can be expressed in terms of the partial derivatives of \\(f\\) at \\(\\mathbf{x}\\) by the formula\n\\[ \\left(D_{\\mathrm{u}} f\\right)(\\mathbf{x})=\\sum_{i=1}^{n}\\left(D_{i} f\\right)(\\mathbf{x}) u_{i} \\]\nSome of these ideas will play a role in the following theorem.\n9.19 Theorem Suppose f maps a convex open set \\(E \\subset R^{n}\\) into \\(R^{m}\\), f is differentiable in \\(E\\), and there is a real number \\(M\\) such that\n\\[ \\left\\|\\mathbf{f}^{\\prime}(\\mathbf{x})\\right\\| \\leq M \\]\nfor every \\(\\mathrm{x} \\in E\\). Then\n\\[ |\\mathbf{f}(\\mathbf{b})-\\mathrm{f}(\\mathbf{a})| \\leq M|\\mathbf{b}-\\mathbf{a}| \\]\nfor all \\(\\mathbf{a} \\in E, \\mathbf{b} \\in E\\).\nProof Fix \\(\\mathbf{a} \\in E, \\mathbf{b} \\in E\\). Define\n\\[ \\gamma(t)=(1-t) \\mathbf{a}+t \\mathbf{b} \\]\nfor all \\(t \\in R^{1}\\) such that \\(\\gamma(t) \\in E\\). Since \\(E\\) is convex, \\(\\gamma(t) \\in E\\) if \\(0 \\leq t \\leq 1\\). Put\n\\[ \\mathbf{g}(t)=\\mathbf{f}(\\gamma(t)) \\]\nThen\n\\[ \\mathbf{g}^{\\prime}(t)=\\mathbf{f}^{\\prime}(\\gamma(t)) \\gamma^{\\prime}(t)=\\mathbf{f}^{\\prime}(\\gamma(t))(\\mathbf{b}-\\mathbf{a}) \\]\nso that\n\\[ \\left|\\mathbf{g}^{\\prime}(t)\\right| \\leq\\left\\|\\mathbf{f}^{\\prime}(\\gamma(t))\\right\\||\\mathbf{b}-\\mathbf{a}| \\leq M|\\mathbf{b}-\\mathbf{a}| \\]\nfor all \\(t \\in[0,1]\\). By Theorem \\(5.19\\),\n\\[ |\\mathbf{g}(1)-\\mathbf{g}(0)| \\leq M|\\mathbf{b}-\\mathbf{a}| \\text {. } \\]\nBut \\(\\mathbf{g}(0)=\\mathbf{f}(\\mathbf{a})\\) and \\(\\mathbf{g}(1)=\\mathbf{f}(\\mathbf{b})\\). This completes the proof.\nCorollary If, in addition, \\(\\mathbf{f}^{\\prime}(\\mathbf{x})=\\mathbf{0}\\) for all \\(\\mathbf{x} \\in E\\), then \\(\\mathbf{f}\\) is constant.\nProof To prove this, note that the hypotheses of the theorem hold now with \\(M=0\\).\n9.20 Definition A differentiable mapping \\(f\\) of an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\) is said to be continuously differentiable in \\(E\\) if \\(\\mathrm{f}^{\\prime}\\) is a continuous mapping of \\(E\\) into \\(L\\left(R^{n}, R^{m}\\right)\\).\nMore explicitly, it is required that to every \\(\\mathbf{x} \\in E\\) and to every \\(\\varepsilon\u0026gt;0\\) corresponds a \\(\\delta\u0026gt;0\\) such that\n\\[ \\left\\|\\mathbf{f}^{\\prime}(\\mathbf{y})-\\mathbf{f}^{\\prime}(\\mathbf{x})\\right\\|\u0026lt;\\varepsilon \\]\nif \\(\\mathbf{y} \\in E\\) and \\(|\\mathbf{x}-\\mathbf{y}|\u0026lt;\\delta\\).\nIf this is so, we also say that \\(f\\) is a \\(\\mathscr{C}^{\\prime}\\)-mapping, or that \\(f \\in \\mathscr{C}^{\\prime}(E)\\).\n9.21 Theorem Suppose \\(\\mathrm{f}\\) maps an open set \\(E \\subset R^{n}\\) into \\(R^{m}\\). Then \\(\\mathrm{f} \\in \\mathscr{C}^{\\prime}(E)\\) if and only if the partial derivatives \\(D_{j} f_{i}\\) exist and are continuous on \\(E\\) for \\(1 \\leq i \\leq m\\), \\(1 \\leq j \\leq n\\).\nProof Assume first that \\(f \\in \\mathscr{C}^{\\prime}(E)\\). By (27),\n\\[ \\left(D_{j} f_{i}\\right)(\\mathbf{x})=\\left(\\mathbf{f}^{\\prime}(\\mathbf{x}) \\mathbf{e}_{j}\\right) \\cdot \\mathbf{u}_{i} \\]\nfor all \\(i, j\\), and for all \\(\\mathbf{x} \\in E\\). Hence\n\\[ \\left(D_{j} f_{i}\\right)(\\mathbf{y})-\\left(D_{j} f_{i}\\right)(\\mathbf{x})=\\left\\{\\left[\\mathbf{f}^{\\prime}(\\mathbf{y})-\\mathbf{f}^{\\prime}(\\mathbf{x})\\right] \\mathbf{e}_{j}\\right\\} \\cdot \\mathbf{u}_{i} \\]\nand since \\(\\left|\\mathbf{u}_{i}\\right|=\\left|\\mathbf{e}_{j}\\right|=1\\), it follows that\n\\[ \\begin{aligned} \\left|\\left(D_{j} f_{i}\\right)(\\mathbf{y})-\\left(D_{j} f_{i}\\right)(\\mathbf{x})\\right| \u0026amp; \\leq\\left|\\left[\\mathbf{f}^{\\prime}(\\mathbf{y})-\\mathbf{f}^{\\prime}(\\mathbf{x})\\right] \\mathbf{e}_{j}\\right| \\\\ \u0026amp; \\leq\\left\\|\\mathbf{f}^{\\prime}(\\mathbf{y})-\\mathbf{f}^{\\prime}(\\mathbf{x})\\right\\| \\end{aligned} \\]\nHence \\(D_{j} f_{i}\\) is continuous.\nFor the converse, it suffices to consider the case \\(m=1\\). (Why?) Fix \\(\\mathrm{x} \\in E\\) and \\(\\varepsilon\u0026gt;0\\). Since \\(E\\) is open, there is an open ball \\(S \\subset E\\), with center at \\(\\mathbf{x}\\) and radius \\(r\\), and the continuity of the functions \\(D_{j} f\\) shows that \\(r\\) can be chosen so that\n\\[ \\left|\\left(D_{j} f\\right)(\\mathbf{y})-\\left(D_{j} f\\right)(\\mathbf{x})\\right|\u0026lt;\\frac{\\varepsilon}{n} \\quad(\\mathbf{y} \\in S, 1 \\leq j \\leq n) . \\]\nSuppose \\(\\mathbf{h}=\\Sigma h_{j} \\mathbf{e}_{j},|\\mathbf{h}|\u0026lt;r\\), put \\(\\mathbf{v}_{0}=\\mathbf{0}\\), and \\(\\mathbf{v}_{k}=h_{1} \\mathbf{e}_{1}+\\cdots+h_{k} \\mathbf{e}_{k}\\), for \\(1 \\leq k \\leq n\\). Then\n\\[ f(\\mathbf{x}+\\mathbf{h})-f(\\mathbf{x})=\\sum_{j=1}^{n}\\left[f\\left(\\mathbf{x}+\\mathbf{v}_{j}\\right)-f\\left(\\mathbf{x}+\\mathbf{v}_{j-1}\\right)\\right] . \\]\nSince \\(\\left|\\mathbf{v}_{k}\\right|\u0026lt;r\\) for \\(1 \\leq k \\leq n\\) and since \\(S\\) is convex, the segments with end points \\(\\mathbf{x}+\\mathbf{v}_{j-1}\\) and \\(\\mathbf{x}+\\mathbf{v}_{j}\\) lie in \\(S\\). Since \\(\\mathbf{v}_{j}=\\mathbf{v}_{j-1}+h_{j} \\mathbf{e}_{j}\\), the mean value theorem (5.10) shows that the \\(j\\) th summand in (42) is equal to\n\\[ h_{j}\\left(D_{j} f\\right)\\left(\\mathbf{x}+\\mathbf{v}_{j-1}+\\theta_{j} h_{j} \\mathbf{e}_{j}\\right) \\]\nfor some \\(\\theta_{j} \\in(0,1)\\), and this differs from \\(h_{j}\\left(D_{j} f\\right)(\\mathbf{x})\\) by less than \\(\\left|h_{j}\\right| \\varepsilon / n\\), using (41). By (42), it follows that\n\\[ \\left|f(\\mathbf{x}+\\mathbf{h})-f(\\mathbf{x})-\\sum_{j=1}^{n} h_{j}\\left(D_{j} f\\right)(\\mathbf{x})\\right| \\leq \\frac{1}{n} \\sum_{j=1}^{n}\\left|h_{j}\\right| \\varepsilon \\leq|\\mathbf{h}| \\varepsilon \\]\nfor all \\(\\mathbf{h}\\) such that \\(|\\mathbf{h}|\u0026lt;r\\).\nThis says that \\(f\\) is differentiable at \\(\\mathbf{x}\\) and that \\(f^{\\prime}(\\mathbf{x})\\) is the linear function which assigns the number \\(\\Sigma h_{j}\\left(D_{j} f\\right)(\\mathbf{x})\\) to the vector \\(\\mathbf{h}=\\Sigma h_{j} \\mathbf{e}_{j}\\). The matrix \\(\\left[f^{\\prime}(\\mathbf{x})\\right]\\) consists of the row \\(\\left(D_{1} f\\right)(\\mathbf{x}), \\ldots,\\left(D_{n} f\\right)(\\mathbf{x})\\); and since \\(D_{1} f, \\ldots, D_{n} f\\) are continuous functions on \\(E\\), the concluding remarks of Sec. \\(9.9\\) show that \\(f \\in \\mathscr{C}^{\\prime}(E)\\).\n","date":"2022-08-12T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/2-differentiation/","section":"baby rudin","tags":null,"title":"2 DIFFERENTIATION"},{"categories":null,"contents":" youtube bilibli note part1 pdf part1 xopp part2 pdf prat2 xopp We now state the existence theorem which is the core of this chapter.\n1.19 Theorem There exists an ordered field \\(R\\) which has the least-upper-bound property. Moreover, \\(R\\) contains \\(Q\\) as a subfield.\nThe second statement means that \\(Q \\subset R\\) and that the operations of addition and multiplication in \\(R\\), when applied to members of \\(Q\\), coincide with the usual operations on rational numbers; also, the positive rational numbers are positive elements of \\(R\\). The members of \\(R\\) are called real numbers. The proof of Theorem \\(1.19\\) is rather long and a bit tedious and is therefore presented in an Appendix to Chap. 1. The proof actually constructs \\(R\\) from \\(Q\\).\nThe next theorem could be extracted from this construction with very little extra effort. However, we prefer to derive it from Theorem \\(1.19\\) since this provides a good illustration of what one can do with the least-upper-bound property.\n\\(1.20\\) Theorem\nIf \\(x \\in R, y \\in R\\), and \\(x\u0026gt;0\\), then there is a positive integer \\(n\\) such that \\(n x\u0026gt;y\\)\nIf \\(x \\in R, y \\in R\\), and \\(x\u0026lt;y\\), then there exists \\(a p \\in Q\\) such that \\(x\u0026lt;p\u0026lt;y\\).\nPart \\((a)\\) is usually referred to as the archimedean property of \\(R\\). Part (b) may be stated by saying that \\(Q\\) is dense in \\(R\\) : Between any two real numbers there is a rational one.\nProof\nLet \\(A\\) be the set of all \\(n x\\), where \\(n\\) runs through the positive integers. If \\((a)\\) were false, then \\(y\\) would be an upper bound of \\(A\\). But then \\(A\\) has a least upper bound in \\(R\\). Put \\(\\alpha=\\sup A\\). Since \\(x\u0026gt;0, \\alpha-x\u0026lt;\\alpha\\), and \\(\\alpha-x\\) is not an upper bound of \\(A\\). Hence \\(\\alpha-x\u0026lt;m x\\) for some positive integer \\(m\\). But then \\(\\alpha\u0026lt;(m+1) x \\in A\\), which is impossible, since \\(\\alpha\\) is an upper bound of \\(A\\).\nSince \\(x\u0026lt;y\\), we have \\(y-x\u0026gt;0\\), and (a) furnishes a positive integer \\(n\\) such that\n\\[ n(y-x)\u0026gt;1 \\text {. } \\]\nApply (a) again, to obtain positive integers \\(m_{1}\\) and \\(m_{2}\\) such that \\(m_{1}\u0026gt;n x\\), \\(m_{2}\u0026gt;-n x\\). Then\n\\[ -m_{2}\u0026lt;n x\u0026lt;m_{1} \\text {. } \\]\nHence there is an integer \\(m\\) (with \\(-m_{2} \\leq m \\leq m_{1}\\) ) such that\n\\[ m-1 \\leq n x\u0026lt;m . \\]\nIf we combine these inequalities, we obtain\n\\[ n x\u0026lt;m \\leq 1+n x\u0026lt;n y . \\]\nSince \\(n\u0026gt;0\\), it follows that\n\\[ x\u0026lt;\\frac{m}{n}\u0026lt;y . \\]\nThis proves \\((b)\\), with \\(p=m / n\\). We shall now prove the existence of \\(n\\)th roots of positive reals. This proof will show how the difficulty pointed out in the Introduction (irrationality of \\(\\sqrt{2}\\) ) can be handled in \\(R\\).\n\\(1.21\\) Theorem For every real \\(x\u0026gt;0\\) and every integer \\(n\u0026gt;0\\) there is one and only one positive real y such that \\(y^{n}=x\\).\nThis number \\(y\\) is written \\(\\sqrt[n]{x}\\) or \\(x^{1 / n}\\).\nProof That there is at most one such \\(y\\) is clear, since \\(0\u0026lt;y_{1}\u0026lt;y_{2}\\) implies \\(y_{1}^{n}\u0026lt;y_{2}^{n}\\).\nLet \\(E\\) be the set consisting of all positive real numbers \\(t\\) such that \\(t^{n}\u0026lt;x\\).\nIf \\(t=x /(1+x)\\) then \\(0 \\leq t\u0026lt;1\\). Hence \\(t^{n} \\leq t\u0026lt;x\\). Thus \\(t \\in E\\), and \\(E\\) is not empty.\nIf \\(t\u0026gt;1+x\\) then \\(t^{n} \\geq t\u0026gt;x\\), so that \\(t \\notin E\\). Thus \\(1+x\\) is an upper bound of \\(E\\).\nHence Theorem \\(1.19\\) implies the existence of\n\\[ y=\\sup E \\text {. } \\]\nTo prove that \\(y^{n}=x\\) we will show that each of the inequalities \\(y^{n}\u0026lt;x\\) and \\(y^{n}\u0026gt;x\\) leads to a contradiction.\nThe identity \\(b^{n}-a^{n}=(b-a)\\left(b^{n-1}+b^{n-2} a+\\cdots+a^{n-1}\\right)\\) yields the inequality\n\\[ b^{n}-a^{n}\u0026lt;(b-a) n b^{n-1} \\]\nwhen \\(0\u0026lt;a\u0026lt;b\\).\nAssume \\(y^{n}\u0026lt;x\\). Choose \\(h\\) so that \\(0\u0026lt;h\u0026lt;1\\) and\n\\[ h\u0026lt;\\frac{x-y^{n}}{n(y+1)^{n-1}} . \\]\nPut \\(a=y, b=y+h\\). Then\n\\[ (y+h)^{n}-y^{n}\u0026lt;h n(y+h)^{n-1}\u0026lt;h n(y+1)^{n-1}\u0026lt;x-y^{n} \\text {. } \\]\nThus \\((y+h)^{n}\u0026lt;x\\), and \\(y+h \\in E\\). Since \\(y+h\u0026gt;y\\), this contradicts the fact that \\(y\\) is an upper bound of \\(E\\).\nAssume \\(y^{n}\u0026gt;x\\). Put\n\\[ k=\\frac{y^{n}-x}{n y^{n-1}} . \\]\nThen \\(0\u0026lt;k\u0026lt;y\\). If \\(t \\geq y-k\\), we conclude that\n\\[ y^{n}-t^{n} \\leq y^{n}-(y-k)^{n}\u0026lt;k n y^{n-1}=y^{n}-x . \\]\nThus \\(t^{n}\u0026gt;x\\), and \\(t \\notin E\\). It follows that \\(y-k\\) is an upper bound of \\(E\\). But \\(y-k\u0026lt;y\\), which contradicts the fact that \\(y\\) is the least upper bound of \\(E\\).\nHence \\(y^{n}=x\\), and the proof is complete.\nCorollary If \\(a\\) and \\(b\\) are positive real numbers and \\(n\\) is a positive integer, then\n\\[ (a b)^{1 / n}=a^{1 / n} b^{1 / n} . \\]\nProof Put \\(\\alpha=a^{1 / n}, \\beta=b^{1 / n}\\). Then\n\\[ a b=\\alpha^{n} \\beta^{n}=(\\alpha \\beta)^{n}, \\]\nsince multiplication is commutative. [Axiom (M2) in Definition 1.12.] The uniqueness assertion of Theorem \\(1.21\\) shows therefore that\n\\[ (a b)^{1 / n}=\\alpha \\beta=a^{1 / n} b^{1 / n} . \\]\n1.22 Decimals We conclude this section by pointing out the relation between real numbers and decimals.\nLet \\(x\u0026gt;0\\) be real. Let \\(n_{0}\\) be the largest integer such that \\(n_{0} \\leq x\\). (Note that the existence of \\(n_{0}\\) depends on the archimedean property of \\(R\\).) Having chosen \\(n_{0}, n_{1}, \\ldots, n_{k-1}\\), let \\(n_{k}\\) be the largest integer such that\n\\[ n_{0}+\\frac{n_{1}}{10}+\\cdots+\\frac{n_{k}}{10^{k}} \\leq x . \\]\nLet \\(E\\) be the set of these numbers\n\\[ n_{0}+\\frac{n_{1}}{10}+\\cdots+\\frac{n_{k}}{10^{k}} \\quad(k=0,1,2, \\ldots) . \\]\nThen \\(x=\\sup E\\). The decimal expansion of \\(x\\) is\n\\[ n_{0} \\cdot n_{1} n_{2} n_{3} \\cdots \\text {. } \\]\nConversely, for any infinite decimal (6) the set \\(E\\) of numbers (5) is bounded above, and (6) is the decimal expansion of \\(\\sup E\\).\nSince we shall never use decimals, we do not enter into a detailed discussion.\n","date":"2022-08-12T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/3-the-real-field/","section":"baby rudin","tags":null,"title":"3 THE REAL FIELD"},{"categories":null,"contents":"We begin this chapter with a discussion of sets of vectors in euclidean \\(n\\)-space \\(R^{n}\\). The algebraic facts presented here extend without change to finite-dimensional vector spaces over any field of scalars. However, for our purposes it is quite sufficient to stay within the familiar framework provided by the euclidean spaces.\n9.1 Definitions\nA nonempty set \\(X \\subset R^{n}\\) is a vector space if \\(\\mathbf{x}+\\mathbf{y} \\in X\\) and \\(c \\mathbf{x} \\in X\\) for all \\(\\mathbf{x} \\in X, \\mathbf{y} \\in X\\), and for all scalars \\(c\\). If \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{k} \\in R^{n}\\) and \\(c_{1}, \\ldots, c_{k}\\) are scalars, the vector \\[ c_{1} \\mathbf{x}_{1}+\\cdots+c_{k} \\mathbf{x}_{k} \\]\nis called a linear combination of \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{k}\\). If \\(S \\subset R^{n}\\) and if \\(E\\) is the set of all linear combinations of elements of \\(S\\), we say that \\(S\\) spans \\(E\\), or that \\(E\\) is the span of \\(S\\).\nObserve that every span is a vector space.\nA set consisting of vectors \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{k}\\) (we shall use the notation \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{k}\\right\\}\\) for such a set) is said to be independent if the relation \\(c_{1} \\mathbf{x}_{1}+\\cdots+c_{k} \\mathbf{x}_{k}=\\mathbf{0}\\) implies that \\(c_{1}=\\cdots=c_{k}=0\\). Otherwise \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{k}\\right\\}\\) is said to be dependent. Observe that no independent set contains the null vector. If a vector space \\(X\\) contains an independent set of \\(r\\) vectors but contains no independent set of \\(r+1\\) vectors, we say that \\(X\\) has dimension \\(r\\), and write: \\(\\operatorname{dim} X=r\\). The set consisting of \\(\\mathbf{0}\\) alone is a vector space; its dimension is 0 . (e) An independent subset of a vector space \\(X\\) which spans \\(X\\) is called a basis of \\(X\\).\nObserve that if \\(B=\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{r}\\right\\}\\) is a basis of \\(X\\), then every \\(\\mathbf{x} \\in X\\) has a unique representation of the form \\(\\mathbf{x}=\\Sigma c_{j} \\mathbf{x}_{j}\\). Such a representation exists since \\(B\\) spans \\(X\\), and it is unique since \\(B\\) is independent. The numbers \\(c_{1}, \\ldots, c_{r}\\) are called the coordinates of \\(\\mathbf{x}\\) with respect to the basis \\(B\\).\nThe most familiar example of a basis is the set \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\), where \\(\\mathbf{e}_{j}\\) is the vector in \\(R^{n}\\) whose \\(j\\) th coordinate is 1 and whose other coordinates are all 0. If \\(\\mathbf{x} \\in R^{n}, \\mathbf{x}=\\left(x_{1}, \\ldots, x_{n}\\right)\\), then \\(\\mathbf{x}=\\Sigma x_{j} \\mathbf{e}_{j}\\). We shall call \\[ \\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\} \\] the standard basis of \\(R^{n}\\).\n9.2 Theorem Let \\(r\\) be a positive integer. If a vector space \\(X\\) is spanned by a set of \\(r\\) vectors, then \\(\\operatorname{dim} X \\leq r\\).\nProof If this is false, there is a vector space \\(X\\) which contains an independent set \\(Q=\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r+1}\\right\\}\\) and which is spanned by a set \\(S_{0}\\) consisting of \\(r\\) vectors.\nSuppose \\(0 \\leq i\u0026lt;r\\), and suppose a set \\(S_{i}\\) has been constructed which spans \\(X\\) and which consists of all \\(\\mathbf{y}_{j}\\) with \\(1 \\leq j \\leq i\\) plus a certain collection of \\(r-i\\) members of \\(S_{0}\\), say \\(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{r-i}\\). (In other words, \\(S_{i}\\) is obtained from \\(S_{0}\\) by replacing \\(i\\) of its elements by members of \\(Q\\), without altering the span.) Since \\(S_{i}\\) spans \\(X, \\mathbf{y}_{i+1}\\) is in the span of \\(S_{i}\\); hence there are scalars \\(a_{1}, \\ldots, a_{i+1}, b_{1}, \\ldots, b_{r-i}\\), with \\(a_{i+1}=1\\), such that\n\\[ \\sum_{j=1}^{i+1} a_{j} \\mathbf{y}_{j}+\\sum_{k=1}^{r-i} b_{k} \\mathbf{x}_{k}=\\mathbf{0} . \\]\nIf all \\(b_{k}\\) ’s were 0 , the independence of \\(Q\\) would force all \\(a_{j}\\) ’s to be 0 , a contradiction. It follows that some \\(\\mathbf{x}_{k} \\in S_{i}\\) is a linear combination of the other members of \\(T_{i}=S_{i} \\cup\\left\\{\\mathbf{y}_{i+1}\\right\\}\\). Remove this \\(\\mathbf{x}_{k}\\) from \\(T_{i}\\) and call the remaining set \\(S_{i+1}\\). Then \\(S_{i+1}\\) spans the same set as \\(T_{i}\\), namely \\(X\\), so that \\(S_{i+1}\\) has the properties postulated for \\(S_{i}\\) with \\(i+1\\) in place of \\(i\\).\nStarting with \\(S_{0}\\), we thus construct sets \\(S_{1}, \\ldots, S_{r}\\). The last of these consists of \\(\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}\\), and our construction shows that it spans \\(X\\). But \\(Q\\) is independent; hence \\(\\mathbf{y}_{r+1}\\) is not in the span of \\(S_{r}\\). This contradiction establishes the theorem.\nCorollary \\(\\operatorname{dim} R^{n}=n\\).\nProof Since \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) spans \\(R^{n}\\), the theorem shows that \\(\\operatorname{dim} R^{n} \\leq n\\). Since \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) is independent, \\(\\operatorname{dim} R^{n} \\geq n\\).\n9.3 Theorem Suppose \\(X\\) is a vector space, and \\(\\operatorname{dim} X=n\\). (a) A set \\(E\\) of \\(n\\) vectors in \\(X\\) spans \\(X\\) if and only if \\(E\\) is independent. (b) \\(X\\) has a basis, and every basis consists of \\(n\\) vectors. (c) If \\(1 \\leq r \\leq n\\) and \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}\\right\\}\\) is an independent set in \\(X\\), then \\(X\\) has \\(a\\) basis containing \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}\\right\\}\\).\nProof Suppose \\(E=\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\). Since \\(\\operatorname{dim} X=n\\), the set \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}, \\mathbf{y}\\right\\}\\) is dependent, for every \\(\\mathbf{y} \\in X\\). If \\(E\\) is independent, it follows that \\(\\mathbf{y}\\) is in the span of \\(E\\); hence \\(E\\) spans \\(X\\). Conversely, if \\(E\\) is dependent, one of its members can be removed without changing the span of \\(E\\). Hence \\(E\\) cannot span \\(X\\), by Theorem 9.2. This proves \\((a)\\).\nSince \\(\\operatorname{dim} X=n, X\\) contains an independent set of \\(n\\) vectors, and (a) shows that every such set is a basis of \\(X ;(b)\\) now follows from \\(9.1(d)\\) and 9.2.\nTo prove \\((c)\\), let \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\) be a basis of \\(X\\). The set\n\\[ S=\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}, \\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\} \\]\nspans \\(X\\) and is dependent, since it contains more than \\(n\\) vectors. The argument used in the proof of Theorem \\(9.2\\) shows that one of the \\(\\mathbf{x}_{i}\\) ’s is a linear combination of the other members of \\(S\\). If we remove this \\(\\mathbf{x}_{i}\\) from \\(S\\), the remaining set still spans \\(X\\). This process can be repeated \\(r\\) times and leads to a basis of \\(X\\) which contains \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{r}\\right\\}\\), by \\((a)\\).\n9.4 Definitions A mapping \\(A\\) of a vector space \\(X\\) into a vector space \\(Y\\) is said to be a linear transformation if\n\\[ A\\left(\\mathbf{x}_{1}+\\mathbf{x}_{2}\\right)=A \\mathbf{x}_{1}+A \\mathbf{x}_{2}, \\quad A(c \\mathrm{x})=c A \\mathbf{x} \\]\nfor all \\(\\mathbf{x}, \\mathbf{x}_{1}, \\mathbf{x}_{2} \\in X\\) and all scalars \\(c\\). Note that one often writes \\(A \\mathbf{x}\\) instead of \\(A(\\mathbf{x})\\) if \\(A\\) is linear.\nObserve that \\(A 0=0\\) if \\(A\\) is linear. Observe also that a linear transformation \\(A\\) of \\(X\\) into \\(Y\\) is completely determined by its action on any basis: If\n\\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\) is a basis of \\(X\\), then every \\(\\mathbf{x} \\in X\\) has a unique representation of the form\n\\[ \\mathbf{x}=\\sum_{i=1}^{n} c_{i} \\mathbf{x}_{i}, \\]\nand the linearity of \\(A\\) allows us to compute \\(A \\mathbf{x}\\) from the vectors \\(A \\mathbf{x}_{1}, \\ldots, A \\mathbf{x}_{n}\\) and the coordinates \\(c_{1}, \\ldots, c_{n}\\) by the formula\n\\[ A \\mathbf{x}=\\sum_{i=1}^{n} c_{i} A \\mathbf{x}_{i} . \\]\nLinear transformations of \\(X\\) into \\(X\\) are often called linear operators on \\(X\\). If \\(A\\) is a linear operator on \\(X\\) which (i) is one-to-one and (ii) maps \\(X\\) onto \\(X\\), we say that \\(A\\) is invertible. In this case we can define an operator \\(A^{-1}\\) on \\(X\\) by requiring that \\(A^{-1}(A \\mathbf{x})=\\mathbf{x}\\) for all \\(\\mathbf{x} \\in X\\). It is trivial to verify that we then also have \\(A\\left(A^{-1} \\mathbf{x}\\right)=\\mathbf{x}\\), for all \\(\\mathbf{x} \\in X\\), and that \\(A^{-1}\\) is linear.\nAn important fact about linear operators on finite-dimensional vector spaces is that each of the above conditions (i) and (ii) implies the other:\n9.5 Theorem \\(A\\) linear operator \\(A\\) on a finite-dimensional vector space \\(X\\) is one-to-one if and only if the range of \\(A\\) is all of \\(X\\).\nProof Let \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\) be a basis of \\(X\\). The linearity of \\(A\\) shows that its range \\(\\mathscr{R}(A)\\) is the span of the set \\(Q=\\left\\{A \\mathbf{x}_{1}, \\ldots, A \\mathbf{x}_{n}\\right\\}\\). We therefore infer from Theorem 9.3(a) that \\(\\mathscr{R}(A)=X\\) if and only if \\(Q\\) is independent. We have to prove that this happens if and only if \\(A\\) is one-to-one.\nSuppose \\(A\\) is one-to-one and \\(\\Sigma c_{i} A \\mathbf{x}_{i}=0\\). Then \\(A\\left(\\Sigma c_{i} \\mathbf{x}_{i}\\right)=\\mathbf{0}\\), hence \\(\\Sigma c_{i} \\mathbf{x}_{i}=\\mathbf{0}\\), hence \\(c_{1}=\\cdots=c_{n}=0\\), and we conclude that \\(Q\\) is independent. Conversely, suppose \\(Q\\) is independent and \\(A\\left(\\Sigma c_{i} \\mathbf{x}_{i}\\right)=0\\). Then \\(\\Sigma c_{i} A \\mathbf{x}_{i}=\\mathbf{0}\\), hence \\(c_{1}=\\cdots=c_{n}=0\\), and we conclude: \\(A \\mathbf{x}=\\mathbf{0}\\) only if \\(\\mathbf{x}=0\\). If now \\(A \\mathbf{x}=A \\mathbf{y}\\), then \\(A(\\mathbf{x}-\\mathbf{y})=A \\mathbf{x}-A \\mathbf{y}=\\mathbf{0}\\), so that \\(\\mathbf{x}-\\mathbf{y}=\\mathbf{0}\\), and this says that \\(A\\) is one-to-one.\n9.6 Definitions (a) Let \\(L(X, Y)\\) be the set of all linear transformations of the vector space \\(X\\) into the vector space \\(Y\\). Instead of \\(L(X, X)\\), we shall simply write \\(L(X)\\). If \\(A_{1}, A_{2} \\in L(X, Y)\\) and if \\(c_{1}, c_{2}\\) are scalars, define \\(c_{1} A_{1}+c_{2} A_{2}\\) by\n\\[ \\left(c_{1} A_{1}+c_{2} A_{2}\\right) \\mathbf{x}=c_{1} A_{1} \\mathbf{x}+c_{2} A_{2} \\mathbf{x} \\quad(\\mathbf{x} \\in X) \\text {. } \\]\nIt is then clear that \\(c_{1} A_{1}+c_{2} A_{2} \\in L(X, Y)\\). (b) If \\(X, Y, Z\\) are vector spaces, and if \\(A \\in L(X, Y)\\) and \\(B \\in L(Y, Z)\\), we define their product \\(B A\\) to be the composition of \\(A\\) and \\(B\\) :\n\\[ (B A) \\mathbf{x}=B(A \\mathbf{x}) \\quad(\\mathbf{x} \\in X) \\text {. } \\]\nThen \\(B A \\in L(X, Z)\\).\nNote that \\(B A\\) need not be the same as \\(A B\\), even if \\(X=Y=Z\\). (c) For \\(A \\in L\\left(R^{n}, R^{m}\\right)\\), define the norm \\(\\|A\\|\\) of \\(A\\) to be the sup of all numbers \\(|A \\mathbf{x}|\\), where \\(\\mathbf{x}\\) ranges over all vectors in \\(R^{n}\\) with \\(|\\mathbf{x}| \\leq 1\\). Observe that the inequality\n\\[ |A \\mathbf{x}| \\leq\\|A\\||\\mathbf{x}| \\]\nholds for all \\(\\mathbf{x} \\in R^{n}\\). Also, if \\(\\lambda\\) is such that \\(|A \\mathbf{x}| \\leq \\lambda|\\mathbf{x}|\\) for all \\(\\mathbf{x} \\in R^{n}\\), then \\(\\|A\\| \\leq \\lambda\\).\n9.7 Theorem (a) If \\(A \\in L\\left(R^{n}, R^{m}\\right)\\), then \\(\\|A\\|\u0026lt;\\infty\\) and \\(A\\) is a uniformly continuous mapping of \\(R^{n}\\) into \\(R^{m}\\).\nIf \\(A, B \\in L\\left(R^{n}, R^{m}\\right)\\) and \\(c\\) is a scalar, then \\[ \\|A+B\\| \\leq\\|A\\|+\\|B\\|, \\quad\\|c A\\|=|c|\\|A\\| \\text {. } \\]\nWith the distance between \\(A\\) and \\(B\\) defined as \\(\\|A-B\\|, L\\left(R^{n}, R^{m}\\right)\\) is a metric space. (c) If \\(A \\in L\\left(R^{n}, R^{m}\\right)\\) and \\(B \\in L\\left(R^{m}, R^{k}\\right)\\), then\n\\[ \\|B A\\| \\leq\\|B\\|\\|A\\| \\text {. } \\]\nProof (a) Let \\(\\left\\{\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{n}\\right\\}\\) be the standard basis in \\(R^{n}\\) and suppose \\(\\mathbf{x}=\\Sigma c_{i} \\mathbf{e}_{i}\\), \\(|\\mathbf{x}| \\leq 1\\), so that \\(\\left|c_{i}\\right| \\leq 1\\) for \\(i=1, \\ldots, n\\). Then \\[ |A \\mathbf{x}|=\\left|\\sum c_{i} A \\mathbf{e}_{i}\\right| \\leq \\sum\\left|c_{i}\\right|\\left|A \\mathbf{e}_{i}\\right| \\leq \\sum\\left|A \\mathbf{e}_{i}\\right| \\]\nso that\n\\[ \\|A\\| \\leq \\sum_{i=1}^{n}\\left|A \\mathbf{e}_{i}\\right|\u0026lt;\\infty . \\]\nSince \\(|A \\mathbf{x}-A \\mathbf{y}| \\leq\\|A\\||\\mathbf{x}-\\mathbf{y}|\\) if \\(\\mathbf{x}, \\mathbf{y} \\in R^{n}\\), we see that \\(A\\) is uniformly continuous. (b) The inequality in \\((b)\\) follows from\n\\[ |(A+B) \\mathbf{x}|=|A \\mathbf{x}+B \\mathbf{x}| \\leq|A \\mathbf{x}|+|B \\mathbf{x}| \\leq(\\|A\\|+\\|B\\|)|\\mathbf{x}| . \\]\nThe second part of \\((b)\\) is proved in the same manner. If\n\\[ A, B, C \\in L\\left(R^{n}, R^{m}\\right) \\]\nwe have the triangle inequality\n\\[ \\|A-C\\|=\\|(A-B)+(B-C)\\| \\leq\\|A-B\\|+\\|B-C\\| \\]\nand it is easily verified that \\(\\|A-B\\|\\) has the other properties of a metric (Definition 2.15). (c) Finally, (c) follows from\n\\[ |(B A) \\mathbf{x}|=|B(A \\mathbf{x})| \\leq\\|B\\||A \\mathbf{x}| \\leq\\|B\\|\\|A\\||\\mathbf{x}| \\]\nSince we now have metrics in the spaces \\(L\\left(R^{n}, R^{m}\\right)\\), the concepts of open set, continuity, etc., make sense for these spaces. Our next theorem utilizes these concepts.\n9.8 Theorem Let \\(\\Omega\\) be the set of all invertible linear operators on \\(R^{n}\\). (a) If \\(A \\in \\Omega, B \\in L\\left(R^{n}\\right)\\), and \\[ \\|B-A\\| \\cdot\\left\\|A^{-1}\\right\\|\u0026lt;1, \\] then \\(B \\in \\Omega\\). (b) \\(\\Omega\\) is an open subset of \\(L\\left(R^{n}\\right)\\), and the mapping \\(A \\rightarrow A^{-1}\\) is continuous on \\(\\Omega\\). (This mapping is also obviously a \\(1-1\\) mapping of \\(\\Omega\\) onto \\(\\Omega\\), which is its own inverse.)\nProof\nPut \\(\\left\\|A^{-1}\\right\\|=1 / \\alpha\\), put \\(\\|B-A\\|=\\beta\\). Then \\(\\beta\u0026lt;\\alpha\\). For every \\(\\mathbf{x} \\in R^{n}\\), \\[ \\begin{aligned} \\alpha|\\mathbf{x}| \u0026amp;=\\alpha\\left|A^{-1} A \\mathbf{x}\\right| \\leq \\alpha\\left\\|A^{-1}\\right\\| \\cdot|A \\mathbf{x}| \\\\ \u0026amp;=|A \\mathbf{x}| \\leq|(A-B) \\mathbf{x}|+|B \\mathbf{x}| \\leq \\beta|\\mathbf{x}|+|B \\mathbf{x}|, \\end{aligned} \\] so that \\[ (\\alpha-\\beta)|\\mathbf{x}| \\leq|B \\mathbf{x}| \\quad\\left(\\mathbf{x} \\in R^{n}\\right) . \\] Since \\(\\alpha-\\beta\u0026gt;0,(1)\\) shows that \\(B \\mathbf{x} \\neq 0\\) if \\(\\mathbf{x} \\neq 0\\). Hence \\(B\\) is \\(1-1\\). By Theorem 9.5, \\(B \\in \\Omega\\). This holds for all \\(B\\) with \\(\\|B-A\\|\u0026lt;\\alpha\\). Thus we have \\((a)\\) and the fact that \\(\\Omega\\) is open. Next, replace \\(\\mathbf{x}\\) by \\(B^{-1} \\mathbf{y}\\) in (1). The resulting inequality \\[ (\\alpha-\\beta)\\left|B^{-1} \\mathbf{y}\\right| \\leq\\left|B B^{-1} \\mathbf{y}\\right|=|\\mathbf{y}| \\quad\\left(\\mathrm{y} \\in R^{n}\\right) \\] shows that \\(\\left\\|B^{-1}\\right\\| \\leq(\\alpha-\\beta)^{-1}\\). The identity \\[ B^{-1}-A^{-1}=B^{-1}(A-B) A^{-1} \\text {, } \\] combined with Theorem 9.7(c), implies therefore that \\[ \\left\\|B^{-1}-A^{-1}\\right\\| \\leq\\left\\|B^{-1}\\right\\|\\|A-B\\|\\left\\|A^{-1}\\right\\| \\leq \\frac{\\beta}{\\alpha(\\alpha-\\beta)} . \\] This establishes the continuity assertion made in \\((b)\\), since \\(\\beta \\rightarrow 0\\) as \\(B \\rightarrow A\\). 9.9 Matrices Suppose \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{m}\\right\\}\\) are bases of vector spaces \\(X\\) and \\(Y\\), respectively. Then every \\(A \\in L(X, Y)\\) determines a set of numbers \\(a_{i j}\\) such that \\[ A \\mathbf{x}_{j}=\\sum_{i=1}^{m} a_{i j} \\mathbf{y}_{i} \\quad(1 \\leq j \\leq n) \\] It is convenient to visualize these numbers in a rectangular array of \\(m\\) rows and \\(n\\) columns, called an \\(m\\) by \\(n\\) matrix:\nObserve that the coordinates \\(a_{\\imath j}\\) of the vector \\(A \\mathbf{x}_{j}\\) (with respect to the basis \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{m}\\right\\}\\) ) appear in the \\(j\\) th column of \\([A]\\). The vectors \\(A \\mathbf{x}_{j}\\) are therefore sometimes called the column vectors of \\([A]\\). With this terminology, the range of \\(A\\) is spanned by the column vectors of \\([A]\\). If \\(\\mathbf{x}=\\Sigma c_{j} \\mathbf{x}_{j}\\), the linearity of \\(A\\), combined with (3), shows that \\[ A \\mathbf{x}=\\sum_{i=1}^{m}\\left(\\sum_{j=1}^{n} a_{i j} c_{j}\\right) \\mathbf{y}_{i} \\] Thus the coordinates of \\(A \\mathbf{x}\\) are \\(\\Sigma_{j} a_{i j} c_{j}\\). Note that in (3) the summation ranges over the first subscript of \\(a_{i j}\\), but that we sum over the second subscript when computing coordinates.\nSuppose next that an \\(m\\) by \\(n\\) matrix is given, with real entries \\(a_{i j}\\). If \\(A\\) is then defined by (4), it is clear that \\(A \\in L(X, Y)\\) and that \\([A]\\) is the given matrix. Thus there is a natural 1-1 correspondence between \\(L(X, Y)\\) and the set of all real \\(m\\) by \\(n\\) matrices. We emphasize, though, that \\([A]\\) depends not only on \\(A\\) but also on the choice of bases in \\(X\\) and \\(Y\\). The same \\(A\\) may give rise to many different matrices if we change bases, and vice versa. We shall not pursue this observation any further, since we shall usually work with fixed bases. (Some remarks on this may be found in Sec. 9.37.)\nIf \\(Z\\) is a third vector space, with basis \\(\\left\\{z_{1}, \\ldots, z_{p}\\right\\}\\), if \\(A\\) is given by (3), and if\n\\[ B \\mathbf{y}_{i}=\\sum_{k} b_{k i} \\mathbf{z}_{k}, \\quad(B A) \\mathbf{x}_{j}=\\sum_{k} c_{k j} \\mathbf{z}_{k} \\text {, } \\]\nthen \\(A \\in L(X, Y), B \\in L(Y, Z), B A \\in L(X, Z)\\), and since\n\\[ \\begin{aligned} B\\left(A \\mathbf{x}_{j}\\right) \u0026amp;=B \\sum_{i} a_{i j} \\mathbf{y}_{i}=\\sum_{i} a_{i j} B \\mathbf{y}_{i} \\\\ \u0026amp;=\\sum a_{i j} \\sum_{k} b_{k i} \\mathbf{z}_{k}=\\sum_{k}\\left(\\sum_{i} b_{k i} a_{i j}\\right) \\mathbf{z}_{k} \\end{aligned} \\]\nthe independence of \\(\\left\\{z_{1}, \\ldots, \\mathbf{z}_{p}\\right\\}\\) implies that\n\\[ c_{k j}=\\sum_{i} b_{k i} a_{i j} \\quad(1 \\leq k \\leq p, 1 \\leq j \\leq n) . \\]\nThis shows how to compute the \\(p\\) by \\(n\\) matrix \\([B A]\\) from \\([B]\\) and \\([A]\\). If we define the product \\([B][A]\\) to be \\([B A]\\), then (5) describes the usual rule of matrix multiplication.\nFinally, suppose \\(\\left\\{\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right\\}\\) and \\(\\left\\{\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{m}\\right\\}\\) are standard bases of \\(R^{n}\\) and \\(R^{m}\\), and \\(A\\) is given by (4). The Schwarz inequality shows that\n\\[ |A \\mathbf{x}|^{2}=\\sum_{i}\\left(\\sum_{j} a_{i j} c_{j}\\right)^{2} \\leq \\sum_{i}\\left(\\sum_{j} a_{i j}^{2} \\cdot \\sum_{j} c_{j}^{2}\\right)=\\sum_{i, j} a_{i j}^{2}|\\mathbf{x}|^{2} \\]\nThus\n\\[ \\|A\\| \\leq\\left\\{\\sum_{i, j} a_{i j}^{2}\\right\\}^{1 / 2} . \\]\nIf we apply (6) to \\(B-A\\) in place of \\(A\\), where \\(A, B \\in L\\left(R^{n}, R^{m}\\right)\\), we see that if the matrix elements \\(a_{i j}\\) are continuous functions of a parameter, then the same is true of \\(A\\). More precisely:\nIf \\(S\\) is a metric space, if \\(a_{11}, \\ldots, a_{m n}\\) are real continuous functions on \\(S\\), and if, for each \\(p \\in S, A_{p}\\) is the linear transformation of \\(R^{n}\\) into \\(R^{m}\\) whose matrix has entries \\(a_{i j}(p)\\), then the mapping \\(p \\rightarrow A_{p}\\) is a continuous mapping of \\(S\\) into \\(L\\left(R^{n}, R^{m}\\right)\\).\n","date":"2022-08-11T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/9-functions-of-several-variables/1-linear-transformations/","section":"baby rudin","tags":null,"title":"1 LINEAR TRANSFORMATIONS.md"},{"categories":null,"contents":" youtube bilibili note pdf xopp \\(1.12\\) Definition A field is a set \\(F\\) with two operations, called addition and multiplication, which satisfy the following so-called “field axioms” (A), (M), and (D):\nAxioms for addition (A1) If \\(x \\in F\\) and \\(y \\in F\\), then their sum \\(x+y\\) is in \\(F\\).\n(A2) Addition is commutative: \\(x+y=y+x\\) for all \\(x, y \\in F\\).\n(A3) Addition is associative: \\((x+y)+z=x+(y+z)\\) for all \\(x, y, z \\in F\\).\n(A4) \\(F\\) contains an element 0 such that \\(0+x=x\\) for every \\(x \\in F\\).\n(A5) To every \\(x \\in F\\) corresponds an element \\(-x \\in F\\) such that\n\\[ x+(-x)=0 \\]\nAxioms for multiplication (M1) If \\(x \\in F\\) and \\(y \\in F\\), then their product \\(x y\\) is in \\(F\\).\n(M2) Multiplication is commutative: \\(x y=y x\\) for all \\(x, y \\in F\\).\n(M3) Multiplication is associative: \\((x y) z=x(y z)\\) for all \\(x, y, z \\in F\\).\n(M4) \\(F\\) contains an element \\(1 \\neq 0\\) such that \\(1 x=x\\) for every \\(x \\in F\\).\n(M5) If \\(x \\in F\\) and \\(x \\neq 0\\) then there exists an element \\(1 / x \\in F\\) such that\n\\[ x \\cdot(1 / x)=1 \\text {. } \\]\nThe distributive law \\[ x(y+z)=x y+x z \\] holds for all \\(x, y, z \\in F\\).\n\\(1.13\\) Remarks\nOne usually writes (in any field) \\[ x-y, \\frac{x}{y}, x+y+z, x y z, x^{2}, x^{3}, 2 x, 3 x, \\ldots \\]\nin place of\n\\[ x+(-y), x \\cdot\\left(\\frac{1}{y}\\right),(x+y)+z,(x y) z, x x, x x x, x+x, x+x+x, \\ldots \\]\nThe field axioms clearly hold in \\(Q\\), the set of all rational numbers, if addition and multiplication have their customary meaning. Thus \\(Q\\) is a field.\nAlthough it is not our purpose to study fields (or any other algebraic structures) in detail, it is worthwhile to prove that some familiar properties of \\(Q\\) are consequences of the field axioms; once we do this, we will not need to do it again for the real numbers and for the complex numbers.\n1.14 Proposition The axioms for addition imply the following statements. (a) If \\(x+y=x+z\\) then \\(y=z\\).\n(b) If \\(x+y=x\\) then \\(y=0\\).\n(c) If \\(x+y=0\\) then \\(y=-x\\).\n(d) \\(-(-x)=x\\).\nStatement (a) is a cancellation law. Note that \\((b)\\) asserts the uniqueness of the element whose existence is assumed in (A4), and that (c) does the same for (A5).\nProof If \\(x+y=x+z\\), the axioms (A) give \\[ \\begin{aligned} y=0+y \u0026amp;=(-x+x)+y=-x+(x+y) \\\\ \u0026amp;=-x+(x+z)=(-x+x)+z=0+z=z \\end{aligned} \\] This proves (a). Take \\(z=0\\) in \\((a)\\) to obtain \\((b)\\). Take \\(z=-x\\) in (a) to obtain \\((c)\\). Since \\(-x+x=0,(c)\\) (with \\(-x\\) in place of \\(x\\) ) gives \\((d)\\)\n1.15 Proposition The axioms for multiplication imply the following statements. (a) If \\(x \\neq 0\\) and \\(x y=x z\\) then \\(y=z\\).\n(b) If \\(x \\neq 0\\) and \\(x y=x\\) then \\(y=1\\).\n(c) If \\(x \\neq 0\\) and \\(x y=1\\) then \\(y=1 / x\\).\n(d) If \\(x \\neq 0\\) then \\(1 /(1 / x)=x\\).\nThe proof is so similar to that of Proposition \\(1.14\\) that we omit it.\n1.16 Proposition The field axioms imply the following statements, for any \\(x, y\\), \\(z \\in F\\) (a) \\(0 x=0\\)\n(b) If \\(x \\neq 0\\) and \\(y \\neq 0\\) then \\(x y \\neq 0\\).\n(c) \\((-x) y=-(x y)=x(-y)\\).\n(d) \\((-x)(-y)=x y\\).\nProof \\(0 x+0 x=(0+0) x=0 x\\). Hence \\(1.14(b)\\) implies that \\(0 x=0\\), and (a) holds. Next, assume \\(x \\neq 0, y \\neq 0\\), but \\(x y=0\\). Then (a) gives \\[ 1=\\left(\\frac{1}{y}\\right)\\left(\\frac{1}{x}\\right) x y=\\left(\\frac{1}{y}\\right)\\left(\\frac{1}{x}\\right) 0=0, \\] a contradiction. Thus \\((b)\\) holds.\nThe first equality in \\((c)\\) comes from \\[ (-x) y+x y=(-x+x) y=0 y=0, \\] combined with \\(1.14(c)\\); the other half of \\((c)\\) is proved in the same way. Finally, \\[ (-x)(-y)=-[x(-y)]=-[-(x y)]=x y \\] by \\((c)\\) and \\(1.14(d)\\).\n1.17 Definition An ordered field is a field \\(F\\) which is also an ordered set, such that\n(i) \\(x+y\u0026lt;x+z\\) if \\(x, y, z \\in F\\) and \\(y\u0026lt;z\\),\n(ii) \\(x y\u0026gt;0\\) if \\(x \\in F, y \\in F, x\u0026gt;0\\), and \\(y\u0026gt;0\\).\nIf \\(x\u0026gt;0\\), we call \\(x\\) positive; if \\(x\u0026lt;0, x\\) is negative.\nFor example, \\(Q\\) is an ordered field. All the familiar rules for working with inequalities apply in every ordered field: Multiplication by positive [negative] quantities preserves [reverses] inequalities, no square is negative, etc. The following proposition lists some of these.\n1.18 Proposition The following statements are true in every ordered field.\n(a) If \\(x\u0026gt;0\\) then \\(-x\u0026lt;0\\), and vice versa.\n(b) If \\(x\u0026gt;0\\) and \\(y\u0026lt;z\\) then \\(x y\u0026lt;x z\\).\n(c) If \\(x\u0026lt;0\\) and \\(y\u0026lt;z\\) then \\(x y\u0026gt;x z\\).\n(d) If \\(x \\neq 0\\) then \\(x^{2}\u0026gt;0\\). In particular, \\(1\u0026gt;0\\).\n(e) If \\(0\u0026lt;x\u0026lt;y\\) then \\(0\u0026lt;1 / y\u0026lt;1 / x\\).\nProof (a) If \\(x\u0026gt;0\\) then \\(0=-x+x\u0026gt;-x+0\\), so that \\(-x\u0026lt;0\\). If \\(x\u0026lt;0\\) then \\(0=-x+x\u0026lt;-x+0\\), so that \\(-x\u0026gt;0\\). This proves \\((a)\\). (b) Since \\(z\u0026gt;y\\), we have \\(z-y\u0026gt;y-y=0\\), hence \\(x(z-y)\u0026gt;0\\), and therefore \\[ x z=x(z-y)+x y\u0026gt;0+x y=x y . \\] (c) By \\((a),(b)\\), and Proposition 1.16(c), \\[ -[x(z-y)]=(-x)(z-y)\u0026gt;0, \\] so that \\(x(z-y)\u0026lt;0\\), hence \\(x z\u0026lt;x y\\).\nIf \\(x\u0026gt;0\\), part (ii) of Definition \\(1.17\\) gives \\(x^{2}\u0026gt;0\\). If \\(x\u0026lt;0\\), then \\(-x\u0026gt;0\\), hence \\((-x)^{2}\u0026gt;0\\). But \\(x^{2}=(-x)^{2}\\), by Proposition \\(1.16(d)\\). Since \\(1=1^{2}, 1\u0026gt;0\\).\nIf \\(y\u0026gt;0\\) and \\(v \\leq 0\\), then \\(y v \\leq 0\\). But \\(y \\cdot(1 / y)=1\u0026gt;0\\). Hence \\(1 / y\u0026gt;0\\). Likewise, \\(1 / x\u0026gt;0\\). If we multiply both sides of the inequality \\(x\u0026lt;y\\) by the positive quantity \\((1 / x)(1 / y)\\), we obtain \\(1 / y\u0026lt;1 / x\\).\n","date":"2022-08-11T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/2-fields/","section":"baby rudin","tags":null,"title":"2 FIELDS"},{"categories":null,"contents":"This function is closely related to factorials and crops up in many unexpected places in analysis. Its origin, history, and development are very well described in an interesting article by P. J. Davis (Amer. Math. Monthly, vol. 66, 1959, pp. 849-869). Artin’s book (cited in the Bibliography) is another good elementary introduction.\nOur presentation will be very condensed, with only a few comments after each theorem. This section may thus be regarded as a large exercise, and as an opportunity to apply some of the material that has been presented so far.\n8.18 Theorem (a) The functional equation \\[ \\Gamma(x+1)=x \\Gamma(x) \\] holds if \\(0\u0026lt;x\u0026lt;\\infty\\). (b) \\(\\Gamma(n+1)=n !\\) for \\(n=1,2,3, \\ldots\\) (c) \\(\\log \\Gamma\\) is convex on \\((0, \\infty)\\).\nProof An integration by parts proves \\((a)\\). Since \\(\\Gamma(1)=1\\), (a) implies (b), by induction. If \\(1\u0026lt;p\u0026lt;\\infty\\) and \\((1 / p)+(1 / q)=1\\), apply Hölder’s inequality (Exercise 10, Chap. 6) to (93), and obtain\n\\[ \\Gamma\\left(\\frac{x}{p}+\\frac{y}{q}\\right) \\leq \\Gamma(x)^{1 / p} \\Gamma(y)^{1 / q} . \\]\nThis is equivalent to \\((c)\\).\nIt is a rather surprising fact, discovered by Bohr and Mollerup, that these three properties characterize \\(\\Gamma\\) completely.\n8.19 Theorem Iff is a positive function on \\((0, \\infty)\\) such that\n\\(f(x+1)=x f(x)\\), \\(f(1)=1\\), \\(\\log f\\) is convex, then \\(f(x)=\\Gamma(x)\\). Proof Since \\(\\Gamma\\) satisfies \\((a),(b)\\), and \\((c)\\), it is enough to prove that \\(f(x)\\) is uniquely determined by \\((a),(b),(c)\\), for all \\(x\u0026gt;0\\). By \\((a)\\), it is enough to do this for \\(x \\in(0,1)\\).\nPut \\(\\varphi=\\log f\\). Then\n\\[ \\varphi(x+1)=\\varphi(x)+\\log x \\quad(0\u0026lt;x\u0026lt;\\infty), \\]\n\\(\\varphi(1)=0\\), and \\(\\varphi\\) is convex. Suppose \\(0\u0026lt;x\u0026lt;1\\), and \\(n\\) is a positive integer. By (94), \\(\\varphi(n+1)=\\log (n\\) !). Consider the difference quotients of \\(\\varphi\\) on the intervals \\([n, n+1],[n+1, n+1+x],[n+1, n+2]\\). Since \\(\\varphi\\) is convex\n\\[ \\log n \\leq \\frac{\\varphi(n+1+x)-\\varphi(n+1)}{x} \\leq \\log (n+1) \\text {. } \\]\nRepeated application of (94) gives\n\\[ \\varphi(n+1+x)=\\varphi(x)+\\log [x(x+1) \\cdots(x+n)] . \\]\nThus\n\\[ 0 \\leq \\varphi(x)-\\log \\left[\\frac{n ! n^{x}}{x(x+1) \\cdots(x+n)}\\right] \\leq x \\log \\left(1+\\frac{1}{n}\\right) \\]\nThe last expression tends to 0 as \\(n \\rightarrow \\infty\\). Hence \\(\\varphi(x)\\) is determined, and the proof is complete. As a by-product we obtain the relation\n\\[ \\Gamma(x)=\\lim _{n \\rightarrow \\infty} \\frac{n ! n^{x}}{x(x+1) \\cdots(x+n)} \\]\nat least when \\(0\u0026lt;x\u0026lt;1\\); from this one can deduce that (95) holds for all \\(x\u0026gt;0\\), since \\(\\Gamma(x+1)=x \\Gamma(x)\\).\n8.20 Theorem If \\(x\u0026gt;0\\) and \\(y\u0026gt;0\\), then\n\\[ \\int_{0}^{1} t^{x-1}(1-t)^{y-1} d t=\\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)} \\]\nThis integral is the so-called beta function \\(B(x, y)\\).\nProof Note that \\(B(1, y)=1 / y\\), that \\(\\log B(x, y)\\) is a convex function of \\(x\\), for each fixed \\(y\\), by Hölder’s inequality, as in Theorem \\(8.18\\), and that\n\\[ B(x+1, y)=\\frac{x}{x+y} B(x, y) \\text {. } \\]\nTo prove (97), perform an integration by parts on\n\\[ B(x+1, y)=\\int_{0}^{1}\\left(\\frac{t}{1-t}\\right)^{x}(1-t)^{x+y-1} d t . \\]\nThese three properties of \\(B(x, y)\\) show, for each \\(y\\), that Theorem \\(8.19\\) applies to the function \\(f\\) defined by\n\\[ f(x)=\\frac{\\Gamma(x+y)}{\\Gamma(y)} B(x, y) \\]\nHence \\(f(x)=\\Gamma(x)\\)\n8.21 Some consequences The substitution \\(t=\\sin ^{2} \\theta\\) turns (96) into \\[ 2 \\int_{0}^{\\pi / 2}(\\sin \\theta)^{2 x-1}(\\cos \\theta)^{2 y-1} d \\theta=\\frac{\\Gamma(x) \\Gamma(y)}{\\Gamma(x+y)} . \\] The special case \\(x=y=\\frac{1}{2}\\) gives \\[ \\Gamma\\left(\\frac{1}{2}\\right)=\\sqrt{\\pi} \\text {. } \\] The substitution \\(t=s^{2}\\) turns (93) into \\[ \\Gamma(x)=2 \\int_{0}^{\\infty} s^{2 x-1} e^{-s^{2}} d s \\quad(0\u0026lt;x\u0026lt;\\infty) . \\] The special case \\(x=\\frac{1}{2}\\) gives \\[ \\int_{-\\infty}^{\\infty} e^{-s^{2}} d s=\\sqrt{\\pi} \\] By (99), the identity \\[ \\Gamma(x)=\\frac{2^{x-1}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{x}{2}\\right) \\Gamma\\left(\\frac{x+1}{2}\\right) \\] follows directly from Theorem 8.19.\n8.22 Stirling’s formula This provides a simple approximate expression for \\(\\Gamma(x+1)\\) when \\(x\\) is large (hence for \\(n !\\) when \\(n\\) is large). The formula is\n\\[ \\lim _{x \\rightarrow \\infty} \\frac{\\Gamma(x+1)}{(x / e)^{x} \\sqrt{2 \\pi x}}=1 \\]\nHere is a proof. Put \\(t=x(1+u)\\) in (93). This gives\n\\[ \\Gamma(x+1)=x^{x+1} e^{-x} \\int_{-1}^{\\infty}\\left[(1+u) e^{-u}\\right]^{x} d u \\]\nDetermine \\(h(u)\\) so that \\(h(0)=1\\) and\n\\[ (1+u) e^{-u}=\\exp \\left[-\\frac{u^{2}}{2} h(u)\\right] \\]\nif \\(-1\u0026lt;u\u0026lt;\\infty, u \\neq 0\\). Then\n\\[ h(u)=\\frac{2}{u^{2}}[u-\\log (1+u)] \\]\nIt follows that \\(h\\) is continuous, and that \\(h(u)\\) decreases monotonically from \\(\\infty\\) to 0 as \\(u\\) increases from \\(-1\\) to \\(\\infty\\).\nThe substitution \\(u=s \\sqrt{2 / x}\\) turns (104) into\n\\[ \\Gamma(x+1)=x^{x} e^{-x} \\sqrt{2 x} \\int_{-\\infty}^{\\infty} \\psi_{x}(s) d s \\]\nwhere\n\\[ \\psi_{x}(s)= \\begin{cases}\\exp \\left[-s^{2} h(s \\sqrt{2 / x})\\right] \u0026amp; (-\\sqrt{x / 2}\u0026lt;s\u0026lt;\\infty) \\\\ 0 \u0026amp; (s \\leq-\\sqrt{x / 2})\\end{cases} \\]\nNote the following facts about \\(\\psi_{x}(s)\\) :\nFor every \\(s, \\psi_{x}(s) \\rightarrow e^{-s^{2}}\\) as \\(x \\rightarrow \\infty\\). The convergence in \\((a)\\) is uniform on \\([-A, A]\\), for every \\(A\u0026lt;\\infty\\). When \\(s\u0026lt;0\\), then \\(0\u0026lt;\\psi_{x}(s)\u0026lt;e^{-s^{2}}\\). When \\(s\u0026gt;0\\) and \\(x\u0026gt;1\\), then \\(0\u0026lt;\\psi_{x}(s)\u0026lt;\\psi_{1}(s)\\). \\(\\int_{0}^{\\infty} \\psi_{1}(s) d s\u0026lt;\\infty\\). The convergence theorem stated in Exercise 12 of Chap. 7 can therefore be applied to the integral (107), and shows that this integral converges to \\(\\sqrt{\\pi}\\) as \\(x \\rightarrow \\infty\\), by (101). This proves (103).\nA more detailed version of this proof may be found in R. C. Buck’s “Advanced Calculus,” pp. 216-218. For two other, entirely different, proofs, see W. Feller’s article in Amer. Math. Monthly, vol. 74, 1967, pp. 1223-1225 (with a correction in vol. 75,1968, p. 518) and pp. 20-24 of Artin’s book.\nExercise 20 gives a simpler proof of a less precise result.\n","date":"2022-08-11T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/6-the-gamma-function/","section":"baby rudin","tags":null,"title":"6 THE GAMMA FUNCTION.md"},{"categories":null,"contents":"Let us define (46) \\[C(x)=\\frac{1}{2}[E(i x)+E(-i x)], \\quad S(x)=\\frac{1}{2 i}[E(i x)-E(-i x)]\\].\nWe shall show that \\(C(x)\\) and \\(S(x)\\) coincide with the functions \\(\\cos x\\) and \\(\\sin x\\), whose definition is usually based on geometric considerations. By \\((25), E(\\bar{z})=\\) \\(\\overline{E(z)}\\). Hence (46) shows that \\(C(x)\\) and \\(S(x)\\) are real for real \\(x\\). Also,\n\\[ E(i x)=C(x)+i S(x) . \\]\nThus \\(C(x)\\) and \\(S(x)\\) are the real and imaginary parts, respectively, of \\(E(i x)\\), if \\(x\\) is real. By (27),\n\\[ |E(i x)|^{2}=E(i x) \\overline{E(i x)}=E(i x) E(-i x)=1, \\]\nso that\n\\[ |E(i x)|=1 \\quad(x \\text { real }) . \\]\nFrom (46) we can read off that \\(C(0)=1, S(0)=0\\), and (28) shows that\n\\[ C^{\\prime}(x)=-S(x), \\quad S^{\\prime}(x)=C(x) . \\]\nWe assert that there exist positive numbers \\(x\\) such that \\(C(x)=0\\). For suppose this is not so. Since \\(C(0)=1\\), it then follows that \\(C(x)\u0026gt;0\\) for all \\(x\u0026gt;0\\), hence \\(S^{\\prime}(x)\u0026gt;0\\), by (49), hence \\(S\\) is strictly increasing; and since \\(S(0)=0\\), we have \\(S(x)\u0026gt;0\\) if \\(x\u0026gt;0\\). Hence if \\(0\u0026lt;x\u0026lt;y\\), we have\n\\[ S(x)(y-x)\u0026lt;\\int_{x}^{y} S(t) d t=C(x)-C(y) \\leq 2 \\text {. } \\]\nThe last inequality follows from (48) and (47). Since \\(S(x)\u0026gt;0,(50)\\) cannot be true for large \\(y\\), and we have a contradiction.\nLet \\(x_{0}\\) be the smallest positive number such that \\(C\\left(x_{0}\\right)=0\\). This exists, since the set of zeros of a continuous function is closed, and \\(C(0) \\neq 0\\). We define the number \\(\\pi\\) by\n\\[ \\pi=2 x_{0} . \\]\nThen \\(C(\\pi / 2)=0\\), and (48) shows that \\(S(\\pi / 2)=\\pm 1\\). Since \\(C(x)\u0026gt;0\\) in \\((0, \\pi / 2), S\\) is increasing in \\((0, \\pi / 2)\\); hence \\(S(\\pi / 2)=1\\). Thus\n\\[ E\\left(\\frac{\\pi i}{2}\\right)=i \\]\nand the addition formula gives\n\\[ E(\\pi i)=-1, \\quad E(2 \\pi i)=1 ; \\]\nhence\n\\[ E(z+2 \\pi i)=E(z) \\quad(z \\text { complex }) \\]\n\\(8.7\\) Theorem (a) The function \\(E\\) is periodic, with period \\(2 \\pi i\\). (b) The functions \\(C\\) and \\(S\\) are periodic, with period \\(2 \\pi\\). (c) If \\(0\u0026lt;t\u0026lt;2 \\pi\\), then \\(E(\\) it \\() \\neq 1\\). (d) If \\(z\\) is a complex number with \\(|z|=1\\), there is a unique \\(t\\) in \\([0,2 \\pi)\\) such that \\(E(i t)=z\\).\nProof By (53), (a) holds; and (b) follows from \\((a)\\) and (46). Suppose \\(0\u0026lt;t\u0026lt;\\pi / 2\\) and \\(E(i t)=x+i y\\), with \\(x, y\\) real. Our preceding work shows that \\(0\u0026lt;x\u0026lt;1,0\u0026lt;y\u0026lt;1\\). Note that\n\\[ E(4 i t)=(x+i y)^{4}=x^{4}-6 x^{2} y^{2}+y^{4}+4 i x y\\left(x^{2}-y^{2}\\right) . \\]\nIf \\(E(4 i t)\\) is real, it follows that \\(x^{2}-y^{2}=0\\); since \\(x^{2}+y^{2}=1\\), by (48), we have \\(x^{2}=y^{2}=\\frac{1}{2}\\), hence \\(E(4 i t)=-1\\). This proves \\((c)\\). If \\(0 \\leq t_{1}\u0026lt;t_{2}\u0026lt;2 \\pi\\), then\n\\[ E\\left(i t_{2}\\right)\\left[E\\left(i t_{1}\\right)\\right]^{-1}=E\\left(i t_{2}-i t_{1}\\right) \\neq 1, \\]\nby \\((c)\\). This establishes the uniqueness assertion in \\((d)\\). To prove the existence assertion in \\((d)\\), fix \\(z\\) so that \\(|z|=1\\). Write \\(z=x+i y\\), with \\(x\\) and \\(y\\) real. Suppose first that \\(x \\geq 0\\) and \\(y \\geq 0\\). On \\([0, \\pi / 2], C\\) decreases from 1 to 0 . Hence \\(C(t)=x\\) for some \\(t \\in[0, \\pi / 2]\\). Since \\(C^{2}+S^{2}=1\\) and \\(S \\geq 0\\) on \\([0, \\pi / 2]\\), it follows that \\(z=E(i t)\\).\nIf \\(x\u0026lt;0\\) and \\(y \\geq 0\\), the preceding conditions are satisfied by \\(-i z\\). Hence \\(-i z=E(i t)\\) for some \\(t \\in[0, \\pi / 2]\\), and since \\(i=E(\\pi i / 2)\\), we obtain \\(z=E(i(t+\\pi / 2))\\). Finally, if \\(y\u0026lt;0\\), the preceding two cases show that\n\\(-z=E(i t)\\) for some \\(t \\in(0, \\pi)\\). Hence \\(z=-E(i t)=E(i(t+\\pi))\\). This proves \\((d)\\), and hence the theorem. It follows from \\((d)\\) and (48) that the curve \\(\\gamma\\) defined by\n\\[ \\gamma(t)=E(i t) \\quad(0 \\leq t \\leq 2 \\pi) \\]\nis a simple closed curve whose range is the unit circle in the plane. Since \\(\\gamma^{\\prime}(t)=i E(i t)\\), the length of \\(\\gamma\\) is\n\\[ \\int_{0}^{2 \\pi}\\left|\\gamma^{\\prime}(t)\\right| d t=2 \\pi, \\]\nby Theorem 6.27. This is of course the expected result for the circumference of a circle of radius 1 . It shows that \\(\\pi\\), defined by \\((51)\\), has the usual geometric significance.\nIn the same way we see that the point \\(\\gamma(t)\\) describes a circular arc of length \\(t_{0}\\) as \\(t\\) increases from 0 to \\(t_{0}\\). Consideration of the triangle whose vertices are\n\\[ z_{1}=0, \\quad z_{2}=\\gamma\\left(t_{0}\\right), \\quad z_{3}=C\\left(t_{0}\\right) \\]\nshows that \\(C(t)\\) and \\(S(t)\\) are indeed identical with \\(\\cos t\\) and \\(\\sin t\\), if the latter are defined in the usual way as ratios of the sides of a right triangle.\nIt should be stressed that we derived the basic properties of the trigonometric functions from (46) and (25), without any appeal to the geometric notion of angle. There are other nongeometric approaches to these functions. The papers by W. F. Eberlein (Amer. Math. Monthly, vol. 74, 1967, pp. 1223-1225) and by G. B. Robison (Math. Mag., vol. 41, 1968, pp. 66-70) deal with these topics.\ntitle changed\n","date":"2022-08-10T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/3-the-trigonometric-functions/","section":"baby rudin","tags":null,"title":"3 THE TRIGONOMETRIC FUNCTIONS"},{"categories":null,"contents":"We are now in a position to give a simple proof of the fact that the complex field is algebraically complete, that is to say, that every nonconstant polynomial with complex coefficients has a complex root.\n\\(8.8\\) Theorem Suppose \\(a_{0}, \\ldots, a_{n}\\) are complex numbers, \\(n \\geq 1, a_{n} \\neq 0\\),\n\\[ P(z)=\\sum_{0}^{n} a_{k} z^{k} . \\]\nThen \\(P(z)=0\\) for some complex number \\(z\\).\nProof Without loss of generality, assume \\(a_{n}=1\\). Put\n\\[ \\mu=\\inf |P(z)| \\quad \\text { ( } z \\text { complex) } \\]\nIf \\(|z|=R\\), then\n\\[ |P(z)| \\geq R^{n}\\left[1-\\left|a_{n-1}\\right| R^{-1}-\\cdots-\\left|a_{0}\\right| R^{-n}\\right] . \\]\nThe right side of (56) tends to \\(\\infty\\) as \\(R \\rightarrow \\infty\\). Hence there exists \\(R_{0}\\) such that \\(|P(z)|\u0026gt;\\mu\\) if \\(|z|\u0026gt;R_{0}\\). Since \\(|P|\\) is continuous on the closed disc with center at 0 and radius \\(R_{0}\\), Theorem \\(4.16\\) shows that \\(\\left|P\\left(z_{0}\\right)\\right|=\\mu\\) for some \\(z_{0}\\). We claim that \\(\\mu=0\\). If not, put \\(Q(z)=P\\left(z+z_{0}\\right) / P\\left(z_{0}\\right)\\). Then \\(Q\\) is a nonconstant polynomial, \\(Q(0)=1\\), and \\(|Q(z)| \\geq 1\\) for all \\(z\\). There is a smallest integer \\(k\\), \\(1 \\leq k \\leq n\\), such that\n\\[ Q(z)=1+b_{k} z^{k}+\\cdots+b_{n} z^{n}, \\quad b_{k} \\neq 0 . \\]\nBy Theorem \\(8.7(d)\\) there is a real \\(\\theta\\) such that\n\\[ e^{i k \\theta} b_{k}=-\\left|b_{k}\\right| \\]\nIf \\(r\u0026gt;0\\) and \\(r^{k}\\left|b_{k}\\right|\u0026lt;1,(58)\\) implies\n\\[ \\left|1+b_{k} r^{k} e^{i k \\theta}\\right|=1-r^{k}\\left|b_{k}\\right| \\]\nso that\n\\[ \\left|Q\\left(r e^{i \\theta}\\right)\\right| \\leq 1-r^{k}\\left\\{\\left|b_{k}\\right|-r\\left|b_{k+1}\\right|-\\cdots-r^{n-k}\\left|b_{n}\\right|\\right\\} \\]\nFor sufficiently small \\(r\\), the expression in braces is positive; hence \\(\\left|Q\\left(r e^{i \\theta}\\right)\\right|\u0026lt;1\\), a contradiction.\nThus \\(\\mu=0\\), that is, \\(P\\left(z_{0}\\right)=0\\).\nExercise 27 contains a more general result.\n","date":"2022-08-10T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/4-the-algebraic-completeness-of-the-complex-filed/","section":"baby rudin","tags":null,"title":"4 THE ALGEBRAIC COMPLETENESS OF THE COMPLEX FILED"},{"categories":null,"contents":"8.9 Definition A trigonometric polynomial is a finite sum of the form\n\\[ f(x)=a_{0}+\\sum_{n=1}^{N}\\left(a_{n} \\cos n x+b_{n} \\sin n x\\right) \\quad(x \\text { real), } \\]\nwhere \\(a_{0}, \\ldots, a_{N}, b_{1}, \\ldots, b_{N}\\) are complex numbers. On account of the identities (46), (59) can also be written in the form\n\\[ f(x)=\\sum_{-N}^{N} c_{n} e^{i n x} \\quad(x \\text { real }), \\]\nwhich is more convenient for most purposes. It is clear that every trigonometric polynomial is periodic, with period \\(2 \\pi\\).\nIf \\(n\\) is a nonzero integer, \\(e^{i n x}\\) is the derivative of \\(e^{i n x} / i n\\), which also has period \\(2 \\pi\\). Hence\n\\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} e^{i n x} d x= \\begin{cases}1 \u0026amp; (\\text { if } n=0) \\\\ 0 \u0026amp; \\text { (if } n=\\pm 1, \\pm 2, \\ldots)\\end{cases} \\]\nLet us multiply (60) by \\(e^{-i m x}\\), where \\(m\\) is an integer; if we integrate the product, (61) shows that\n\\[ c_{m}=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(x) e^{-i m x} d x \\]\nfor \\(|m| \\leq N\\). If \\(|m|\u0026gt;N\\), the integral in (62) is 0 . The following observation can be read off from (60) and (62): The trigonometric polynomial \\(f\\), given by (60), is real if and only if \\(c_{-n}=\\bar{c}_{n}\\) for \\(n=0, \\ldots, N\\).\nIn agreement with (60), we define a trigonometric series to be a series of the form\n\\[ \\sum_{-\\infty}^{\\infty} c_{n} e^{i n x} \\quad(x \\text { real }) \\text {; } \\]\nthe \\(N\\) th partial sum of (63) is defined to be the right side of (60). If \\(f\\) is an integrable function on \\([-\\pi, \\pi]\\), the numbers \\(c_{m}\\) defined by (62) for all integers \\(m\\) are called the Fourier coefficients of \\(f\\), and the series (63) formed with these coefficients is called the Fourier series of \\(f\\).\nThe natural question which now arises is whether the Fourier series of \\(f\\) converges to \\(f\\), or, more generally, whether \\(f\\) is determined by its Fourier series. That is to say, if we know the Fourier coefficients of a function, can we find the function, and if so, how?\nThe study of such series, and, in particular, the problem of representing a given function by a trigonometric series, originated in physical problems such as the theory of oscillations and the theory of heat conduction (Fourier’s “Théorie analytique de la chaleur” was published in 1822). The many difficult and delicate problems which arose during this study caused a thorough revision and reformulation of the whole theory of functions of a real variable. Among many prominent names, those of Riemann, Cantor, and Lebesgue are intimately connected with this field, which nowadays, with all its generalizations and ramifications, may well be said to occupy a central position in the whole of analysis.\nWe shall be content to derive some basic theorems which are easily accessible by the methods developed in the preceding chapters. For more thorough investigations, the Lebesgue integral is a natural and indispensable tool.\nWe shall first study more general systems of functions which share a property analogous to (61).\n8.10 Definition Let \\(\\left\\{\\phi_{n}\\right\\}(n=1,2,3, \\ldots)\\) be a sequence of complex functions on \\([a, b]\\), such that\n\\[ \\int_{a}^{b} \\phi_{n}(x) \\overline{\\phi_{m}(x)} d x=0 \\quad(n \\neq m) \\]\nThen \\(\\left\\{\\phi_{n}\\right\\}\\) is said to be an orthogonal system of functions on \\([a, b]\\). If, in addition,\n\\[ \\int_{a}^{b}\\left|\\phi_{n}(x)\\right|^{2} d x=1 \\]\nfor all \\(n,\\left\\{\\phi_{n}\\right\\}\\) is said to be orthonormal. For example, the functions \\((2 \\pi)^{-\\frac{1}{2}} e^{i n x}\\) form an orthonormal system on \\([-\\pi, \\pi]\\). So do the real functions\n\\[ \\frac{1}{\\sqrt{2 \\pi}}, \\frac{\\cos x}{\\sqrt{\\pi}}, \\frac{\\sin x}{\\sqrt{\\pi}}, \\frac{\\cos 2 x}{\\sqrt{\\pi}}, \\frac{\\sin 2 x}{\\sqrt{\\pi}}, \\cdots . \\]\nIf \\(\\left\\{\\phi_{n}\\right\\}\\) is orthonormal on \\([a, b]\\) and if\n\\[ c_{n}=\\int_{a}^{b} f(t) \\overline{\\phi_{n}(t)} d t \\quad(n=1,2,3, \\ldots), \\]\nwe call \\(c_{n}\\) the \\(n\\)th Fourier coefficient of \\(f\\) relative to \\(\\left\\{\\phi_{n}\\right\\}\\). We write\n\\[ f(x) \\sim \\sum_{1}^{\\infty} c_{n} \\phi_{n}(x) \\]\nand call this series the Fourier series of \\(f\\) (relative to \\(\\left\\{\\phi_{n}\\right\\}\\) ).\nNote that the symbol \\(\\sim\\) used in (67) implies nothing about the convergence of the series; it merely says that the coefficients are given by (66).\nThe following theorems show that the partial sums of the Fourier series of \\(f\\) have a certain minimum property. We shall assume here and in the rest of this chapter that \\(f \\in \\mathscr{R}\\), although this hypothesis can be weakened.\n8.11 Theorem Let \\(\\left\\{\\phi_{n}\\right\\}\\) be orthonormal on \\([a, b]\\). Let\n\\[ s_{n}(x)=\\sum_{m=1}^{n} c_{m} \\phi_{m}(x) \\]\nbe the nth partial sum of the Fourier series of \\(f\\), and suppose\n\\[ t_{n}(x)=\\sum_{m=1}^{n} \\gamma_{m} \\phi_{m}(x) . \\]\nThen\n\\[ \\int_{a}^{b}\\left|f-s_{n}\\right|^{2} d x \\leq \\int_{a}^{b}\\left|f-t_{n}\\right|^{2} d x, \\]\nand equality holds if and only if\n\\[ \\gamma_{m}=c_{m} \\quad(m=1, \\ldots, n) . \\]\nThat is to say, among all functions \\(t_{n}\\), \\(s_{n}\\) gives the best possible mean square approximation to \\(f\\).\nProof Let \\(\\int\\) denote the integral over \\([a, b], \\Sigma\\) the sum from 1 to \\(n\\). Then\n\\[ \\int f \\bar{t}_{n}=\\int f \\sum \\bar{\\gamma}_{m} \\bar{\\phi}_{m}=\\sum c_{m} \\bar{\\gamma}_{m} \\]\nby the definition of \\(\\left\\{c_{m}\\right\\}\\),\n\\[ \\int\\left|t_{n}\\right|^{2}=\\int t_{n} \\bar{t}_{n}=\\int \\sum \\gamma_{m} \\phi_{m} \\sum \\bar{\\gamma}_{k} \\bar{\\phi}_{k}=\\sum\\left|\\gamma_{m}\\right|^{2} \\]\nsince \\(\\left\\{\\phi_{m}\\right\\}\\) is orthonormal, and so\n\\[ \\begin{aligned} \\int\\left|f-t_{n}\\right|^{2} \u0026amp;=\\int|f|^{2}-\\int f \\bar{t}_{n}-\\int f t_{n}+\\int\\left|t_{n}\\right|^{2} \\\\ \u0026amp;=\\int|f|^{2}-\\sum c_{m} \\bar{\\gamma}_{m}-\\sum \\bar{c}_{m} \\gamma_{m}+\\sum \\gamma_{m} \\bar{\\gamma}_{m} \\\\ \u0026amp;=\\int|f|^{2}-\\sum\\left|c_{m}\\right|^{2}+\\sum\\left|\\gamma_{m}-c_{m}\\right|^{2} \\end{aligned} \\]\nwhich is evidently minimized if and only if \\(\\gamma_{m}=c_{m}\\). Putting \\(\\gamma_{m}=c_{m}\\) in this calculation, we obtain\n\\[ \\begin{aligned} \u0026amp;\\int_{a}^{b}\\left|s_{n}(x)\\right|^{2} d x=\\sum_{1}^{n}\\left|c_{m}\\right|^{2} \\leq \\int_{a}^{b}|f(x)|^{2} d x, \\\\ \u0026amp;\\text { since } \\int\\left|f-t_{n}\\right|^{2} \\geq 0 \\end{aligned} \\]\n8.12 Theorem If \\(\\left\\{\\phi_{n}\\right\\}\\) is orthonormal on \\([a, b]\\), and if\n\\[ f(x) \\sim \\sum_{n=1}^{\\infty} c_{n} \\phi_{n}(x) \\]\nthen\n\\[ \\sum_{n=1}^{\\infty}\\left|c_{n}\\right|^{2} \\leq \\int_{a}^{b}|f(x)|^{2} d x \\]\nIn particular,\n\\[ \\lim _{n \\rightarrow \\infty} c_{n}=0 . \\]\nProof Letting \\(n \\rightarrow \\infty\\) in (72), we obtain (73), the so-called “Bessel inequality.”\n8.13 Trigonometric series From now on we shall deal only with the trigonometric system. We shall consider functions \\(f\\) that have period \\(2 \\pi\\) and that are Riemann-integrable on \\([-\\pi, \\pi]\\) (and hence on every bounded interval). The Fourier series of \\(f\\) is then the series (63) whose coefficients \\(c_{n}\\) are given by the integrals \\((62)\\), and\n\\[ s_{N}(x)=s_{N}(f ; x)=\\sum_{-N}^{N} c_{n} e^{i n x} \\]\nis the \\(N\\) th partial sum of the Fourier series of \\(f\\). The inequality (72) now takes the form\n\\(\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|s_{N}(x)\\right|^{2} d x=\\sum_{-N}^{N}\\left|c_{n}\\right|^{2} \\leq \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}|f(x)|^{2} d x\\). In order to obtain an expression for \\(s_{N}\\) that is more manageable than (75) we introduce the Dirichlet kernel\n\\[ D_{N}(x)=\\sum_{n=-N}^{N} e^{i n x}=\\frac{\\sin \\left(N+\\frac{1}{2}\\right) x}{\\sin (x / 2)} . \\]\nThe first of these equalities is the definition of \\(D_{N}(x)\\). The second follows if both sides of the identity\n\\[ \\left(e^{i x}-1\\right) D_{N}(x)=e^{i(N+1) x}-e^{-i N x} \\]\nare multiplied by \\(e^{-i x / 2}\\). By (62) and (75), we have\n\\[ \\begin{aligned} s_{N}(f ; x) \u0026amp;=\\sum_{-N}^{N} \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(t) e^{-i n t} d t e^{i n x} \\\\ \u0026amp;=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(t) \\sum_{-N}^{N} e^{i n(x-t)} d t \\end{aligned} \\]\nso that\n\\[ s_{N}(f ; x)=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(t) D_{N}(x-t) d t=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(x-t) D_{N}(t) d t \\]\nThe periodicity of all functions involved shows that it is immaterial over which interval we integrate, as long as its length is \\(2 \\pi\\). This shows that the two integrals in (78) are equal.\nWe shall prove just one theorem about the pointwise convergence of Fourier series.\n8.14 Theorem If, for some \\(x\\), there are constants \\(\\delta\u0026gt;0\\) and \\(M\u0026lt;\\infty\\) such that\n\\[ |f(x+t)-f(x)| \\leq M|t| \\]\nfor all \\(t \\in(-\\delta, \\delta)\\), then\n\\[ \\lim _{N \\rightarrow \\infty} s_{N}(f ; x)=f(x) \\]\nProof Define\n\\[ g(t)=\\frac{f(x-t)-f(x)}{\\sin (t / 2)} \\]\nfor \\(0\u0026lt;|t| \\leq \\pi\\), and put \\(g(0)=0\\). By the definition (77),\n\\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} D_{N}(x) d x=1 . \\]\nHence (78) shows that\n\\[ \\begin{aligned} s_{N}(f ; x) \u0026amp;-f(x)=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} g(t) \\sin \\left(N+\\frac{1}{2}\\right) t d t \\\\ \u0026amp;=\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left[g(t) \\cos \\frac{t}{2}\\right] \\sin N t d t+\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left[g(t) \\sin \\frac{t}{2}\\right] \\cos N t d t \\end{aligned} \\]\nBy (79) and (81), \\(g(t) \\cos (t / 2)\\) and \\(g(t) \\sin (t / 2)\\) are bounded. The last two integrals thus tend to 0 as \\(N \\rightarrow \\infty\\), by (74). This proves \\((80)\\).\nCorollary If \\(f(x)=0\\) for all \\(x\\) in some segment \\(J\\), then \\(\\lim s_{N}(f ; x)=0\\) for every \\(x \\in J\\). Here is another formulation of this corollary: If \\(f(t)=g(t)\\) for all \\(t\\) in some neighborhood of \\(x\\), then\n\\[ s_{N}(f ; x)-s_{N}(g ; x)=s_{N}(f-g ; x) \\rightarrow 0 \\text { as } N \\rightarrow \\infty . \\]\nThis is usually called the localization theorem. It shows that the behavior of the sequence \\(\\left\\{s_{N}(f ; x)\\right\\}\\), as far as convergence is concerned, depends only on the values of \\(f\\) in some (arbitrarily small) neighborhood of \\(x\\). Two Fourier series may thus have the same behavior in one interval, but may behave in entirely different ways in some other interval. We have here a very striking contrast between Fourier series and power series (Theorem 8.5).\nWe conclude with two other approximation theorems.\n8.15 Theorem If \\(f\\) is continuous (with period \\(2 \\pi\\) ) and if \\(\\varepsilon\u0026gt;0\\), then there is a trigonometric polynomial \\(P\\) such that\n\\[ |P(x)-f(x)|\u0026lt;\\varepsilon \\]\nfor all real \\(x\\).\nProof If we identify \\(x\\) and \\(x+2 \\pi\\), we may regard the \\(2 \\pi\\)-periodic functions on \\(R^{1}\\) as functions on the unit circle \\(T\\), by means of the mapping \\(x \\rightarrow e^{i x}\\). The trigonometric polynomials, i.e., the functions of the form (60), form a self-adjoint algebra \\(\\mathscr{A}\\), which separates points on \\(T\\), and which vanishes at no point of \\(T\\). Since \\(T\\) is compact, Theorem \\(7.33\\) tells us that \\(\\mathscr{A}\\) is dense in \\(\\mathscr{C}(T)\\). This is exactly what the theorem asserts.\nA more precise form of this theorem appears in Exercise \\(15 .\\)\n8.16 Parseval’s theorem Suppose \\(f\\) and \\(g\\) are Riemann-integrable functions with period \\(2 \\pi\\), and\n\\[ f(x) \\sim \\sum_{-\\infty}^{\\infty} c_{n} e^{i n x}, \\quad g(x) \\sim \\sum_{-\\infty}^{\\infty} \\gamma_{n} e^{i n x} \\]\nThen\n\\[ \\begin{aligned} \\lim _{N \\rightarrow \\infty} \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}\\left|f(x)-s_{N}(f ; x)\\right|^{2} d x \u0026amp;=0 \\\\ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} f(x) \\overline{g(x)} d x \u0026amp;=\\sum_{-\\infty}^{\\infty} c_{n} \\bar{\\gamma}_{n} \\\\ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}|f(x)|^{2} d x \u0026amp;=\\sum_{-\\infty}^{\\infty}\\left|c_{n}\\right|^{2} \\end{aligned} \\]\nProof Let us use the notation\n\\[ \\|h\\|_{2}=\\left\\{\\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi}|h(x)|^{2} d x\\right\\}^{1 / 2} \\]\nLet \\(\\varepsilon\u0026gt;0\\) be given. Since \\(f \\in \\mathscr{R}\\) and \\(f(\\pi)=f(-\\pi)\\), the construction described in Exercise 12 of Chap. 6 yields a continuous \\(2 \\pi\\)-periodic function \\(h\\) with\n\\[ \\|f-h\\|_{2}\u0026lt;\\varepsilon . \\]\nBy Theorem \\(8.15\\), there is a trigonometric polynomial \\(P\\) such that \\(|h(x)-P(x)|\u0026lt;\\varepsilon\\) for all \\(x\\). Hence \\(\\|h-P\\|_{2}\u0026lt;\\varepsilon\\). If \\(P\\) has degree \\(N_{0}\\), Theorem \\(8.11\\) shows that\n\\[ \\left\\|h-s_{N}(h)\\right\\|_{2} \\leq\\|h-P\\|_{2}\u0026lt;\\varepsilon \\]\nfor all \\(N \\geq N_{0}\\). By \\((72)\\), with \\(h-f\\) in place of \\(f\\),\n\\[ \\left\\|s_{N}(h)-s_{N}(f)\\right\\|_{2}=\\left\\|s_{N}(h-f)\\right\\|_{2} \\leq\\|h-f\\|_{2}\u0026lt;\\varepsilon . \\]\nNow the triangle inequality (Exercise 11, Chap. 6), combined with \\((87),(88)\\), and \\((89)\\), shows that\n\\[ \\left\\|f-s_{N}(f)\\right\\|_{2}\u0026lt;3 \\varepsilon \\quad\\left(N \\geq N_{0}\\right) \\]\nThis proves (83). Next,\n\\[ \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} s_{N}(f) \\bar{g} d x=\\sum_{-N}^{N} c_{n} \\frac{1}{2 \\pi} \\int_{-\\pi}^{\\pi} e^{i n x} \\overline{g(x)} d x=\\sum_{-N}^{N} c_{n} \\bar{\\gamma}_{n} \\]\nand the Schwarz inequality shows that\n\\[ \\left|\\int f \\bar{g}-\\int s_{N}(f) \\bar{g}\\right| \\leq \\int\\left|f-s_{N}(f) \\| g\\right| \\leq\\left\\{\\int\\left|f-s_{N}\\right|^{2} \\int|g|^{2}\\right\\}^{1 / 2} \\]\nwhich tends to 0 , as \\(N \\rightarrow \\infty\\), by (83). Comparison of (91) and (92) gives (84). Finally, (85) is the special case \\(g=f\\) of (84). A more general version of Theorem \\(8.16\\) appears in Chap. \\(11 .\\)\n","date":"2022-08-10T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/5-fourier-series/","section":"baby rudin","tags":null,"title":"5 FOURIER SERIES.md"},{"categories":null,"contents":"A satisfactory discussion of the main concepts of analysis (such as convergence, continuity, differentiation, and integration) must be based on an accurately defined number concept. We shall not, however, enter into any discussion of the axioms that govern the arithmetic of the integers, but assume familiarity with the rational numbers (i.e., the numbers of the form \\(m / n\\), where \\(m\\) and \\(n\\) are integers and \\(n \\neq 0\\) ).\nThe rational number system is inadequate for many purposes, both as a field and as an ordered set. (These terms will be defined in Secs. \\(1.6\\) and 1.12.) For instance, there is no rational \\(p\\) such that \\(p^{2}=2\\). (We shall prove this presently.) This leads to the introduction of so-called “irrational numbers” which are often written as infinite decimal expansions and are considered to be “approximated” by the corresponding finite decimals. Thus the sequence\n\\[ 1,1.4,1.41,1.414,1.4142, \\ldots \\]\n“tends to \\(\\sqrt{2}\\).” But unless the irrational number \\(\\sqrt{2}\\) has been clearly defined, the question must arise: Just what is it that this sequence “tends to”?\nThis sort of question can be answered as soon as the so-called “real number system” is constructed.\n1.1 Example We now show that the equation\n\\[ p^{2}=2 \\]\nis not satisfied by any rational \\(p\\). If there were such a \\(p\\), we could write \\(p=m / n\\) where \\(m\\) and \\(n\\) are integers that are not both even. Let us assume this is done. Then (1) implies\n\\[ m^{2}=2 n^{2}, \\]\nThis shows that \\(m^{2}\\) is even. Hence \\(m\\) is even (if \\(m\\) were odd, \\(m^{2}\\) would be odd), and so \\(m^{2}\\) is divisible by 4 . It follows that the right side of \\((2)\\) is divisible by 4 , so that \\(n^{2}\\) is even, which implies that \\(n\\) is even.\nThe assumption that (1) holds thus leads to the conclusion that both \\(m\\) and \\(n\\) are even, contrary to our choice of \\(m\\) and \\(n\\). Hence (1) is impossible for rational \\(p\\).\nWe now examine this situation a little more closely. Let \\(A\\) be the set of all positive rationals \\(p\\) such that \\(p^{2}\u0026lt;2\\) and let \\(B\\) consist of all positive rationals \\(p\\) such that \\(p^{2}\u0026gt;2\\). We shall show that \\(A\\) contains no largest number and \\(B\\) contains no smallest.\nMore explicitly, for every \\(p\\) in \\(A\\) we can find a rational \\(q\\) in \\(A\\) such that \\(p\u0026lt;q\\), and for every \\(p\\) in \\(B\\) we can find a rational \\(q\\) in \\(B\\) such that \\(q\u0026lt;p\\).\nTo do this, we associate with each rational \\(p\u0026gt;0\\) the number\n\\[ q=p-\\frac{p^{2}-2}{p+2}=\\frac{2 p+2}{p+2} \\]\nThen\n\\[ q^{2}-2=\\frac{2\\left(p^{2}-2\\right)}{(p+2)^{2}} \\]\nIf \\(p\\) is in \\(A\\) then \\(p^{2}-2\u0026lt;0\\), (3) shows that \\(q\u0026gt;p\\), and (4) shows that \\(q^{2}\u0026lt;2\\). Thus \\(q\\) is in \\(A\\).\nIf \\(p\\) is in \\(B\\) then \\(p^{2}-2\u0026gt;0\\), (3) shows that \\(0\u0026lt;q\u0026lt;p\\), and (4) shows that \\(q^{2}\u0026gt;2\\). Thus \\(q\\) is in \\(B\\).\n1.2 Remark The purpose of the above discussion has been to show that the rational number system has certain gaps, in spite of the fact that between any two rationals there is another: If \\(r\u0026lt;s\\) then \\(r\u0026lt;(r+s) / 2\u0026lt;s\\). The real number system fills these gaps. This is the principal reason for the fundamental role which it plays in analysis.\nIn order to elucidate its structure, as well as that of the complex numbers, we start with a brief discussion of the general concepts of ordered set and field. Here is some of the standard set-theoretic terminology that will be used throughout this book.\n\\(1.3\\) Definitions If \\(A\\) is any set (whose elements may be numbers or any other objects), we write \\(x \\in A\\) to indicate that \\(x\\) is a member (or an element) of \\(A\\). If \\(x\\) is not a member of \\(A\\), we write: \\(x \\notin A\\).\nThe set which contains no element will be called the empty set. If a set has at least one element, it is called nonempty.\nIf \\(A\\) and \\(B\\) are sets, and if every element of \\(A\\) is an element of \\(B\\), we say that \\(A\\) is a subset of \\(B\\), and write \\(A \\subset B\\), or \\(B \\supset A\\). If, in addition, there is an element of \\(B\\) which is not in \\(A\\), then \\(A\\) is said to be a proper subset of \\(B\\). Note that \\(A \\subset A\\) for every set \\(A\\).\nIf \\(A \\subset B\\) and \\(B \\subset A\\), we write \\(A=B\\). Otherwise \\(A \\neq B\\).\n\\(1.4\\) Definition Throughout Chap. 1, the set of all rational numbers will be denoted by \\(Q\\).\n","date":"2022-08-08T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/0-intro/","section":"baby rudin","tags":null,"title":"0 INTRO"},{"categories":null,"contents":" youtube: part1 part2 bilibili: part1 part2 note: pdf xopp 1.5 Definition Let \\(S\\) be a set. An order on \\(S\\) is a relation, denoted by \\(\u0026lt;\\), with the following two properties:\nIf \\(x \\in S\\) and \\(y \\in S\\) then one and only one of the statements \\[ x\u0026lt;y, \\quad x=y, \\quad y\u0026lt;x \\]\nis true.\nIf \\(x, y, z \\in S\\), if \\(x\u0026lt;y\\) and \\(y\u0026lt;z\\), then \\(x\u0026lt;z\\). The statement ” \\(x\u0026lt;y\\) ” may be read as ” \\(x\\) is less than \\(y\\) ” or ” \\(x\\) is smaller than \\(y\\) ” or ” \\(x\\) precedes \\(y\\) “.\nIt is often convenient to write \\(y\u0026gt;x\\) in place of \\(x\u0026lt;y\\).\nThe notation \\(x \\leq y\\) indicates that \\(x\u0026lt;y\\) or \\(x=y\\), without specifying which of these two is to hold. In other words, \\(x \\leq y\\) is the negation of \\(x\u0026gt;y\\).\n1.6 Definition An ordered set is a set \\(S\\) in which an order is defined.\nFor example, \\(Q\\) is an ordered set if \\(r\u0026lt;s\\) is defined to mean that \\(s-r\\) is a positive rational number.\n1.7 Definition Suppose \\(S\\) is an ordered set, and \\(E \\subset S\\). If there exists a \\(\\beta \\in S\\) such that \\(x \\leq \\beta\\) for every \\(x \\in E\\), we say that \\(E\\) is bounded above, and call \\(\\beta\\) an upper bound of \\(E\\). Lower bounds are defined in the same way (with \\(\\geq\\) in place of \\(\\leq\\) ).\n\\(1.8\\) Definition Suppose \\(S\\) is an ordered set, \\(E \\subset S\\), and \\(E\\) is bounded above. Suppose there exists an \\(\\alpha \\in S\\) with the following properties:\n\\(\\alpha\\) is an upper bound of \\(E\\).\nIf \\(\\gamma\u0026lt;\\alpha\\) then \\(\\gamma\\) is not an upper bound of \\(E\\).\nThen \\(\\alpha\\) is called the least upper bound of \\(E\\) [that there is at most one such \\(\\alpha\\) is clear from (ii)] or the supremum of \\(E\\), and we write\n\\[ \\alpha=\\sup E . \\]\nThe greatest lower bound, or infimum, of a set \\(E\\) which is bounded below is defined in the same manner: The statement\n\\[ \\alpha=\\inf E \\]\nmeans that \\(\\alpha\\) is a lower bound of \\(E\\) and that no \\(\\beta\\) with \\(\\beta\u0026gt;\\alpha\\) is a lower bound of \\(E\\).\n\\(1.9\\) Examples\nConsider the sets \\(A\\) and \\(B\\) of Example \\(1.1\\) as subsets of the ordered set \\(Q\\). The set \\(A\\) is bounded above. In fact, the upper bounds of \\(A\\) are exactly the members of \\(B\\). Since \\(B\\) contains no smallest member, \\(A\\) has no least upper bound in \\(Q\\). Similarly, \\(B\\) is bounded below: The set of all lower bounds of \\(B\\) consists of \\(A\\) and of all \\(r \\in Q\\) with \\(r \\leq 0\\). Since \\(A\\) has no lasgest member, \\(B\\) has no greatest lower bound in \\(Q\\).\nIf \\(\\alpha=\\sup E\\) exists, then \\(\\alpha\\) may or may not be a member of \\(E\\). For instance, let \\(E_{1}\\) be the set of all \\(r \\in Q\\) with \\(r\u0026lt;0\\). Let \\(E_{2}\\) be the set of all \\(r \\in Q\\) with \\(r \\leq 0\\). Then \\[ \\sup E_{1}=\\sup E_{2}=0, \\]\nand \\(0 \\notin E_{1}, 0 \\in \\mathrm{E}_{2}\\). (c) Let \\(E\\) consist of all numbers \\(1 / n\\), where \\(n=1,2,3, \\ldots\\) Then \\(\\sup E=1\\), which is in \\(E\\), and inf \\(E=0\\), which is not in \\(E\\).\n1.10 Definition An ordered set \\(S\\) is said to have the least-upper-bound property if the following is true:\nIf \\(E \\subset \\mathrm{S}, E\\) is not empty, and \\(E\\) is bounded above, then sup \\(E\\) exists in \\(S\\).\nExample 1.9(a) shows that \\(Q\\) does not have the least-upper-bound property. We shall now show that there is a close relation between greatest lower bounds and least upper bounds, and that every ordered set with the least-upperbound property also has the greatest-lower-bound property.\n\\(1.11\\) Theorem Suppose \\(S\\) is an ordered set with the least-upper-bound property, \\(B \\subset S, B\\) is not empty, and \\(B\\) is bounded below. Let \\(L\\) be the set of all lower bounds of \\(B\\). Then exists in \\(S\\), and \\(\\alpha=\\inf B\\).\n\\[ \\alpha=\\sup L \\]\nIn particular, inf \\(B\\) exists in \\(S\\).\nProof Since \\(B\\) is bounded below, \\(L\\) is not empty. Since \\(L\\) consists of exactly those \\(y \\in S\\) which satisfy the inequality \\(y \\leq x\\) for every \\(x \\in B\\), we see that every \\(x \\in B\\) is an upper bound of \\(L\\). Thus \\(L\\) is bounded above. Our hypothesis about \\(S\\) implies therefore that \\(L\\) has a supremum in \\(S\\); call it \\(\\alpha\\).\nIf \\(\\gamma\u0026lt;\\alpha\\) then (see Definition 1.8) \\(\\gamma\\) is not an upper bound of \\(L\\), hence \\(\\gamma \\notin B\\). It follows that \\(\\alpha \\leq x\\) for every \\(x \\in B\\). Thus \\(\\alpha \\in L\\).\nIf \\(\\alpha\u0026lt;\\beta\\) then \\(\\beta \\notin L\\), since \\(\\alpha\\) is an upper bound of \\(L\\).\nWe have shown that \\(\\alpha \\in L\\) but \\(\\beta \\notin L\\) if \\(\\beta\u0026gt;\\alpha\\). In other words, \\(\\alpha\\) is a lower bound of \\(B\\), but \\(\\beta\\) is not if \\(\\beta\u0026gt;\\alpha\\). This means that \\(\\alpha=\\inf B\\).\n","date":"2022-08-08T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch1/1-ordered-sets/","section":"baby rudin","tags":null,"title":"1 ORDERED SETS"},{"categories":null,"contents":"In this section we shall derive some properties of functions which are represented by power series, i.e., functions of the form\n\\[ f(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n} \\]\nor, more generally,\n\\[ f(x)=\\sum_{n=0}^{\\infty} c_{n}(x-a)^{n} . \\]\nThese are called analytic functions. We shall restrict ourselves to real values of \\(x\\). Instead of circles of convergence (see Theorem 3.39) we shall therefore encounter intervals of convergence.\nIf (1) converges for all \\(x\\) in \\((-R, R)\\), for some \\(R\u0026gt;0(R\\) may be \\(+\\infty)\\), we say that \\(f\\) is expanded in a power series about the point \\(x=0\\). Similarly, if (2) converges for \\(|x-a|\u0026lt;R, f\\) is said to be expanded in a power series about the point \\(x=a\\). As a matter of convenience, we shall often take \\(a=0\\) without any loss of generality.\n8.1 Theorem Suppose the series \\[ \\sum_{n=0}^{\\infty} c_{n} x^{n} \\] converges for \\(|x|\u0026lt;R\\), and define \\[ f(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n} \\quad(|x|\u0026lt;R) . \\] Then (3) converges uniformly on \\([-R+\\varepsilon, R-\\varepsilon]\\), no matter which \\(\\varepsilon\u0026gt;0\\) is chosen. The function \\(f\\) is continuous and differentiable in \\((-R, R)\\), and \\[ f^{\\prime}(x)=\\sum_{n=1}^{\\infty} n c_{n} x^{n-1} \\quad(|x|\u0026lt;R) . \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. For \\(|x| \\leq R-\\varepsilon\\), we have\n\\[ \\left|c_{n} x^{n}\\right| \\leq\\left|c_{n}(R-\\varepsilon)^{n}\\right| \\]\nand since\n\\[ \\Sigma c_{n}(R-\\varepsilon)^{n} \\]\nconverges absolutely (every power series converges absolutely in the interior of its interval of convergence, by the root test), Theorem \\(7.10\\) shows the uniform convergence of \\((3)\\) on \\([-R+\\varepsilon, R-\\varepsilon]\\). Since \\(\\sqrt[n]{n} \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\), we have\n\\[ \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{n\\left|c_{n}\\right|}=\\limsup _{n \\rightarrow \\infty} \\sqrt[n]{\\left|c_{n}\\right|} \\]\nso that the series (4) and (5) have the same interval of convergence. Since (5) is a power series, it converges uniformly in \\([-R+\\varepsilon\\), \\(R-\\varepsilon\\) ], for every \\(\\varepsilon\u0026gt;0\\), and we can apply Theorem \\(7.17\\) (for series instead of sequences). It follows that (5) holds if \\(|x| \\leq R-\\varepsilon\\).\nBut, given any \\(x\\) such that \\(|x|\u0026lt;R\\), we can find an \\(\\varepsilon\u0026gt;0\\) such that \\(|x|\u0026lt;R-\\varepsilon\\). This shows that (5) holds for \\(|x|\u0026lt;R\\). Continuity of \\(f\\) follows from the existence of \\(f^{\\prime}\\) (Theorem 5.2).\nCorollary Under the hypotheses of Theorem 8.1, \\(f\\) has derivatives of all orders in \\((-R, R)\\), which are given by \\[ f^{(k)}(x)=\\sum_{n=k}^{\\infty} n(n-1) \\cdots(n-k+1) c_{n} x^{n-k} \\] In particular, \\[ f^{(k)}(0)=k ! c_{k} \\quad(k=0,1,2, \\ldots) \\text {. } \\] (Here \\(f^{(0)}\\) means \\(f\\), and \\(f^{(k)}\\) is the \\(k\\) th derivative of \\(f\\), for \\(k=1,2,3, \\ldots\\) ).\nProof Equation (6) follows if we apply Theorem \\(8.1\\) successively to \\(f\\), \\(f^{\\prime}, f^{\\prime \\prime}, \\ldots\\). Putting \\(x=0\\) in (6), we obtain (7).\nFormula (7) is very interesting. It shows, on the one hand, that the coefficients of the power series development of \\(f\\) are determined by the values of \\(f\\) and of its derivatives at a single point. On the other hand, if the coefficients are given, the values of the derivatives of \\(f\\) at the center of the interval of convergence can be read off immediately from the power series.\nNote, however, that although a function \\(f\\) may have derivatives of all orders, the series \\(\\Sigma c_{n} x^{n}\\), where \\(c_{n}\\) is computed by (7), need not converge to \\(f(x)\\) for any \\(x \\neq 0\\). In this case, \\(f\\) cannot be expanded in a power series about \\(x=0\\). For if we had \\(f(x)=\\Sigma a_{n} x^{n}\\), we should have\n\\[ n ! a_{n}=f^{(n)}(0) ; \\]\nhence \\(a_{n}=c_{n}\\). An example of this situation is given in Exercise 1 .\nIf the series (3) converges at an endpoint, say at \\(x=R\\), then \\(f\\) is continuous not only in \\((-R, R)\\), but also at \\(x=R\\). This follows from Abel’s theorem (for simplicity of notation, we take \\(R=1\\) ):\n8.2 Theorem Suppose \\(\\Sigma c_{n}\\) converges. Put \\[ f(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n} \\quad(-1\u0026lt;x\u0026lt;1) . \\]\nThen\n\\[ \\lim _{x \\rightarrow 1} f(x)=\\sum_{n=0}^{\\infty} c_{n} . \\]\nProof Let \\(s_{n}=c_{0}+\\cdots+c_{n}, s_{-1}=0\\). Then\n\\[ \\sum_{n=0}^{m} c_{n} x^{n}=\\sum_{n=0}^{m}\\left(s_{n}-s_{n-1}\\right) x^{n}=(1-x) \\sum_{n=0}^{m-1} s_{n} x^{n}+s_{m} x^{m} \\]\nFor \\(|x|\u0026lt;1\\), we let \\(m \\rightarrow \\infty\\) and obtain\n\\[ f(x)=(1-x) \\sum_{n=0}^{\\infty} s_{n} x^{n} . \\]\nSuppose \\(s=\\lim _{n \\rightarrow \\infty} s_{n}\\). Let \\(\\varepsilon\u0026gt;0\\) be given. Choose \\(N\\) so that \\(n\u0026gt;N\\) implies\n\\[ \\left|s-s_{n}\\right|\u0026lt;\\frac{\\varepsilon}{2} . \\]\nThen, since\n\\[ (1-x) \\sum_{n=0}^{\\infty} x^{n}=1 \\quad(|x|\u0026lt;1), \\]\nwe obtain from (9)\n\\[ |f(x)-s|=\\left|(1-x) \\sum_{n=0}^{\\infty}\\left(s_{n}-s\\right) x^{n}\\right| \\leq(1-x) \\sum_{n=0}^{N}\\left|s_{n}-s\\right||x|^{n}+\\frac{\\varepsilon}{2} \\leq \\varepsilon \\]\nif \\(x\u0026gt;1-\\delta\\), for some suitably chosen \\(\\delta\u0026gt;0\\). This implies (8). As an application, let us prove Theorem 3.51, which asserts: If \\(\\Sigma a_{n}, \\Sigma b_{n}\\), \\(\\Sigma c_{n}\\), converge to \\(A, B, C\\), and if \\(c_{n}=a_{0} b_{n}+\\cdots+a_{n} b_{0}\\), then \\(C=A B\\). We let\n\\[ f(x)=\\sum_{n=0}^{\\infty} a_{n} x^{n}, \\quad g(x)=\\sum_{n=0}^{\\infty} b_{n} x^{n}, \\quad h(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n}, \\]\nfor \\(0 \\leq x \\leq 1\\). For \\(x\u0026lt;1\\), these series converge absolutely and hence may be multiplied according to Definition 3.48; when the multiplication is carried out, we see that\n\\[ f(x) \\cdot g(x)=h(x) \\quad(0 \\leq x\u0026lt;1) . \\]\nBy Theorem 8.2,\n\\[ f(x) \\rightarrow A, \\quad g(x) \\rightarrow B, \\quad h(x) \\rightarrow C \\]\nas \\(x \\rightarrow 1\\). Equations (10) and (11) imply \\(A B=C\\). We now require a theorem concerning an inversion in the order of summation. (See Exercises 2 and 3.)\n8.3 Theorem Given a double sequence \\(\\left\\{a_{i j}\\right\\}, i=1,2,3, \\ldots, j=1,2,3, \\ldots\\), suppose that \\[ \\sum_{j=1}^{\\infty}\\left|a_{i j}\\right|=b_{i} \\quad(i=1,2,3, \\ldots) \\] and \\(\\Sigma b_{i}\\) converges. Then \\[ \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} a_{i j}=\\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} a_{i j} . \\]\nProof We could establish (13) by a direct procedure similar to (although more involved than) the one used in Theorem 3.55. However, the following method seems more interesting.\nLet \\(E\\) be a countable set, consisting of the points \\(x_{0}, x_{1}, x_{2}, \\ldots\\), and suppose \\(x_{n} \\rightarrow x_{0}\\) as \\(n \\rightarrow \\infty\\). Define\n\\[ \\begin{aligned} f_{i}\\left(x_{0}\\right)=\\sum_{j=1}^{\\infty} a_{i j} \u0026amp;(i=1,2,3, \\ldots), \\\\ f_{i}\\left(x_{n}\\right)=\\sum_{j=1}^{n} a_{i j} \u0026amp;(i, n=1,2,3, \\ldots), \\\\ g(x)=\\sum_{i=1}^{\\infty} f_{i}(x) \u0026amp;(x \\in E) . \\end{aligned} \\]\nNow, (14) and (15), together with (12), show that each \\(f_{i}\\) is continuous at \\(x_{0}\\). Since \\(\\left|f_{i}(x)\\right| \\leq b_{i}\\) for \\(x \\in E\\), (16) converges uniformly, so that \\(g\\) is continuous at \\(x_{0}\\) (Theorem 7.11). It follows that\n\\[ \\begin{aligned} \\sum_{i=1}^{\\infty} \\sum_{j=1}^{\\infty} a_{i j} \u0026amp;=\\sum_{i=1}^{\\infty} f_{i}\\left(x_{0}\\right)=g\\left(x_{0}\\right)=\\lim _{n \\rightarrow \\infty} g\\left(x_{n}\\right) \\\\ \u0026amp;=\\lim _{n \\rightarrow \\infty} \\sum_{i=1}^{\\infty} f_{i}\\left(x_{n}\\right)=\\lim _{n \\rightarrow \\infty} \\sum_{i=1}^{\\infty} \\sum_{j=1}^{n} a_{i j} \\\\ \u0026amp;=\\lim _{n \\rightarrow \\infty} \\sum_{j=1}^{n} \\sum_{i=1}^{\\infty} a_{i j}=\\sum_{j=1}^{\\infty} \\sum_{i=1}^{\\infty} a_{i j} \\end{aligned} \\]\n8.4 Theorem Suppose \\[ f(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n}, \\] the series converging in \\(|x|\u0026lt;R\\). If \\(-R\u0026lt;a\u0026lt;R\\), then \\(f\\) can be expanded in a power series about the point \\(x=a\\) which converges in \\(|x-a|\u0026lt;R-|a|\\), and\n\\[ f(x)=\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^{n} \\quad(|x-a|\u0026lt;R-|a|) . \\]\nThis is an extension of Theorem \\(5.15\\) and is also known as Taylor’s theorem.\nProof We have \\[ \\begin{aligned} f(x) \u0026amp;=\\sum_{n=0}^{\\infty} c_{n}[(x-a)+a]^{n} \\\\ \u0026amp;=\\sum_{n=0}^{\\infty} c_{n} \\sum_{m=0}^{n}\\left(\\begin{array}{c} n \\\\ m \\end{array}\\right) a^{n-m}(x-a)^{m} \\\\ \u0026amp;=\\sum_{m=0}^{\\infty}\\left[\\sum_{n=m}^{\\infty}\\left(\\begin{array}{c} n \\\\ m \\end{array}\\right) c_{n} a^{n-m}\\right](x-a)^{m} . \\end{aligned} \\]\nThis is the desired expansion about the point \\(x=a\\). To prove its validity, we have to justify the change which was made in the order of summation. Theorem \\(8.3\\) shows that this is permissible if\n\\[ \\sum_{n=0}^{\\infty} \\sum_{m=0}^{n}\\left|c_{n}\\left(\\begin{array}{c} n \\\\ m \\end{array}\\right) a^{n-m}(x-a)^{m}\\right| \\]\nconverges. But (18) is the same as\n\\[ \\sum_{n=0}^{\\infty}\\left|c_{n}\\right| \\cdot(|x-a|+|a|)^{n} \\]\nand (19) converges if \\(|x-a|+|a|\u0026lt;R\\). Finally, the form of the coefficients in (17) follows from (7). It should be noted that (17) may actually converge in a larger interval than the one given by \\(|x-a|\u0026lt;R-|a|\\).\nIf two power series converge to the same function in \\((-R, R),(7)\\) shows that the two series must be identical, i.e., they must have the same coefficients. It is interesting that the same conclusion can be deduced from much weaker hypotheses:\n8.5 Theorem Suppose the series \\(\\Sigma a_{n} x^{n}\\) and \\(\\Sigma b_{n} x^{n}\\) converge in the segment \\(S=(-R, R)\\). Let \\(E\\) be the set of all \\(x \\in S\\) at which \\[ \\sum_{n=0}^{\\infty} a_{n} x^{n}=\\sum_{n=0}^{\\infty} b_{n} x^{n} \\] If \\(E\\) has a limit point in \\(S\\), then \\(a_{n}=b_{n}\\) for \\(n=0,1,2, \\ldots\\).Hence (20) holds for all \\(x \\in S\\).\nProof Put \\(c_{n}=a_{n}-b_{n}\\) and\n\\[ f(x)=\\sum_{n=0}^{\\infty} c_{n} x^{n} \\quad(x \\in S) . \\]\nThen \\(f(x)=0\\) on \\(E\\).\nLet \\(A\\) be the set of all limit points of \\(E\\) in \\(S\\), and let \\(B\\) consist of all other points of \\(S\\). It is clear from the definition of “limit point” that \\(B\\) is open. Suppose we can prove that \\(A\\) is open. Then \\(A\\) and \\(B\\) are disjoint open sets. Hence they are separated (Definition 2.45). Since \\(S=A \\cup B\\), and \\(S\\) is connected, one of \\(A\\) and \\(B\\) must be empty. By hypothesis, \\(A\\) is not empty. Hence \\(B\\) is empty, and \\(A=S\\). Since \\(f\\) is continuous in \\(S\\), \\(A \\subset E\\). Thus \\(E=S\\), and (7) shows that \\(c_{n}=0\\) for \\(n=0,1,2, \\ldots\\), which is the desired conclusion.\nThus we have to prove that \\(A\\) is open. If \\(x_{0} \\in A\\), Theorem \\(8.4\\) shows that\n\\[ f(x)=\\sum_{n=0}^{\\infty} d_{n}\\left(x-x_{0}\\right)^{n} \\quad\\left(\\left|x-x_{0}\\right|\u0026lt;R-\\left|x_{0}\\right|\\right) . \\]\nWe claim that \\(d_{n}=0\\) for all \\(n\\). Otherwise, let \\(k\\) be the smallest nonnegative integer such that \\(d_{k} \\neq 0\\). Then\n\\[ f(x)=\\left(x-x_{0}\\right)^{k} g(x) \\quad\\left(\\left|x-x_{0}\\right|\u0026lt;R-\\left|x_{0}\\right|\\right), \\]\nwhere\n\\[ g(x)=\\sum_{m=0}^{\\infty} d_{k+m}\\left(x-x_{0}\\right)^{m} . \\]\nSince \\(g\\) is continuous at \\(x_{0}\\) and\n\\[ g\\left(x_{0}\\right)=d_{k} \\neq 0, \\]\nthere exists a \\(\\delta\u0026gt;0\\) such that \\(g(x) \\neq 0\\) if \\(\\left|x-x_{0}\\right|\u0026lt;\\delta\\). It follows from (23) that \\(f(x) \\neq 0\\) if \\(0\u0026lt;\\left|x-x_{0}\\right|\u0026lt;\\delta\\). But this contradicts the fact that \\(x_{0}\\) is a limit point of \\(E\\).\nThus \\(d_{n}=0\\) for all \\(n\\), so that \\(f(x)=0\\) for all \\(x\\) for which (22) holds, i.e., in a neighborhood of \\(x_{0}\\). This shows that \\(A\\) is open, and completes the proof.\n","date":"2022-08-07T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/1-power-series/","section":"baby rudin","tags":null,"title":"1 POWER SERIES"},{"categories":null,"contents":"We define\n\\[ E(z)=\\sum_{n=0}^{\\infty} \\frac{z^{n}}{n !} \\]\nThe ratio test shows that this series converges for every complex z. Applying Theorem \\(3.50\\) on multiplication of absolutely convergent series, we obtain\n\\[ \\begin{aligned} E(z) E(w) \u0026amp;=\\sum_{n=0}^{\\infty} \\frac{z^{n}}{n !} \\sum_{m=0}^{\\infty} \\frac{w^{m}}{m !}=\\sum_{n=0}^{\\infty} \\sum_{k=0}^{n} \\frac{z^{k} w^{n-k}}{k !(n-k) !} \\\\ \u0026amp;=\\sum_{n=0}^{\\infty} \\frac{1}{n !} \\sum_{k=0}^{n}\\left(\\begin{array}{l} n \\\\ k \\end{array}\\right) z^{k} w^{n-k}=\\sum_{n=0}^{\\infty} \\frac{(z+w)^{n}}{n !} \\end{aligned} \\]\nwhich gives us the important addition formula\n\\[ E(z+w)=E(z) E(w) \\quad(z, w \\text { complex }) \\]\nOne consequence is that\n\\[ E(z) E(-z)=E(z-z)=E(0)=1 \\quad \\text { ( } z \\text { complex). } \\]\nThis shows that \\(E(z) \\neq 0\\) for all \\(z\\). By (25), \\(E(x)\u0026gt;0\\) if \\(x\u0026gt;0\\); hence (27) shows that \\(E(x)\u0026gt;0\\) for all real \\(x\\). By \\((25), E(x) \\rightarrow+\\infty\\) as \\(x \\rightarrow+\\infty\\); hence (27) shows that \\(E(x) \\rightarrow 0\\) as \\(x \\rightarrow-\\infty\\) along the real axis. By \\((25), 0\u0026lt;x\u0026lt;y\\) implies that \\(E(x)\u0026lt;E(y)\\); by (27), it follows that \\(E(-y)\u0026lt;E(-x)\\); hence \\(E\\) is strictly increasing on the whole real axis. The addition formula also shows that\n\\[ \\lim _{h=0} \\frac{E(z+h)-E(z)}{h}=E(z) \\lim _{h=0} \\frac{E(h)-1}{h}=E(z) \\text {; } \\]\nthe last equality follows directly from (25). Iteration of (26) gives\n\\[ E\\left(z_{1}+\\cdots+z_{n}\\right)=E\\left(z_{1}\\right) \\cdots E\\left(z_{n}\\right) . \\]\nLet us take \\(z_{1}=\\cdots=z_{n}=1\\). Since \\(E(1)=e\\), where \\(e\\) is the number defined in Definition 3.30, we obtain\n\\[ E(n)=e^{n} \\quad(n=1,2,3, \\ldots) . \\]\nIf \\(p=n / m\\), where \\(n, m\\) are positive integers, then\n\\[ [E(p)]^{m}=E(m p)=E(n)=e^{n} \\]\nso that\n\\[ E(p)=e^{p} \\quad(p\u0026gt;0, p \\text { rational). } \\]\nIt follows from (27) that \\(E(-p)=e^{-p}\\) if \\(p\\) is positive and rational. Thus (32) holds for all rational \\(p\\). In Exercise 6, Chap. 1, we suggested the definition\n\\[ x^{y}=\\sup x^{p}, \\]\nwhere the sup is taken over all rational \\(p\\) such that \\(p\u0026lt;y\\), for any real \\(y\\), and \\(x\u0026gt;1\\). If we thus define, for any real \\(x\\),\n\\[ e^{x}=\\sup e^{p} \\quad(p\u0026lt;x, p \\text { rational }), \\]\nthe continuity and monotonicity properties of \\(E\\), together with (32), show that\n\\[ E(x)=e^{x} \\]\nfor all real \\(x\\). Equation (35) explains why \\(E\\) is called the exponential function. The notation \\(\\exp (x)\\) is often used in place of \\(e^{x}\\), expecially when \\(x\\) is a complicated expression.\nActually one may very well use (35) instead of (34) as the definition of \\(e^{x}\\); (35) is a much more convenient starting point for the investigation of the properties of \\(e^{x}\\). We shall see presently that (33) may also be replaced by a more convenient definition [see (43)].\n8.6 Theorem Let \\(e^{x}\\) be defined on \\(R^{1}\\) by (35) and (25). Then (a) \\(e^{x}\\) is continuous and differentiable for all \\(x\\); (b) \\(\\left(e^{x}\\right)^{\\prime}=e^{x}\\) (c) \\(e^{x}\\) is a strictly increasing function of \\(x\\), and \\(e^{x}\u0026gt;0\\) (d) \\(e^{x+y}=e^{x} e^{y}\\) (e) \\(e^{x} \\rightarrow+\\infty\\) as \\(x \\rightarrow+\\infty, e^{x} \\rightarrow 0\\) as \\(x \\rightarrow-\\infty\\); (f) \\(\\lim _{x \\rightarrow+\\infty} x^{n} e^{-x}=0\\), for every \\(n\\).\nProof We have already proved \\((a)\\) to \\((e) ;(25)\\) shows that\n\\[ e^{x}\u0026gt;\\frac{x^{n+1}}{(n+1) !} \\]\nfor \\(x\u0026gt;0\\), so that\n\\[ x^{n} e^{-x}\u0026lt;\\frac{(n+1) !}{x} \\]\nand \\((f)\\) follows. Part \\((f)\\) shows that \\(e^{x}\\) tends to \\(+\\infty\\) “faster” than any power of \\(x\\), as \\(x \\rightarrow+\\infty\\).\nSince \\(E\\) is strictly increasing and differentiable on \\(R^{1}\\), it has an inverse function \\(L\\) which is also strictly increasing and differentiable and whose domain is \\(E\\left(R^{1}\\right)\\), that is, the set of all positive numbers. \\(L\\) is defined by\n\\[ E(L(y))=y \\quad(y\u0026gt;0), \\]\nor, equivalently, by\n\\[ L(E(x))=x \\quad(x \\text { real }) . \\]\nDifferentiating (37), we get (compare Theorem 5.5)\n\\[ L^{\\prime}(E(x)) \\cdot E(x)=1 . \\]\nWriting \\(y=E(x)\\), this gives us\n\\[ L^{\\prime}(y)=\\frac{1}{y} \\quad(y\u0026gt;0) \\]\nTaking \\(x=0\\) in (37), we see that \\(L(1)=0\\). Hence (38) implies\n\\[ L(y)=\\int_{1}^{y} \\frac{d x}{x} . \\]\nQuite frequently, (39) is taken as the starting point of the theory of the logarithm and the exponential function. Writing \\(u=E(x), v=E(y)\\), (26) gives\n\\[ L(u v)=L(E(x) \\cdot E(y))=L(E(x+y))=x+y, \\]\nso that\n\\[ L(u v)=L(u)+L(v) \\quad(u\u0026gt;0, v\u0026gt;0) . \\]\nThis shows that \\(L\\) has the familiar property which makes logarithms useful tools for computation. The customary notation for \\(L(x)\\) is of course \\(\\log x\\). As to the behavior of \\(\\log x\\) as \\(x \\rightarrow+\\infty\\) and as \\(x \\rightarrow 0\\), Theorem \\(8.6(e)\\) shows that\n\\[ \\begin{array}{ll} \\log x \\rightarrow+\\infty \u0026amp; \\text { as } x \\rightarrow+\\infty \\\\ \\log x \\rightarrow-\\infty \u0026amp; \\text { as } x \\rightarrow 0 \\end{array} \\]\nIt is easily seen that\n\\[ x^{n}=E(n L(x)) \\]\nif \\(x\u0026gt;0\\) and \\(n\\) is an integer. Similarly, if \\(m\\) is a positive integer, we have\n\\[ x^{1 / m}=E\\left(\\frac{1}{m} L(x)\\right), \\]\nsince each term of (42), when raised to the \\(m\\) th power, yields the corresponding term of (36). Combining (41) and (42), we obtain\n\\[ x^{\\alpha}=E(\\alpha L(x))=e^{\\alpha \\log x} \\]\nfor any rational \\(\\alpha\\).\nWe now define \\(x^{\\alpha}\\), for any real \\(\\alpha\\) and any \\(x\u0026gt;0\\), by (43). The continuity and monotonicity of \\(E\\) and \\(L\\) show that this definition leads to the same result as the previously suggested one. The facts stated in Exercise 6 of Chap. 1, are trivial consequences of (43). If we differentiate (43), we obtain, by Theorem \\(5.5\\),\n\\[ \\left(x^{\\alpha}\\right)^{\\prime}=E(\\alpha L(x)) \\cdot \\frac{\\alpha}{x}=\\alpha x^{\\alpha-1} . \\]\nNote that we have previously used (44) only for integral values of \\(\\alpha\\), in which case (44) follows easily from Theorem 5.3(b). To prove (44) directly from the definition of the derivative, if \\(x^{\\alpha}\\) is defined by (33) and \\(\\alpha\\) is irrational, is quite troublesome.\nThe well-known integration formula for \\(x^{\\alpha}\\) follows from (44) if \\(\\alpha \\neq-1\\), and from (38) if \\(\\alpha=-1\\). We wish to demonstrate one more property of \\(\\log x\\), namely,\n\\[ \\lim _{x \\rightarrow+\\infty} x^{-\\alpha} \\log x=0 \\]\nfor every \\(\\alpha\u0026gt;0\\). That is, \\(\\log x \\rightarrow+\\infty\\) “slower” than any positive power of \\(x\\), as \\(x \\rightarrow+\\infty\\)\nFor if \\(0\u0026lt;\\varepsilon\u0026lt;\\alpha\\), and \\(x\u0026gt;1\\), then\n\\[ \\begin{aligned} x^{-\\alpha} \\log x \u0026amp;=x^{-\\alpha} \\int_{1}^{x} t^{-1} d t\u0026lt;x^{-\\alpha} \\int_{1}^{x} t^{\\varepsilon-1} d t \\\\ \u0026amp;=x^{-\\alpha} \\cdot \\frac{x^{\\varepsilon}-1}{\\varepsilon}\u0026lt;\\frac{x^{\\varepsilon-\\alpha}}{\\varepsilon} \\end{aligned} \\]\nand (45) follows. We could also have used Theorem \\(8.6(f)\\) to derive (45).\n","date":"2022-08-07T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/8-some-special-function/2-the-exponential-and-logarithmic-functions/","section":"baby rudin","tags":null,"title":"2 THE EXPONENTIAL AND LOGARITHMIC FUNCTIONS"},{"categories":null,"contents":"samba install pacman -S samba gvfs-smb copy samba configure file\nadd user for samba\nsmbpasswd -a zong setup hostname (config file in samba) netbios name = gserver wins support = yes if connect using hostname, the subfolder name is required. It can not connect to the server without subfolder name.\nnginx ","date":"2022-08-07T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/server_setup/","section":"posts","tags":null,"title":"setup samba and nginx server"},{"categories":null,"contents":"7.26 Theorem If \\(f\\) is a continuous complex function on \\([a, b]\\), there exists a sequence of polynomials \\(P_{n}\\) such that \\[ \\lim _{n \\rightarrow \\infty} P_{n}(x)=f(x) \\] uniformly on \\([a, b]\\). If \\(f\\) is real, the \\(P_{n}\\) may be taken real.\nThis is the form in which the theorem was originally discovered by Weierstrass.\nProof We may assume, without loss of generality, that \\([a, b]=[0,1]\\). We may also assume that \\(f(0)=f(1)=0\\). For if the theorem is proved for this case, consider\n\\[ g(x)=f(x)-f(0)-x[f(1)-f(0)] \\quad(0 \\leq x \\leq 1) . \\]\nHere \\(g(0)=g(1)=0\\), and if \\(g\\) can be obtained as the limit of a uniformly convergent sequence of polynomials, it is clear that the same is true for \\(f\\), since \\(f-g\\) is a polynomial.\nFurthermore, we define \\(f(x)\\) to be zero for \\(x\\) outside \\([0,1]\\). Then \\(f\\) is uniformly continuous on the whole line.\nWe put\n\\[ Q_{n}(x)=c_{n}\\left(1-x^{2}\\right)^{n} \\quad(n=1,2,3, \\ldots), \\]\nwhere \\(c_{n}\\) is chosen so that\n\\[ \\int_{-1}^{1} Q_{n}(x) d x=1 \\quad(n=1,2,3, \\ldots) . \\]\nWe need some information about the order of magnitude of \\(c_{n}\\). Since\n\\[ \\begin{aligned} \\int_{-1}^{1}\\left(1-x^{2}\\right)^{n} d x=2 \\int_{0}^{1}\\left(1-x^{2}\\right)^{n} d x \u0026amp; \\geq 2 \\int_{0}^{1 / \\sqrt{n}}\\left(1-x^{2}\\right)^{n} d x \\\\ \u0026amp; \\geq 2 \\int_{0}^{1 / \\sqrt{n}}\\left(1-n x^{2}\\right) d x \\\\ \u0026amp;=\\frac{4}{3 \\sqrt{n}} \\\\ \u0026amp;\u0026gt;\\frac{1}{\\sqrt{n}} \\end{aligned} \\]\nit follows from (48) that\n\\[ c_{n}\u0026lt;\\sqrt{n} . \\]\nThe inequality \\(\\left(1-x^{2}\\right)^{n} \\geq 1-n x^{2}\\) which we used above is easily shown to be true by considering the function\n\\[ \\left(1-x^{2}\\right)^{n}-1+n x^{2} \\]\nwhich is zero at \\(x=0\\) and whose derivative is positive in \\((0,1)\\). For any \\(\\delta\u0026gt;0\\), (49) implies\n\\[ Q_{n}(x) \\leq \\sqrt{n}\\left(1-\\delta^{2}\\right)^{n} \\quad(\\delta \\leq|x| \\leq 1) \\]\nso that \\(Q_{n} \\rightarrow 0\\) uniformly in \\(\\delta \\leq|x| \\leq 1\\). Now set\n\\[ P_{n}(x)=\\int_{-1}^{1} f(x+t) Q_{n}(t) d t \\quad(0 \\leq x \\leq 1) . \\]\nOur assumptions about \\(f\\) show, by a simple change of variable, that\n\\[ P_{n}(x)=\\int_{-x}^{1-x} f(x+t) Q_{n}(t) d t=\\int_{0}^{1} f(t) Q_{n}(t-x) d t, \\]\nand the last integral is clearly a polynomial in \\(x\\). Thus \\(\\left\\{P_{n}\\right\\}\\) is a sequence of polynomials, which are real if \\(f\\) is real. Given \\(\\varepsilon\u0026gt;0\\), we choose \\(\\delta\u0026gt;0\\) such that \\(|y-x|\u0026lt;\\delta\\) implies\n\\[ |f(y)-f(x)|\u0026lt;\\frac{\\varepsilon}{2} . \\]\nLet \\(M=\\sup |f(x)|\\). Using (48), (50), and the fact that \\(Q_{n}(x) \\geq 0\\), we see that for \\(0 \\leq x \\leq 1\\),\n\\[ \\begin{aligned} \\left|P_{n}(x)-f(x)\\right| \u0026amp;=\\left|\\int_{-1}^{1}[f(x+t)-f(x)] Q_{n}(t) d t\\right| \\\\ \u0026amp; \\leq \\int_{-1}^{1}|f(x+t)-f(x)| Q_{n}(t) d t \\\\ \u0026amp; \\leq 2 M \\int_{-1}^{-\\delta} Q_{n}(t) d t+\\frac{\\varepsilon}{2} \\int_{-\\delta}^{\\delta} Q_{n}(t) d t+2 M \\int_{\\delta}^{1} Q_{n}(t) d t \\\\ \u0026amp; \\leq 4 M \\sqrt{n}\\left(1-\\delta^{2}\\right)^{n}+\\frac{\\varepsilon}{2} \\\\ \u0026amp;\u0026lt;\\varepsilon \\end{aligned} \\]\nfor all large enough \\(n\\), which proves the theorem. It is instructive to sketch the graphs of \\(Q_{n}\\) for a few values of \\(n\\); also, note that we needed uniform continuity of \\(f\\) to deduce uniform convergence of \\(\\left\\{P_{n}\\right\\}\\)\nIn the proof of Theorem \\(7.32\\) we shall not need the full strength of Theorem 7.26, but only the following special case, which we state as a corollary.\n7.27 Corollary For every interval \\([-a, a]\\) there is a sequence of real polynomials \\(P_{n}\\) such that \\(P_{n}(0)=0\\) and such that \\[ \\lim _{n \\rightarrow \\infty} P_{n}(x)=|x| \\]\nuniformly on \\([-a, a]\\). Proof By Theorem \\(7.26\\), there exists a sequence \\(\\left\\{P_{n}^{*}\\right\\}\\) of real polynomials which converges to \\(|x|\\) uniformly on \\([-a, a]\\). In particular, \\(P_{n}^{*}(0) \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). The polynomials \\[ P_{n}(x)=P_{n}^{*}(x)-P_{n}^{*}(0) \\quad(n=1,2,3, \\ldots) \\] have desired properties. We shall now isolate those properties of the polynomials which make the Weierstrass theorem possible.\n7.28 Definition A family \\(\\mathscr{A}\\) of complex functions defined on a set \\(E\\) is said to be an algebra if (i) \\(f+g \\in \\mathscr{A}\\), (ii) \\(f g \\in \\mathscr{A}\\), and (iii) \\(c f \\in \\mathscr{A}\\) for all \\(f \\in \\mathscr{A}, g \\in \\mathscr{A}\\) and for all complex constants \\(c\\), that is, if \\(\\mathscr{A}\\) is closed under addition, multiplication, and scalar multiplication. We shall also have to consider algebras of real functions; in this case, (iii) is of course only required to hold for all real \\(c\\).\nIf \\(\\mathscr{A}\\) has the property that \\(f \\in \\mathscr{A}\\) whenever \\(f_{n} \\in \\mathscr{A}(n=1,2,3, \\ldots)\\) and \\(f_{n} \\rightarrow f\\) uniformly on \\(E\\), then \\(\\mathscr{A}\\) is said to be uniformly closed.\nLet \\(\\mathscr{B}\\) be the set of all functions which are limits of uniformly convergent sequences of members of \\(\\mathscr{A}\\). Then \\(\\mathscr{B}\\) is called the uniform closure of \\(\\mathscr{A}\\). (See Definition 7.14.)\nFor example, the set of all polynomials is an algebra, and the Weierstrass theorem may be stated by saying that the set of continuous functions on \\([a, b]\\) is the uniform closure of the set of polynomials on \\([a, b]\\).\n\\(7.29\\) Theorem Let \\(\\mathscr{B}\\) be the uniform closure of an algebra \\(\\mathscr{A}\\) of bounded functions. Then \\(\\mathscr{B}\\) is a uniformly closed algebra.\nProof If \\(f \\in \\mathscr{B}\\) and \\(g \\in \\mathscr{B}\\), there exist uniformly convergent sequences \\(\\left\\{f_{n}\\right\\},\\left\\{g_{n}\\right\\}\\) such that \\(f_{n} \\rightarrow f, g_{n} \\rightarrow g\\) and \\(f_{n} \\in \\mathscr{A}, g_{n} \\in \\mathscr{A}\\). Since we are dealing with bounded functions, it is easy to show that \\[ f_{n}+g_{n} \\rightarrow f+g, \\quad f_{n} g_{n} \\rightarrow f g, \\quad c f_{n} \\rightarrow c f, \\] where \\(c\\) is any constant, the convergence being uniform in each case. Hence \\(f+g \\in \\mathscr{B}, f g \\in \\mathscr{B}\\), and \\(c f \\in \\mathscr{B}\\), so that \\(\\mathscr{B}\\) is an algebra. By Theorem \\(2.27, \\mathscr{B}\\) is (uniformly) closed.\n7.30 Definition Let \\(\\mathscr{A}\\) be a family of functions on a set \\(E\\). Then \\(\\mathscr{A}\\) is said to separate points on \\(E\\) if to every pair of distinct points \\(x_{1}, x_{2} \\in E\\) there corresponds a function \\(f \\in \\mathscr{A}\\) such that \\(f\\left(x_{1}\\right) \\neq f\\left(x_{2}\\right)\\).\nIf to each \\(x \\in E\\) there corresponds a function \\(g \\in \\mathscr{A}\\) such that \\(g(x) \\neq 0\\), we say that \\(\\mathscr{A}\\) vanishes at no point of \\(E\\).\nThe algebra of all polynomials in one variable clearly has these properties on \\(R^{1}\\). An example of an algebra which does not separate points is the set of all even polynomials, say on \\([-1,1]\\), since \\(f(-x)=f(x)\\) for every even function \\(f\\). The following theorem will illustrate these concepts further.\n7.31 Theorem Suppose \\(A\\) is an algebra of functions on a set \\(E, \\mathscr{A}\\) separates points on \\(E\\), and \\(\\mathscr{A}\\) vanishes at no point of \\(E\\). Suppose \\(x_{1}, x_{2}\\) are distinct points of \\(E\\), and \\(c_{1}, c_{2}\\) are constants (real if \\(\\mathscr{A}\\) is a real algebra). Then \\(\\mathscr{A}\\) contains a function \\(f\\) such that \\[ f\\left(x_{1}\\right)=c_{1}, \\quad f\\left(x_{2}\\right)=c_{2} . \\]\nProof The assumptions show that \\(A\\) contains functions \\(g, h\\), and \\(k\\) such that\n\\[ g\\left(x_{1}\\right) \\neq g\\left(x_{2}\\right), \\quad h\\left(x_{1}\\right) \\neq 0, \\quad k\\left(x_{2}\\right) \\neq 0 . \\]\nPut\n\\[ u=g k-g\\left(x_{1}\\right) k, \\quad v=g h-g\\left(x_{2}\\right) h . \\]\nThen \\(u \\in \\mathscr{A}, v \\in \\mathscr{A}, u\\left(x_{1}\\right)=v\\left(x_{2}\\right)=0, u\\left(x_{2}\\right) \\neq 0\\), and \\(v\\left(x_{1}\\right) \\neq 0\\). Therefore\n\\[ f=\\frac{c_{1} v}{v\\left(x_{1}\\right)}+\\frac{c_{2} u}{u\\left(x_{2}\\right)} \\]\nhas the desired properties.\nWe now have all the material needed for Stone’s generalization of the Weierstrass theorem.\n7.32 Theorem Let \\(A\\) be an algebra of real continuous functions on a compact set \\(K\\). If separates points on \\(K\\) and if \\(\\mathscr{A}\\) vanishes at no point of \\(K\\), then the uniform closure \\(\\mathscr{B}\\) of \\(\\mathscr{A}\\) consists of all real continuous functions on \\(K\\).\nWe shall divide the proof into four steps.\nSTEP 1 If \\(f \\in \\mathscr{B}\\), then \\(|f| \\in \\mathscr{B} .\\)\nProof Let\n\\[ a=\\sup |f(x)| \\quad(x \\in K) \\]\nand let \\(\\varepsilon\u0026gt;0\\) be given. By Corollary \\(7.27\\) there exist real numbers \\(c_{1}, \\ldots, c_{n}\\) such that\n\\[ \\left|\\sum_{i=1}^{n} c_{i} y^{i}-\\right| y||\u0026lt;\\varepsilon \\quad(-a \\leq y \\leq a) . \\]\nSince \\(\\mathscr{B}\\) is an algebra, the function\n\\[ g=\\sum_{i=1}^{n} c_{i} f^{i} \\]\nis a member of \\(\\mathscr{B}\\). By (52) and (53), we have\n\\[ |g(x)-| f(x)||\u0026lt;\\varepsilon \\quad(x \\in K) . \\]\nSince \\(\\mathscr{B}\\) is uniformly closed, this shows that \\(|f| \\in \\mathscr{B}\\).\nSTEP 2 If \\(f \\in \\mathscr{B}\\) and \\(g \\in \\mathscr{B}\\), then \\(\\max (f, g) \\in \\mathscr{B}\\) and \\(\\min (f, g) \\in \\mathscr{B}\\).\nBy \\(\\max (f, g)\\) we mean the function \\(h\\) defined by\n\\[ h(x)= \\begin{cases}f(x) \u0026amp; \\text { if } f(x) \\geq g(x), \\\\ g(x) \u0026amp; \\text { if } f(x)\u0026lt;g(x),\\end{cases} \\]\nand \\(\\min (f, g)\\) is defined likewise.\nProof Step 2 follows from step 1 and the identities\n\\[ \\begin{aligned} \u0026amp;\\max (f, g)=\\frac{f+g}{2}+\\frac{|f-g|}{2} \\\\ \u0026amp;\\min (f, g)=\\frac{f+g}{2}-\\frac{|f-g|}{2} \\end{aligned} \\]\nBy iteration, the result can of course be extended to any finite set of functions: If \\(f_{1}, \\ldots, f_{n} \\in \\mathscr{B}\\), then \\(\\max \\left(f_{1}, \\ldots, f_{n}\\right) \\in \\mathscr{B}\\), and\n\\[ \\min \\left(f_{1}, \\ldots, f_{n}\\right) \\in \\mathscr{B} . \\]\nSTEP 3 Given a real function \\(f\\), continuous on \\(K\\), a point \\(x \\in K\\), and \\(\\varepsilon\u0026gt;0\\), there exists a function \\(g_{x} \\in \\mathscr{B}\\) such that \\(g_{x}(x)=f(x)\\) and \\[ g_{x}(t)\u0026gt;f(t)-\\varepsilon \\quad(t \\in K) . \\]\nProof Since \\(\\mathscr{A} \\subset \\mathscr{B}\\) and \\(\\mathscr{A}\\) satisfies the hypotheses of Theorem \\(7.31\\) so does \\(\\mathscr{B}\\). Hence, for every \\(y \\in K\\), we can find a function \\(h_{y} \\in \\mathscr{B}\\) such that\n\\[ h_{y}(x)=f(x), \\quad h_{y}(y)=f(y) . \\]\nBy the continuity of \\(h_{y}\\) there exists an open set \\(J_{y}\\), containing \\(y\\), such that\n\\[ h_{y}(t)\u0026gt;f(t)-\\varepsilon \\quad\\left(t \\in J_{y}\\right) . \\]\nSince \\(K\\) is compact, there is a finite set of points \\(y_{1}, \\ldots, y_{n}\\) such that\n\\[ K \\subset J_{y_{1}} \\cup \\cdots \\cup J_{y_{n}} . \\]\nPut\n\\[ g_{x}=\\max \\left(h_{y_{1}}, \\ldots, h_{y_{n}}\\right) . \\]\nBy step \\(2, g_{x} \\in \\mathscr{8}\\), and the relations (55) to (57) show that \\(g_{x}\\) has the other required properties.\nSTEP 4 Given a real function \\(f\\), continuous on \\(K\\), and \\(\\varepsilon\u0026gt;0\\), there exists a function \\(h \\in \\mathscr{B}\\) such that \\[ |h(x)-f(x)|\u0026lt;\\varepsilon \\quad(x \\in K) . \\]\nSince \\(\\mathscr{B}\\) is uniformly closed, this statement is equivalent to the conclusion of the theorem.\nProof Let us consider the functions \\(g_{x}\\), for each \\(x \\in K\\), constructed in step 3. By the continuity of \\(g_{x}\\), there exist open sets \\(V_{x}\\) containing \\(x\\), such that\n\\[ g_{x}(t)\u0026lt;f(t)+\\varepsilon \\quad\\left(t \\in V_{x}\\right) . \\]\nSince \\(K\\) is compact, there exists a finite set of points \\(x_{1}, \\ldots, x_{m}\\) such that (60)\n\\[ K \\subset V_{x_{1}} \\cup \\cdots \\cup V_{x_{m}} . \\]\nPut\n\\[ h=\\min \\left(g_{x_{1}}, \\ldots, g_{x_{m}}\\right) . \\]\nBy step 2, \\(h \\in \\mathscr{B}\\), and (54) implies\n\\[ h(t)\u0026gt;f(t)-\\varepsilon \\quad(t \\in K), \\]\nwhereas (59) and (60) imply\n\\[ h(t)\u0026lt;f(t)+\\varepsilon \\quad(t \\in K) . \\]\nFinally, (58) follows from (61) and (62).\nTheorem \\(7.32\\) does not hold for complex algebras. A counterexample is given in Exercise 21. However, the conclusion of the theorem does hold, even for complex algebras, if an extra condition is imposed on \\(\\mathscr{A}\\), namely, that \\(\\mathscr{A}\\) be self-adjoint. This means that for every \\(f \\in \\mathscr{A}\\) its complex conjugate \\(f\\) must also belong to \\(\\mathscr{A} ; f\\) is defined by \\(f(x)=\\overline{f(x)}\\).\n7.33 Theorem Suppose \\(\\mathscr{A}\\) is a self-adjoint algebra of complex continuous functions on a compact set \\(K, \\mathscr{A}\\) separates points on \\(K\\), and \\(\\mathscr{A}\\) vanishes at no point of \\(K\\). Then the uniform closure \\(\\mathscr{B}\\) of \\(\\mathscr{A}\\) consists of all complex continuous functions on \\(K\\). In other words, \\(\\mathscr{A}\\) is dense \\(\\mathscr{C}(K)\\).\nProof Let \\(\\mathscr{A}_{R}\\) be the set of all real functions on \\(K\\) which belong to \\(\\mathscr{A}\\).\nIf \\(f \\in \\mathscr{A}\\) and \\(f=u+i v\\), with \\(u, v\\) real, then \\(2 u=f+\\bar{f}\\), and since \\(\\mathscr{A}\\) is self-adjoint, we see that \\(u \\in \\mathscr{A}_{R}\\). If \\(x_{1} \\neq x_{2}\\), there exists \\(f \\in \\mathscr{A}\\) such that \\(f\\left(x_{1}\\right)=1, f\\left(x_{2}\\right)=0\\); hence \\(0=u\\left(x_{2}\\right) \\neq u\\left(x_{1}\\right)=1\\), which shows that \\(\\mathscr{A}_{R}\\) separates points on \\(K\\). If \\(x \\in K\\), then \\(g(x) \\neq 0\\) for some \\(g \\in \\mathscr{A}\\), and there is a complex number \\(\\lambda\\) such that \\(\\lambda g(x)\u0026gt;0\\); if \\(f=\\lambda g, f=u+i v\\), it follows that \\(u(x)\u0026gt;0\\); hence \\(\\mathscr{A}_{R}\\) vanishes at no point of \\(K\\).\nThus \\(\\mathscr{A}_{R}\\) satisfies the hypotheses of Theorem 7.32. It follows that every real continuous function on \\(K\\) lies in the uniform closure of \\(\\mathscr{A}_{R}\\), hence lies in \\(\\mathscr{B}\\). If \\(f\\) is a complex continuous function on \\(K, f=u+i v\\), then \\(u \\in \\mathscr{B}, v \\in \\mathscr{B}\\), hence \\(f \\in \\mathscr{B}\\). This completes the proof.\n","date":"2022-08-04T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/7-the-stone-weierstrass-theorem/","section":"baby rudin","tags":null,"title":"7 THE STONE-WEIERSTRASS THEOREM"},{"categories":null,"contents":"In Theorem \\(3.6\\) we saw that every bounded sequence of complex numbers contains a convergent subsequence, and the question arises whether something similar is true for sequences of functions. To make the question more precise, we shall define two kinds of boundedness.\n7.19 Definition Let \\(\\left\\{f_{n}\\right\\}\\) be a sequence of functions defined on a set \\(E\\).\nWe say that \\(\\left\\{f_{n}\\right\\}\\) is pointwise bounded on \\(E\\) if the sequence \\(\\left\\{f_{n}(x)\\right\\}\\) is bounded for every \\(x \\in E\\), that is, if there exists a finite-valued function \\(\\phi\\) defined on \\(E\\) such that\n\\[ \\left|f_{n}(x)\\right|\u0026lt;\\phi(x) \\quad(x \\in E, n=1,2,3, \\ldots) \\text {. } \\]\nWe say that \\(\\left\\{f_{n}\\right\\}\\) is uniformly bounded on \\(E\\) if there exists a number \\(M\\) such that\n\\[ \\left|f_{n}(x)\\right|\u0026lt;M \\quad(x \\in E, n=1,2,3, \\ldots) \\text {. } \\]\nNow if \\(\\left\\{f_{n}\\right\\}\\) is pointwise bounded on \\(E\\) and \\(E_{1}\\) is a countable subset of \\(E\\), it is always possible to find a subsequence \\(\\left\\{f_{n_{k}}\\right\\}\\) such that \\(\\left\\{f_{n_{k}}(x)\\right\\}\\) converges for every \\(x \\in E_{1}\\). This can be done by the diagonal process which is used in the proof of Theorem 7.23.\nHowever, even if \\(\\left\\{f_{n}\\right\\}\\) is a uniformly bounded sequence of continuous functions on a compact set \\(E\\), there need not exist a subsequence which converges pointwise on \\(E\\). In the following example, this would be quite troublesome to prove with the equipment which we have at hand so far, but the proof is quite simple if we appeal to a theorem from Chap. \\(11 .\\)\n7.20 Example Let\n\\[ f_{n}(x)=\\sin n x \\quad(0 \\leq x \\leq 2 \\pi, n=1,2,3, \\ldots) . \\]\nSuppose there exists a sequence \\(\\left\\{n_{k}\\right\\}\\) such that \\(\\left\\{\\sin n_{k} x\\right\\}\\) converges, for every \\(x \\in[0,2 \\pi]\\). In that case we must have\n\\[ \\lim _{k \\rightarrow \\infty}\\left(\\sin n_{k} x-\\sin n_{k+1} x\\right)=0 \\quad(0 \\leq x \\leq 2 \\pi) ; \\]\nhence\n\\[ \\lim _{k \\rightarrow \\infty}\\left(\\sin n_{k} x-\\sin n_{k+1} x\\right)^{2}=0 \\quad(0 \\leq x \\leq 2 \\pi) . \\]\nBy Lebesgue’s theorem concerning integration of boundedly convergent sequences (Theorem 11.32), (40) implies\n\\[ \\lim _{k \\rightarrow \\infty} \\int_{0}^{2 \\pi}\\left(\\sin n_{k} x-\\sin n_{k+1} x\\right)^{2} d x=0 . \\]\nBut a simple calculation shows that\n\\[ \\int_{0}^{2 \\pi}\\left(\\sin n_{k} x-\\sin n_{k+1} x\\right)^{2} d x=2 \\pi \\]\nwhich contradicts (41). Another question is whether every convergent sequence contains a uniformly convergent subsequence. Our next example will show that this need not be so, even if the sequence is uniformly bounded on a compact set. (Example \\(7.6\\) shows that a sequence of bounded functions may converge without being uniformly bounded; but it is trivial to see that uniform convergence of a sequence of bounded functions implies uniform boundedness.)\n7.21 Example Let\n\\[ f_{n}(x)=\\frac{x^{2}}{x^{2}+(1-n x)^{2}} \\quad(0 \\leq x \\leq 1, n=1,2,3, \\ldots) \\text {. } \\]\nThen \\(\\left|f_{n}(x)\\right| \\leq 1\\), so that \\(\\left\\{f_{n}\\right\\}\\) is uniformly bounded on \\([0,1]\\). Also\n\\[ \\lim _{n \\rightarrow \\infty} f_{n}(x)=0 \\quad(0 \\leq x \\leq 1), \\]\nbut\n\\[ f_{n}\\left(\\frac{1}{n}\\right)=1 \\quad(n=1,2,3, \\ldots), \\]\nso that no subsequence can converge uniformly on \\([0,1]\\).\nThe concept which is needed in this connection is that of equicontinuity; it is given in the following definition.\n7.22 Definition A family \\(\\mathscr{F}\\) of complex functions \\(f\\) defined on a set \\(E\\) in a metric space \\(X\\) is said to be equicontinuous on \\(E\\) if for every \\(\\varepsilon\u0026gt;0\\) there exists a \\(\\delta\u0026gt;0\\) such that\n\\[ |f(x)-f(y)|\u0026lt;\\varepsilon \\]\nwhenever \\(d(x, y)\u0026lt;\\delta, x \\in E, y \\in E\\), and \\(f \\in \\mathscr{F}\\). Here \\(d\\) denotes the metric of \\(X\\). (compare to 4.18)\nIt is clear that every member of an equicontinuous family is uniformly continuous.\nThe sequence of Example \\(7.21\\) is not equicontinuous.\nTheorems \\(7.24\\) and \\(7.25\\) will show that there is a very close relation between equicontinuity, on the one hand, and uniform convergence of sequences of continuous functions, on the other. But first we describe a selection process which has nothing to do with continuity.\n7.23 Theorem If \\(\\left\\{f_{n}\\right\\}\\) is a pointwise bounded sequence of complex functions on a countable set \\(E\\), then \\(\\left\\{f_{n}\\right\\}\\) has a subsequence \\(\\left\\{f_{n_{k}}\\right\\}\\) such that \\(\\left\\{f_{n_{k}}(x)\\right\\}\\) converges for every \\(x \\in E\\).\nProof Let \\(\\left\\{x_{i}\\right\\}, i=1,2,3, \\ldots\\), be the points of \\(E\\), arranged in a sequence. Since \\(\\left\\{f_{n}\\left(x_{1}\\right)\\right\\}\\) is bounded, there exists a subsequence, which we shall denote by \\(\\left\\{f_{1, k}\\right\\}\\), such that \\(\\left\\{f_{1, k}\\left(x_{1}\\right)\\right\\}\\) converges as \\(k \\rightarrow \\infty\\).\nLet us now consider sequences \\(S_{1}, S_{2}, S_{3}, \\ldots\\), which we represent by the array\n\\[ \\begin{array}{cccccc} S_{1}: \u0026amp; f_{1,1} \u0026amp; f_{1,2} \u0026amp; f_{1,3} \u0026amp; f_{1,4} \u0026amp; \\ldots \\\\ S_{2}: \u0026amp; f_{2,1} \u0026amp; f_{2,2} \u0026amp; f_{2,3} \u0026amp; f_{2,4} \u0026amp; \\ldots \\\\ S_{3}: \u0026amp; f_{3,1} \u0026amp; f_{3,2} \u0026amp; f_{3,3} \u0026amp; f_{3,4} \u0026amp; \\cdots \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\end{array} \\]\nand which have the following properties:\n\\(S_{n}\\) is a subsequence of \\(S_{n-1}\\), for \\(n=2,3,4, \\ldots\\)\n\\(\\left\\{f_{n, k}\\left(x_{n}\\right)\\right\\}\\) converges, as \\(k \\rightarrow \\infty\\) (the boundedness of \\(\\left\\{f_{n}\\left(x_{n}\\right)\\right\\}\\) makes it possible to choose \\(S_{n}\\) in this way);\nThe order in which the functions appear is the same in each sequence; i.e., if one function precedes another in \\(S_{1}\\), they are in the same relation in every \\(S_{n}\\), until one or the other is deleted. Hence, when going from one row in the above array to the next below, functions may move to the left but never to the right.\nWe now go down the diagonal of the array; i.e., we consider the sequence\n\\[ S: f_{1,1} \\quad f_{2,2} \\quad f_{3,3} \\quad f_{4,4} \\cdots . \\]\nBy \\((c)\\), the sequence \\(S\\) (except possibly its first \\(n-1\\) terms) is a subsequence of \\(S_{n}\\), for \\(n=1,2,3, \\ldots\\). Hence \\((b)\\) implies that \\(\\left\\{f_{n, n}\\left(x_{i}\\right)\\right\\}\\) converges, as \\(n \\rightarrow \\infty\\), for every \\(x_{i} \\in E\\).\n7.24 Theorem If \\(K\\) is a compact metric space, if \\(f_{n} \\in \\mathscr{C}(K)\\) for \\(n=1,2,3, \\ldots\\), and if \\(\\left\\{f_{n}\\right\\}\\) converges uniformly on \\(K\\), then \\(\\left\\{f_{n}\\right\\}\\) is equicontinuous on \\(K\\).\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Since \\(\\left\\{f_{n}\\right\\}\\) converges uniformly, there is an integer \\(N\\) such that\n\\[ \\left\\|f_{n}-f_{N}\\right\\|\u0026lt;\\varepsilon \\quad(n\u0026gt;N) . \\]\n(See Definition 7.14.) Since continuous functions are uniformly continuous on compact sets, there is a \\(\\delta\u0026gt;0\\) such that\n\\[ \\left|f_{i}(x)-f_{i}(y)\\right|\u0026lt;\\varepsilon \\]\nif \\(1 \\leq i \\leq N\\) and \\(d(x, y)\u0026lt;\\delta\\).\nIf \\(n\u0026gt;N\\) and \\(d(x, y)\u0026lt;\\delta\\), it follows that\n\\[ \\left|f_{n}(x)-f_{n}(y)\\right| \\leq\\left|f_{n}(x)-f_{N}(x)\\right|+\\left|f_{N}(x)-f_{N}(y)\\right|+\\left|f_{N}(y)-f_{n}(y)\\right|\u0026lt;3 \\varepsilon \\text {. } \\]\nIn conjunction with (43), this proves the theorem.\n7.25 Theorem If \\(K\\) is compact, if \\(f_{n} \\in \\mathscr{C}(K)\\) for \\(n=1,2,3, \\ldots\\), and if \\(\\left\\{f_{n}\\right\\}\\) is pointwise bounded and equicontinuous on \\(K\\), then (a) \\(\\left\\{f_{n}\\right\\}\\) is uniformly bounded on \\(K\\),\n(b) \\(\\left\\{f_{n}\\right\\}\\) contains a uniformly convergent subsequence.\nProof\nLet \\(\\varepsilon\u0026gt;0\\) be given and choose \\(\\delta\u0026gt;0\\), in accordance with Definition 7.22, so that \\[ \\left|f_{n}(x)-f_{n}(y)\\right|\u0026lt;\\varepsilon \\]\nfor all \\(n\\), provided that \\(d(x, y)\u0026lt;\\delta\\).\nSince \\(K\\) is compact, there are finitely many points \\(p_{1}, \\ldots, p_{r}\\) in \\(K\\) such that to every \\(x \\in K\\) corresponds at least one \\(p_{i}\\) with \\(d\\left(x, p_{i}\\right)\u0026lt;\\delta\\). Since \\(\\left\\{f_{n}\\right\\}\\) is pointwise bounded, there exist \\(M_{i}\u0026lt;\\infty\\) such that \\(\\left|f_{n}\\left(p_{i}\\right)\\right|\u0026lt;M_{i}\\) for all \\(n\\). If \\(M=\\max \\left(M_{1}, \\ldots, M_{r}\\right)\\), then \\(\\left|f_{n}(x)\\right|\u0026lt;M+\\varepsilon\\) for every \\(x \\in K\\). This proves \\((a)\\).\nLet \\(E\\) be a countable dense subset of \\(K\\). (For the existence of such a set \\(E\\), see Exercise 25, Chap. 2.) Theorem \\(7.23\\) shows that \\(\\left\\{f_{n}\\right\\}\\) has a subsequence \\(\\left\\{f_{n_{i}}\\right\\}\\) such that \\(\\left\\{f_{n_{i}}(x)\\right\\}\\) converges for every \\(x \\in E\\). Put \\(f_{n_{i}}=g_{i}\\), to simplify the notation. We shall prove that \\(\\left\\{g_{i}\\right\\}\\) converges uniformly on \\(K\\).\nLet \\(\\varepsilon\u0026gt;0\\), and pick \\(\\delta\u0026gt;0\\) as in the beginning of this proof. Let \\(V(x, \\delta)\\) be the set of all \\(y \\in K\\) with \\(d(x, y)\u0026lt;\\delta\\). Since \\(E\\) is dense in \\(K\\), and \\(K\\) is compact, there are finitely many points \\(x_{1}, \\ldots, x_{m}\\) in \\(E\\) such that\n\\[ K \\subset V\\left(x_{1}, \\delta\\right) \\cup \\cdots \\cup V\\left(x_{m}, \\delta\\right) . \\]\nSince \\(\\left\\{g_{i}(x)\\right\\}\\) converges for every \\(x \\in E\\), there is an integer \\(N\\) such that\n\\[ \\left|g_{l}\\left(x_{s}\\right)-g_{j}\\left(x_{s}\\right)\\right|\u0026lt;\\varepsilon \\]\nwhenever \\(i \\geq N, j \\geq N, 1 \\leq s \\leq m\\).\nIf \\(x \\in K\\), (45) shows that \\(x \\in V\\left(x_{s}, \\delta\\right)\\) for some \\(s\\), so that\n\\[ \\left|g_{i}(x)-g_{i}\\left(x_{\\mathrm{s}}\\right)\\right|\u0026lt;\\varepsilon \\]\nfor every \\(i\\). If \\(i \\geq N\\) and \\(j \\geq N\\), it follows from (46) that\n\\[ \\begin{aligned} \\left|g_{i}(x)-g_{j}(x)\\right| \u0026amp; \\leq\\left|g_{i}(x)-g_{i}\\left(x_{s}\\right)\\right|+\\left|g_{i}\\left(x_{s}\\right)-g_{j}\\left(x_{s}\\right)\\right|+\\left|g_{j}\\left(x_{s}\\right)-g_{j}(x)\\right| \\\\ \u0026amp;\u0026lt;3 \\varepsilon . \\end{aligned} \\]\nThis completes the proof.\n","date":"2022-08-03T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/6-equicontinuous-families-of-functions/","section":"baby rudin","tags":null,"title":"6 EQUICONTINUOUS FAMILIES OF FUNCTIONS"},{"categories":null,"contents":"We have already seen, in Example 7.5, that uniform convergence of \\(\\left\\{f_{n}\\right\\}\\) implies nothing about the sequence \\(\\left\\{f_{n}^{\\prime}\\right\\}\\). Thus stronger hypotheses are required for the assertion that \\(f_{n}^{\\prime} \\rightarrow f^{\\prime}\\) if \\(f_{n} \\rightarrow f\\).\n7.17 Theorem Suppose \\(\\left\\{f_{n}\\right\\}\\) is a sequence of functions, differentiable on \\([a, b]\\) and such that \\(\\left\\{f_{n}\\left(x_{0}\\right)\\right\\}\\) converges for some point \\(x_{0}\\) on \\([a, b]\\). If \\(\\left\\{f_{n}^{\\prime}\\right\\}\\) converges uniformly on \\([a, b]\\), then \\(\\left\\{f_{n}\\right\\}\\) converges uniformly on \\([a, b]\\), to a function \\(f\\), and \\[ f^{\\prime}(x)=\\lim _{n \\rightarrow \\infty} f_{n}^{\\prime}(x) \\quad(a \\leq x \\leq b) . \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Choose \\(N\\) such that \\(n \\geq N, m \\geq N\\), implies\n\\[ \\left|f_{n}\\left(x_{0}\\right)-f_{m}\\left(x_{0}\\right)\\right|\u0026lt;\\frac{\\varepsilon}{2} \\]\nand\n\\[ \\left|f_{n}^{\\prime}(t)-f_{m}^{\\prime}(t)\\right|\u0026lt;\\frac{\\varepsilon}{2(b-a)} \\quad(a \\leq t \\leq b) \\]\nIf we apply the mean value theorem \\(5.19\\) to the function \\(f_{n}-f_{m},(29)\\) shows that\n\\[ \\left|f_{n}(x)-f_{m}(x)-f_{n}(t)+f_{m}(t)\\right| \\leq \\frac{|x-t| \\varepsilon}{2(b-a)} \\leq \\frac{\\varepsilon}{2} \\]\nfor any \\(x\\) and \\(t\\) on \\([a, b]\\), if \\(n \\geq N, m \\geq N\\). The inequality\n\\[ \\left|f_{n}(x)-f_{m}(x)\\right| \\leq\\left|f_{n}(x)-f_{m}(x)-f_{n}\\left(x_{0}\\right)+f_{m}\\left(x_{0}\\right)\\right|+\\left|f_{n}\\left(x_{0}\\right)-f_{m}\\left(x_{0}\\right)\\right| \\]\nimplies, by (28) and (30), that\n\\[ \\left|f_{n}(x)-f_{m}(x)\\right|\u0026lt;\\varepsilon \\quad(a \\leq x \\leq b, n \\geq N, m \\geq N) \\text {, } \\]\nso that \\(\\left\\{f_{n}\\right\\}\\) converges uniformly on \\([a, b]\\). Let\n\\[ f(x)=\\lim _{n \\rightarrow \\infty} f_{n}(x) \\quad(a \\leq x \\leq b) . \\]\nLet us now fix a point \\(x\\) on \\([a, b]\\) and define\n\\[ \\phi_{n}(t)=\\frac{f_{n}(t)-f_{n}(x)}{t-x}, \\quad \\phi(t)=\\frac{f(t)-f(x)}{t-x} \\]\nfor \\(a \\leq t \\leq b, t \\neq x\\). Then\n\\[ \\lim _{t \\rightarrow x} \\phi_{n}(t)=f_{n}^{\\prime}(x) \\quad(n=1,2,3, \\ldots) \\text {. } \\]\nThe first inequality in (30) shows that\n\\[ \\left|\\phi_{n}(t)-\\phi_{m}(t)\\right| \\leq \\frac{\\varepsilon}{2(b-a)} \\quad(n \\geq N, m \\geq N), \\]\nso that \\(\\left\\{\\phi_{n}\\right\\}\\) converges uniformly, for \\(t \\neq x\\). Since \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\), we conclude from (31) that\n\\[ \\lim _{n \\rightarrow \\infty} \\phi_{n}(t)=\\phi(t) \\]\nuniformly for \\(a \\leq t \\leq b, t \\neq x\\).\nIf we now apply Theorem \\(7.11\\) to \\(\\left\\{\\phi_{n}\\right\\},(32)\\) and (33) show that\n\\[ \\lim _{t \\rightarrow x} \\phi(t)=\\lim _{n \\rightarrow \\infty} f_{n}^{\\prime}(x) \\text {; } \\]\nand this is (27), by the definition of \\(\\phi(t)\\).\nRemark: If the continuity of the functions \\(f_{n}^{\\prime}\\) is assumed in addition to the above hypotheses, then a much shorter proof of (27) can be based on Theorem \\(7.16\\) and the fundamental theorem of calculus.\n7.18 Theorem There exists a real continuous function on the real line which is nowhere differentiable.\nProof Define\n\\[ \\varphi(x)=|x| \\quad(-1 \\leq x \\leq 1) \\]\nand extend the definition of \\(\\varphi(x)\\) to all real \\(x\\) by requiring that\n\\[ \\varphi(x+2)=\\varphi(x) . \\]\nThen, for all \\(s\\) and \\(t\\)\n\\[ |\\varphi(s)-\\varphi(t)| \\leq|s-t| . \\]\nIn particular, \\(\\varphi\\) is continuous on \\(R^{1}\\). Define\n\\[ f(x)=\\sum_{n=0}^{\\infty}\\left(\\frac{3}{4}\\right)^{n} \\varphi\\left(4^{n} x\\right) . \\]\nSince \\(0 \\leq \\varphi \\leq 1\\), Theorem \\(7.10\\) shows that the series (37) converges uniformly on \\(R^{1}\\). By Theorem 7.12, \\(f\\) is continuous on \\(R^{1}\\).\nNow fix a real number \\(x\\) and a positive integer \\(m\\). Put\n\\[ \\delta_{m}=\\pm \\frac{1}{2} \\cdot 4^{-m} \\]\nwhere the sign is so chosen that no integer lies between \\(4^{m} x\\) and \\(4^{m}\\left(x+\\delta_{m}\\right)\\). This can be done, since \\(4^{m}\\left|\\delta_{m}\\right|=\\frac{1}{2}\\). Define\n\\[ \\gamma_{n}=\\frac{\\varphi\\left(4^{n}\\left(x+\\delta_{m}\\right)\\right)-\\varphi\\left(4^{n} x\\right)}{\\delta_{m}} \\text {. } \\]\nWhen \\(n\u0026gt;m\\), then \\(4^{n} \\delta_{m}\\) is an even integer, so that \\(\\gamma_{n}=0\\). When \\(0 \\leq n \\leq m\\), (36) implies that \\(\\left|\\gamma_{n}\\right| \\leq 4^{n}\\).\nSince \\(\\left|\\gamma_{m}\\right|=4^{m}\\), we conclude that\n\\[ \\begin{aligned} \\left|\\frac{f\\left(x+\\delta_{m}\\right)-f(x)}{\\delta_{m}}\\right| \u0026amp;=\\mid \\sum_{n=0}^{m}\\left(\\frac{3}{4}\\right)^{n} \\gamma_{n} \\\\ \u0026amp; \\geq 3^{m}-\\sum_{n=0}^{m-1} 3^{n} \\\\ \u0026amp;=\\frac{1}{2}\\left(3^{m}+1\\right) . \\end{aligned} \\]\nAs \\(m \\rightarrow \\infty, \\delta_{m} \\rightarrow 0\\). It follows that \\(f\\) is not differentiable at \\(x\\).\n","date":"2022-08-02T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/5-uniform-convergence-and-differentiation/","section":"baby rudin","tags":null,"title":"5 UNIFORM CONVERGENCE AND DIFFERENTIATION"},{"categories":null,"contents":"7.7 Definition We say that a sequence of functions \\(\\left\\{f_{n}\\right\\}, n=1,2,3, \\ldots\\), converges uniformly on \\(E\\) to a function \\(f\\) if for every \\(\\varepsilon\u0026gt;0\\) there is an integer \\(N\\) such that \\(n \\geq N\\) implies\n\\[ \\left|f_{n}(x)-f(x)\\right| \\leq \\varepsilon \\]\nfor all \\(x \\in E\\).\nIt is clear that every uniformly convergent sequence is pointwise convergent. Quite explicitly, the difference between the two concepts is this: If \\(\\left\\{f_{n}\\right\\}\\) converges pointwise on \\(E\\), then there exists a function \\(f\\) such that, for every \\(\\varepsilon\u0026gt;0\\), and for every \\(x \\in E\\), there is an integer \\(N\\), depending on \\(\\varepsilon\\) and on \\(x\\), such that (12) holds if \\(n \\geq N\\); if \\(\\left\\{f_{n}\\right\\}\\) converges uniformly on \\(E\\), it is possible, for each \\(\\varepsilon\u0026gt;0\\), to find one integer \\(N\\) which will do for all \\(x \\in E\\).\nWe say that the series \\(\\Sigma f_{n}(x)\\) converges uniformly on \\(E\\) if the sequence \\(\\left\\{s_{n}\\right\\}\\) of partial sums defined by\n\\[ \\sum_{i=1}^{n} f_{i}(x)=s_{n}(x) \\]\nconverges uniformly on \\(E\\).\nThe Cauchy criterion for uniform convergence is as follows.\n\\(7.8\\) Theorem The sequence of functions \\(\\left\\{f_{n}\\right\\}\\), defined on \\(E\\), converges uniformly on \\(E\\) if and only if for every \\(\\varepsilon\u0026gt;0\\) there exists an integer \\(N\\) such that \\(m \\geq N\\), \\(n \\geq N, x \\in E\\) implies\n\\[ \\left|f_{n}(x)-f_{m}(x)\\right| \\leq \\varepsilon . \\]\nProof Suppose \\(\\left\\{f_{n}\\right\\}\\) converges uniformly on \\(E\\), and let \\(f\\) be the limit function. Then there is an integer \\(N\\) such that \\(n \\geq N, x \\in E\\) implies\n\\[ \\left|f_{n}(x)-f(x)\\right| \\leq \\frac{\\varepsilon}{2}, \\]\nso that\n\\[ \\left|f_{n}(x)-f_{m}(x)\\right| \\leq\\left|f_{n}(x)-f(x)\\right|+\\left|f(x)-f_{m}(x)\\right| \\leq \\varepsilon \\]\nif \\(n \\geq N, m \\geq N, x \\in E\\). Conversely, suppose the Cauchy condition holds. By Theorem 3.11, the sequence \\(\\left\\{f_{n}(x)\\right\\}\\) converges, for every \\(x\\), to a limit which we may call \\(f(x)\\). Thus the sequence \\(\\left\\{f_{n}\\right\\}\\) converges on \\(E\\), to \\(f\\). We have to prove that the convergence is uniform.\nLet \\(\\varepsilon\u0026gt;0\\) be given, and choose \\(N\\) such that (13) holds. Fix \\(n\\), and let \\(m \\rightarrow \\infty\\) in (13). Since \\(f_{m}(x) \\rightarrow f(x)\\) as \\(m \\rightarrow \\infty\\), this gives\n\\[ \\left|f_{n}(x)-f(x)\\right| \\leq \\varepsilon \\]\nfor every \\(n \\geq N\\) and every \\(x \\in E\\), which completes the proof. \\(\\blacksquare\\)\nThe following criterion is sometimes useful.\n7.9 Theorem Suppose\n\\[ \\lim _{n \\rightarrow \\infty} f_{n}(x)=f(x) \\quad(x \\in E) . \\]\nPut\n\\[ M_{n}=\\sup _{x \\in E}\\left|f_{n}(x)-f(x)\\right| . \\]\nThen \\(f_{n} \\rightarrow f\\) uniformly on \\(E\\) if and only if \\(M_{n} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\).\nSince this is an immediate consequence of Definition 7.7, we omit the details of the proof.\nFor series, there is a very convenient test for uniform convergence, due to Weierstrass.\n7.10 Theorem Suppose \\(\\left\\{f_{n}\\right\\}\\) is a sequence of functions defined on \\(E\\), and suppose\n\\[ \\left|f_{n}(x)\\right| \\leq M_{n} \\quad(x \\in E, n=1,2,3, \\ldots) \\text {. } \\]\nThen \\(\\Sigma f_{n}\\) converges uniformly on \\(E\\) if \\(\\Sigma M_{n}\\) converges.\nNote that the converse is not asserted (and is, in fact, not true).\nProof If \\(\\Sigma M_{n}\\) converges, then, for arbitrary \\(\\varepsilon\u0026gt;0\\),\n\\[ \\left|\\sum_{i=n}^{m} f_{i}(x)\\right| \\leq \\sum_{i=n}^{m} M_{i} \\leq \\varepsilon \\quad(x \\in E), \\]\nprovided \\(m\\) and \\(n\\) are large enough. \\(\\blacksquare\\)\nUniform convergence now follows from Theorem 7.8.\n","date":"2022-07-31T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/2-uniform-convergence/","section":"baby rudin","tags":null,"title":"2 UNIFORM CONVERGENCE"},{"categories":null,"contents":"7.11 Theorem Suppose \\(f_{n} \\rightarrow f\\) uniformly on a set \\(E\\) in a metric space. Let \\(x\\) be a limit point of \\(E\\), and suppose that \\[ \\lim _{t \\rightarrow x} f_{n}(t)=A_{n} \\quad(n=1,2,3, \\ldots) . \\] Then \\(\\left\\{A_{n}\\right\\}\\) converges, and \\[ \\lim _{t \\rightarrow x} f(t)=\\lim _{n \\rightarrow \\infty} A_{n} . \\]\nIn other words, the conclusion is that\n\\[ \\lim _{t \\rightarrow x} \\lim _{n \\rightarrow \\infty} f_{n}(t)=\\lim _{n \\rightarrow \\infty} \\lim _{t \\rightarrow x} f_{n}(t) . \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. By the uniform convergence of \\(\\left\\{f_{n}\\right\\}\\), there exists \\(N\\) such that \\(n \\geq N, m \\geq N, t \\in E\\) imply\n\\[ \\left|f_{n}(t)-f_{m}(t)\\right| \\leq \\varepsilon . \\]\nLetting \\(t \\rightarrow x\\) in (18), we obtain\n\\[ \\left|A_{n}-A_{m}\\right| \\leq \\varepsilon \\]\nfor \\(n \\geq N, m \\geq N\\), so that \\(\\left\\{A_{n}\\right\\}\\) is a Cauchy sequence and therefore converges, say to \\(A\\).\nNext,\n\\[ |f(t)-A| \\leq\\left|f(t)-f_{n}(t)\\right|+\\left|f_{n}(t)-A_{n}\\right|+\\left|A_{n}-A\\right| . \\]\nWe first choose \\(n\\) such that\n\\[ \\left|f(t)-f_{n}(t)\\right| \\leq \\frac{\\varepsilon}{3} \\]\nfor all \\(t \\in E\\) (this is possible by the uniform convergence), and such that\n\\[ \\left|A_{n}-A\\right| \\leq \\frac{\\varepsilon}{3} . \\]\nThen, for this \\(n\\), we choose a neighborhood \\(V\\) of \\(x\\) such that\n\\[ \\left|f_{n}(t)-A_{n}\\right| \\leq \\frac{\\varepsilon}{3} \\]\nif \\(t \\in V \\cap E, t \\neq x\\).\nSubstituting the inequalities (20) to (22) into (19), we see that\n\\[ |f(t)-A| \\leq \\varepsilon, \\]\nprovided \\(t \\in V \\cap E, t \\neq x\\). This is equivalent to (16).\n7.12 Theorem If \\(\\left\\{f_{n}\\right\\}\\) is a sequence of continuous functions on \\(E\\), and if \\(f_{n} \\rightarrow f\\) uniformly on \\(E\\), then \\(f\\) is continuous on \\(E\\).\nThis very important result is an immediate corollary of Theorem 7.11.\nThe converse is not true; that is, a sequence of continuous functions may converge to a continuous function, although the convergence is not uniform. Example \\(7.6\\) is of this kind (to see this, apply Theorem 7.9). But there is a case in which we can assert the converse.\n7.13 Theorem Suppose \\(K\\) is compact, and\n(a) \\(\\left\\{f_{n}\\right\\}\\) is a sequence of continuous functions on \\(K\\), (b) \\(\\left\\{f_{n}\\right\\}\\) converges pointwise to a continuous function \\(f\\) on \\(K\\),\n(c) \\(f_{n}(x) \\geq f_{n+1}(x)\\) for all \\(x \\in K, n=1,2,3, \\ldots\\) Then \\(f_{n} \\rightarrow f\\) uniformly on \\(K\\).\nProof Put \\(g_{n}=f_{n}-f\\). Then \\(g_{n}\\) is continuous, \\(g_{n} \\rightarrow 0\\) pointwise, and \\(g_{n} \\geq g_{n+1}\\). We have to prove that \\(g_{n} \\rightarrow 0\\) uniformly on \\(K\\).\nLet \\(\\varepsilon\u0026gt;0\\) be given. Let \\(K_{n}\\) be the set of all \\(x \\in K\\) with \\(g_{n}(x) \\geq \\varepsilon\\). Since \\(g_{n}\\) is continuous, \\(K_{n}\\) is closed (Theorem 4.8), hence compact (Theorem 2.35). Since \\(g_{n} \\geq g_{n+1}\\), we have \\(K_{n} \\supset K_{n+1}\\). Fix \\(x \\in K\\). Since \\(g_{n}(x) \\rightarrow 0\\), we see that \\(x \\notin K_{n}\\) if \\(n\\) is sufficiently large. Thus \\(x \\notin \\bigcap K_{n}\\). In other words, \\(\\bigcap K_{n}\\) is empty. Hence \\(K_{N}\\) is empty for some \\(N\\) (Theorem 2.36). It follows that \\(0 \\leq g_{n}(x)\u0026lt;\\varepsilon\\) for all \\(x \\in K\\) and for all \\(n \\geq N\\). This proves the theorem.\\(\\blacksquare\\)\nLet us note that compactness is really needed here. For instance, if\n\\[ f_{n}(x)=\\frac{1}{n x+1} \\quad(0\u0026lt;x\u0026lt;1 ; n=1,2,3, \\ldots) \\]\nthen \\(f_{n}(x) \\rightarrow 0\\) monotonically in \\((0,1)\\), but the convergence is not uniform.\n7.14 Definition If \\(X\\) is a metric space, \\(\\mathscr{C}(X)\\) will denote the set of all complexvalued, continuous, bounded functions with domain \\(X\\).\n[Note that boundedness is redundant if \\(X\\) is compact (Theorem 4.15). Thus \\(\\mathscr{C}(X)\\) consists of all complex continuous functions on \\(X\\) if \\(X\\) is compact.]\nWe associate with each \\(f \\in \\mathscr{C}(X)\\) its supremum norm\n\\[ \\|f\\|=\\sup _{x \\in X}|f(x)| . \\]\nSince \\(f\\) is assumed to be bounded, \\(\\|f\\|\u0026lt;\\infty\\). It is obvious that \\(\\|f\\|=0\\) only if \\(f(x)=0\\) for every \\(x \\in X\\), that is, only if \\(f=0\\). If \\(h=f+g\\), then\n\\[ |h(x)| \\leq|f(x)|+|g(x)| \\leq\\|f\\|+\\|g\\| \\]\nfor all \\(x \\in X\\); hence\n\\[ \\|f+g\\| \\leq\\|f\\|+\\|g\\| . \\]\nIf we define the distance between \\(f \\in \\mathscr{C}(X)\\) and \\(g \\in \\mathscr{C}(X)\\) to be \\(\\|f-g\\|\\), it follows that Axioms \\(2.15\\) for a metric are satisfied.\nWe have thus made \\(\\mathscr{C}(X)\\) into a metric space.\nTheorem \\(7.9\\) can be rephrased as follows:\nA sequence \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\) with respect to the metric of \\(\\mathscr{C}(X)\\) if and only if \\(f_{n} \\rightarrow f\\) uniformly on \\(X\\).\nAccordingly, closed subsets of \\(\\mathscr{C}(X)\\) are sometimes called uniformly closed, the closure of a set \\(\\mathscr{A} \\subset \\mathscr{C}(X)\\) is called its uniform closure, and so on.\n7.15 Theorem The above metric makes \\(\\mathscr{B}(X)\\) into a complete metric space.\nProof Let \\(\\left\\{f_{n}\\right\\}\\) be a Cauchy sequence in \\(\\mathscr{C}(X)\\). This means that to each \\(\\varepsilon\u0026gt;0\\) corresponds an \\(N\\) such that \\(\\left\\|f_{n}-f_{m}\\right\\|\u0026lt;\\varepsilon\\) if \\(n \\geq N\\) and \\(m \\geq N\\). It follows (by Theorem 7.8) that there is a function \\(f\\) with domain \\(X\\) to which \\(\\left\\{f_{n}\\right\\}\\) converges uniformly. By Theorem 7.12, \\(f\\) is continuous. Moreover, \\(f\\) is bounded, since there is an \\(n\\) such that \\(\\left|f(x)-f_{n}(x)\\right|\u0026lt;1\\) for all \\(x \\in X\\), and \\(f_{n}\\) is bounded.\\(\\blacksquare\\)\nThus \\(f \\in \\mathscr{C}(X)\\), and since \\(f_{n} \\rightarrow f\\) uniformly on \\(X\\), we have \\(\\left\\|f-f_{n}\\right\\| \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\).\n","date":"2022-07-31T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/3-uniform-convergence-and-continuity/","section":"baby rudin","tags":null,"title":"3 UNIFORM CONVERGENCE AND CONTINUITY"},{"categories":null,"contents":"7.16 Theorem Let \\(\\alpha\\) be monotonically increasing on [a, b]. Suppose \\(f_{n} \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\), for \\(n=1,2,3, \\ldots\\), and suppose \\(f_{n} \\rightarrow f\\) uniformly on \\([a, b]\\). Then \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\), and \\[ \\int_{a}^{b} f d \\alpha=\\lim _{n \\rightarrow \\infty} \\int_{a}^{b} f_{n} d \\alpha . \\]\n(The existence of the limit is part of the conclusion.)\n**Proof* It suffices to prove this for real \\(f_{n}\\). Put\n\\[ \\varepsilon_{n}=\\sup \\left|f_{n}(x)-f(x)\\right|, \\] the supremum being taken over \\(a \\leq x \\leq b\\). Then\n\\[ f_{n}-\\varepsilon_{n} \\leq f \\leq f_{n}+\\varepsilon_{n}, \\]\nso that the upper and lower integrals of \\(f\\) (see Definition 6.2) satisfy\n\\[ \\int_{a}^{b}\\left(f_{n}-\\varepsilon_{n}\\right) d \\alpha \\leq \\int_{-} f d \\alpha \\leq \\bar{\\int} f d \\alpha \\leq \\int_{a}^{b}\\left(f_{n}+\\varepsilon_{n}\\right) d \\alpha . \\]\nHence\n\\[ 0 \\leq \\bar{\\int} f d \\alpha-\\int_{-} f d \\alpha \\leq 2 \\varepsilon_{n}[\\alpha(b)-\\alpha(a)] \\]\nSince \\(\\varepsilon_{n} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\) (Theorem 7.9), the upper and lower integrals of \\(f\\) are equal.\nThus \\(f \\in \\mathscr{R}(\\alpha)\\). Another application of (25) now yields\n\\[ \\left|\\int_{a}^{b} f d \\alpha-\\int_{a}^{b} f_{n} d \\alpha\\right| \\leq \\varepsilon_{n}[\\alpha(b)-\\alpha(a)] . \\]\nThis implies (23).\\(\\blacksquare\\)\nCorollary If \\(f_{n} \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\) and if \\[ f(x)=\\sum_{n=1}^{\\infty} f_{n}(x) \\quad(a \\leq x \\leq b), \\] the series converging uniformly on \\([a, b]\\), then \\[ \\int_{a}^{b} f d \\alpha=\\sum_{n=1}^{\\infty} \\int_{a}^{b} f_{n} d \\alpha . \\]\nIn other words, the series may be integrated term by term.\n","date":"2022-07-31T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/4-uniform-convergence-and-integration/","section":"baby rudin","tags":null,"title":"4 UNIFORM CONVERGENCE AND INTEGRATION"},{"categories":null,"contents":"In the present chapter we confine our attention to complex-valued functions (including the real-valued ones, of course), although many of the theorems and proofs which follow extend without difficulty to vector-valued functions, and even to mappings into general metric spaces. We choose to stay within this simple framework in order to focus attention on the most important aspects of the problems that arise when limit processes are interchanged.\n","date":"2022-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/0-intro/","section":"baby rudin","tags":null,"title":"0 SEQUENCES AND SERIES OF FUNCTIONS"},{"categories":null,"contents":"7.1 Definition Suppose \\(\\left\\{f_{n}\\right\\}, n=1,2,3, \\ldots\\), is a sequence of functions defined on a set \\(E\\), and suppose that the sequence of numbers \\(\\left\\{f_{n}(x)\\right\\}\\) converges for every \\(x \\in E\\). We can then define a function \\(f\\) by \\[ f(x)=\\lim _{n \\rightarrow \\infty} f_{n}(x) \\quad(x \\in E) . \\]\nUnder these circumstances we say that \\(\\left\\{f_{n}\\right\\}\\) converges on \\(E\\) and that \\(f\\) is the limit, or the limit function, of \\(\\left\\{f_{n}\\right\\}\\). Sometimes we shall use a more descriptive terminology and shall say that ” \\(\\left\\{f_{n}\\right\\}\\) converges to \\(f\\) point-wise on \\(E\\) ” if (1) holds. Similarly, if \\(\\Sigma f_{n}(x)\\) converges for every \\(x \\in E\\), and if we define\n\\[ f(x)=\\sum_{n=1}^{\\infty} f_{n}(x) \\quad(x \\in E), \\]\nthe function \\(f\\) is called the sum of the series \\(\\Sigma f_{n}\\).\nThe main problem which arises is to determine whether important properties of functions are preserved under the limit operations (1) and (2). For instance, if the functions \\(f_{n}\\) are continuous, or differentiable, or integrable, is the same true of the limit function? What are the relations between \\(f_{n}^{\\prime}\\) and \\(f^{\\prime}\\), say, or between the integrals of \\(f_{n}\\) and that of \\(f\\) ?\nTo say that \\(f\\) is continuous at a limit point \\(x\\) means\n\\[ \\lim _{t \\rightarrow x} f(t)=f(x) . \\]\nHence, to ask whether the limit of a sequence of continuous functions is continuous is the same as to ask whether\n\\[ \\lim _{t \\rightarrow x} \\lim _{n \\rightarrow \\infty} f_{n}(t)=\\lim _{n \\rightarrow \\infty} \\lim _{t \\rightarrow x} f_{n}(t), \\]\ni.e., whether the order in which limit processes are carried out is immaterial. On the left side of (3), we first let \\(n \\rightarrow \\infty\\), then \\(t \\rightarrow x\\); on the right side, \\(t \\rightarrow x\\) first, then \\(n \\rightarrow \\infty\\).\nWe shall now show by means of several examples that limit processes cannot in general be interchanged without affecting the result. Afterward, we shall prove that under certain conditions the order in which limit operations are carried out is immaterial.\nOur first example, and the simplest one, concerns a “double sequence.”\n7.2 Example For \\(m=1,2,3, \\ldots, n=1,2,3, \\ldots\\), let \\[ s_{m, n}=\\frac{m}{m+n} . \\]\nThen, for every fixed \\(n\\),\n\\[ \\lim _{m \\rightarrow \\infty} s_{m, n}=1, \\]\nso that\n\\[ \\lim _{n \\rightarrow \\infty} \\lim _{m \\rightarrow \\infty} s_{m, n}=1 . \\]\nOn the other hand, for every fixed \\(m\\),\n\\[ \\lim _{n \\rightarrow \\infty} s_{m, n}=0, \\]\nso that\n\\[ \\lim _{m \\rightarrow \\infty} \\lim _{n \\rightarrow \\infty} s_{m, n}=0 . \\]\n\\(7.3\\) Example Let \\[ f_{n}(x)=\\frac{x^{2}}{\\left(1+x^{2}\\right)^{n}} \\quad(x \\text { real; } n=0,1,2, \\ldots), \\]\nand consider\n\\[ f(x)=\\sum_{n=0}^{\\infty} f_{n}(x)=\\sum_{n=0}^{\\infty} \\frac{x^{2}}{\\left(1+x^{2}\\right)^{n}} . \\]\nSince \\(f_{n}(0)=0\\), we have \\(f(0)=0\\). For \\(x \\neq 0\\), the last series in (6) is a convergent geometric series with sum \\(1+x^{2}\\) (Theorem 3.26). Hence\n\\[ f(x)= \\begin{cases}0 \u0026amp; (x=0) \\\\ 1+x^{2} \u0026amp; (x \\neq 0)\\end{cases} \\]\nso that a convergent series of continuous functions may have a discontinuous sum.\n7.4 Example For \\(m=1,2,3, \\ldots\\), put\n\\[ f_{m}(x)=\\lim _{n \\rightarrow \\infty}(\\cos m ! \\pi x)^{2 n} . \\]\nWhen \\(m ! x\\) is an integer, \\(f_{m}(x)=1\\). For all other values of \\(x, f_{m}(x)=0\\). Now let\n\\[ f(x)=\\lim _{m \\rightarrow \\infty} f_{m}(x) . \\]\nFor irrational \\(x, f_{m}(x)=0\\) for every \\(m\\); hence \\(f(x)=0\\). For rational \\(x\\), say \\(x=p / q\\), where \\(p\\) and \\(q\\) are integers, we see that \\(m ! x\\) is an integer if \\(m \\geq q\\), so that \\(f(x)=1\\). Hence\n\\[ \\lim _{m \\rightarrow \\infty} \\lim _{n \\rightarrow \\infty}(\\cos m ! \\pi x)^{2 n}= \\begin{cases}0 \u0026amp; (x \\text { irrational }), \\\\ 1 \u0026amp; (x \\text { rational }) .\\end{cases} \\]\nWe have thus obtained an everywhere discontinuous limit function, which is not Riemann-integrable (Exercise 4, Chap. 6).\n7.5 Example Let\n\\[ f_{n}(x)=\\frac{\\sin n x}{\\sqrt{n}} \\quad(x \\text { real, } n=1,2,3, \\ldots) \\]\nand\n\\[ f(x)=\\lim _{n \\rightarrow \\infty} f_{n}(x)=0 . \\]\nThen \\(f^{\\prime}(x)=0\\), and\n\\[ f_{n}^{\\prime}(x)=\\sqrt{n} \\cos n x, \\]\nso that \\(\\left\\{f_{n}^{\\prime}\\right\\}\\) does not converge to \\(f^{\\prime}\\). For instance,\n\\[ f_{n}^{\\prime}(0)=\\sqrt{n} \\rightarrow+\\infty \\]\nas \\(n \\rightarrow \\infty\\), whereas \\(f^{\\prime}(0)=0\\).\n7.6 Example Let\n\\[ f_{n}(x)=n^{2} x\\left(1-x^{2}\\right)^{n} \\quad(0 \\leq x \\leq 1, n=1,2,3, \\ldots) . \\]\nFor \\(0\u0026lt;x \\leq 1\\), we have\n\\[ \\lim _{n \\rightarrow \\infty} f_{n}(x)=0, \\]\nby Theorem \\(3.20(d)\\). Since \\(f_{n}(0)=0\\), we see that\n\\[ \\lim _{n \\rightarrow \\infty} f_{n}(x)=0 \\quad(0 \\leq x \\leq 1) . \\]\nA simple calculation shows that\n\\[ \\int_{0}^{1} x\\left(1-x^{2}\\right)^{n} d x=\\frac{1}{2 n+2} . \\]\nThus, in spite of (11),\n\\[ \\int_{0}^{1} f_{n}(x) d x=\\frac{n^{2}}{2 n+2} \\rightarrow+\\infty \\]\nas \\(n \\rightarrow \\infty\\).\nIf, in (10), we replace \\(n^{2}\\) by \\(n,(11)\\) still holds, but we now have\n\\[ \\lim _{n \\rightarrow \\infty} \\int_{0}^{1} f_{n}(x) d x=\\lim _{n \\rightarrow \\infty} \\frac{n}{2 n+2}=\\frac{1}{2}, \\]\nwhereas\n\\[ \\int_{0}^{1}\\left[\\lim _{n \\rightarrow \\infty} f_{n}(x)\\right] d x=0 \\]\nThus the limit of the integral need not be equal to the integral of the limit, even if both are finite.\nAfter these examples, which show what can go wrong if limit processes are interchanged carelessly, we now define a new mode of convergence, stronger than pointwise convergence as defined in Definition 7.1, which will enable us to arrive at positive results.\n","date":"2022-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/7-sequences-and-series-of-functions/1-discussion-of-main-problem/","section":"baby rudin","tags":null,"title":"1 DISCUSSION OF MAIN PROBLEM"},{"categories":null,"contents":"6.1 Definition Let \\([a, b]\\) be a given interval. By a partition \\(P\\) of \\([a, b]\\) we mean a finite set of points \\(x_{0}, x_{1}, \\ldots, x_{n}\\), where\n\\[ a=x_{0} \\leq x_{1} \\leq \\cdots \\leq x_{n-1} \\leq x_{n}=b . \\]\nWe write\n\\[ \\Delta x_{i}=x_{i}-x_{i-1} \\quad(i=1, \\ldots, n) \\]\nNow suppose \\(f\\) is a bounded real function defined on \\([a, b]\\). Corresponding to each partition \\(P\\) of \\([a, b]\\) we put\n\\[ \\begin{aligned} M_{i} \u0026amp;=\\sup f(x) \u0026amp; \u0026amp;\\left(x_{i-1} \\leq x \\leq x_{i}\\right) \\\\ m_{i} \u0026amp;=\\inf f(x) \u0026amp; \u0026amp;\\left(x_{i-1} \\leq x \\leq x_{i}\\right) \\\\ U(P, f) \u0026amp;=\\sum_{i=1}^{n} M_{i} \\Delta x_{i}, \u0026amp; \\\\ L(P, f) \u0026amp;=\\sum_{i=1}^{n} m_{i} \\Delta x_{i}, \u0026amp; \\end{aligned} \\]\nand finally\n\\[ \\begin{aligned} \u0026amp;\\int_{a}^{b} f d x=\\inf U(P, f) \\\\ \u0026amp;\\int_{a}^{b} f d x=\\sup L(P, f) \\end{aligned} \\]\nwhere the inf and the sup are taken over all partitions \\(P\\) of \\([a, b]\\). The left members of (1) and (2) are called the upper and lower Riemann integrals of \\(f\\) over \\([a, b]\\), respectively.\nIf the upper and lower integrals are equal, we say that \\(f\\) is Riemannintegrable on \\([a, b]\\), we write \\(f \\in \\mathscr{R}\\) (that is, \\(\\mathscr{R}\\) denotes the set of Riemannintegrable functions), and we denote the common value of (1) and (2) by\n\\[ \\int_{a}^{b} f d x \\]\nor by\n\\[ \\int_{a}^{b} f(x) d x . \\]\nThis is the Riemann integral of \\(f\\) over \\([a, b]\\). Since \\(f\\) is bounded, there exist two numbers, \\(m\\) and \\(M\\), such that\n\\[ m \\leq f(x) \\leq M \\quad(a \\leq x \\leq b) . \\]\nHence, for every \\(P\\)\n\\[ m(b-a) \\leq L(P, f) \\leq U(P, f) \\leq M(b-a), \\]\nso that the numbers \\(L(P, f)\\) and \\(U(P, f)\\) form a bounded set. This shows that the upper and lower integrals are defined for every bounded function \\(f\\). The question of their equality, and hence the question of the integrability of \\(f\\), is a more delicate one. Instead of investigating it separately for the Riemann integral, we shall immediately consider a more general situation.\n6.2 Definition Let \\(\\alpha\\) be a monotonically increasing function on \\([a, b]\\) (since \\(\\alpha(a)\\) and \\(\\alpha(b)\\) are finite, it follows that \\(\\alpha\\) is bounded on \\([a, b])\\). Corresponding to each partition \\(P\\) of \\([a, b]\\), we write \\[ \\Delta \\alpha_{i}=\\alpha\\left(x_{i}\\right)-\\alpha\\left(x_{i-1}\\right) . \\]\nIt is clear that \\(\\Delta \\alpha_{i} \\geq 0\\). For any real function \\(f\\) which is bounded on \\([a, b]\\) we put\n\\[ \\begin{aligned} \u0026amp;U(P, f, \\alpha)=\\sum_{i=1}^{n} M_{i} \\Delta \\alpha_{i}, \\\\ \u0026amp;L(P, f, \\alpha)=\\sum_{i=1}^{n} m_{i} \\Delta \\alpha_{i}, \\end{aligned} \\]\nwhere \\(M_{i}, m_{i}\\) have the same meaning as in Definition 6.1, and we define\n\\[ \\begin{aligned} \u0026amp;\\bar{\\int}_{a}^{b} f d \\alpha=\\inf U(P, f, \\alpha), \\\\ \u0026amp;\\int_{a}^{b} f d \\alpha=\\sup L(P, f, \\alpha), \\end{aligned} \\]\nthe inf and sup again being taken over all partitions.\nIf the left members of (5) and (6) are equal, we denote their common value by\n\\[ \\int_{a}^{b} f d \\alpha \\]\nor sometimes by\n\\[ \\int_{a}^{b} f(x) d \\alpha(x) . \\]\nThis is the Riemann-Stieltjes integral (or simply the Stieltjes integral) of \\(f\\) with respect to \\(\\alpha\\), over \\([a, b]\\).\nIf (7) exists, i.e., if (5) and (6) are equal, we say that \\(f\\) is integrable with respect to \\(\\alpha\\), in the Riemann sense, and write \\(f \\in \\mathscr{R}(\\alpha)\\).\nBy taking \\(\\alpha(x)=x\\), the Riemann integral is seen to be a special case of the Riemann-Stieltjes integral. Let us mention explicitly, however, that in the general case \\(\\alpha\\) need not even be continuous.\nA few words should be said about the notation. We prefer (7) to (8), since the letter \\(x\\) which appears in (8) adds nothing to the content of (7). It is immaterial which letter we use to represent the so-called “variable of integration.” For instance, (8) is the same as\n\\[ \\int_{a}^{b} f(y) d \\alpha(y) \\]\nThe integral depends on \\(f, \\alpha, a\\) and \\(b\\), but not on the variable of integration, which may as well be omitted.\nThe role played by the variable of integration is quite analogous to that of the index of summation: The two symbols\n\\[ \\sum_{i=1}^{n} c_{i}, \\quad \\sum_{k=1}^{n} c_{k} \\]\nmean the same thing, since each means \\(c_{1}+c_{2}+\\cdots+c_{n}\\).\nOf course, no harm is done by inserting the variable of integration, and in many cases it is actually convenient to do so.\nWe shall now investigate the existence of the integral (7). Without saying so every time, \\(f\\) will be assumed real and bounded, and \\(\\alpha\\) monotonically increasing on \\([a, b]\\); and, when there can be no misunderstanding, we shall write \\(\\int\\) in place of \\(\\int_{a}^{b}\\)\n6.3 Definition We say that the partition \\(P^{*}\\) is a refinement of \\(P\\) if \\(P^{*} \\supset P\\) (that is, if every point of \\(P\\) is a point of \\(P^{*}\\) ). Given two partitions, \\(P_{1}\\) and \\(P_{2}\\), we say that \\(P^{*}\\) is their common refinement if \\(P^{*}=P_{1} \\cup P_{2}\\).\n6.4 Theorem If \\(P^{*}\\) is a refinement of \\(P\\), then \\[ L(P, f, \\alpha) \\leq L\\left(P^{*}, f, \\alpha\\right) \\]\nand\n\\[ U\\left(P^{*}, f, \\alpha\\right) \\leq U(P, f, \\alpha) . \\]\nProof To prove (9), suppose first that \\(P^{*}\\) contains just one point more than \\(P\\). Let this extra point be \\(x^{*}\\), and suppose \\(x_{i-1}\u0026lt;x^{*}\u0026lt;x_{i}\\), where \\(x_{i-1}\\) and \\(x_{i}\\) are two consecutive points of \\(P\\). Put\nClearly \\(w_{1} \\geq m_{i}\\) and \\(w_{2} \\geq m_{i}\\), where, as before,\n\\[ m_{i}=\\inf f(x) \\quad\\left(x_{i-1} \\leq x \\leq x_{i}\\right) . \\]\nHence\n\\[ \\begin{aligned} \u0026amp;L\\left(P^{*}, f, \\alpha\\right)-L(P, f, \\alpha) \\\\ \u0026amp;\\quad=w_{1}\\left[\\alpha\\left(x^{*}\\right)-\\alpha\\left(x_{i-1}\\right)\\right]+w_{2}\\left[\\alpha\\left(x_{i}\\right)-\\alpha\\left(x^{*}\\right)\\right]-m_{i}\\left[\\alpha\\left(x_{i}\\right)-\\alpha\\left(x_{i-1}\\right)\\right] \\\\ \u0026amp;\\quad=\\left(w_{1}-m_{i}\\right)\\left[\\alpha\\left(x^{*}\\right)-\\alpha\\left(x_{i-1}\\right)\\right]+\\left(w_{2}-m_{i}\\right)\\left[\\alpha\\left(x_{i}\\right)-\\alpha\\left(x^{*}\\right)\\right] \\geq 0 . \\end{aligned} \\]\nIf \\(P^{*}\\) contains \\(k\\) points more than \\(P\\), we repeat this reasoning \\(k\\) times, and arrive at (9). The proof of \\((10)\\) is analogous.\n\\[ \\begin{aligned} \u0026amp; w_{1}=\\inf f(x) \\quad\\left(x_{i-1} \\leq x \\leq x^{*}\\right), \\\\ \u0026amp; w_{2}=\\inf f(x) \\quad\\left(x^{*} \\leq x \\leq x_{i}\\right) \\text {. } \\end{aligned} \\]\n6.5 Theorem \\(\\int_{a}^{b} f d \\alpha \\leq \\int_{a}^{b} f d \\alpha\\).\nProof Let \\(P^{*}\\) be the common refinement of two partitions \\(P_{1}\\) and \\(P_{2}\\). By Theorem 6.4,\n\\[ L\\left(P_{1}, f, \\alpha\\right) \\leq L\\left(P^{*}, f, \\alpha\\right) \\leq U\\left(P^{*}, f, \\alpha\\right) \\leq U\\left(P_{2}, f, \\alpha\\right) . \\]\nHence\n\\[ L\\left(P_{1}, f, \\alpha\\right) \\leq U\\left(P_{2}, f, \\alpha\\right) . \\]\nIf \\(P_{2}\\) is fixed and the sup is taken over all \\(P_{1},(11)\\) gives\n\\[ \\int f d \\alpha \\leq U\\left(P_{2}, f, \\alpha\\right) \\]\nThe theorem follows by taking the inf over all \\(P_{2}\\) in (12).\n6.6 Theorem \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\) if and only if for every \\(\\varepsilon\u0026gt;0\\) there exists a partition \\(P\\) such that \\[ U(P, f, \\alpha)-L(P, f, \\alpha)\u0026lt;\\varepsilon . \\]\nProof For every \\(P\\) we have\n\\[ L(P, f, \\alpha) \\leq \\int f d \\alpha \\leq \\bar{\\int} f d \\alpha \\leq U(P, f, \\alpha) . \\]\nThus (13) implies\n\\[ 0 \\leq \\bar{\\int} f d \\alpha-\\int_{-} f d \\alpha\u0026lt;\\varepsilon . \\]\nHence, if (13) can be satisfied for every \\(\\varepsilon\u0026gt;0\\), we have\n\\[ \\int f d \\alpha=\\int_{-} f d \\alpha \\]\nthat is, \\(f \\in \\mathscr{R}(\\alpha)\\).\nConversely, suppose \\(f \\in \\mathscr{R}(\\alpha)\\), and let \\(\\varepsilon\u0026gt;0\\) be given. Then there exist partitions \\(P_{1}\\) and \\(P_{2}\\) such that\n\\[ \\begin{aligned} \u0026amp;U\\left(P_{2}, f, \\alpha\\right)-\\int f d \\alpha\u0026lt;\\frac{\\varepsilon}{2} \\\\ \u0026amp;\\int f d \\alpha-L\\left(P_{1}, f, \\alpha\\right)\u0026lt;\\frac{\\varepsilon}{2} \\end{aligned} \\]\nWe choose \\(P\\) to be the common refinement of \\(P_{1}\\) and \\(P_{2}\\). Then Theorem \\(6.4\\), together with (14) and (15), shows that\n\\[ U(P, f, \\alpha) \\leq U\\left(P_{2}, f, \\alpha\\right)\u0026lt;\\int f d \\alpha+\\frac{\\varepsilon}{2}\u0026lt;L\\left(P_{1}, f, \\alpha\\right)+\\varepsilon \\leq L(P, f, \\alpha)+\\varepsilon \\]\nso that (13) holds for this partition \\(P\\).\nTheorem \\(6.6\\) furnishes a convenient criterion for integrability. Before we apply it, we state some closely related facts.\n6.7 Theorem\nIf (13) holds for some \\(P\\) and some \\(\\varepsilon\\), then (13) holds (with the same \\(\\varepsilon\\) ) for every refinement of \\(P\\).\nIf (13) holds for \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) and if \\(s_{i}, t_{i}\\) are arbitrary points in \\(\\left[x_{i-1}, x_{i}\\right]\\), then\n\\[ \\sum_{i=1}^{n}\\left|f\\left(s_{i}\\right)-f\\left(t_{i}\\right)\\right| \\Delta \\alpha_{i}\u0026lt;\\varepsilon . \\]\nIf \\(f \\in \\mathscr{R}(\\alpha)\\) and the hypotheses of \\((b)\\) hold, then \\[ \\left|\\sum_{i=1}^{n} f\\left(t_{i}\\right) \\Delta \\alpha_{i}-\\int_{a}^{b} f d \\alpha\\right|\u0026lt;\\varepsilon . \\]\nProof Theorem \\(6.4\\) implies (a). Under the assumptions made in (b), both \\(f\\left(s_{i}\\right)\\) and \\(f\\left(t_{i}\\right)\\) lie in \\(\\left[m_{i}, M_{i}\\right]\\), so that \\(\\left|f\\left(s_{i}\\right)-f\\left(t_{i}\\right)\\right| \\leq M_{i}-m_{i}\\). Thus\n\\[ \\sum_{i=1}^{n}\\left|f\\left(s_{i}\\right)-f\\left(t_{i}\\right)\\right| \\Delta \\alpha_{i} \\leq U(P, f, \\alpha)-L(P, f, \\alpha), \\]\nwhich proves \\((b)\\). The obvious inequalities\n\\[ L(P, f, \\alpha) \\leq \\sum f\\left(t_{i}\\right) \\Delta \\alpha_{i} \\leq U(P, f, \\alpha) \\]\nand\n\\[ L(P, f, \\alpha) \\leq \\int f d \\alpha \\leq U(P, f, \\alpha) \\]\nprove \\((c)\\)\n6.8 Theorem If \\(f\\) is continuous on \\([a, b]\\) then \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\).\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Choose \\(\\eta\u0026gt;0\\) so that\n\\[ [\\alpha(b)-\\alpha(a)] \\eta\u0026lt;\\varepsilon . \\]\nSince \\(f\\) is uniformly continuous on \\([a, b]\\) (Theorem 4.19), there exists a \\(\\delta\u0026gt;0\\) such that\n\\[ |f(x)-f(t)|\u0026lt;\\eta \\]\nif \\(x \\in[a, b], t \\in[a, b]\\), and \\(|x-t|\u0026lt;\\delta\\).\nIf \\(P\\) is any partition of \\([a, b]\\) such that \\(\\Delta x_{i}\u0026lt;\\delta\\) for all \\(i\\), then (16) implies that\n\\[ M_{i}-m_{i} \\leq \\eta \\quad(i-1, \\ldots, n) \\]\nand therefore\n\\[ \\begin{array}{r} U(P, f, \\alpha)-L(P, f, \\alpha)=\\sum_{i=1}^{n}\\left(M_{i}-m_{i}\\right) \\Delta \\alpha_{i} \\\\ \\leq \\eta \\sum_{i=1}^{n} \\Delta \\alpha_{i}=\\eta[\\alpha(b)-\\alpha(a)]\u0026lt;\\varepsilon . \\end{array} \\]\nBy Theorem 6.6, \\(f \\in \\mathscr{R}(\\alpha)\\).\n6.9 Theorem If \\(f\\) is monotonic on \\([a, b]\\), and if \\(\\alpha\\) is continuous on \\([a, b]\\), then \\(f \\in \\mathscr{R}(\\alpha)\\). (We still assume, of course, that \\(\\alpha\\) is monotonic.)\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. For any positive integer \\(n\\), choose a partition such that\n\\[ \\Delta \\alpha_{i}=\\frac{\\alpha(b)-\\alpha(a)}{n} \\quad(i=1, \\ldots, n) . \\]\nThis is possible since \\(\\alpha\\) is continuous (Theorem 4.23).\nWe suppose that \\(f\\) is monotonically increasing (the proof is analogous in the other case). Then\n\\[ M_{i}=f\\left(x_{i}\\right), \\quad m_{i}=f\\left(x_{i-1}\\right) \\quad(i=1, \\ldots, n), \\]\nso that\n\\[ \\begin{aligned} U(P, f, \\alpha)-L(P, f, \\alpha) \u0026amp;=\\frac{\\alpha(b)-\\alpha(a)}{n} \\sum_{i=1}^{n}\\left[f\\left(x_{i}\\right)-f\\left(x_{i-1}\\right)\\right] \\\\ \u0026amp;=\\frac{\\alpha(b)-\\alpha(a)}{n} \\cdot[f(b)-f(a)]\u0026lt;\\varepsilon \\end{aligned} \\]\nif \\(n\\) is taken large enough. By Theorem 6.6, \\(f \\in \\mathscr{R}(\\alpha)\\).\n6.10 Theorem Suppose \\(f\\) is bounded on \\([a, b], f\\) has only finitely many points of discontinuity on \\([a, b]\\), and \\(\\alpha\\) is continuous at every point at which \\(f\\) is discontinuous. Then \\(f \\in \\mathscr{R}(\\alpha)\\).\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Put \\(M=\\sup |f(x)|\\), let \\(E\\) be the set of points at which \\(f\\) is discontinuous. Since \\(E\\) is finite and \\(\\alpha\\) is continuous at every point of \\(E\\), we can cover \\(E\\) by finitely many disjoint intervals \\(\\left[u_{j}, v_{j}\\right] \\subset\\) \\([a, b]\\) such that the sum of the corresponding differences \\(\\alpha\\left(v_{j}\\right)-\\alpha\\left(u_{j}\\right)\\) is less than \\(\\varepsilon\\). Furthermore, we can place these intervals in such a way that every point of \\(E \\cap(a, b)\\) lies in the interior of some \\(\\left[u_{j}, v_{j}\\right]\\). Remove the segments \\(\\left(u_{j}, v_{j}\\right)\\) from \\([a, b]\\). The remaining set \\(K\\) is compact. Hence \\(f\\) is uniformly continuous on \\(K\\), and there exists \\(\\delta\u0026gt;0\\) such that \\(|f(s)-f(t)|\u0026lt;\\varepsilon\\) if \\(s \\in K, t \\in K,|s-t|\u0026lt;\\delta\\).\nNow form a partition \\(P=\\left\\{x_{0}, x_{1}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\), as follows: Each \\(u_{j}\\) occurs in \\(P\\). Each \\(v_{j}\\) occurs in \\(P\\). No point of any segment \\(\\left(u_{j}, v_{j}\\right)\\) occurs in \\(P\\). If \\(x_{i-1}\\) is not one of the \\(u_{j}\\), then \\(\\Delta x_{i}\u0026lt;\\delta\\).\nNote that \\(M_{i}-m_{i} \\leq 2 M\\) for every \\(i\\), and that \\(M_{i}-m_{i} \\leq \\varepsilon\\) unless \\(x_{i-1}\\) is one of the \\(u_{j}\\). Hence, as in the proof of Theorem \\(6.8\\),\n\\[ U(P, f, \\alpha)-L(P, f, \\alpha) \\leq[\\alpha(b)-\\alpha(a)] \\varepsilon+2 M \\varepsilon . \\]\nSince \\(\\varepsilon\\) is arbitrary, Theorem \\(6.6\\) shows that \\(f \\in \\mathscr{R}(\\alpha)\\).\nNote: If \\(f\\) and \\(\\alpha\\) have a common point of discontinuity, then \\(f\\) need not be in \\(\\mathscr{R}(\\alpha)\\). Exercise 3 shows this.\n6.11 Theorem Suppose \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b], m \\leq f \\leq M, \\phi\\) is continuous on \\([m, M]\\), and \\(h(x)=\\phi(f(x))\\) on \\([a, b]\\). Then \\(h \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\).\nProof Choose \\(\\varepsilon\u0026gt;0\\). Since \\(\\phi\\) is uniformly continuous on \\([m, M]\\), there exists \\(\\delta\u0026gt;0\\) such that \\(\\delta\u0026lt;\\varepsilon\\) and \\(|\\phi(s)-\\phi(t)|\u0026lt;\\varepsilon\\) if \\(|s-t| \\leq \\delta\\) and \\(s, t \\in[m, M]\\).\nSince \\(f \\in \\mathscr{R}(\\alpha)\\), there is a partition \\(P=\\left\\{x_{0}, x_{1}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\) such that\n\\[ U(P, f, \\alpha)-L(P, f, \\alpha)\u0026lt;\\delta^{2} . \\]\nLet \\(M_{i}, m_{i}\\) have the same meaning as in Definition 6.1, and let \\(M_{i}^{*}, m_{i}^{*}\\) be the analogous numbers for \\(h\\). Divide the numbers \\(1, \\ldots, n\\) into two classes: \\(i \\in A\\) if \\(M_{i}-m_{i}\u0026lt;\\delta, i \\in B\\) if \\(M_{i}-m_{i} \\geq \\delta\\).\nFor \\(i \\in A\\), our choice of \\(\\delta\\) shows that \\(M_{i}^{*}-m_{i}^{*} \\leq \\varepsilon\\).\nFor \\(i \\in B, M_{i}^{*}-m_{i}^{*} \\leq 2 K\\), where \\(K=\\sup |\\phi(t)|, m \\leq t \\leq M\\). By (18), we have\n\\[ \\delta \\sum_{i \\in B} \\Delta \\alpha_{i} \\leq \\sum_{i \\in B}\\left(M_{i}-m_{i}\\right) \\Delta \\alpha_{i}\u0026lt;\\delta^{2} \\]\nso that \\(\\sum_{i \\in B} \\Delta \\alpha_{i}\u0026lt;\\delta\\). It follows that\n\\[ \\begin{aligned} U(P, h, \\alpha)-L(P, h, \\alpha) \u0026amp;=\\sum_{i \\in A}\\left(M_{i}^{*}-m_{i}^{*}\\right) \\Delta \\alpha_{i}+\\sum_{i \\in B}\\left(M_{i}^{*}-m_{i}^{*}\\right) \\Delta \\alpha_{i} \\\\ \u0026amp; \\leq \\varepsilon[\\alpha(b)-\\alpha(a)]+2 K \\delta\u0026lt;\\varepsilon[\\alpha(b)-\\alpha(a)+2 K] . \\end{aligned} \\]\nSince \\(\\varepsilon\\) was arbitrary, Theorem \\(6.6\\) implies that \\(h \\in \\mathscr{R}(\\alpha)\\).\nRemark: This theorem suggests the question: Just what functions are Riemann-integrable? The answer is given by Theorem \\(11.33(b)\\).\n","date":"2022-07-29T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/1-definition-and-existence-of-the-integral/","section":"baby rudin","tags":null,"title":"DEFINITION AND EXISTENCE OF THE INTEGRAL"},{"categories":null,"contents":"The present chapter is based on a definition of the Riemann integral which depends very explicitly on the order structure of the real line. Accordingly, we begin by discussing integration of real-valued functions on intervals. Extensions to complex- and vector-valued functions on intervals follow in later sections. Integration over sets other than intervals is discussed in Chaps. 10 and \\(11 .\\)\n","date":"2022-07-29T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/0-intro/","section":"baby rudin","tags":null,"title":"THE RIEMANN-STIELTJES INTEGRAL"},{"categories":null,"contents":"#include \u0026lt;iostream\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;assert.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; class g_bitset { private: vector\u0026lt;unsigned long long\u0026gt; val; int max_index = 0; int max = 0; public: g_bitset() { val.resize(5010); std::fill(val.begin(), val.end(), 0ull); } // add the nth bit void add(int n) { int index = n / 64; int pos = n % 64; while (__builtin_add_overflow(val[index],(1ull \u0026lt;\u0026lt; pos), \u0026amp;val[index])) { pos = 0; index++; } if (index \u0026gt;= max_index) { max_index = index; max =(63 - __builtin_clzll(val[max_index])) + 64 * (max_index); } } void sub(int n) { int index = n / 64; int pos = n % 64; unsigned long long tmp = 0; while (__builtin_sub_overflow(val[index], (1ull \u0026lt;\u0026lt; pos), \u0026amp;val[index])) { pos = 0; index++; } if (index \u0026gt;= max_index) { while(val[index] == 0) index --; max_index = index; max =(63 - __builtin_clzll(val[max_index])) + 64 * (max_index); } } int get_max() { return max; } }; int main() { ios_base::sync_with_stdio(false); cin.tie(0), cout.tie(0); int n; int q; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; vector\u0026lt;int\u0026gt; a(n); g_bitset bs; for (auto \u0026amp;i: a) { cin \u0026gt;\u0026gt; i; bs.add(i); } while (q--) { int k, l; cin \u0026gt;\u0026gt; k \u0026gt;\u0026gt; l; bs.sub(a[k - 1]); bs.add(l); a[k - 1] = l; cout \u0026lt;\u0026lt; bs.get_max() \u0026lt;\u0026lt; endl; } } ","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/algorithm/bitset/","section":"algorithm","tags":null,"title":"bitset"},{"categories":null,"contents":"5.14 Definition If \\(f\\) has a derivative \\(f^{\\prime}\\) on an interval, and if \\(f^{\\prime}\\) is itself differentiable, we denote the derivative of \\(f^{\\prime}\\) by \\(f^{\\prime \\prime}\\) and call \\(f^{\\prime \\prime}\\) the second derivative of \\(f\\). Continuing in this manner, we obtain functions \\[ f, f^{\\prime}, f^{\\prime \\prime}, f^{(3)}, \\ldots, f^{(n)}, \\]\neach of which is the derivative of the preceding one. \\(f^{(n)}\\) is called the \\(n\\)th derivative, or the derivative of order \\(n\\), of \\(f\\).\nIn order for \\(f^{(n)}(x)\\) to exist at a point \\(x, f^{(n-1)}(t)\\) must exist in a neighborhood of \\(x\\) (or in a one-sided neighborhood, if \\(x\\) is an endpoint of the interval on which \\(f\\) is defined), and \\(f^{(n-1)}\\) must be differentiable at \\(x\\). Since \\(f^{(n-1)}\\) must exist in a neighborhood of \\(x, f^{(n-2)}\\) must be differentiable in that neighborhood.\n","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/5-derivatives-of-higher-order/","section":"baby rudin","tags":null,"title":"DERIVATIVES OF HIGHER ORDER"},{"categories":null,"contents":"5.16 Remarks Definition \\(5.1\\) applies without any change to complex functions \\(f\\) defined on \\([a, b]\\), and Theorems \\(5.2\\) and \\(5.3\\), as well as their proofs, remain valid. If \\(f_{1}\\) and \\(f_{2}\\) are the real and imaginary parts of \\(f\\), that is, if \\[ f(t)=f_{1}(t)+i f_{2}(t) \\]\nfor \\(a \\leq t \\leq b\\), where \\(f_{1}(t)\\) and \\(f_{2}(t)\\) are real, then we clearly have\n\\[ f^{\\prime}(x)=f_{1}^{\\prime}(x)+i f_{2}^{\\prime}(x) \\]\nalso, \\(f\\) is differentiable at \\(x\\) if and only if both \\(f_{1}\\) and \\(f_{2}\\) are differentiable at \\(x\\). Passing to vector-valued functions in general, i.e., to functions \\(\\mathbf{f}\\) which map \\([a, b]\\) into some \\(R^{k}\\), we may still apply Definition \\(5.1\\) to define \\(\\mathrm{f}^{\\prime}(x)\\). The term \\(\\phi(t)\\) in (1) is now, for each \\(t\\), a point in \\(R^{k}\\), and the limit in (2) is taken with respect to the norm of \\(R^{k}\\). In other words, \\(f^{\\prime}(x)\\) is that point of \\(R^{k}\\) (if there is one) for which\n\\[ \\lim _{t \\rightarrow x}\\left|\\frac{f(t)-f(x)}{t-x}-f^{\\prime}(x)\\right|=0, \\]\nand \\(\\mathrm{f}^{\\prime}\\) is again a function with values in \\(R^{k}\\).\nIf \\(f_{1}, \\ldots, f_{k}\\) are the components of \\(f\\), as defined in Theorem \\(4.10\\), then\n\\[ \\mathbf{f}^{\\prime}=\\left(f_{1}^{\\prime}, \\ldots, f_{k}^{\\prime}\\right), \\]\nand \\(f\\) is differentiable at a point \\(x\\) if and only if each of the functions \\(f_{1}, \\ldots, f_{k}\\) is differentiable at \\(x\\).\nTheorem \\(5.2\\) is true in this context as well, and so is Theorem 5.3(a) and (b), if \\(f g\\) is replaced by the inner product \\(f \\cdot g\\) (see Definition 4.3).\nWhen we turn to the mean value theorem, however, and to one of its consequences, namely, L’Hospital’s rule, the situation changes. The next two examples will show that each of these results fails to be true for complex-valued functions.\n5.17 Example Define, for real \\(x\\), \\[ f(x)=e^{i x}=\\cos x+i \\sin x . \\]\n(The last expression may be taken as the definition of the complex exponential \\(e^{i x}\\); see Chap. 8 for a full discussion of these functions.) Then\n\\[ f(2 \\pi)-f(0)=1-1=0 \\]\nbut\n\\[ f^{\\prime}(x)=i e^{i x} \\]\nso that \\(\\left|f^{\\prime}(x)\\right|=1\\) for all real \\(x\\).\nThus Theorem \\(5.10\\) fails to hold in this case.\n5.18 Example On the segment \\((0,1)\\), define \\(f(x)=x\\) and \\[ g(x)=x+x^{2} e^{i / x^{2}} . \\]\nSince \\(\\left|e^{i t}\\right|=1\\) for all real \\(t\\), we see that\n\\[ \\lim _{x \\rightarrow 0} \\frac{f(x)}{g(x)}=1 . \\]\nNext,\n\\[ g^{\\prime}(x)=1+\\left\\{2 x-\\frac{2 i}{x}\\right\\} e^{i / x^{2}} \\quad(0\u0026lt;x\u0026lt;1) \\]\nso that\n\\[ \\left|g^{\\prime}(x)\\right| \\geq\\left|2 x-\\frac{2 i}{x}\\right|-1 \\geq \\frac{2}{x}-1 \\]\nHence\n\\[ \\left|\\frac{f^{\\prime}(x)}{g^{\\prime}(x)}\\right|=\\frac{1}{\\left|g^{\\prime}(x)\\right|} \\leq \\frac{x}{2-x} \\]\nand so\n\\[ \\lim _{x \\rightarrow 0} \\frac{f^{\\prime}(x)}{g^{\\prime}(x)}=0 . \\]\nBy (36) and (40), L’Hospital’s rule fails in this case. Note also that \\(g^{\\prime}(x) \\neq 0\\) on \\((0,1)\\), by (38).\nHowever, there is a consequence of the mean value theorem which, for purposes of applications, is almost as useful as Theorem 5.10, and which remains true for vector-valued functions: From Theorem \\(5.10\\) it follows that\n\\[ |f(b)-f(a)| \\leq(b-a) \\sup _{a\u0026lt;x\u0026lt;b}\\left|f^{\\prime}(x)\\right| . \\]\n5.19 Theorem Suppose \\(\\mathrm{f}\\) is a continuous mapping of \\([a, b]\\) into \\(R^{k}\\) and \\(\\mathrm{f}\\) is differentiable in \\((a, b)\\). Then there exists \\(x \\in(a, b)\\) such that \\[ |\\mathbf{f}(b)-\\mathbf{f}(a)| \\leq(b-a)\\left|\\mathbf{f}^{\\prime}(x)\\right| . \\]\nProof \\(^{1} \\quad\\) Put \\(\\mathbf{z}=\\mathbf{f}(b)-\\mathbf{f}(a)\\), and define\n\\[ \\varphi(t)=\\mathbf{z} \\cdot \\mathbf{f}(t) \\quad(a \\leq t \\leq b) . \\]\nThen \\(\\varphi\\) is a real-valued continuous function on \\([a, b]\\) which is differentiable in \\((a, b)\\). The mean value theorem shows therefore that\n\\[ \\varphi(b)-\\varphi(a)=(b-a) \\varphi^{\\prime}(x)=(b-a) \\mathbf{z} \\cdot \\mathbf{f}^{\\prime}(x) \\]\nfor some \\(x \\in(a, b)\\). On the other hand, \\[ \\varphi(b)-\\varphi(a)=\\mathbf{z} \\cdot \\mathbf{f}(b)-\\mathbf{z} \\cdot \\mathbf{f}(a)=\\mathbf{z} \\cdot \\mathbf{z}=|\\mathbf{z}|^{2} . \\]\nThe Schwarz inequality now gives\n\\[ |\\mathbf{z}|^{2}=(b-a)\\left|\\mathbf{z} \\cdot \\mathbf{f}^{\\prime}(x)\\right| \\leq(b-a)|\\mathbf{z}|\\left|\\mathbf{f}^{\\prime}(x)\\right| \\text {. } \\]\nHence \\(|\\mathbf{z}| \\leq(b-a)\\left|\\mathbf{f}^{\\prime}(x)\\right|\\), which is the desired conclusion.\n","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/7-differentiation-of-vector-valued-functions/","section":"baby rudin","tags":null,"title":"DIFFERENTIATION OF VECTOR-VALUED FUNCTIONS"},{"categories":null,"contents":"The following theorem is frequently useful in the evaluation of limits.\n5.13 Theorem Suppose \\(f\\) and \\(g\\) are real and differentiable in \\((a, b)\\), and \\(g^{\\prime}(x) \\neq 0\\) for all \\(x \\in(a, b)\\), where \\(-\\infty \\leq a\u0026lt;b \\leq+\\infty\\). Suppose \\[ \\frac{f^{\\prime}(x)}{g^{\\prime}(x)} \\rightarrow A \\text { as } x \\rightarrow a \\text {. } \\]\nIf\n\\[ f(x) \\rightarrow 0 \\text { and } g(x) \\rightarrow 0 \\text { as } x \\rightarrow a, \\]\nor if\n\\[ g(x) \\rightarrow+\\infty \\text { as } x \\rightarrow a, \\]\nthen\n\\[ \\frac{f(x)}{g(x)} \\rightarrow A \\text { as } x \\rightarrow a . \\]\nThe analogous statement is of course also true if \\(x \\rightarrow b\\), or if \\(g(x) \\rightarrow-\\infty\\) in (15). Let us note that we now use the limit concept in the extended sense of Definition 4.33.\nProof We first consider the case in which \\(-\\infty \\leq A\u0026lt;+\\infty\\). Choose a real number \\(q\\) such that \\(A\u0026lt;q\\), and then choose \\(r\\) such that \\(A\u0026lt;r\u0026lt;q\\). By (13) there is a point \\(c \\in(a, b)\\) such that \\(a\u0026lt;x\u0026lt;c\\) implies\n\\[ \\frac{f^{\\prime}(x)}{g^{\\prime}(x)}\u0026lt;r . \\]\nIf \\(a\u0026lt;x\u0026lt;y\u0026lt;c\\), then Theorem \\(5.9\\) shows that there is a point \\(t \\in(x, y)\\) such that\n\\[ \\frac{f(x)-f(y)}{g(x)-g(y)}=\\frac{f^{\\prime}(t)}{g^{\\prime}(t)}\u0026lt;r . \\]\nSuppose (14) holds. Letting \\(x \\rightarrow a\\) in (18), we see that\n\\[ \\frac{f(y)}{g(y)} \\leq r\u0026lt;q \\quad(a\u0026lt;y\u0026lt;c) . \\]\nNext, suppose (15) holds. Keeping \\(y\\) fixed in (18), we can choose a point \\(c_{1} \\in(a, y)\\) such that \\(g(x)\u0026gt;g(y)\\) and \\(g(x)\u0026gt;0\\) if \\(a\u0026lt;x\u0026lt;c_{1}\\). Multiplying (18) by \\([g(x)-g(y)] / g(x)\\), we obtain\n\\[ \\frac{f(x)}{g(x)}\u0026lt;r-r \\frac{g(y)}{g(x)}+\\frac{f(y)}{g(x)} \\quad\\left(a\u0026lt;x\u0026lt;c_{1}\\right) . \\]\nIf we let \\(x \\rightarrow a\\) in \\((20),(15)\\) shows that there is a point \\(c_{2} \\in\\left(a, c_{1}\\right)\\) such that\n\\[ \\frac{f(x)}{g(x)}\u0026lt;q \\quad\\left(a\u0026lt;x\u0026lt;c_{2}\\right) . \\]\nSumming up, (19) and (21) show that for any \\(q\\), subject only to the condition \\(A\u0026lt;q\\), there is a point \\(c_{2}\\) such that \\(f(x) / g(x)\u0026lt;q\\) if \\(a\u0026lt;x\u0026lt;c_{2}\\).\nIn the same manner, if \\(-\\infty\u0026lt;A \\leq+\\infty\\), and \\(p\\) is chosen so that \\(p\u0026lt;A\\), we can find a point \\(c_{3}\\) such that\n\\[ p\u0026lt;\\frac{f(x)}{g(x)} \\quad\\left(a\u0026lt;x\u0026lt;c_{3}\\right), \\]\nand (16) follows from these two statements.\n","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/4-lhospitals-rule/","section":"baby rudin","tags":null,"title":"L'HOSPITAL'S RULE"},{"categories":null,"contents":"5.15 Theorem Suppose \\(f\\) is a real function on \\([a, b], n\\) is a positive integer, \\(f^{(n-1)}\\) is continuous on \\([a, b], f^{(n)}(t)\\) exists for every \\(t \\in(a, b)\\). Let \\(\\alpha, \\beta\\) be distinct points of \\([a, b]\\), and define \\[ P(t)=\\sum_{k=0}^{n-1} \\frac{f^{(k)}(\\alpha)}{k !}(t-\\alpha)^{k} . \\]\nThen there exists a point \\(x\\) between \\(\\alpha\\) and \\(\\beta\\) such that\n\\[ f(\\beta)=P(\\beta)+\\frac{f^{(n)}(x)}{n !}(\\beta-\\alpha)^{n} . \\]\nFor \\(n=1\\), this is just the mean value theorem. In general, the theorem shows that \\(f\\) can be approximated by a polynomial of degree \\(n-1\\), and that (24) allows us to estimate the error, if we know bounds on \\(\\left|f^{(n)}(x)\\right|\\).\nProof Let \\(M\\) be the number defined by\n\\[ f(\\beta)=P(\\beta)+M(\\beta-\\alpha)^{n} \\]\nand put\n\\[ g(t)=f(t)-P(t)-M(t-\\alpha)^{n} \\quad(a \\leq t \\leq b) . \\]\nWe have to show that \\(n ! M=f^{(n)}(x)\\) for some \\(x\\) between \\(\\alpha\\) and \\(\\beta\\). By (23) and (26),\n\\[ g^{(n)}(t)=f^{(n)}(t)-n ! M \\quad(a\u0026lt;t\u0026lt;b) . \\]\nHence the proof will be complete if we can show that \\(g^{(n)}(x)=0\\) for some \\(x\\) between \\(\\alpha\\) and \\(\\beta\\).\nSince \\(P^{(k)}(\\alpha)=f^{(k)}(\\alpha)\\) for \\(k=0, \\ldots, n-1\\), we have\n\\[ g(\\alpha)=g^{\\prime}(\\alpha)=\\cdots=g^{(n-1)}(\\alpha)=0 . \\]\nOur choice of \\(M\\) shows that \\(g(\\beta)=0\\), so that \\(g^{\\prime}\\left(x_{1}\\right)=0\\) for some \\(x_{1}\\) between \\(\\alpha\\) and \\(\\beta\\), by the mean value theorem. Since \\(g^{\\prime}(\\alpha)=0\\), we conclude similarly that \\(g^{\\prime \\prime}\\left(x_{2}\\right)=0\\) for some \\(x_{2}\\) between \\(\\alpha\\) and \\(x_{1}\\). After \\(n\\) steps we arrive at the conclusion that \\(g^{(n)}\\left(x_{n}\\right)=0\\) for some \\(x_{n}\\) between \\(\\alpha\\) and \\(x_{n-1}\\), that is, between \\(\\alpha\\) and \\(\\beta\\).\n","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/6-taylors-theorem/","section":"baby rudin","tags":null,"title":"TAYLOR'S THEOREM"},{"categories":null,"contents":"We have already seen [Example 5.6(b)] that a function \\(f\\) may have a derivative \\(f^{\\prime}\\) which exists at every point, but is discontinuous at some point. However, not every function is a derivative. In particular, derivatives which exist at every point of an interval have one important property in common with functions which are continuous on an interval: Intermediate values are assumed (compare Theorem 4.23). The precise statement follows.\n5.12 Theorem Suppose \\(f\\) is a real differentiable function on \\([a, b]\\) and suppose \\(f^{\\prime}(a)\u0026lt;\\lambda\u0026lt;f^{\\prime}(b)\\). Then there is a point \\(x \\in(a, b)\\) such that \\(f^{\\prime}(x)=\\lambda\\).\nA similar result holds of course if \\(f^{\\prime}(a)\u0026gt;f^{\\prime}(b)\\).\nProof Put \\(g(t)=f(t)-\\lambda t\\). Then \\(g^{\\prime}(a)\u0026lt;0\\), so that \\(g\\left(t_{1}\\right)\u0026lt;g(a)\\) for some \\(t_{1} \\in(a, b)\\), and \\(g^{\\prime}(b)\u0026gt;0\\), so that \\(g\\left(t_{2}\\right)\u0026lt;g(b)\\) for some \\(t_{2} \\in(a, b)\\). Hence \\(g\\) attains its minimum on \\([a, b]\\) (Theorem 4.16) at some point \\(x\\) such that \\(a\u0026lt;x\u0026lt;b\\). By Theorem 5.8, \\(g^{\\prime}(x)=0\\). Hence \\(f^{\\prime}(x)=\\lambda\\). Corollary If \\(f\\) is differentiable on \\([a, b]\\), then \\(f^{\\prime}\\) cannot have any simple discontinuities on \\([a, b]\\).\nBut \\(f^{\\prime}\\) may very well have discontinuities of the second kind.\n","date":"2022-07-28T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/3-the-continuity-of-derivatives/","section":"baby rudin","tags":null,"title":"THE CONTINUITY OF DERIVATIVES"},{"categories":null,"contents":"5.7 Definition Let \\(f\\) be a real function defined on a metric space \\(X\\). We say that \\(f\\) has a local maximum at a point \\(p \\in X\\) if there exists \\(\\delta\u0026gt;0\\) such that \\(f(q) \\leq\\) \\(f(p)\\) for all \\(q \\in X\\) with \\(d(p, q)\u0026lt;\\delta\\).\nLocal minima are defined likewise.\nOur next theorem is the basis of many applications of differentiation.\n\\(5.8\\) Theorem Let \\(f\\) be defined on \\([a, b]\\); if \\(f\\) has a local maximum at a point \\(x \\in(a, b)\\), and if \\(f^{\\prime}(x)\\) exists, then \\(f^{\\prime}(x)=0\\).\nThe analogous statement for local minima is of course also true.\nProof Choose \\(\\delta\\) in accordance with Definition 5.7, so that\n\\[ a\u0026lt;x-\\delta\u0026lt;x\u0026lt;x+\\delta\u0026lt;b \\text {. } \\]\nIf \\(x-\\delta\u0026lt;t\u0026lt;x\\), then\n\\[ \\frac{f(t)-f(x)}{t-x} \\geq 0 \\]\nLetting \\(t \\rightarrow x\\), we see that \\(f^{\\prime}(x) \\geq 0\\).\nIf \\(x\u0026lt;t\u0026lt;x+\\delta\\), then\n\\[ \\frac{f(t)-f(x)}{t-x} \\leq 0, \\]\nwhich shows that \\(f^{\\prime}(x) \\leq 0\\). Hence \\(f^{\\prime}(x)=0\\).\n5.9 Theorem If \\(f\\) and \\(g\\) are continuous real functions on \\([a, b]\\) which are differentiable in \\((a, b)\\), then there is a point \\(x \\in(a, b)\\) at which \\[ [f(b)-f(a)] g^{\\prime}(x)=[g(b)-g(a)] f^{\\prime}(x) . \\]\nNote that differentiability is not required at the endpoints.\nProof Put\n\\[ h(t)=[f(b)-f(a)] g(t)-[g(b)-g(a)] f(t) \\quad(a \\leq t \\leq b) . \\]\nThen \\(h\\) is continuous on \\([a, b], h\\) is differentiable in \\((a, b)\\), and\n\\[ h(a)=f(b) g(a)-f(a) g(b)=h(b) . \\]\nTo prove the theorem, we have to show that \\(h^{\\prime}(x)=0\\) for some \\(x \\in(a, b)\\).\nIf \\(h\\) is constant, this holds for every \\(x \\in(a, b)\\). If \\(h(t)\u0026gt;h(a)\\) for some \\(t \\in(a, b)\\), let \\(x\\) be a point on \\([a, b]\\) at which \\(h\\) attains its maximum (Theorem 4.16). By (12), \\(x \\in(a, b)\\), and Theorem \\(5.8\\) shows that \\(h^{\\prime}(x)=0\\). If \\(h(t)\u0026lt;h(a)\\) for some \\(t \\in(a, b)\\), the same argument applies if we choose for \\(x\\) a point on \\([a, b]\\) where \\(h\\) attains its minimum.\nThis theorem is often called a generalized mean value theorem; the following special case is usually referred to as “the” mean value theorem:\n5.10 Theorem Iff is a real continuous function on \\([a, b]\\) which is differentiable in \\((a, b)\\), then there is a point \\(x \\in(a, b)\\) at which \\[ f(b)-f(a)=(b-a) f^{\\prime}(x) . \\]\nProof Take \\(g(x)=x\\) in Theorem 5.9.\n5.11 Theorem Suppose \\(f\\) is differentiable in \\((a, b)\\).\nIf \\(f^{\\prime}(x) \\geq 0\\) for all \\(x \\in(a, b)\\), then \\(f\\) is monotonically increasing.\nIf \\(f^{\\prime}(x)=0\\) for all \\(x \\in(a, b)\\), then \\(f\\) is constant.\nIf \\(f^{\\prime}(x) \\leq 0\\) for all \\(x \\in(a, b)\\), then \\(f\\) is monotonically decreasing.\nProof All conclusions can be read off from the equation\n\\[ f\\left(x_{2}\\right)-f\\left(x_{1}\\right)=\\left(x_{2}-x_{1}\\right) f^{\\prime}(x), \\]\nwhich is valid, for each pair of numbers \\(x_{1}, x_{2}\\) in \\((a, b)\\), for some \\(x\\) between \\(x_{1}\\) and \\(x_{2}\\).\n","date":"2022-07-27T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/2-mean-value-theorems/","section":"baby rudin","tags":null,"title":"MEAN VALUE THEOREMS"},{"categories":null,"contents":"In this chapter we shall (except in the final section) confine our attention to real functions defined on intervals or segments. This is not just a matter of convenience, since genuine differences appear when we pass from real functions to vector-valued ones. Differentiation of functions defined on \\(R^{k}\\) will be discussed in Chap. \\(9 .\\)\n","date":"2022-07-26T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/0-intro/","section":"baby rudin","tags":null,"title":"DIFFERENTIATION"},{"categories":null,"contents":"To enable us to operate in the extended real number system, we shall now enlarge the scope of Definition 4.1, by reformulating it in terms of neighborhoods.\nFor any real number \\(x\\), we have already defined a neighborhood of \\(x\\) to be any segment \\((x-\\delta, x+\\delta)\\). 4.32 Definition For any real \\(c\\), the set of real numbers \\(x\\) such that \\(x\u0026gt;c\\) is called a neighborhood of \\(+\\infty\\) and is written \\((c,+\\infty)\\). Similarly, the set \\((-\\infty, c)\\) is a neighborhood of \\(-\\infty\\).\n4.33 Definition Let \\(f\\) be a real function defined on \\(E \\subset R\\). We say that \\[ f(t) \\rightarrow A \\text { as } t \\rightarrow x, \\]\nwhere \\(A\\) and \\(x\\) are in the extended real number system, if for every neighborhood \\(U\\) of \\(A\\) there is a neighborhood \\(V\\) of \\(x\\) such that \\(V \\cap E\\) is not empty, and such that \\(f(t) \\in U\\) for all \\(t \\in V \\cap E, t \\neq x\\).\nA moment’s consideration will show that this coincides with Definition 4.1 when \\(A\\) and \\(x\\) are real.\nThe analogue of Theorem \\(4.4\\) is still true, and the proof offers nothing new. We state it, for the sake of completeness.\n4.34 Theorem Let \\(f\\) and \\(g\\) be defined on \\(E \\subset R\\). Suppose \\[ f(t) \\rightarrow A, \\quad g(t) \\rightarrow B \\quad \\text { as } t \\rightarrow x . \\]\nThen\n\\(f(t) \\rightarrow A^{\\prime}\\) implies \\(A^{\\prime}=A\\).\n\\((f+g)(t) \\rightarrow A+B\\)\n\\((f g)(t) \\rightarrow A B\\)\n\\((f / g)(t) \\rightarrow A / B\\)\nprovided the right members of \\((b),(c)\\), and \\((d)\\) are defined.\nNote that \\(\\infty-\\infty, 0 \\cdot \\infty, \\infty / \\infty, A / 0\\) are not defined (see Definition 1.23).\n","date":"2022-07-26T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/7-infinite-limits-and-limits-at-infinity/","section":"baby rudin","tags":null,"title":"INFINITE LIMITS AND LIMITS AT INFINITY"},{"categories":null,"contents":"5.1 Definition Let \\(f\\) be defined (and real-valued) on \\([a, b]\\). For any \\(x \\in[a, b]\\) form the quotient \\[ \\phi(t)=\\frac{f(t)-f(x)}{t-x} \\quad(a\u0026lt;t\u0026lt;b, t \\neq x), \\]\nand define\n\\[ f^{\\prime}(x)=\\lim _{t \\rightarrow x} \\phi(t), \\]\nprovided this limit exists in accordance with Definition 4.1.\nWe thus associate with the function \\(f\\) a function \\(f^{\\prime}\\) whose domain is the set of points \\(x\\) at which the limit (2) exists; \\(f^{\\prime}\\) is called the derivative of \\(f\\).\nIf \\(f^{\\prime}\\) is defined at a point \\(x\\), we say that \\(f\\) is differentiable at \\(x\\). If \\(f^{\\prime}\\) is defined at every point of a set \\(E \\subset[a, b]\\), we say that \\(f\\) is differentiable on \\(E\\).\nIt is possible to consider right-hand and left-hand limits in (2); this leads to the definition of right-hand and left-hand derivatives. In particular, at the endpoints \\(a\\) and \\(b\\), the derivative, if it exists, is a right-hand or left-hand derivative, respectively. We shall not, however, discuss one-sided derivatives in any detail.\nIf \\(f\\) is defined on a segment \\((a, b)\\) and if \\(a\u0026lt;x\u0026lt;b\\), then \\(f^{\\prime}(x)\\) is defined by (1) and (2), as above. But \\(f^{\\prime}(a)\\) and \\(f^{\\prime}(b)\\) are not defined in this case.\n\\(5.2\\) Theorem Let \\(f\\) be defined on \\([a, b]\\). If \\(f\\) is differentiable at a point \\(x \\in[a, b]\\), then \\(f\\) is continuous at \\(x\\).\nProof As \\(t \\rightarrow x\\), we have, by Theorem 4.4,\n\\[ f(t)-f(x)=\\frac{f(t)-f(x)}{t-x} \\cdot(t-x) \\rightarrow f^{\\prime}(x) \\cdot 0=0 . \\] The converse of this theorem is not true. It is easy to construct continuous functions which fail to be differentiable at isolated points. In Chap. 7 we shall even become acquainted with a function which is continuous on the whole line without being differentiable at any point!\n5.3 Theorem Suppose \\(f\\) and \\(g\\) are defined on \\([a, b]\\) and are differentiable at a point \\(x \\in[a, b]\\). Then \\(f+g, f g\\), and \\(f / g\\) are differentiable at \\(x\\), and\n\\((f+g)^{\\prime}(x)=f^{\\prime}(x)+g^{\\prime}(x)\\)\n\\((f g)^{\\prime}(x)=f^{\\prime}(x) g(x)+f(x) g^{\\prime}(x)\\);\n\\(\\left(\\frac{f}{g}\\right)^{\\prime}(x)=\\frac{g(x) f^{\\prime}(x)-g^{\\prime}(x) f(x)}{g^{2}(x)}\\).\nIn \\((c)\\), we assume of course that \\(g(x) \\neq 0\\)\nProof \\((a)\\) is clear, by Theorem 4.4. Let \\(h=f g\\). Then\n\\[ h(t)-h(x)=f(t)[g(t)-g(x)]+g(x)[f(t)-f(x)] . \\]\nIf we divide this by \\(t-x\\) and note that \\(f(t) \\rightarrow f(x)\\) as \\(t \\rightarrow x\\) (Theorem 5.2), (b) follows. Next, let \\(h=f / g\\). Then\n\\[ \\frac{h(t)-h(x)}{t-x}=\\frac{1}{g(t) g(x)}\\left[g(x) \\frac{f(t)-f(x)}{t-x}-f(x) \\frac{g(t)-g(x)}{t-x}\\right] \\text {. } \\]\nLetting \\(t \\rightarrow x\\), and applying Theorems \\(4.4\\) and \\(5.2\\), we obtain \\((c)\\).\n5.4 Examples The derivative of any constant is clearly zero. If \\(f\\) is defined by \\(f(x)=x\\), then \\(f^{\\prime}(x)=1\\). Repeated application of \\((b)\\) and \\((c)\\) then shows that \\(x^{n}\\) is differentiable, and that its derivative is \\(n x^{n-1}\\), for any integer \\(n\\) (if \\(n\u0026lt;0\\), we have to restrict ourselves to \\(x \\neq 0\\) ). Thus every polynomial is differentiable, and so is every rational function, except at the points where the denominator is zero.\nThe following theorem is known as the “chain rule” for differentiation. It deals with differentiation of composite functions and is probably the most important theorem about derivatives. We shall meet more general versions of it in Chap. \\(9 .\\)\n5.5 Theorem Suppose \\(f\\) is continuous on \\([a, b], f^{\\prime}(x)\\) exists at some point \\(x \\in[a, b], g\\) is defined on an interval \\(I\\) which contains the range of \\(f\\), and \\(g\\) is differentiable at the point \\(f(x)\\). If \\[ h(t)=g(f(t)) \\quad(a \\leq t \\leq b), \\]\nthen \\(h\\) is differentiable at \\(x\\), and\n\\[ h^{\\prime}(x)=g^{\\prime}(f(x)) f^{\\prime}(x) \\]\nProof Let \\(y=f(x)\\). By the definition of the derivative, we have \\[ \\begin{gathered} f(t)-f(x)=(t-x)\\left[f^{\\prime}(x)+u(t)\\right], \\\\ g(s)-g(y)=(s-y)\\left[g^{\\prime}(y)+v(s)\\right] \\end{gathered} \\]\nwhere \\(t \\in[a, b], s \\in I\\), and \\(u(t) \\rightarrow 0\\) as \\(t \\rightarrow x, v(s) \\rightarrow 0\\) as \\(s \\rightarrow y\\). Let \\(s=f(t)\\). Using first (5) and then (4), we obtain\n\\[ \\begin{aligned} h(t)-h(x) \u0026amp;=g(f(t))-g(f(x)) \\\\ \u0026amp;=[f(t)-f(x)] \\cdot\\left[g^{\\prime}(y)+v(s)\\right] \\\\ \u0026amp;=(t-x) \\cdot\\left[f^{\\prime}(x)+u(t)\\right] \\cdot\\left[g^{\\prime}(y)+v(s)\\right], \\end{aligned} \\]\nor, if \\(t \\neq x\\),\n\\[ \\frac{h(t)-h(x)}{t-x}=\\left[g^{\\prime}(y)+v(s)\\right] \\cdot\\left[f^{\\prime}(x)+u(t)\\right] \\]\nLetting \\(t \\rightarrow x\\), we see that \\(s \\rightarrow y\\), by the continuity of \\(f\\), so that the right side of \\((6)\\) tends to \\(g^{\\prime}(y) f^{\\prime}(x)\\), which gives (3).\n5.6 Examples\nLet \\(f\\) be defined by \\[ f(x)= \\begin{cases}x \\sin \\frac{1}{x} \u0026amp; (x \\neq 0) \\\\ 0 \u0026amp; (x=0)\\end{cases} \\]\nTaking for granted that the derivative of \\(\\sin x\\) is \\(\\cos x\\) (we shall discuss the trigonometric functions in Chap. 8), we can apply Theorems \\(5.3\\) and \\(5.5\\) whenever \\(x \\neq 0\\), and obtain \\[ f^{\\prime}(x)=\\sin \\frac{1}{x}-\\frac{1}{x} \\cos \\frac{1}{x} \\quad(x \\neq 0) . \\]\nAt \\(x=0\\), these theorems do not apply any longer, since \\(1 / x\\) is not defined there, and we appeal directly to the definition: for \\(t \\neq 0\\),\n\\[ \\frac{f(t)-f(0)}{t-0}=\\sin \\frac{1}{t} . \\]\nAs \\(t \\rightarrow 0\\), this does not tend to any limit, so that \\(f^{\\prime}(0)\\) does not exist.\nLet \\(f\\) be defined by \\[ f(x)= \\begin{cases}x^{2} \\sin \\frac{1}{x} \u0026amp; (x \\neq 0) \\\\ 0 \u0026amp; (x=0)\\end{cases} \\]\nAs above, we obtain\n\\[ f^{\\prime}(x)=2 x \\sin \\frac{1}{x}-\\cos \\frac{1}{x} \\quad(x \\neq 0) \\]\nAt \\(x=0\\), we appeal to the definition, and obtain\n\\[ \\left|\\frac{f(t)-f(0)}{t-0}\\right|=\\left|t \\sin \\frac{1}{t}\\right| \\leq|t| \\quad(t \\neq 0) \\]\nletting \\(t \\rightarrow 0\\), we see that\n\\[ f^{\\prime}(0)=0 . \\]\nThus \\(f\\) is differentiable at all points \\(x\\), but \\(f^{\\prime}\\) is not a continuous function, since \\(\\cos (1 / x)\\) in (10) does not tend to a limit as \\(x \\rightarrow 0\\).\n\\[ f^{\\prime}(x)=\\sin \\frac{1}{x}-\\frac{1}{x} \\cos \\frac{1}{x} \\quad(x \\neq 0) . \\]\nAt \\(x=0\\), these theorems do not apply any longer, since \\(1 / x\\) is not defined there, and we appeal directly to the definition: for \\(t \\neq 0\\),\n\\[ \\frac{f(t)-f(0)}{t-0}=\\sin \\frac{1}{t} . \\]\nAs \\(t \\rightarrow 0\\), this does not tend to any limit, so that \\(f^{\\prime}(0)\\) does not exist.\nLet \\(f\\) be defined by \\[ f(x)= \\begin{cases}x^{2} \\sin \\frac{1}{x} \u0026amp; (x \\neq 0) \\\\ 0 \u0026amp; (x=0)\\end{cases} \\]\nAs above, we obtain\n\\[ f^{\\prime}(x)=2 x \\sin \\frac{1}{x}-\\cos \\frac{1}{x} \\quad(x \\neq 0) \\]\nAt \\(x=0\\), we appeal to the definition, and obtain\n\\[ \\left|\\frac{f(t)-f(0)}{t-0}\\right|=\\left|t \\sin \\frac{1}{t}\\right| \\leq|t| \\quad(t \\neq 0) \\]\nletting \\(t \\rightarrow 0\\), we see that\n\\[ f^{\\prime}(0)=0 . \\]\nThus \\(f\\) is differentiable at all points \\(x\\), but \\(f^{\\prime}\\) is not a continuous function, since \\(\\cos (1 / x)\\) in (10) does not tend to a limit as \\(x \\rightarrow 0\\).\n","date":"2022-07-26T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/5-differentiation/1-the-derivative-of-a-real-function/","section":"baby rudin","tags":null,"title":"THE DERIVATIVE OF A REAL FUNCTION"},{"categories":null,"contents":"If \\(x\\) is a point in the domain of definition of the function \\(f\\) at which \\(f\\) is not continuous, we say that \\(f\\) is discontinuous at \\(x\\), or that \\(f\\) has a discontinuity at \\(x\\). If \\(f\\) is defined on an interval or on a segment, it is customary to divide discontinuities into two types. Before giving this classification, we have to define the right-hand and the left-hand limits of \\(f\\) at \\(x\\), which we denote by \\(f(x+)\\) and \\(f(x-)\\), respectively. 4.25 Definition Let \\(f\\) be defined on \\((a, b)\\). Consider any point \\(x\\) such that \\(a \\leq x\u0026lt;b\\). We write \\[ f(x+)=q \\] if \\(f\\left(t_{n}\\right) \\rightarrow q\\) as \\(n \\rightarrow \\infty\\), for all sequences \\(\\left\\{t_{n}\\right\\}\\) in \\((x, b)\\) such that \\(t_{n} \\rightarrow x\\). To obtain the definition of \\(f(x-)\\), for \\(a\u0026lt;x \\leq b\\), we restrict ourselves to sequences \\(\\left\\{t_{n}\\right\\}\\) in \\((a, x)\\). It is clear that any point \\(x\\) of \\((a, b), \\lim _{t \\rightarrow x} f(t)\\) exists if and only if \\[ f(x+)=f(x-)=\\lim _{t \\rightarrow x} f(t) . \\] 4.26 Definition Let \\(f\\) be defined on \\((a, b)\\). If \\(f\\) is discontinuous at a point \\(x\\), and if \\(f(x+)\\) and \\(f(x-)\\) exist, then \\(f\\) is said to have a discontinuity of the first kind, or a simple discontinuity, at \\(x\\). Otherwise the discontinuity is said to be of the second kind.\nThere are two ways in which a function can have a simple discontinuity: either \\(f(x+) \\neq f(x-)\\) [in which case the value \\(f(x)\\) is immaterial], or \\(f(x+)=\\) \\(f(x-) \\neq f(x)\\).\n4.27 Examples\nDefine \\[ f(x)= \\begin{cases}1 \u0026amp; (x \\text { rational }), \\\\ 0 \u0026amp; (x \\text { irrational })\\end{cases} \\]\nThen \\(f\\) has a discontinuity of the second kind at every point \\(x\\), since neither \\(f(x+)\\) nor \\(f(x-)\\) exists.\nDefine \\[ f(x)= \\begin{cases}x \u0026amp; (x \\text { rational }), \\\\ 0 \u0026amp; (x \\text { irrational) }\\end{cases} \\]\nThen \\(f\\) is continuous at \\(x=0\\) and has a discontinuity of the second kind at every other point.\nDefine \\[ f(x)= \\begin{cases}x+2 \u0026amp; (-3\u0026lt;x\u0026lt;-2) \\\\ -x-2 \u0026amp; (-2 \\leq x\u0026lt;0) \\\\ x+2 \u0026amp; (0 \\leq x\u0026lt;1)\\end{cases} \\]\nThen \\(f\\) has a simple discontinuity at \\(x=0\\) and is continuous at every other point of \\((-3,1)\\).\nDefine \\[ f(x)= \\begin{cases}\\sin \\frac{1}{x} \u0026amp; (x \\neq 0) \\\\ 0 \u0026amp; (x=0)\\end{cases} \\]\nSince neither \\(f(0+)\\) nor \\(f(0-)\\) exists, \\(f\\) has a discontinuity of the second kind at \\(x=0\\). We have not yet shown that \\(\\sin x\\) is a continuous function. If we assume this result for the moment, Theorem \\(4.7\\) implies that \\(f\\) is continuous at every point \\(x \\neq 0\\).\n","date":"2022-07-24T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/5-discontinuities/","section":"baby rudin","tags":null,"title":"DISCONTINUITIES"},{"categories":null,"contents":"We shall now study those functions which never decrease (or never increase) on a given segment.\n4.28 Definition Let \\(f\\) be real on \\((a, b)\\). Then \\(f\\) is said to be monotonically increasing on \\((a, b)\\) if \\(a\u0026lt;x\u0026lt;y\u0026lt;b\\) implies \\(f(x) \\leq f(y)\\). If the last inequality is reversed, we obtain the definition of a monotonically decreasing function. The class of monotonic functions consists of both the increasing and the decreasing functions.\n4.29 Theorem Let \\(f\\) be monotonically increasing on \\((a, b)\\). Then \\(f(x+)\\) and \\(f(x-)\\) exist at every point of \\(x\\) of \\((a, b)\\). More precisely, \\[ \\sup _{a\u0026lt;t\u0026lt;x} f(t)=f(x-) \\leq f(x) \\leq f(x+)=\\inf _{x\u0026lt;t\u0026lt;b} f(t) . \\]\nFurthermore, if \\(a\u0026lt;x\u0026lt;y\u0026lt;b\\), then\n\\[ f(x+) \\leq f(y-) . \\]\nAnalogous results evidently hold for monotonically decreasing functions. Proof By hypothesis, the set of numbers \\(f(t)\\), where \\(a\u0026lt;t\u0026lt;x\\), is bounded above by the number \\(f(x)\\), and therefore has a least upper bound which we shall denote by \\(A\\). Evidently \\(A \\leq f(x)\\). We have to show that \\(A=f(x-)\\).\nLet \\(\\varepsilon\u0026gt;0\\) be given. It follows from the definition of \\(A\\) as a least upper bound that there exists \\(\\delta\u0026gt;0\\) such that \\(a\u0026lt;x-\\delta\u0026lt;x\\) and \\[ A-\\varepsilon\u0026lt;f(x-\\delta) \\leq A \\text {. } \\]\nSince \\(f\\) is monotonic, we have\n\\[ f(x-\\delta) \\leq f(t) \\leq A \\quad(x-\\delta\u0026lt;t\u0026lt;x) . \\]\nCombining (27) and (28), we see that\n\\[ |f(t)-A|\u0026lt;\\varepsilon \\quad(x-\\delta\u0026lt;t\u0026lt;x) . \\]\nHence \\(f(x-)=A\\).\nThe second half of \\((25)\\) is proved in precisely the same way. Next, if \\(a\u0026lt;x\u0026lt;y\u0026lt;b\\), we see from (25) that\n\\[ f(x+)=\\inf _{x\u0026lt;t\u0026lt;b} f(t)=\\inf _{x\u0026lt;t\u0026lt;y} f(t) . \\]\nThe last equality is obtained by applying \\((25)\\) to \\((a, y)\\) in place of \\((a, b)\\). Similarly,\n\\[ f(y-)=\\sup _{a\u0026lt;t\u0026lt;y} f(t)=\\sup _{x\u0026lt;t\u0026lt;y} f(t) . \\]\nComparison of (29) and (30) gives (26).\nCorollary Monotonic functions have no discontinuities of the second kind.\nThis corollary implies that every monotonic function is discontinuous at a countable set of points at most. Instead of appealing to the general theorem whose proof is sketched in Exercise 17, we give here a simple proof which is applicable to monotonic functions.\n4.30 Theorem Let \\(f\\) be monotonic on \\((a, b)\\). Then the set of points of \\((a, b) a t\\) which \\(f\\) is discontinuous is at most countable.\nProof Suppose, for the sake of definiteness, that \\(f\\) is increasing, and let \\(E\\) be the set of points at which \\(f\\) is discontinuous.\nWith every point \\(x\\) of \\(E\\) we associate a rational number \\(r(x)\\) such that\n\\[ f(x-)\u0026lt;r(x)\u0026lt;f(x+) \\]\nSince \\(x_{1}\u0026lt;x_{2}\\) implies \\(f\\left(x_{1}+\\right) \\leq f\\left(x_{2}-\\right)\\), we see that \\(r\\left(x_{1}\\right) \\neq r\\left(x_{2}\\right)\\) if \\(x_{1} \\neq x_{2}\\).\nWe have thus established a 1-1 correspondence between the set \\(E\\) and a subset of the set of rational numbers. The latter, as we know, is countable.\n4.31 Remark It should be noted that the discontinuities of a monotonic function need not be isolated. In fact, given any countable subset \\(E\\) of \\((a, b)\\), which may even be dense, we can construct a function \\(f\\), monotonic on \\((a, b)\\), discontinuous at every point of \\(E\\), and at no other point of \\((a, b)\\).\nTo show this, let the points of \\(E\\) be arranged in a sequence \\(\\left\\{x_{n}\\right\\}\\), \\(n=1,2,3, \\ldots\\) Let \\(\\left\\{c_{n}\\right\\}\\) be a sequence of positive numbers such that \\(\\Sigma c_{n}\\) converges. Define\n\\[ f(x)=\\sum_{x_{n}\u0026lt;x} c_{n} \\quad(a\u0026lt;x\u0026lt;b) . \\]\nThe summation is to be understood as follows: Sum over those indices \\(n\\) for which \\(x_{n}\u0026lt;x\\). If there are no points \\(x_{n}\\) to the left of \\(x\\), the sum is empty; following the usual convention, we define it to be zero. Since (31) converges absolutely, the order in which the terms are arranged is immaterial.\nWe leave the verification of the following properties of \\(f\\) to the reader:\n\\(f\\) is monotonically increasing on \\((a, b)\\);\n\\(f\\) is discontinuous at every point of \\(E\\); in fact,\n\\[ f\\left(x_{n}+\\right)-f\\left(x_{n}-\\right)=c_{n} . \\]\n\\(f\\) is continuous at every other point of \\((a, b)\\). Moreover, it is not hard to see that \\(f(x-)=f(x)\\) at all points of \\((a, b)\\). If a function satisfies this condition, we say that \\(f\\) is continuous from the left. If the summation in (31) were taken over all indices \\(n\\) for which \\(x_{n} \\leq x\\), we would have \\(f(x+)=f(x)\\) at every point of \\((a, b)\\); that is, \\(f\\) would be continuous from the right.\nFunctions of this sort can also be defined by another method; for an example we refer to Theorem \\(6.16\\).\n","date":"2022-07-24T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/6-monotonic-functions/","section":"baby rudin","tags":null,"title":"MONOTONIC FUNCTIONS"},{"categories":null,"contents":"4.22 Theorem If \\(f\\) is a continuous mapping of a metric space \\(X\\) into a metric space \\(Y\\), and if \\(E\\) is a connected subset of \\(X\\), then \\(f(E)\\) is connected.\nProof Assume, on the contrary, that \\(f(E)=A \\cup B\\), where \\(A\\) and \\(B\\) are nonempty separated subsets of \\(Y\\). Put \\(G=E \\cap f^{-1}(A), H=E \\cap f^{-1}(B)\\). Then \\(E=G \\cup H\\), and neither \\(G\\) nor \\(H\\) is empty.\nSince \\(A \\subset \\bar{A}\\) (the closure of \\(A\\) ), we have \\(G \\subset f^{-1}(\\bar{A})\\); the latter set is closed, since \\(f\\) is continuous; hence \\(\\bar{G} \\subset f^{-1}(\\bar{A})\\). It follows that \\(f(\\bar{G}) \\subset \\bar{A}\\). Since \\(f(H)=B\\) and \\(\\bar{A} \\cap B\\) is empty, we conclude that \\(\\bar{G} \\cap H\\) is empty. The same argument shows that \\(G \\cap \\bar{H}\\) is empty. Thus \\(G\\) and \\(H\\) are separated. This is impossible if \\(E\\) is connected.\n4.23 Theorem Let \\(f\\) be a continuous real function on the interval \\([a, b]\\). If \\(f(a)\u0026lt;f(b)\\) and if \\(c\\) is a number such that \\(f(a)\u0026lt;c\u0026lt;f(b)\\), then there exists a point \\(x \\in(a, b)\\) such that \\(f(x)=c\\).\nA similar result holds, of course, if \\(f(a)\u0026gt;f(b)\\). Roughly speaking, the theorem says that a continuous real function assumes all intermediate values on an interval.\nProof By Theorem 2.47, \\([a, b]\\) is connected; hence Theorem \\(4.22\\) shows that \\(f([a, b])\\) is a connected subset of \\(R^{1}\\), and the assertion follows if we appeal once more to Theorem 2.47.\n4.24 Remark At first glance, it might seem that Theorem \\(4.23\\) has a converse. That is, one might think that if for any two points \\(x_{1}\u0026lt;x_{2}\\) and for any number \\(c\\) between \\(f\\left(x_{1}\\right)\\) and \\(f\\left(x_{2}\\right)\\) there is a point \\(x\\) in \\(\\left(x_{1}, x_{2}\\right)\\) such that \\(f(x)=c\\), then \\(f\\) must be continuous. That this is not so may be concluded from Example \\(4.27(d)\\).\n","date":"2022-07-23T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/4-continuity-and-connectedness/","section":"baby rudin","tags":null,"title":"CONTINUITY AND CONNECTEDNESS"},{"categories":null,"contents":"4.13 Definition A mapping \\(f\\) of a set \\(E\\) into \\(R^{k}\\) is said to be bounded if there is a real number \\(M\\) such that \\(|\\mathrm{f}(x)| \\leq M\\) for all \\(x \\in E\\).\n4.14 Theorem Suppose \\(f\\) is a continuous mapping of a compact metric space \\(X\\) into a metric space \\(Y\\). Then \\(f(X)\\) is compact.\nProof Let \\(\\left\\{V_{\\alpha}\\right\\}\\) be an open cover of \\(f(X)\\). Since \\(f\\) is continuous, Theorem \\(4.8\\) shows that each of the sets \\(f^{-1}\\left(V_{\\alpha}\\right)\\) is open. Since \\(X\\) is compact, there are finitely many indices, say \\(\\alpha_{1}, \\ldots, \\alpha_{n}\\), such that \\[ X \\subset f^{-1}\\left(V_{\\alpha_{1}}\\right) \\cup \\cdots \\cup f^{-1}\\left(V_{\\alpha_{n}}\\right) . \\] Since \\(f\\left(f^{-1}(E)\\right) \\subset E\\) for every \\(E \\subset Y,(12)\\) implies that \\[ f(X) \\subset V_{\\alpha_{1}} \\cup \\cdots \\cup V_{\\alpha_{n}} . \\] This completes the proof. Note: We have used the relation \\(f\\left(f^{-1}(E)\\right) \\subset E\\), valid for \\(E \\subset Y\\). If \\(E \\subset X\\), then \\(f^{-1}(f(E)) \\supset E\\); equality need not hold in either case. We shall now deduce some consequences of Theorem 4.14.\n4.15 Theorem If \\(\\mathbf{f}\\) is a continuous mapping of a compact metric space \\(X\\) into \\(R^{k}\\), then \\(\\mathbf{f}(X)\\) is closed and bounded. Thus, \\(\\mathrm{f}\\) is bounded.\nThis follows from Theorem 2.41. The result is particularly important when \\(f\\) is real: 4.16 Theorem Suppose \\(f\\) is a continuous real function on a compact metric space \\(X\\), and \\[ M=\\sup _{p \\in X} f(p), \\quad m=\\inf _{p \\in X} f(p) . \\] Then there exist points \\(p, q \\in X\\) such that \\(f(p)=M\\) and \\(f(q)=m\\). The notation in (14) means that \\(M\\) is the least upper bound of the set of all numbers \\(f(p)\\), where \\(p\\) ranges over \\(X\\), and that \\(m\\) is the greatest lower bound of this set of numbers.\n4.16 Theorem Suppose \\(f\\) is a continuous real function on a compact metric space \\(X\\), and \\[ M=\\sup _{p \\in X} f(p), \\quad m=\\inf _{p \\in X} f(p) . \\] Then there exist points \\(p, q \\in X\\) such that \\(f(p)=M\\) and \\(f(q)=m\\). The notation in (14) means that \\(M\\) is the least upper bound of the set of all numbers \\(f(p)\\), where \\(p\\) ranges over \\(X\\), and that \\(m\\) is the greatest lower bound of this set of numbers.\nThe conclusion may also be stated as follows: There exist points \\(p\\) and \\(q\\) in \\(X\\) such that \\(f(q) \\leq f(x) \\leq f(p)\\) for all \\(x \\in X\\); that is, \\(f\\) attains its maximum (at \\(p\\) ) and its minimum (at \\(q)\\).\nProof By Theorem 4.15, \\(f(X)\\) is a closed and bounded set of real numbers; hence \\(f(X)\\) contains \\[ M=\\sup f(X) \\quad \\text { and } \\quad m=\\inf f(X), \\] by Theorem \\(2.28\\).\n4.17 Theorem Suppose \\(f\\) is a continuous 1-1 mapping of a compact metric space \\(X\\) onto a metric space \\(Y\\). Then the inverse mapping \\(f^{-1}\\) defined on \\(Y\\) by \\[ f^{-1}(f(x))=x \\quad(x \\in X) \\] is a continuous mapping of \\(Y\\) onto \\(X\\). Proof Applying Theorem \\(4.8\\) to \\(f^{-1}\\) in place of \\(f\\), we see that it suffices to prove that \\(f(V)\\) is an open set in \\(Y\\) for every open set \\(V\\) in \\(X\\). Fix such a set \\(V\\).\nThe complement \\(V^{c}\\) of \\(V\\) is closed in \\(X\\), hence compact (Theorem 2.35); hence \\(f\\left(V^{c}\\right)\\) is a compact subset of \\(Y\\) (Theorem 4.14) and so is closed in \\(Y\\) (Theorem 2.34). Since \\(f\\) is one-to-one and onto, \\(f(V)\\) is the complement of \\(f\\left(V^{c}\\right)\\). Hence \\(f(V)\\) is open.\n4.18 Definition Let \\(f\\) be a mapping of a metric space \\(X\\) into a metric space \\(Y\\). We say that \\(f\\) is uniformly continuous on \\(X\\) if for every \\(\\varepsilon\u0026gt;0\\) there exists \\(\\delta\u0026gt;0\\) such that \\[ d_{\\mathbf{Y}}(f(p), f(q))\u0026lt;\\varepsilon \\] for all \\(p\\) and \\(q\\) in \\(X\\) for which \\(d_{\\mathbf{X}}(p, q)\u0026lt;\\delta\\). Let us consider the differences between the concepts of continuity and of uniform continuity. First, uniform continuity is a property of a function on a set, whereas continuity can be defined at a single point. To ask whether a given function is uniformly continuous at a certain point is meaningless. Second, if \\(f\\) is continuous on \\(X\\), then it is possible to find, for each \\(\\varepsilon\u0026gt;0\\) and for each point \\(p\\) of \\(X\\), a number \\(\\delta\u0026gt;0\\) having the property specified in Definition 4.5. This \\(\\delta\\) depends on \\(\\varepsilon\\) and on \\(p\\). If \\(f\\) is, however, uniformly continuous on \\(X\\), then it is possible, for each \\(\\varepsilon\u0026gt;0\\), to find one number \\(\\delta\u0026gt;0\\) which will do for all points \\(p\\) of \\(X\\).\nEvidently, every uniformly continuous function is continuous. That the two concepts are equivalent on compact sets follows from the next theorem.\n4.19 Theorem Let \\(f\\) be a continuous mapping of a compact metric space \\(X\\) into a metric space \\(Y\\). Then \\(f\\) is uniformly continuous on \\(X\\).\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Since \\(f\\) is continuous, we can associate to each point \\(p \\in X\\) a positive number \\(\\phi(p)\\) such that (16) \\(q \\in X, d_{X}(p, q)\u0026lt;\\phi(p)\\) implies \\(d_{Y}(f(p), f(q))\u0026lt;\\frac{\\varepsilon}{2}\\). Let \\(J(p)\\) be the set of all \\(q \\in X\\) for which \\[ d_{\\mathbf{x}}(p, q)\u0026lt;\\frac{1}{2} \\phi(p) . \\] Since \\(p \\in J(p)\\), the collection of all sets \\(J(p)\\) is an open cover of \\(X\\); and since \\(X\\) is compact, there is a finite set of points \\(p_{1}, \\ldots, p_{n}\\) in \\(X\\), such that \\[ \\text { We put } \\quad \\delta=\\frac{1}{2} \\min \\left[\\phi\\left(p_{1}\\right), \\ldots, \\phi\\left(p_{n}\\right)\\right] \\text {. } \\] Then \\(\\delta\u0026gt;0\\). (This is one point where the finiteness of the covering, inherent in the definition of compactness, is essential. The minimum of a finite set of positive numbers is positive, whereas the inf of an infinite set of positive numbers may very well be 0 .)\nNow let \\(q\\) and \\(p\\) be points of \\(X\\), such that \\(d_{X}(p, q)\u0026lt;\\delta\\). By (18), there is an integer \\(m, 1 \\leq m \\leq n\\), such that \\(p \\in J\\left(p_{m}\\right)\\); hence \\[ d_{x}\\left(p, p_{m}\\right)\u0026lt;\\frac{1}{2} \\phi\\left(p_{m}\\right), \\] and we also have \\[ d_{X}\\left(q, p_{m}\\right) \\leq d_{X}(p, q)+d_{X}\\left(p, p_{m}\\right)\u0026lt;\\delta+\\frac{1}{2} \\phi\\left(p_{m}\\right) \\leq \\phi\\left(p_{m}\\right) . \\] Finally, (16) shows that therefore \\[ d_{Y}(f(p), f(q)) \\leq d_{Y}\\left(f(p), f\\left(p_{m}\\right)\\right)+d_{Y}\\left(f(q), f\\left(p_{m}\\right)\\right)\u0026lt;\\varepsilon . \\] This completes the proof. An alternative proof is sketched in Exercise \\(10 .\\) We now proceed to show that compactness is essential in the hypotheses of Theorems \\(4.14,4.15,4.16\\), and 4.19.\n4.20 Theorem Let \\(E\\) be a noncompact set in \\(R^{1}\\). Then (a) there exists a continuous function on \\(E\\) which is not bounded; (b) there exists a continuous and bounded function on \\(E\\) which has no maximum. If, in addition, \\(E\\) is bounded, then\nthere exists a continuous function on \\(E\\) which is not uniformly continuous. Proof Suppose first that \\(E\\) is bounded, so that there exists a limit point \\(x_{0}\\) of \\(E\\) which is not a point of \\(E\\). Consider \\[ f(x)=\\frac{1}{x-x_{0}} \\quad(x \\in E) \\] This is continuous on \\(E\\) (Theorem 4.9), but evidently unbounded. To see that (21) is not uniformly continuous, let \\(\\varepsilon\u0026gt;0\\) and \\(\\delta\u0026gt;0\\) be arbitrary, and choose a point \\(x \\in E\\) such that \\(\\left|x-x_{0}\\right|\u0026lt;\\delta\\). Taking \\(t\\) close enough to \\(x_{0}\\), we can then make the difference \\(|f(t)-f(x)|\\) greater than \\(\\varepsilon\\), although \\(|t-x|\u0026lt;\\delta\\). Since this is true for every \\(\\delta\u0026gt;0, f\\) is not uniformly continuous on \\(E\\).\nThe function \\(g\\) given by \\[ g(x)=\\frac{1}{1+\\left(x-x_{0}\\right)^{2}} \\quad(x \\in E) \\] is continuous on \\(E\\), and is bounded, since \\(0\u0026lt;g(x)\u0026lt;1\\). It is clear that \\[ \\sup _{x \\in E} g(x)=1, \\] whereas \\(g(x)\u0026lt;1\\) for all \\(x \\in E\\). Thus \\(g\\) has no maximum on \\(E\\). Having proved the theorem for bounded sets \\(E\\), let us now suppose that \\(E\\) is unbounded. Then \\(f(x)=x\\) establishes \\((a)\\), whereas \\[ h(x)=\\frac{x^{2}}{1+x^{2}} \\quad(x \\in E) \\] establishes \\((b)\\), since \\[ \\sup _{x \\in E} h(x)=1 \\] and \\(h(x)\u0026lt;1\\) for all \\(x \\in E\\). Assertion \\((c)\\) would be false if boundedness were omitted from the hypotheses. For, let \\(E\\) be the set of all integers. Then every function defined on \\(E\\) is uniformly continuous on \\(E\\). To see this, we need merely take \\(\\delta\u0026lt;1\\) in Definition \\(4.18\\).\nWe conclude this section by showing that compactness is also essential in Theorem 4.17.\n4.21 Example Let \\(X\\) be the half-open interval \\([0,2 \\pi)\\) on the real line, and let \\(\\mathrm{f}\\) be the mapping of \\(X\\) onto the circle \\(Y\\) consisting of all points whose distance from the origin is 1 , given by \\[ \\mathbf{f}(t)=(\\cos t, \\sin t) \\quad(0 \\leq t\u0026lt;2 \\pi) . \\] The continuity of the trigonometric functions cosine and sine, as well as their periodicity properties, will be established in Chap. 8. These results show that \\(\\mathrm{f}\\) is a continuous 1-1 mapping of \\(X\\) onto \\(Y\\).\nHowever, the inverse mapping (which exists, since \\(f\\) is one-to-one and onto) fails to be continuous at the point \\((1,0)=f(0)\\). Of course, \\(X\\) is not compact in this example. (It may be of interest to observe that \\(\\mathbf{f}^{-1}\\) fails to be continuous in spite of the fact that \\(Y\\) is compact!)\n","date":"2022-07-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/3-continuity-and-compactness/","section":"baby rudin","tags":null,"title":"CONTINUITY AND COMPACTNESS"},{"categories":null,"contents":"4.5 Definition Suppose \\(X\\) and \\(Y\\) are metric spaces, \\(E \\subset X, p \\in E\\), and \\(f\\) maps \\(E\\) into \\(Y\\). Then \\(f\\) is said to be continuous at \\(p\\) if for every \\(\\varepsilon\u0026gt;0\\) there exists a \\(\\delta\u0026gt;0\\) such that \\[ d_{\\mathbf{Y}}(f(x), f(p))\u0026lt;\\varepsilon \\] for all points \\(x \\in E\\) for which \\(d_{X}(x, p)\u0026lt;\\delta\\). If \\(f\\) is continuous at every point of \\(E\\), then \\(f\\) is said to be continuous on \\(E\\). It should be noted that \\(f\\) has to be defined at the point \\(p\\) in order to be continuous at \\(p\\). (Compare this with the remark following Definition 4.1.)\nIf \\(p\\) is an isolated point of \\(E\\), then our definition implies that every function \\(f\\) which has \\(E\\) as its domain of definition is continuous at \\(p\\). For, no matter which \\(\\varepsilon\u0026gt;0\\) we choose, we can pick \\(\\delta\u0026gt;0\\) so that the only point \\(x \\in E\\) for which \\(d_{X}(x, p)\u0026lt;\\delta\\) is \\(x=p ;\\) then \\[ d_{Y}(f(x), f(p))=0\u0026lt;\\varepsilon \\] 4.6 Theorem In the situation given in Definition \\(4.5\\), assume also that \\(p\\) is a limit point of \\(E\\). Then \\(f\\) is continuous at \\(p\\) if and only if \\(\\lim _{x \\rightarrow p} f(x)=f(p)\\). Proof This is clear if we compare Definitions \\(4.1\\) and 4.5. We now turn to compositions of functions. A brief statement of the following theorem is that a continuous function of a continuous function is continuous.\n4.7 Theorem Suppose \\(X, Y, Z\\) are metric spaces, \\(E \\subset X, f\\) maps \\(E\\) into \\(Y, g\\) maps the range of \\(f, f(E)\\), into \\(Z\\), and \\(h\\) is the mapping of \\(E\\) into \\(Z\\) defined by \\[ h(x)=g(f(x)) \\quad(x \\in E) . \\] If \\(f\\) is continuous at a point \\(p \\in E\\) and if \\(g\\) is continuous at the point \\(f(p)\\), then \\(h\\) is continuous at \\(p .\\)\nThis function \\(h\\) is called the composition or the composite of \\(f\\) and \\(g\\). The notation \\[ h=g \\circ f \\] is frequently used in this context.\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Since \\(g\\) is continuous at \\(f(p)\\), there exists \\(\\eta\u0026gt;0\\) such that \\[ d_{Z}(g(y), g(f(p)))\u0026lt;\\varepsilon \\text { if } d_{Y}(y, f(p))\u0026lt;\\eta \\text { and } y \\in f(E) \\] Since \\(f\\) is continuous at \\(p\\), there exists \\(\\delta\u0026gt;0\\) such that \\[ d_{Y}(f(x), f(p))\u0026lt;\\eta \\text { if } d_{X}(x, p)\u0026lt;\\delta \\text { and } x \\in E . \\] It follows that \\[ d_{\\mathrm{Z}}(h(x), h(p))=d_{\\mathrm{Z}}(g(f(x)), g(f(p)))\u0026lt;\\varepsilon \\] if \\(d_{X}(x, p)\u0026lt;\\delta\\) and \\(x \\in E\\). Thus \\(h\\) is continuous at \\(p\\).\n4.8 Theorem \\(A\\) mapping \\(f\\) of a metric space \\(X\\) into a metric space \\(Y\\) is continuous on \\(X\\) if and only if \\(f^{-1}(V)\\) is open in \\(X\\) for every open set \\(V\\) in \\(Y\\). (Inverse images are defined in Definition 2.2.) This is a very useful haracterization of continuity.\nProof Suppose \\(f\\) is continuous on \\(X\\) and \\(V\\) is an open set in \\(Y\\). We have to show that every point of \\(f^{-1}(V)\\) is an interior point of \\(f^{-1}(V)\\). So, suppose \\(p \\in X\\) and \\(f(p) \\in V\\). Since \\(V\\) is open, there exists \\(\\varepsilon\u0026gt;0\\) such that \\(y \\in V\\) if \\(d_{\\mathbf{Y}}(f(p), y)\u0026lt;\\varepsilon ;\\) and since \\(f\\) is continuous at \\(p\\), there exists \\(\\delta\u0026gt;0\\) such that \\(d_{Y}(f(x), f(p))\u0026lt;\\varepsilon\\) if \\(d_{X}(x, p)\u0026lt;\\delta\\). Thus \\(x \\in f^{-1}(V)\\) as soon as \\(d_{X}(x, p)\u0026lt;\\delta\\)\nConversely, suppose \\(f^{-1}(V)\\) is open in \\(X\\) for every open set \\(V\\) in \\(Y\\). Fix \\(p \\in X\\) and \\(\\varepsilon\u0026gt;0\\), let \\(V\\) be the set of all \\(y \\in Y\\) such that \\(d_{Y}(y, f(p))\u0026lt;\\varepsilon\\). Then \\(V\\) is open; hence \\(f^{-1}(V)\\) is open; hence there exists \\(\\delta\u0026gt;0\\) such that \\(x \\in f^{-1}(V)\\) as soon as \\(d_{x}(p, x)\u0026lt;\\delta\\). But if \\(x \\in f^{-1}(V)\\), then \\(f(x) \\in V\\), so that \\(d_{Y}(f(x), f(p))\u0026lt;\\varepsilon\\). This completes the proof.\nCorollary A mapping \\(f\\) of a metric space \\(X\\) into a metric space \\(Y\\) is continuous if and only if \\(f^{-1}(C)\\) is closed in \\(X\\) for every closed set \\(C\\) in \\(Y\\).\nThis follows from the theorem, since a set is closed if and only if its complement is open, and since \\(f^{-1}\\left(E^{c}\\right)=\\left[f^{-1}(E)\\right]^{c}\\) for every \\(E \\subset Y\\).\nWe now turn to complex-valued and vector-valued functions, and to functions defined on subsets of \\(R^{k}\\).\n4.9 Theorem Let \\(f\\) and \\(g\\) be complex continuous functions on a metric space \\(X\\). Then \\(f+g, f g\\), and \\(f / g\\) are continuous on \\(X\\). In the last case, we must of course assume that \\(g(x) \\neq 0\\), for all \\(x \\in X\\). Proof At isolated points of \\(X\\) there is nothing to prove. At limit points, the statement follows from Theorems \\(4.4\\) and \\(4.6\\).\n\\(4.10\\) Theorem (a) Let \\(f_{1}, \\ldots, f_{k}\\) be real functions on a metric space \\(X\\), and let \\(\\mathbf{f}\\) be the mapping of \\(X\\) into \\(R^{k}\\) defined by \\[ \\mathbf{f}(x)=\\left(f_{1}(x), \\ldots, f_{k}(x)\\right) \\quad(x \\in X) ; \\] then \\(\\mathbf{f}\\) is continuous if and only if each of the functions \\(f_{1}, \\ldots, f_{k}\\) is continuous. (b) If \\(\\mathbf{f}\\) and \\(\\mathbf{g}\\) are continuous mappings of \\(X\\) into \\(R^{k}\\), then \\(\\mathbf{f}+\\mathbf{g}\\) and \\(\\mathbf{f} \\cdot \\mathbf{g}\\) are continuous on \\(X\\).\nThe functions \\(f_{1}, \\ldots, f_{k}\\) are called the components of \\(\\mathbf{f}\\). Note that \\(\\mathbf{f}+\\mathbf{g}\\) is a mapping into \\(R^{k}\\), whereas \\(\\mathbf{f} \\cdot \\mathbf{g}\\) is a real function on \\(X\\).\nProof Part (a) follows from the inequalities \\[ \\left|f_{j}(x)-f_{j}(y)\\right| \\leq|\\mathbf{f}(x)-\\mathbf{f}(y)|=\\left\\{\\sum_{i=1}^{k}\\left|f_{i}(x)-f_{i}(y)\\right|^{2}\\right\\}^{\\mathbf{t}}, \\] for \\(j=1, \\ldots, k\\). Part \\((b)\\) follows from ( \\(a\\) ) and Theorem 4.9.\n4.11 Examples If \\(x_{1}, \\ldots, x_{k}\\) are the coordinates of the point \\(\\mathrm{x} \\in R^{k}\\), the functions \\(\\phi_{i}\\) defined by \\[ \\phi_{i}(\\mathbf{x})=x_{i} \\quad\\left(\\mathbf{x} \\in R^{k}\\right) \\] are continuous on \\(R^{k}\\), since the inequality \\[ \\left|\\phi_{i}(\\mathbf{x})-\\phi_{i}(\\mathbf{y})\\right| \\leq|\\mathbf{x}-\\mathbf{y}| \\] shows that we may take \\(\\delta=\\varepsilon\\) in Definition 4.5. The functions \\(\\phi_{i}\\) are sometimes called the coordinate functions.\nRepeated application of Theorem \\(4.9\\) then shows that every monomial \\[ x_{1}^{n_{1}} x_{2}^{n_{2}} \\ldots x_{k}^{n_{k}} \\] where \\(n_{1}, \\ldots, n_{k}\\) are nonnegative integers, is continuous on \\(R^{k}\\). The same is true of constant multiples of \\((9)\\), since constants are evidently continuous. It follows that every polynomial \\(P\\), given by \\[ P(\\mathbf{x})=\\Sigma c_{n_{1} \\cdots n_{k}} x_{1}^{n_{1}} \\ldots x_{k}^{n_{k}} \\quad\\left(\\mathbf{x} \\in R^{k}\\right), \\] is continuous on \\(R^{k}\\). Here the coefficients \\(c_{n_{1} \\cdots n_{k}}\\) are complex numbers, \\(n_{1}, \\ldots, n_{k}\\) are nonnegative integers, and the sum in (10) has finitely many terms.\nFurthermore, every rational function in \\(x_{1}, \\ldots, x_{k}\\), that is, every quotient of two polynomials of the form (10), is continuous on \\(R^{k}\\) wherever the denominator is different from zero. From the triangle inequality one sees easily that \\[ || \\mathbf{x}|-| \\mathbf{y}|| \\leq|\\mathbf{x}-\\mathbf{y}| \\quad\\left(\\mathbf{x}, \\mathbf{y} \\in R^{k}\\right) . \\] Hence the mapping \\(\\mathbf{x} \\rightarrow|\\mathbf{x}|\\) is a continuous real function on \\(R^{k}\\). If now \\(f\\) is a continuous mapping from a metric space \\(X\\) into \\(R^{k}\\), and if \\(\\phi\\) is defined on \\(X\\) by setting \\(\\phi(p)=|\\mathbf{f}(p)|\\), it follows, by Theorem 4.7, that \\(\\phi\\) is a continuous real function on \\(X\\).\n4.12 Remark We defined the notion of continuity for functions defined on a subset \\(E\\) of a metric space \\(X\\). However, the complement of \\(E\\) in \\(X\\) plays no role whatever in this definition (note that the situation was somewhat different for limits of functions). Accordingly, we lose nothing of interest by discarding the complement of the domain of \\(f\\). This means that we may just as well talk only about continuous mappings of one metric space into another, rather than of mappings of subsets. This simplifies statements and proofs of some theorems. We have already made use of this principle in Theorems \\(4.8\\) to \\(4.10\\), and will continue to do so in the following section on compactness.\n","date":"2022-07-20T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/2-continuous-functions/","section":"baby rudin","tags":null,"title":"CONTINUOUS FUNCTIONS"},{"categories":null,"contents":"The function concept and some of the related terminology were introduced in Definitions \\(2.1\\) and \\(2.2\\). Although we shall (in later chapters) be mainly interested in real and complex functions (i.e., in functions whose values are real or complex numbers) we shall also discuss vector-valued functions (i.e., functions with values in \\(R^{k}\\) ) and functions with values in an arbitrary metric space. The theorems we shall discuss in this general setting would not become any easier if we restricted ourselves to real functions, for instance, and it actually simplifies and clarifies the picture to discard unnecessary hypotheses and to state and prove theorems in an appropriately general context.\nThe domains of definition of our functions will also be metric spaces, suitably specialized in various instances.\n","date":"2022-07-18T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/0-intro/","section":"baby rudin","tags":null,"title":"CONTINUITY"},{"categories":null,"contents":"4.1 Definition Let \\(X\\) and \\(Y\\) be metric spaces; suppose \\(E \\subset X, f\\) maps \\(E\\) into \\(Y\\), and \\(p\\) is a limit point of \\(E\\). We write \\(f(x) \\rightarrow q\\) as \\(x \\rightarrow p\\), or \\[ \\lim _{x \\rightarrow p} f(x)=q \\] if there is a point \\(q \\in Y\\) with the following property: For every \\(\\varepsilon\u0026gt;0\\) there exists a \\(\\delta\u0026gt;0\\) such that \\[ d_{Y}(f(x), q)\u0026lt;\\varepsilon \\] for all points \\(x \\in E\\) for which\n\\[ 0\u0026lt;d_{X}(x, p)\u0026lt;\\delta . \\]\nThe symbols \\(d_{X}\\) and \\(d_{Y}\\) refer to the distances in \\(X\\) and \\(Y\\), respectively. If \\(X\\) and/or \\(Y\\) are replaced by the real line, the complex plane, or by some euclidean space \\(R^{k}\\), the distances \\(d_{X}, d_{Y}\\) are of course replaced by absolute values, or by norms of differences (see Sec. 2.16).\nIt should be noted that \\(p \\in X\\), but that \\(p\\) need not be a point of \\(E\\) in the above definition. Moreover, even if \\(p \\in E\\), we may very well have \\(f(p) \\neq \\lim _{x \\rightarrow p} f(x)\\). We can recast this definition in terms of limits of sequences:\n4.2 Theorem Let \\(X, Y, E, f\\), and \\(p\\) be as in Definition 4.1. Then \\[ \\lim _{x \\rightarrow p} f(x)=q \\] if and only if \\[ \\lim _{n \\rightarrow \\infty} f\\left(p_{n}\\right)=q \\] for every sequence \\(\\left\\{p_{n}\\right\\}\\) in \\(E\\) such that \\[ p_{n} \\neq p, \\quad \\lim _{n \\rightarrow \\infty} p_{n}=p . \\] Proof Suppose (4) holds. Choose \\(\\left\\{p_{n}\\right\\}\\) in \\(E\\) satisfying (6). Let \\(\\varepsilon\u0026gt;0\\) be given. Then there exists \\(\\delta\u0026gt;0\\) such that \\(d_{Y}(f(x), q)\u0026lt;\\varepsilon\\) if \\(x \\in E\\) and \\(0\u0026lt;d_{X}(x, p)\u0026lt;\\delta\\). Also, there exists \\(N\\) such that \\(n\u0026gt;N\\) implies \\(0\u0026lt;d_{X}\\left(p_{n}, p\\right)\u0026lt;\\delta\\). Thus, for \\(n\u0026gt;N\\), we have \\(d_{Y}\\left(f\\left(p_{n}\\right), q\\right)\u0026lt;\\varepsilon\\), which shows that (5) holds.\nConversely, suppose (4) is false. Then there exists some \\(\\varepsilon\u0026gt;0\\) such that for every \\(\\delta\u0026gt;0\\) there exists a point \\(x \\in E\\) (depending on \\(\\delta\\) ), for which \\(d_{Y}(f(x), q) \\geq \\varepsilon\\) but \\(0\u0026lt;d_{X}(x, p)\u0026lt;\\delta\\). Taking \\(\\delta_{n}=1 / n(n=1,2,3, \\ldots)\\), we thus find a sequence in \\(E\\) satisfying (6) for which (5) is false.\nCorollary If \\(f\\) has a limit at \\(p\\), this limit is unique. This follows from Theorems \\(3.2(b)\\) and 4.2.\n4.3 Definition Suppose we have two complex functions, \\(f\\) and \\(g\\), both defined on \\(E\\). By \\(f+g\\) we mean the function which assigns to each point \\(x\\) of \\(E\\) the number \\(f(x)+g(x)\\). Similarly we define the difference \\(f-g\\), the product \\(f g\\), and the quotient \\(f / g\\) of the two functions, with the understanding that the quotient is defined only at those points \\(x\\) of \\(E\\) at which \\(g(x) \\neq 0\\). If \\(f\\) assigns to each point \\(x\\) of \\(E\\) the same number \\(c\\), then \\(f\\) is said to be a constant function, or simply a constant, and we write \\(f=c\\). If \\(f\\) and \\(g\\) are real functions, and if \\(f(x) \\geq g(x)\\) for every \\(x \\in E\\), we shall sometimes write \\(f \\geq g\\), for brevity. Similarly, if \\(\\mathbf{f}\\) and \\(\\mathbf{g}\\) map \\(E\\) into \\(R^{k}\\), we define \\(\\mathbf{f}+\\mathbf{g}\\) and \\(\\mathbf{f} \\cdot \\mathbf{g}\\) by \\[ (\\mathbf{f}+\\mathbf{g})(x)=\\mathbf{f}(x)+\\mathbf{g}(x), \\quad(\\mathbf{f} \\cdot \\mathbf{g})(x)=\\mathbf{f}(x) \\cdot \\mathbf{g}(x) \\text {; } \\] and if \\(\\lambda\\) is a real number, \\((\\lambda f)(x)=\\lambda f(x)\\).\n4.4 Theorem Suppose \\(E \\subset X\\), a metric space, \\(p\\) is a limit point of \\(E, f\\) and \\(g\\) are complex functions on \\(E\\), and \\[ \\lim _{x \\rightarrow p} f(x)=A, \\quad \\lim _{x \\rightarrow p} g(x)=B . \\] Then \\((a) \\lim (f+g)(x)=A+B\\) (b) \\(\\lim _{x \\rightarrow p}^{x \\rightarrow p}(f g)(x)=A B\\); (c) \\(\\lim _{x \\rightarrow p}\\left(\\frac{f}{g}\\right)(x)=\\frac{A}{B}\\), if \\(B \\neq 0\\). Proof In view of Theorem 4.2, these assertions follow immediately from the analogous properties of sequences (Theorem 3.3). Remark If \\(\\mathbf{f}\\) and \\(\\mathbf{g}\\) map \\(E\\) into \\(R^{k}\\), then (a) remains true, and (b) becomes (b’) \\(\\lim _{x \\rightarrow p}(\\mathbf{f} \\cdot \\mathbf{g})(x)=\\mathbf{A} \\cdot \\mathbf{B}\\). (Compare Theorem 3.4.)\n","date":"2022-07-18T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch-4/1-limits-of-functions/","section":"baby rudin","tags":null,"title":"LIMITS OF FUNCTIONS"},{"categories":null,"contents":"3.52 Definition Let \\(\\left\\{k_{n}\\right\\}, n=1,2,3, \\ldots\\), be a sequence in which every positive integer appears once and only once (that is, \\(\\left\\{k_{n}\\right\\}\\) is a 1-1 function from \\(J\\) onto \\(J\\), in the notation of Definition 2.2). Putting \\[ a_{n}^{\\prime}=a_{k_{n}} \\quad(n=1,2,3, \\ldots), \\] we say that \\(\\Sigma a_{n}^{\\prime}\\) is a rearrangement of \\(\\Sigma a_{n}\\).\nIf \\(\\left\\{s_{n}\\right\\},\\left\\{s_{n}^{\\prime}\\right\\}\\) are the sequences of partial sums of \\(\\Sigma a_{n}, \\Sigma a_{n}^{\\prime}\\), it is easily seen that, in general, these two sequences consist of entirely different numbers. We are thus led to the problem of determining under what conditions all rearrangements of a convergent series will converge and whether the sums are necessarily the same. 3.53 Example Consider the convergent series \\[ 1-\\frac{1}{2}+\\frac{1}{3}-\\frac{1}{4}+\\frac{1}{5}-\\frac{1}{6}+\\cdots \\] and one of its rearrangements \\[ 1+\\frac{1}{3}-\\frac{1}{2}+\\frac{1}{5}+\\frac{1}{7}-\\frac{1}{4}+\\frac{1}{9}+\\frac{1}{11}-\\frac{1}{6}+\\cdots \\] in which two positive terms are always followed by one negative. If \\(s\\) is the sum of (22), then \\[ s\u0026lt;1-\\frac{1}{2}+\\frac{1}{3}=\\frac{5}{6} . \\] Since \\[ \\frac{1}{4 k-3}+\\frac{1}{4 k-1}-\\frac{1}{2 k}\u0026gt;0 \\] for \\(k \\geq 1\\), we see that \\(s_{3}^{\\prime}\u0026lt;s_{6}^{\\prime}\u0026lt;s_{9}^{\\prime}\u0026lt;\\cdots\\), where \\(s_{n}^{\\prime}\\) is \\(n\\)th partial sum of (23). Hence \\[ \\limsup _{n \\rightarrow \\infty} s_{n}^{\\prime}\u0026gt;s_{3}^{\\prime}=\\frac{5}{6}, \\] so that (23) certainly does not converge to \\(s\\) [we leave it to the reader to verify that (23) does, however, converge]. This example illustrates the following theorem, due to Riemann.\n3.54 Theorem Let \\(\\Sigma a_{n}\\) be a series of real numbers which converges, but not absolutely. Suppose \\[ -\\infty \\leq \\alpha \\leq \\beta \\leq \\infty \\text {. } \\] Then there exists a rearrangement \\(\\Sigma a_{n}^{\\prime}\\) with partial sums \\(s_{n}^{\\prime}\\) such that \\[ \\liminf _{n \\rightarrow \\infty} s_{n}^{\\prime}=\\alpha, \\quad \\limsup _{n \\rightarrow \\infty} s_{n}^{\\prime}=\\beta . \\] Proof Let \\[ p_{n}=\\frac{\\left|a_{n}\\right|+a_{n}}{2}, \\quad q_{n}=\\frac{\\left|a_{n}\\right|-a_{n}}{2} \\quad(n=1,2,3, \\ldots) \\] Then \\(p_{n}-q_{n}=a_{n}, p_{n}+q_{n}=\\left|a_{n}\\right|, p_{n} \\geq 0, q_{n} \\geq 0\\). The series \\(\\Sigma p_{n}, \\Sigma q_{n}\\) must both diverge. For if both were convergent, then \\[ \\Sigma\\left(p_{n}+q_{n}\\right)=\\Sigma\\left|a_{n}\\right| \\] would converge, contrary to hypothesis. Since \\[ \\sum_{n=1}^{N} a_{n}=\\sum_{n=1}^{N}\\left(p_{n}-q_{n}\\right)=\\sum_{n=1}^{N} p_{n}-\\sum_{n=1}^{N} q_{n}, \\] divergence of \\(\\Sigma p_{n}\\) and convergence of \\(\\Sigma q_{n}\\) (or vice versa) implies divergence of \\(\\Sigma a_{n}\\), again contrary to hypothesis.\nNow let \\(P_{1}, P_{2}, P_{3}, \\ldots\\) denote the nonnegative terms of \\(\\Sigma a_{n}\\), in the order in which they occur, and let \\(Q_{1}, Q_{2}, Q_{3}, \\ldots\\) be the absolute values of the negative terms of \\(\\Sigma a_{n}\\), also in their original order.\nThe series \\(\\Sigma P_{n}, \\Sigma Q_{n}\\) differ from \\(\\Sigma p_{n}, \\Sigma q_{n}\\) only by zero terms, and are therefore divergent. We shall construct sequences \\(\\left\\{m_{n}\\right\\},\\left\\{k_{n}\\right\\}\\), such that the series (25) \\(P_{1}+\\cdots+P_{m_{1}}-Q_{1}-\\cdots-Q_{k_{1}}+P_{m_{1}+1}+\\cdots\\) \\[ +P_{m_{2}}-Q_{k_{1}+1}-\\cdots-Q_{k_{2}}+\\cdots, \\] which clearly is a rearrangement of \\(\\Sigma a_{n}\\), satisfies (24). Choose real-valued sequences \\(\\left\\{\\alpha_{n}\\right\\},\\left\\{\\beta_{n}\\right\\}\\) such that \\(\\alpha_{n} \\rightarrow \\alpha, \\beta_{n} \\rightarrow \\beta\\), \\(\\alpha_{n}\u0026lt;\\beta_{n}, \\beta_{1}\u0026gt;0\\). \\(\\quad\\) Let \\(m_{1}, k_{1}\\) be the smallest integers such that Let \\(m_{1}, k_{1}\\) be the smallest integers such that \\[ \\begin{gathered} P_{1}+\\cdots+P_{m_{1}}\u0026gt;\\beta_{1} \\\\ P_{1}+\\cdots+P_{m_{1}}-Q_{1}-\\cdots-Q_{k_{1}}\u0026lt;\\alpha_{1} \\end{gathered} \\] let \\(m_{2}, k_{2}\\) be the smallest integers such that \\[ \\begin{aligned} P_{1}+\\cdots+P_{m_{1}}-Q_{1}-\\cdots-Q_{k_{1}}+P_{m_{1}+1}+\\cdots+P_{m_{2}}\u0026gt;\\beta_{2} \\\\ P_{1}+\\cdots+P_{m_{1}}-Q_{1}-\\cdots-Q_{k_{1}}+P_{m_{1}+1}+\\cdots+P_{m_{2}}-Q_{k_{1}+1} \\\\ \u0026amp;-\\cdots-Q_{k_{2}}\u0026lt;\\alpha_{2} \\end{aligned} \\] and continue in this way. This is possible since \\(\\Sigma P_{n}\\) and \\(\\Sigma Q_{n}\\) diverge. If \\(x_{n}, y_{n}\\) denote the partial sums of (25) whose last terms are \\(P_{m_{n}}\\), \\(-Q_{k_{n}}\\), then \\[ \\left|x_{n}-\\beta_{n}\\right| \\leq P_{m_{n}}, \\quad\\left|y_{n}-\\alpha_{n}\\right| \\leq Q_{k_{n}} . \\] Since \\(P_{n} \\rightarrow 0\\) and \\(Q_{n} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\), we see that \\(x_{n} \\rightarrow \\beta, y_{n} \\rightarrow \\alpha\\). Finally, it is clear that no number less than \\(\\alpha\\) or greater than \\(\\beta\\) can be a subsequential limit of the partial sums of ( 25\\()\\).\n3.55 Theorem If \\(\\Sigma a_{n}\\) is a series of complex numbers which converges absolutely, then every rearrangement of \\(\\Sigma a_{n}\\) converges, and they all converge to the same sum. Proof Let \\(\\Sigma a_{n}^{\\prime}\\) be a rearrangement, with partial sums \\(s_{n}^{\\prime}\\). Given \\(\\varepsilon\u0026gt;0\\), there exists an integer \\(N\\) such that \\(m \\geq n \\geq N\\) implies \\[ \\sum_{i=n}^{m}\\left|a_{i}\\right| \\leq \\varepsilon . \\] Now choose \\(p\\) such that the integers \\(1,2, \\ldots, N\\) are all contained in the set \\(k_{1}, k_{2}, \\ldots, k_{p}\\) (we use the notation of Definition 3.52). Then if \\(n\u0026gt;p\\), the numbers \\(a_{1}, \\ldots, a_{N}\\) will cancel in the difference \\(s_{n}-s_{n}^{\\prime}\\), so that \\(\\left|s_{n}-s_{n}^{\\prime}\\right| \\leq \\varepsilon\\), by (26). Hence \\(\\left\\{s_{n}^{\\prime}\\right\\}\\) converges to the same sum as \\(\\left\\{s_{n}\\right\\}\\).\n","date":"2022-07-18T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/14-rearrangements/","section":"baby rudin","tags":null,"title":"REARRANGEMENTS"},{"categories":null,"contents":" youtube bili note pdf xopp 3.38 Definition Given a sequence \\(\\left\\{c_{n}\\right\\}\\) of complex numbers, the series\n\\[ \\sum_{n=0}^{\\infty} c_{n} z^{n} \\]\nis called a power series. The numbers \\(c_{n}\\) are called the coefficients of the series; \\(z\\) is a complex number.\nIn general, the series will converge or diverge, depending on the choice of \\(z\\). More specifically, with every power series there is associated a circle, the circle of convergence, such that (19) converges if \\(z\\) is in the interior of the circle and diverges if \\(z\\) is in the exterior (to cover all cases, we have to consider the plane as the interior of a circle of infinite radius, and a point as a circle of radius zero). The behavior on the circle of convergence is much more varied and cannot be described so simply.\n3.39 Theorem Given the power series \\(\\Sigma c_{n} z^{n}\\), put \\[ \\alpha=\\limsup _{n \\rightarrow \\infty} \\sqrt[n]{\\left|c_{n}\\right|}, \\quad R=\\frac{1}{\\alpha} \\] (If \\(\\alpha=0, R=+\\infty\\); if \\(\\alpha=+\\infty, R=0\\).) Then \\(\\Sigma c_{n} z^{n}\\) converges if \\(|z|\u0026lt;R\\), and diverges if \\(|z|\u0026gt;R\\). Proof Put \\(a_{n}=c_{n} z^{n}\\), and apply the root test: \\[ \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{\\left|a_{n}\\right|}=|z| \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{\\left|c_{n}\\right|}=\\frac{|z|}{R} \\] Note: \\(R\\) is called the radius of convergence of \\(\\Sigma c_{n} z^{n}\\).\n3.40 Examples (a) The series \\(\\Sigma n^{n} z^{n}\\) has \\(R=0\\). (b) The series \\(\\sum \\frac{z^{n}}{n !}\\) has \\(R=+\\infty\\). (In this case the ratio test is easier to apply than the root test.) (c) The series \\(\\Sigma z^{n}\\) has \\(R=1\\). If \\(|z|=1\\), the series diverges, since \\(\\left\\{z^{n}\\right\\}\\) does not tend to 0 as \\(n \\rightarrow \\infty\\). (d) The series \\(\\sum \\frac{z^{n}}{n}\\) has \\(R=1\\). It diverges if \\(z=1\\). It converges for all other \\(z\\) with \\(|z|=1\\). (The last assertion will be proved in Theorem 3.44.) (e) The series \\(\\sum \\frac{z^{n}}{n^{2}}\\) has \\(R=1\\). It converges for all \\(z\\) with \\(|z|=1\\), by the comparison test, since \\(\\left|z^{n} / n^{2}\\right|=1 / n^{2}\\).\n","date":"2022-07-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/10-power-series/","section":"baby rudin","tags":null,"title":"10 POWER SERIES"},{"categories":null,"contents":" youtube bilibili note pdf xopp 3.41 Theorem Given two sequences \\(\\left\\{a_{n}\\right\\},\\left\\{b_{n}\\right\\}\\), put\n\\[ \\begin{equation} A_{n}=\\sum_{k=0}^{n} a_{k} \\end{equation} \\]\nif \\(n \\geq 0\\); put \\(A_{-1}=0\\). Then, if \\(0 \\leq p \\leq q\\), we have\n\\[ \\begin{equation} \\sum_{n=p}^{q} a_{n} b_{n}=\\sum_{n=p}^{q-1} A_{n}\\left(b_{n}-b_{n+1}\\right)+A_{q} b_{q}-A_{p-1} b_{p} \\end{equation} \\]\nProof\n\\[ \\begin{equation} \\sum_{n=p}^{q} a_{n} b_{n}=\\sum_{n=p}^{q}\\left(A_{n}-A_{n-1}\\right) b_{n}=\\sum_{n=p}^{q} A_{n} b_{n}-\\sum_{n=p-1}^{q-1} A_{n} b_{n+1}, \\end{equation} \\]\nand the last expression on the right is clearly equal to the right side of (2).\nFormula (2), the so-called “partial summation formula,” is useful in the investigation of series of the form \\(\\Sigma a_{n} b_{n}\\), particularly when \\(\\left\\{b_{n}\\right\\}\\) is monotonic. We shall now give applications.\n3.42 Theorem Suppose\nthe partial sums \\(A_{n}\\) of \\(\\Sigma a_{n}\\) form a bounded sequence; \\(b_{0} \\geq b_{1} \\geq b_{2} \\geq \\cdots\\); \\(\\lim _{n \\rightarrow \\infty} b_{n}=0\\). Then \\(\\Sigma a_{n} b_{n}\\) converges.\nProof Choose \\(M\\) such that \\(\\left|A_{n}\\right| \\leq M\\) for all \\(n\\). Given \\(\\varepsilon\u0026gt;0\\), there is an integer \\(N\\) such that \\(b_{N} \\leq(\\varepsilon / 2 M)\\). For \\(N \\leq p \\leq q\\), we have \\[ \\begin{aligned} \\left|\\sum_{n=p}^{q} a_{n} b_{n}\\right| \u0026amp;=\\left|\\sum_{n=p}^{q-1} A_{n}\\left(b_{n}-b_{n+1}\\right)+A_{q} b_{q}-A_{p-1} b_{p}\\right| \\\\ \u0026amp; \\leq M\\left|\\sum_{n=p}^{q-1}\\left(b_{n}-b_{n+1}\\right)+b_{q}+b_{p}\\right| \\\\ \u0026amp;=2 M b_{p} \\leq 2 M b_{N} \\leq \\varepsilon . \\end{aligned} \\] Convergence now follows from the Cauchy criterion. We note that the first inequality in the above chain depends of course on the fact that \\(b_{n}-b_{n+1} \\geq 0\\).\n3.43 Theorem Suppose\n\\(\\left|c_{1}\\right| \\geq\\left|c_{2}\\right| \\geq\\left|c_{3}\\right| \\geq \\cdots\\) \\(c_{2 m-1} \\geq 0, c_{2 m} \\leq 0 \\quad(m=1,2,3, \\ldots)\\); \\(\\lim _{n \\rightarrow \\infty} c_{n}=0\\). Then \\(\\Sigma c_{n}\\) converges. Series for which \\((b)\\) holds are called “alternating series”; the theorem was known to Leibnitz.\nProof Apply Theorem 3.42, with \\(a_{n}=(-1)^{n+1}, b_{n}=\\left|c_{n}\\right|\\).\n3.44 Theorem Suppose the radius of convergence of \\(\\Sigma c_n z^n\\) is 1 , and suppose \\(c_0 \\geq c_1 \\geq c_2 \\geq \\cdots, \\lim _{n \\rightarrow \\infty} c_n=0\\). Then \\(\\Sigma c_n z^n\\) converges at every point on the circle \\(|z|=1\\), except possibly at \\(z=1\\).\nProof Put \\(a_n=z^n, b_n=c_n\\). The hypotheses of Theorem \\(3.42\\) are then satisfied, since\n\\[ \\begin{equation} \\left|A_n\\right|=\\left|\\sum_{m=0}^n z^m\\right|=\\left|\\frac{1-z^{n+1}}{1-z}\\right| \\leq \\frac{2}{|1-z|}, \\end{equation} \\]\nif \\(|z|=1, z \\neq 1\\)\n","date":"2022-07-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/11-summation-by-parts/","section":"baby rudin","tags":null,"title":"11 SUMMATION BY PARTS"},{"categories":null,"contents":" bilibili youtube The series \\(\\Sigma a_{n}\\) is said to converge absolutely if the series \\(\\Sigma\\left|a_{n}\\right|\\) converges.\n3.45 Theorem If \\(Σ a_n\\) converges absolutely, then \\(\\Sigma a_{n}\\) converges.\nProof The assertion follows from the inequality\n\\[ \\begin{equation} \\left| ∑ {k=n}^{m} a_{k}\\right| \\leq ∑_{k=n}^{m}\\left|a_{k}\\right| \\end{equation} \\]\n3.46 Remarks For series of positive terms, absolute convergence is the same as convergence.\nIf \\(\\Sigma a_{n}\\) converges, but \\(\\Sigma\\left|a_{n}\\right|\\) diverges, we say that \\(\\Sigma a_{n}\\) converges nonabsolutely. For instance, the series\n\\[ \\begin{equation} \\sum \\frac{(-1)^{n}}{n} \\end{equation} \\]\nconverges nonabsolutely (Theorem 3.43). The comparison test, as well as the root and ratio tests, is really a test for absolute convergence, and therefore cannot give any information about nonabsolutely convergent series. Summation by parts can sometimes be used to handle the latter. In particular, power series converge absolutely in the interior of the circle of convergence.\nWe shall see that we may operate with absolutely convergent series very much as with finite sums. We may multiply them term by term and we may change the order in which the additions are carried out, without affecting the sum of the series. But for nonabsolutely convergent series this is no longer true, and more care has to be taken when dealing with them.\n","date":"2022-07-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/12-absolute-convergence/","section":"baby rudin","tags":null,"title":"12 ABSOLUTE CONVERGENCE"},{"categories":null,"contents":" youtube bili 3.33 Theorem (Root Test) Given \\(\\Sigma a_{n}\\), put \\(\\alpha=\\limsup _{n \\rightarrow \\infty} \\sqrt[n]{\\left|a_{n}\\right|}\\). Then (a) if \\(\\alpha\u0026lt;1, \\Sigma a_{n}\\) converges; (b) if \\(\\alpha\u0026gt;1, \\Sigma a_{n}\\) diverges; (c) if \\(\\alpha=1\\), the test gives no information.\nProof If \\(\\alpha\u0026lt;1\\), we can choose \\(\\beta\\) so that \\(\\alpha\u0026lt;\\beta\u0026lt;1\\), and an integer \\(N\\) such that\n\\[ \\sqrt[n]{\\left|a_{n}\\right|}\u0026lt;\\beta \\]\nfor \\(n \\geq N\\) [by Theorem 3.17(b)]. That is, \\(n \\geq N\\) implies\n\\[ \\left|a_{n}\\right|\u0026lt;\\beta^{n} . \\]\nSince \\(0\u0026lt;\\beta\u0026lt;1, \\Sigma \\beta^{n}\\) converges. Convergence of \\(\\Sigma a_{n}\\) follows now from the comparison test.\nIf \\(\\alpha\u0026gt;1\\), then, again by Theorem 3.17, there is a sequence \\(\\left\\{n_{k}\\right\\}\\) such that \\[ \\sqrt[n_{k}]{\\left|a_{n_{k}}\\right|} \\rightarrow \\alpha . \\] Hence \\(\\left|a_{n}\\right|\u0026gt;1\\) for infinitely many values of \\(n\\), so that the condition \\(a_{n} \\rightarrow 0\\), necessary for convergence of \\(\\Sigma a_{n}\\), does not hold (Theorem 3.23). To prove \\((c)\\), we consider the series \\[ \\sum \\frac{1}{n}, \\sum \\frac{1}{n^{2}} . \\] For each of these series \\(\\alpha=1\\), but the first diverges, the second converges.\n3.34 Theorem (Ratio Test) The series \\(\\Sigma a_{n}\\) (a) converges if \\(\\limsup _{n \\rightarrow \\infty}\\left|\\frac{a_{n+1}}{a_{n}}\\right|\u0026lt;1\\), (b) diverges if \\(\\left|\\frac{a_{n+1}}{a_{n}}\\right| \\geq 1\\) for all \\(n \\geq n_{0}\\), where \\(n_{0}\\) is some fixed integer.\nProof If condition \\((a)\\) holds, we can find \\(\\beta\u0026lt;1\\), and an integer \\(N\\), such that\n\\[ \\left|\\frac{a_{n+1}}{a_{n}}\\right|\u0026lt;\\beta \\] for \\(n \\geq N\\). In particular, \\[ \\begin{aligned} \u0026amp;\\left|a_{N+1}\\right|\u0026lt;\\beta\\left|a_{N}\\right|, \\\\ \u0026amp;\\left|a_{N+2}\\right|\u0026lt;\\beta\\left|a_{N+1}\\right|\u0026lt;\\beta^{2}\\left|a_{N}\\right|, \\\\ \u0026amp;\\ldots \\ldots \\ldots \\ldots \\ldots . . \\ldots \\ldots . \\\\ \u0026amp;\\left|a_{N+p}\\right|\u0026lt;\\beta^{p}\\left|a_{N}\\right| . \\end{aligned} \\] That is, \\[ \\left|a_{n}\\right|\u0026lt;\\left|a_{N}\\right| \\beta^{-N} \\cdot \\beta^{n} \\] for \\(n \\geq N\\), and (a) follows from the comparison test, since \\(\\Sigma \\beta^{n}\\) converges. If \\(\\left|a_{n+1}\\right| \\geq\\left|a_{n}\\right|\\) for \\(n \\geq n_{0}\\), it is easily seen that the condition \\(a_{n} \\rightarrow 0\\) does not hold, and \\((b)\\) follows.\nNote: The knowledge that \\(\\lim a_{n+1} / a_{n}=1\\) implies nothing about the convergence of \\(\\Sigma a_{n}\\). The series \\(\\Sigma 1 / n\\) and \\(\\Sigma 1 / n^{2}\\) demonstrate this.\n3.35 Examples (a) Consider the series \\[ \\frac{1}{2}+\\frac{1}{3}+\\frac{1}{2^{2}}+\\frac{1}{3^{2}}+\\frac{1}{2^{3}}+\\frac{1}{3^{3}}+\\frac{1}{2^{4}}+\\frac{1}{3^{4}}+\\cdots, \\] for which \\[ \\begin{aligned} \u0026amp;\\liminf _{n \\rightarrow \\infty} \\frac{a_{n+1}}{a_{n}}=\\lim _{n \\rightarrow \\infty}\\left(\\frac{2}{3}\\right)^{n}=0, \\\\ \u0026amp;\\liminf _{n \\rightarrow \\infty} \\sqrt[n]{a_{n}}=\\lim _{n \\rightarrow \\infty} \\sqrt[2 n]{\\frac{1}{3^{n}}}=\\frac{1}{\\sqrt{3}} \\\\ \u0026amp;\\limsup _{n \\rightarrow \\infty} \\sqrt[n]{a_{n}}=\\lim _{n \\rightarrow \\infty} \\sqrt[2 n]{\\frac{1}{2^{n}}}=\\frac{1}{\\sqrt{2}} \\\\ \u0026amp;\\limsup _{n \\rightarrow \\infty} \\frac{a_{n+1}}{a_{n}}=\\lim _{n \\rightarrow \\infty} \\frac{1}{2}\\left(\\frac{3}{2}\\right)^{n}=+\\infty \\end{aligned} \\] The root test indicates convergence; the ratio test does not apply. (b) The same is true for the series \\[ \\frac{1}{2}+1+\\frac{1}{8}+\\frac{1}{4}+\\frac{1}{32}+\\frac{1}{16}+\\frac{1}{128}+\\frac{1}{64}+\\cdots, \\] where \\[ \\begin{aligned} \u0026amp;\\liminf _{n \\rightarrow \\infty} \\frac{a_{n+1}}{a_{n}}=\\frac{1}{8}, \\\\ \u0026amp;\\limsup _{n \\rightarrow \\infty} \\frac{a_{n+1}}{a_{n}}=2, \\end{aligned} \\] but \\[ \\lim \\sqrt[n]{a_{n}}=\\frac{1}{2} \\] 3.36 Remarks The ratio test is frequently easier to apply than the root test, since it is usually easier to compute ratios than \\(n\\)th roots. However, the root test has wider scope. More precisely: Whenever the ratio test shows convergence, the root test does too; whenever the root test is inconclusive, the ratio test is too. This is a consequence of Theorem 3.37, and is illustrated by the above examples.\nNeither of the two tests is subtle with regard to divergence. Both deduce divergence from the fact that \\(a_{n}\\) does not tend to zero as \\(n \\rightarrow \\infty\\).\n3.37 Theorem For any sequence \\(\\left\\{c_{n}\\right\\}\\) of positive numbers, \\[ \\begin{gathered} \\liminf _{n \\rightarrow \\infty} \\frac{c_{n+1}}{c_{n}} \\leq \\liminf _{n \\rightarrow \\infty} \\sqrt[n]{c_{n}}, \\\\ \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{c_{n}} \\leq \\limsup _{n \\rightarrow \\infty} \\frac{c_{n+1}}{c_{n}} \\end{gathered} \\]\nProof We shall prove the second inequality; the proof of the first is quite similar. Put\n\\[ \\alpha=\\limsup _{n \\rightarrow \\infty} \\frac{c_{n+1}}{c_{n}} . \\]\nIf \\(\\alpha=+\\infty\\), there is nothing to prove. If \\(\\alpha\\) is finite, choose \\(\\beta\u0026gt;\\alpha\\). There is an integer \\(N\\) such that\n\\[ \\frac{c_{n+1}}{c_{n}} \\leq \\beta \\]\nfor \\(n \\geq N\\). In particular, for any \\(p\u0026gt;0\\),\n\\[ c_{N+k+1} \\leq \\beta c_{N+k} \\quad(k=0,1, \\ldots, p-1) . \\]\nMultiplying these inequalities, we obtain\n\\[ c_{N+p} \\leq \\beta^{p} c_{N}, \\] or \\[ c_{n} \\leq c_{N} \\beta^{-N} \\cdot \\beta^{n} \\quad(n \\geq N) \\] Hence \\[ \\sqrt[n]{c_{n}} \\leq \\sqrt[n]{c_{N} \\beta^{-N}} \\cdot \\beta \\] so that \\[ \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{c_{n}} \\leq \\beta \\] by Theorem 3.20(b). Since (18) is true for every \\(\\beta\u0026gt;\\alpha\\), we have \\[ \\limsup _{n \\rightarrow \\infty} \\sqrt[n]{c_{n}} \\leq \\alpha . \\]\n","date":"2022-07-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/9-the-root-and-ratio-tests/","section":"baby rudin","tags":null,"title":"9 THE ROOT AND RATIO TESTS"},{"categories":null,"contents":"3.47 Theorem If \\(\\Sigma a_{n}=A\\), and \\(\\Sigma b_{n}=B\\), then \\(\\Sigma\\left(a_{n}+b_{n}\\right)=A+B\\), and \\(\\Sigma c a_{n}=c A\\), for any fixed \\(c\\). Proof Let \\[ A_{n}=\\sum_{k=0}^{n} a_{k}, \\quad B_{n}=\\sum_{k=0}^{n} b_{k} \\] Then \\[ A_{n}+B_{n}=\\sum_{k=0}^{n}\\left(a_{k}+b_{k}\\right) \\text {. } \\] Since \\(\\lim _{n \\rightarrow \\infty} A_{n}=A\\) and \\(\\lim _{n \\rightarrow \\infty} B_{n}=B\\), we see that \\[ \\lim _{n \\rightarrow \\infty}\\left(A_{n}+B_{n}\\right)=A+B \\] The proof of the second assertion is even simpler.\nThus two convergent series may be added term by term, and the resulting series converges to the sum of the two series. The situation becomes more complicated when we consider multiplication of two series. To begin with, we have to define the product. This can be done in several ways; we shall consider the so-called “’Cauchy product.”\n3.48 Definition Given \\(\\Sigma a_{n}\\) and \\(\\Sigma b_{n}\\), we put \\[ c_{n}=\\sum_{k=0}^{n} a_{k} b_{n-k} \\quad(n=0,1,2, \\ldots) \\] and call \\(\\Sigma c_{n}\\) the product of the two given series. This definition may be motivated as follows. If we take two power series \\(\\Sigma a_{n} z^{n}\\) and \\(\\Sigma b_{n} z^{n}\\), multiply them term by term, and collect terms containing the same power of \\(z\\), we get \\[ \\begin{aligned} \\sum_{n=0}^{\\infty} a_{n} z^{n} \\cdot \\sum_{n=0}^{\\infty} b_{n} z^{n} \u0026amp;=\\left(a_{0}+a_{1} z+a_{2} z^{2}+\\cdots\\right)\\left(b_{0}+b_{1} z+b_{2} z^{2}+\\cdots\\right) \\\\ \u0026amp;=a_{0} b_{0}+\\left(a_{0} b_{1}+a_{1} b_{0}\\right) z+\\left(a_{0} b_{2}+a_{1} b_{1}+a_{2} b_{0}\\right) z^{2}+\\cdots \\\\ \u0026amp;=c_{0}+c_{1} z+c_{2} z^{2}+\\cdots \\end{aligned} \\] Setting \\(z=1\\), we arrive at the above definition.\n3.49 Example If \\[ A_{n}=\\sum_{k=0}^{n} a_{k}, \\quad B_{n}=\\sum_{k=0}^{n} b_{k}, \\quad C_{n}=\\sum_{k=0}^{n} c_{k}, \\] and \\(A_{n} \\rightarrow A, B_{n} \\rightarrow B\\), then it is not at all clear that \\(\\left\\{C_{n}\\right\\}\\) will converge to \\(A B\\), since we do not have \\(C_{n}=A_{n} B_{n}\\). The dependence of \\(\\left\\{C_{n}\\right\\}\\) on \\(\\left\\{A_{n}\\right\\}\\) and \\(\\left\\{B_{n}\\right\\}\\) is quite a complicated one (see the proof of Theorem 3.50). We shall now show that the product of two convergent series may actually diverge. The series \\[ \\sum_{n=0}^{\\infty} \\frac{(-1)^{n}}{\\sqrt{n+1}}=1-\\frac{1}{\\sqrt{2}}+\\frac{1}{\\sqrt{3}}-\\frac{1}{\\sqrt{4}}+\\cdots \\] converges (Theorem 3.43). We form the product of this series with itself and obtain \\[ \\begin{aligned} \\sum_{n=0}^{\\infty} c_{n}=1-\\left(\\frac{1}{\\sqrt{2}}+\\frac{1}{\\sqrt{2}}\\right)+\\left(\\frac{1}{\\sqrt{3}}+\\frac{1}{\\sqrt{2} \\sqrt{2}}+\\frac{1}{\\sqrt{3}}\\right) \\\\ \u0026amp;-\\left(\\frac{1}{\\sqrt{4}}+\\frac{1}{\\sqrt{3} \\sqrt{2}}+\\frac{1}{\\sqrt{2} \\sqrt{3}}+\\frac{1}{\\sqrt{4}}\\right)+\\cdots, \\end{aligned} \\] so that \\[ c_{n}=(-1)^{n} \\sum_{k=0}^{n} \\frac{1}{\\sqrt{(n-k+1)(k+1)}} . \\] Since \\[ (n-k+1)(k+1)=\\left(\\frac{n}{2}+1\\right)^{2}-\\left(\\frac{n}{2}-k\\right)^{2} \\leq\\left(\\frac{n}{2}+1\\right)^{2} . \\] we have \\[ \\left|c_{n}\\right| \\geq \\sum_{k=0}^{n} \\frac{2}{n+2}=\\frac{2(n+1)}{n+2} \\] so that the condition \\(c_{n} \\rightarrow 0\\), which is necessary for the convergence of \\(\\Sigma c_{n}\\), is not satisfied.\nIn view of the next theorem, due to Mertens, we note that we have here considered the product of two nonabsolutely convergent series.\n3.50 Theorem Suppose (a) \\(\\sum_{n=0}^{\\infty} a_{n}\\) converges absolutely, (b) \\(\\sum_{n=0}^{\\infty} a_{n}=A\\) (c) \\(\\sum_{n=0}^{\\infty} b_{n}=B\\) (d) \\(c_{n}=\\sum_{k=0}^{n} a_{k} b_{n-k} \\quad(n=0,1,2, \\ldots)\\) Then \\[ \\sum_{n=0}^{\\infty} c_{n}=A B . \\] That is, the product of two convergent series converges, and to the right value, if at least one of the two series converges absolutely. Proof Put \\[ A_{n}=\\sum_{k=0}^{n} a_{k}, \\quad B_{n}=\\sum_{k=0}^{n} b_{k}, \\quad C_{n}=\\sum_{k=0}^{n} c_{k}, \\quad \\beta_{n}=B_{n}-B \\] Then \\[ \\begin{aligned} C_{n} \u0026amp;=a_{0} b_{0}+\\left(a_{0} b_{1}+a_{1} b_{0}\\right)+\\cdots+\\left(a_{0} b_{n}+a_{1} b_{n-1}+\\cdots+a_{n} b_{0}\\right) \\\\ \u0026amp;=a_{0} B_{n}+a_{1} B_{n-1}+\\cdots+a_{n} B_{0} \\\\ \u0026amp;=a_{0}\\left(B+\\beta_{n}\\right)+a_{1}\\left(B+\\beta_{n-1}\\right)+\\cdots+a_{n}\\left(B+\\beta_{0}\\right) \\\\ \u0026amp;=A_{n} B+a_{0} \\beta_{n}+a_{1} \\beta_{n-1}+\\cdots+a_{n} \\beta_{0} \\end{aligned} \\] Put \\[ \\gamma_{n}=a_{0} \\beta_{n}+a_{1} \\beta_{n-1}+\\cdots+a_{n} \\beta_{0} . \\] We wish to show that \\(C_{n} \\rightarrow A B\\). Since \\(A_{n} B \\rightarrow A B\\), it suffices to show that \\[ \\lim _{n \\rightarrow \\infty} \\gamma_{n}=0 \\] Put \\[ \\alpha=\\sum_{n=0}^{\\infty}\\left|a_{n}\\right| \\] [It is here that we use (a).] Let \\(\\varepsilon\u0026gt;0\\) be given. By \\((c), \\beta_{n} \\rightarrow 0\\). Hence we can choose \\(N\\) such that \\(\\left|\\beta_{n}\\right| \\leq \\varepsilon\\) for \\(n \\geq N\\), in which case \\[ \\begin{aligned} \\left|\\gamma_{n}\\right| \u0026amp; \\leq\\left|\\beta_{0} a_{n}+\\cdots+\\beta_{N} a_{n-N}\\right|+\\left|\\beta_{N+1} a_{n-N-1}+\\cdots+\\beta_{n} a_{0}\\right| \\\\ \u0026amp; \\leq\\left|\\beta_{0} a_{n}+\\cdots+\\beta_{N} a_{n-N}\\right|+\\varepsilon \\alpha . \\end{aligned} \\] Keeping \\(N\\) fixed, and letting \\(n \\rightarrow \\infty\\), we get \\[ \\limsup _{n \\rightarrow \\infty}\\left|\\gamma_{n}\\right| \\leq \\varepsilon \\alpha, \\] since \\(a_{k} \\rightarrow 0\\) as \\(k \\rightarrow \\infty\\). Since \\(\\varepsilon\\) is arbitrary, (21) follows. Another question which may be asked is whether the series \\(\\Sigma c_{n}\\), if convergent, must have the sum \\(A B\\). Abel showed that the answer is in the affirmative.\n3.51 Theorem If the series \\(\\Sigma a_{n}, \\Sigma b_{n}, \\Sigma c_{n}\\) converge to \\(A, B, C\\), and \\(c_{n}=a_{0} b_{n}+\\cdots+a_{n} b_{0}\\), then \\(C=A B\\).\nHere no assumption is made concerning absolute convergence. We shall give a simple proof (which depends on the continuity of power series) after Theorem 8.2.\n","date":"2022-07-17T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/13-addition-and-multiplication-of-series/","section":"baby rudin","tags":null,"title":"ADDITION AND MULTIPLICATION OF SERIES"},{"categories":null,"contents":" youtube bilibili note pdf xopp In the remainder of this chapter, all sequences and series under consideration will be complex-valued, unless the contrary is explicitly stated. Extensions of some of the theorems which follow, to series with terms in \\(R^{k}\\), are mentioned in Exercise \\(15 .\\)\n3.21 Definition Given a sequence \\(\\left\\{a_{n}\\right\\}\\), we use the notation \\[ \\sum_{n=p}^{q} a_{n} \\quad(p \\leq q) \\] to denote the sum \\(a_{p}+a_{p+1}+\\cdots+a_{q}\\). With \\(\\left\\{a_{n}\\right\\}\\) we associate a sequence \\(\\left\\{s_{n}\\right\\}\\), where \\[ s_{n}=\\sum_{k=1}^{n} a_{k} . \\] For \\(\\left\\{s_{n}\\right\\}\\) we also use the symbolic expression or, more concisely, \\[ a_{1}+a_{2}+a_{3}+\\cdots \\] (4) \\[ \\sum_{n=1}^{\\infty} a_{n} \\] The symbol (4) we call an infinite series, or just a series. The numbers \\(s_{n}\\) are called the partial sums of the series. If \\(\\left\\{s_{n}\\right\\}\\) converges to \\(s\\), we say that the series converges, and write \\[ \\sum_{n=1}^{\\infty} a_{n}=s . \\] The number \\(s\\) is called the sum of the series; but it should be clearly understood that \\(s\\) is the limit of a sequence of sums, and is not obtained simply by addition. If \\(\\left\\{s_{n}\\right\\}\\) diverges, the series is said to diverge. Sometimes, for convenience of notation, we shall consider series of the form \\[ \\sum_{n=0}^{\\infty} a_{n} . \\] And frequently, when there is no possible ambiguity, or when the distinction is immaterial, we shall simply write \\(\\Sigma a_{n}\\) in place of (4) or (5).\nIt is clear that every theorem about sequences can be stated in terms of series (putting \\(a_{1}=s_{1}\\), and \\(a_{n}=s_{n}-s_{n-1}\\) for \\(n\u0026gt;1\\) ), and vice versa. But it is nevertheless useful to consider both concepts.\nThe Cauchy criterion (Theorem 3.11) can be restated in the following form:\n3.22 Theorem \\(\\Sigma a_{n}\\) converges if and only if for every \\(\\varepsilon\u0026gt;0\\) there is an integer \\(N\\) such that (6) \\[ \\left|\\sum_{k=n}^{m} a_{k}\\right| \\leq \\varepsilon \\] if \\(m \\geq n \\geq N\\). In particular, by taking \\(m=n\\), (6) becomes \\[ \\left|a_{n}\\right| \\leq \\varepsilon \\quad(n \\geq N) \\text {. } \\] In other words:\n3.23 Theorem If \\(\\Sigma a_{n}\\) converges, then \\(\\lim _{n \\rightarrow \\infty} a_{n}=0\\) The condition \\(a_{n} \\rightarrow 0\\) is not, however, sufficient to ensure convergence of \\(\\Sigma a_{n}\\). For instance, the series \\[ \\sum_{n=1}^{\\infty} \\frac{1}{n} \\] diverges; for the proof we refer to Theorem 3.28. Theorem 3.14, concerning monotonic sequences, also has an immediate counterpart for series.\n3.24 Theorem A series of nonnegative terms converges if and only if its partial sums form a bounded sequence.\nWe now turn to a convergence test of a different nature, the so-called “comparison test.”’\n3.25 Theorem (a) If \\(\\left|a_{n}\\right| \\leq c_{n}\\) for \\(n \\geq N_{0}\\), where \\(N_{0}\\) is some fixed integer, and if \\(\\Sigma c_{n}\\) converges, then \\(\\Sigma a_{n}\\) converges. (b) If \\(a_{n} \\geq d_{n} \\geq 0\\) for \\(n \\geq N_{0}\\), and if \\(\\Sigma d_{n}\\) diverges, then \\(\\Sigma a_{n}\\) diverges. Note that \\((b)\\) applies only to series of nonnegative terms \\(a_{n}\\). Proof Given \\(\\varepsilon\u0026gt;0\\), there exists \\(N \\geq N_{0}\\) such that \\(m \\geq n \\geq N\\) implies \\[ \\sum_{k=n}^{m} c_{k} \\leq \\varepsilon \\] by the Cauchy criterion. Hence \\[ \\left|\\sum_{k=n}^{m} a_{k}\\right| \\leq \\sum_{k=n}^{m}\\left|a_{k}\\right| \\leq \\sum_{k=n}^{m} c_{k} \\leq \\varepsilon \\] and (a) follows. Next, \\((b)\\) follows from \\((a)\\), for if \\(\\Sigma a_{n}\\) converges, so must \\(\\Sigma d_{n}\\) [note that \\((b)\\) also follows from Theorem 3.24]. The comparison test is a very useful one; to use it efficiently, we have to become familiar with a number of series of nonnegative terms whose convergence or divergence is known.\n","date":"2022-07-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/6-series/","section":"baby rudin","tags":null,"title":"6 SERIES"},{"categories":null,"contents":" youtube bilibili note pdf xopp The simplest of all is perhaps the geometric series.\n3.26 Theorem If \\(0 \\leq x\u0026lt;1\\), then \\[ \\sum_{n=0}^{\\infty} x^{n}=\\frac{1}{1-x} \\] If \\(x \\geq 1\\), the series diverges.\nProof If \\(x \\neq 1\\), \\[ s_{n}=\\sum_{k=0}^{n} x^{k}=\\frac{1-x^{n+1}}{1-x} . \\] The result follows if we let \\(n \\rightarrow \\infty\\). For \\(x=1\\), we get \\[ 1+1+1+\\cdots \\text {, } \\] which evidently diverges.\nIn many cases which occur in applications, the terms of the series decrease monotonically. The following theorem of Cauchy is therefore of particular interest. The striking feature of the theorem is that a rather “thin’s subsequence of \\(\\left\\{a_{n}\\right\\}\\) determines the convergence or divergence of \\(\\Sigma a_{n}\\).\n3.27 Theorem Suppose \\(a_{1} \\geq a_{2} \\geq a_{3} \\geq \\cdots \\geq 0\\). Then the series \\(\\sum_{n=1}^{\\infty} a_{n}\\) converges if and only if the series \\[ \\sum_{k=0}^{\\infty} 2^{k} a_{2^{k}}=a_{1}+2 a_{2}+4 a_{4}+8 a_{8}+\\cdots \\] converges.\nProof By Theorem 3.24, it suffices to consider boundedness of the partial sums. Let \\[ \\begin{aligned} \u0026amp;s_{n}=a_{1}+a_{2}+\\cdots+a_{n}, \\\\ \u0026amp;t_{k}=a_{1}+2 a_{2}+\\cdots+2^{k} a_{2^{k}} \\end{aligned} \\] For \\(n\u0026lt;2^{k}\\) \\[ \\begin{aligned} s_{n} \u0026amp; \\leq a_{1}+\\left(a_{2}+a_{3}\\right)+\\cdots+\\left(a_{2^{k}}+\\cdots+a_{2^{k+1}-1}\\right) \\\\ \u0026amp; \\leq a_{1}+2 a_{2}+\\cdots+2^{k} a_{2^{k}} \\\\ \u0026amp;=t_{k} \\end{aligned} \\] so that \\[ s_{n} \\leq t_{k} \\] On the other hand, if \\(n\u0026gt;2^{k}\\), \\[ \\begin{aligned} s_{n} \u0026amp; \\geq a_{1}+a_{2}+\\left(a_{3}+a_{4}\\right)+\\cdots+\\left(a_{2^{k-1}+1}+\\cdots+a_{2^{k}}\\right) \\\\ \u0026amp; \\geq \\frac{1}{2} a_{1}+a_{2}+2 a_{4}+\\cdots+2^{k-1} a_{2^{k}} \\\\ \u0026amp;=\\frac{1}{2} t_{k} \\end{aligned} \\] so that (9) \\[ 2 s_{n} \\geq t_{k} \\text {. } \\] By (8) and (9), the sequences \\(\\left\\{s_{n}\\right\\}\\) and \\(\\left\\{t_{k}\\right\\}\\) are either both bounded or both unbounded. This completes the proof.\n3.28 Theorem \\(\\sum \\frac{1}{n^{p}}\\) converges if \\(p\u0026gt;1\\) and diverges if \\(p \\leq 1\\). Proof If \\(p \\leq 0\\), divergence follows from Theorem 3.23. If \\(p\u0026gt;0\\), Theorem \\(3.27\\) is applicable, and we are led to the series \\[ \\sum_{k=0}^{\\infty} 2^{k} \\cdot \\frac{1}{2^{k p}}=\\sum_{k=0}^{\\infty} 2^{(1-p) k} \\] Now, \\(2^{1-p}\u0026lt;1\\) if and only if \\(1-p\u0026lt;0\\), and the result follows by comparison with the geometric series (take \\(x=2^{1-p}\\) in Theorem 3.26). As a further application of Theorem 3.27, we prove:\n3.29 Theorem If \\(p\u0026gt;1\\), \\[ \\sum_{n=2}^{\\infty} \\frac{1}{n(\\log n)^{p}} \\] converges; if \\(p \\leq 1\\), the series diverges. Remark “‘log \\(n\\) “’ denotes the logarithm of \\(n\\) to the base \\(e\\) (compare Exercise 7, Chap. 1); the number \\(e\\) will be defined in a moment (see Definition 3.30). We let the series start with \\(n=2\\), since \\(\\log 1=0\\).\nProof The monotonicity of the logarithmic function (which will be discussed in more detail in Chap. 8) implies that \\(\\{\\log n\\}\\) increases. Hence \\(\\{1 / n \\log n\\}\\) decreases, and we can apply Theorem \\(3.27\\) to (10); this leads us to the series \\[ \\sum_{k=1}^{\\infty} 2^{k} \\cdot \\frac{1}{2^{k}\\left(\\log 2^{k}\\right)^{p}}=\\sum_{k=1}^{\\infty} \\frac{1}{(k \\log 2)^{p}}=\\frac{1}{(\\log 2)^{p}} \\sum_{k=1}^{\\infty} \\frac{1}{k^{p}} \\] and Theorem \\(3.29\\) follows from Theorem 3.28. This procedure may evidently be continued. For instance, \\[ \\sum_{n=3}^{\\infty} \\frac{1}{n \\log n \\log \\log n} \\] diverges, whereas \\[ \\sum_{n=3}^{\\infty} \\frac{1}{n \\log n(\\log \\log n)^{2}} \\] converges.\nWe may now observe that the terms of the series (12) differ very little from those of (13). Still, one diverges, the other converges. If we continue the process which led us from Theorem \\(3.28\\) to Theorem 3.29, and then to (12) and (13), we get pairs of convergent and divergent series whose terms differ even less than those of (12) and (13). One might thus be led to the conjecture that there is a limiting situation of some sort, a “boundary” with all convergent series on one side, all divergent series on the other side-at least as far as series with monotonic coefficients are concerned. This notion of “boundary” is of course quite vague. The point we wish to make is this: No matter how we make this notion precise, the conjecture is false. Exercises \\(11(b)\\) and \\(12(b)\\) may serve as illustrations.\nWe do not wish to go any deeper into this aspect of convergence theory, and refer the reader to Knopp’s “Theory and Application of Infinite Series,” Chap. IX, particularly Sec. \\(41 .\\)\n","date":"2022-07-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/7-series-of-nonnegative-terms/","section":"baby rudin","tags":null,"title":"7 SERIES OF NONNEGATIVE TERMS"},{"categories":null,"contents":" youtube bili note pdf xopp 3.30 Definition \\(e=\\sum_{n=0}^{\\infty} \\frac{1}{n !}\\) Here \\(n !=1 \\cdot 2 \\cdot 3 \\cdots n\\) if \\(n \\geq 1\\), and \\(0 !=1\\)\nSince \\[ \\begin{aligned} s_{n} \u0026amp;=1+1+\\frac{1}{1 \\cdot 2}+\\frac{1}{1 \\cdot 2 \\cdot 3}+\\cdots+\\frac{1}{1 \\cdot 2 \\cdots n} \\\\ \u0026amp;\u0026lt;1+1+\\frac{1}{2}+\\frac{1}{2^{2}}+\\cdots+\\frac{1}{2^{n-1}}\u0026lt;3, \\end{aligned} \\] the series converges, and the definition makes sense. In fact, the series converges very rapidly and allows us to compute \\(e\\) with great accuracy.\nIt is of interest to note that \\(e\\) can also be defined by means of another limit process; the proof provides a good illustration of operations with limits:\n3.31 Theorem \\(\\lim _{n \\rightarrow \\infty}\\left(1+\\frac{1}{n}\\right)^{n}=e\\). Proof Let \\[ s_{n}=\\sum_{k=0}^{n} \\frac{1}{k !}, \\quad t_{n}=\\left(1+\\frac{1}{n}\\right)^{n} \\] By the binomial theorem, \\[ \\begin{aligned} t_{n}=1+1+\\frac{1}{2 !}\\left(1-\\frac{1}{n}\\right)+\\frac{1}{3 !}\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right) \u0026amp;+\\cdots \\\\ \u0026amp;+\\frac{1}{n !}\\left(1-\\frac{1}{n}\\right)\\left(1-\\frac{2}{n}\\right) \\cdots\\left(1-\\frac{n-1}{n}\\right) \\end{aligned} \\] Hence \\(t_{n} \\leq s_{n}\\), so that \\[ \\limsup _{n \\rightarrow \\infty} t_{n} \\leq e \\] by Theorem 3.19. Next, if \\(n \\geq m\\), \\[ t_{n} \\geq 1+1+\\frac{1}{2 !}\\left(1-\\frac{1}{n}\\right)+\\cdots+\\frac{1}{m !}\\left(1-\\frac{1}{n}\\right) \\cdots\\left(1-\\frac{m-1}{n}\\right) . \\] Let \\(n \\rightarrow \\infty\\), keeping \\(m\\) fixed. We get \\[ \\liminf _{n \\rightarrow \\infty} t_{n} \\geq 1+1+\\frac{1}{2 !}+\\cdots+\\frac{1}{m !}, \\] so that \\[ s_{m} \\leq \\underset{n \\rightarrow \\infty}{\\liminf } t_{n} . \\] Letting \\(m \\rightarrow \\infty\\), we finally get \\[ e \\leq \\liminf _{n \\rightarrow \\infty} t_{n} . \\] The theorem follows from (14) and (15).\nThe rapidity with which the series \\(\\sum \\frac{1}{n !}\\) converges can be estimated as follows: If \\(s_{n}\\) has the same meaning as above, we have \\[ \\begin{aligned} e-s_{n} \u0026amp;=\\frac{1}{(n+1) !}+\\frac{1}{(n+2) !}+\\frac{1}{(n+3) !}+\\cdots \\\\ \u0026amp;\u0026lt;\\frac{1}{(n+1) !}\\left\\{1+\\frac{1}{n+1}+\\frac{1}{(n+1)^{2}}+\\cdots\\right\\}=\\frac{1}{n ! n} \\end{aligned} \\] so that \\[ 0\u0026lt;e-s_{n}\u0026lt;\\frac{1}{n ! n} \\] Thus \\(s_{10}\\), for instance, approximates \\(e\\) with an error less than \\(10^{-7}\\). The inequality (16) is of theoretical interest as well, since it enables us to prove the irrationality of \\(e\\) very easily.\n3.32 Theorem e is irrational. Proof Suppose \\(e\\) is rational. Then \\(e=p / q\\), where \\(p\\) and \\(q\\) are positive integers. By (16), \\[ 0\u0026lt;q !\\left(e-s_{q}\\right)\u0026lt;\\frac{1}{q} \\] By our assumption, \\(q ! e\\) is an integer. Since \\[ q ! s_{q}=q !\\left(1+1+\\frac{1}{2 !}+\\cdots+\\frac{1}{q !}\\right) \\] is an integer, we see that \\(q !\\left(e-s_{q}\\right)\\) is an integer. Since \\(q \\geq 1,(17)\\) implies the existence of an integer between 0 and 1 . We have thus reached a contradiction.\nActually, \\(e\\) is not even an algebraic number. For a simple proof of this, see page 25 of Niven’s book, or page 176 of Herstein’s, cited in the Bibliography.\n","date":"2022-07-16T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/ch3/8-the-number-e/","section":"baby rudin","tags":null,"title":"8 THE NUMBER e"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1679/problem/C\nSolution For this problem, we only need position of \\(x\\) and position of \\(y\\) separately. We don’t need combine them.\nSo use fenwick tree to store the presum of \\(x\\) and \\(y\\). The value in each node only 1 and 0. Stand for has \\(x\\) or haven’t \\(x\\). Then the answer just the \\(presum(x2) - presum(x1) == x2 - x1 || presum(y2) - presum(y1) == y2 - y1\\)\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; #define endl \u0026quot;\\n\u0026quot; #include \u0026lt;stdbool.h\u0026gt; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); #include \u0026lt;stdio.h\u0026gt; //#define int long long template \u0026lt;typename T\u0026gt; class fenwick { public: vector\u0026lt;T\u0026gt; fenw; int n; fenwick(int _n) : n(_n) { fenw.resize(n); } void modify(int x, T v) { while (x \u0026lt; n) { fenw[x] += v; x |= (x + 1); } } T get(int x) { T v{}; while (x \u0026gt;= 0) { v += fenw[x]; x = (x \u0026amp; (x + 1)) - 1; } return v; } }; int32_t main() { fastio; int n, q; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; map\u0026lt;int, int\u0026gt; mx; map\u0026lt;int, int\u0026gt; my; set\u0026lt;int\u0026gt; sx; set\u0026lt;int\u0026gt; sy; fenwick\u0026lt;int\u0026gt; fen_x(4 * n); fenwick\u0026lt;int\u0026gt; fen_y(4 * n); while (q--) { int type; cin \u0026gt;\u0026gt; type; if (type == 1) { int x, y; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; mx[x]++, my[y]++; if (mx[x] == 1) fen_x.modify(x, 1); if (my[y] == 1) fen_y.modify(y, 1); } else if (type == 2) { int x, y; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; mx[x]--, my[y]--; if (mx[x] == 0) fen_x.modify(x, -1); if (my[y] == 0) fen_y.modify(y, -1); } else if (type == 3) { int x1, y1, x2, y2; cin \u0026gt;\u0026gt; x1 \u0026gt;\u0026gt; y1 \u0026gt;\u0026gt; x2 \u0026gt;\u0026gt; y2; if (fen_x.get(x2) - fen_x.get(x1-1) == x2 - x1 + 1) { cout \u0026lt;\u0026lt; \u0026quot;Yes\u0026quot; \u0026lt;\u0026lt;endl; continue ; } if (fen_y.get(y2) - fen_y.get(y1-1) == y2 - y1 + 1) { cout \u0026lt;\u0026lt; \u0026quot;Yes\u0026quot; \u0026lt;\u0026lt;endl; continue ; } cout \u0026lt;\u0026lt; \u0026quot;No\u0026quot; \u0026lt;\u0026lt; endl; } } } ","date":"2022-05-14T00:00:00Z","permalink":"https://zongpitt.com/codeforces/791/c/","section":"codeforces","tags":null,"title":"791C Rooks Defenders"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1675/problem/G\nSolution We consider using dynamic programming to solve this problem since we don’t have any other method related to this problem.\nwe have \\(dp[i][last][sum]\\), This stand for \\(i\\) prefix element, the value of \\(last\\) element in the array, the sum of \\(i\\) prefix. \\(dp[i][last][sum]\\) stand the number of action we need to construct \\(i\\) prefix element with last element is \\(last\\) and sum the array is \\(sum\\). At the begin we set all element to \\(\\infty\\). Let’s consider some case to understand this dynamic programming process. First we set \\(dp[0][0][0]\\) to 0, because it does not need any action. Then we can consider for \\(dp[1]\\), \\(dp[1]\\) means there have 1 element in the array. We can compute \\(dp[1][0][0] = a[1]\\), because we need to remove all element from \\(a[1]\\). We don’t care where it move to, but we need remove all of it. So in this setting we can computer \\(dp[1][j][k]\\) for $ j , k $ by \\(dp[1][j][j] = abs(j - a[j])\\) and \\(dp[1][j][k] = \\infty\\) when \\(j \\neq k\\).\nNow let’s start compute \\(dp[2][j][k]\\). \\(dp[2][j][k]\\) can transfer from \\(dp[1][j-l][k-j]\\) the sum in the \\(dp[1]\\) does not contain \\(j\\) so the last element is \\(k-j\\) and the last element \\(j\\) should greater equal to \\(dp[1][last]\\). So we put \\(j-l\\) for all \\(l \\in [0,m], j-l \\geq 0\\).\nNow the problem is how to update the states.\nSince we don’t care about where it move to the first element or where it move to. But we can suppose there are \\(k-j\\) element already finished. And now \\(i\\) the prefix has \\(sum[i]\\) element. There are \\(k-j\\) element fill in \\(i-1\\) prefix. Now we need to move \\(sum[i] - k +j\\) elements to \\(i\\) th position in the array. We also don’t need to care about where it move from. Because the transfer function will take care about the cost.\nThis algorithm will take \\(O(nm^3)\\) we will getTLE in the test. So we need to optimize this algorithm. This just need to look at the carefully.\nThe transfer function \\[ dp[i][last][sum] + sum[i+1] - (last-l) \\rightarrow dp[i+1][last-l][sum-(last-l)], l \\in[0, last] \\] notice\n\\(dp[i+1][m][sum+m]\\) can only transfer from \\(dp[i][m][sum]\\), because the last element need to grater than \\(m\\) in \\(dp[i]\\) . Let’s consider for \\(dp[i+1][m-1][sum + m - 1]\\).\n\\(dp[i+1][m-1][sum + m - 1]\\) can transfer from \\(dp[i][m][sum]\\) and \\(dp[i][m-1][sum]\\). Now we notice we need minimum value for \\(dp[i][m][sum]\\) and \\(dp[i][m-1][sum]\\). Then we calculate the transfer process in one calculation. The transfer for \\(dp[i+1][m-1][sum + m - 1]\\) is\n\\[ \\begin{align} dp[i][m-1][sum] + sum[i+1] - (m-1) \\rightarrow dp[i+1][m-1][sum+m-1] \\\\ dp[i][m][sum] + sum[i+1] - (m-1) \\rightarrow dp[i+1][m-1][sum+m-1] \\end{align} \\]\nThus we just need to maintain a min value for \\(dp[i+1][m-1][sum+m-1]\\) is good enough to calculate the transfer function by 1 calculation. Now the time complexity become \\(O(nm^2)\\) . This is good enough for pass the judgment.\n#include \u0026lt;bits/stdc++.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int32_t main() { fastio; int n, m; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m; vector\u0026lt;int\u0026gt; a(n + 1); vector\u0026lt;int\u0026gt; sum(n + 1); for (int i = 1; i \u0026lt;= n; i++) { cin \u0026gt;\u0026gt; a[i]; sum[i] = sum[i - 1] + a[i]; } const int INF = 1e8; uint32_t dp[n + 1][m + 1][m + 1]; for (int i = 0; i \u0026lt; n + 1; i++) { for (int j = 0; j \u0026lt; m + 1; j++) for (int k = 0; k \u0026lt; m + 1; k++) { dp[i][j][k] = INF; } } for (int i = 0; i \u0026lt;= m; i++) dp[1][i][i] = abs(a[1] - i); for (int i = 1; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt;= m; j++) { uint32_t mn = INF; for (int k = m; k \u0026gt;= 0; k--) { mn = min(mn, dp[i][k][j]); if (j+k \u0026lt;=m) dp[i + 1][k][j + k] = min(dp[i + 1][k][j + k], mn + abs(sum[i + 1] - (j+k))); } } } uint32_t ans = -1; for (int i = 0; i \u0026lt;= m; i++) { ans = min(ans, dp[n][i][m]); } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; } ","date":"2022-05-07T00:00:00Z","permalink":"https://zongpitt.com/codeforces/787-div3/g/","section":"codeforces","tags":null,"title":"787G Sorting Pancakes"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1670/problem/E\nSolution The numbers we need to assign is \\([1, 2n-1]\\) . there are \\(2n-1\\) numbers, if we take \\(2^p\\) out, we can divide rest of number to 2 group \\([x, x+2^p]\\). where \\(p = log_2(n)\\). Now we select random node as root and assign \\(2^p\\) to the root. If father greater equal than \\(2^p\\) we assign node as \\(x\\), edge as \\(x + 2^p\\). else we will do inversely.\n#include \u0026lt;bits/stdc++.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); #define get(n) int n; cin \u0026gt;\u0026gt; n; int x = 1; vector\u0026lt;int\u0026gt; node, edge; vector\u0026lt;vector\u0026lt;pair\u0026lt;int, int\u0026gt; \u0026gt; \u0026gt; g; int n; void dfs(int cur, int parent) { if (parent == -1) { node[cur] = n; } auto children = g[cur]; for(int i=0;i\u0026lt;children.size();i++){ if (children[i].first == parent) continue ; if (node[cur] \u0026gt;= n) { node[children[i].first] = x; edge[children[i].second] = x^n; } else { node[children[i].first] = x^n; edge[children[i].second] = x; } x ++; dfs(children[i].first, cur); } } int32_t main() { fastio; get(t); while (t--) { get(p); n = 1 \u0026lt;\u0026lt; p; x = 1; g.clear(); node.clear(); edge.clear(); node.resize(n+1); edge.resize(n+1); g.resize(n+1); int edge_cnt = 1; for (int i=0;i\u0026lt;n-1;i++) { get(u) get(v) g[u].push_back({v, edge_cnt}); g[v].push_back({u, edge_cnt++}); } dfs(1, -1); cout \u0026lt;\u0026lt; 1 \u0026lt;\u0026lt; endl; for(int i=1;i\u0026lt;=n;i++) { cout \u0026lt;\u0026lt; node[i] \u0026lt;\u0026lt; \u0026quot; \u0026quot; ; } cout \u0026lt;\u0026lt; endl; for(int i=1;i\u0026lt;n;i++) { cout \u0026lt;\u0026lt; edge[i] \u0026lt;\u0026lt; \u0026quot; \u0026quot; ; } cout \u0026lt;\u0026lt; endl; } } ","date":"2022-05-07T00:00:00Z","permalink":"https://zongpitt.com/codeforces/788-div2/e/","section":"codeforces","tags":null,"title":"788E Hemose on the Tree"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1670/problem/F\nThis is two dimension dynamic problem. There are three constrains in the original problem. \\[ \\begin{align} a_1 + a_2 + \\dots + a_n \\geq l\\\\ a_1 + a_2 + \\dots + a_n \\leq r \\\\ a_1 \\oplus a_2 \\oplus \\dots \\oplus a_n = z \\end{align} \\] Let’s consider only second condition from the beginning.\nWe going start add bits one by one to satisfy the constrain since there is a constrains related to bitwise xor.\n\\(dp[bit][num]\\) stand for combination in \\(bit\\) and \\(num\\) bits total in \\(bit\\)\nwe will have following equation \\[ dp[bit][num] = {n \\choose c} dp[bit+1][(num + c - current\\_bit\\_on)/2] \\]\n","date":"2022-05-07T00:00:00Z","permalink":"https://zongpitt.com/codeforces/788-div2/f/","section":"codeforces","tags":null,"title":"788F  Jee, You See?"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1670/problem/B\nSolution If we do simulate the process. We will get TLE since the test case is large enough. We can find the all character can be delete before and special character. If there an special character after an special character, this special character will be delete. But even this special character is been deleted it will not affect how many times need to delete the character before this character. So we just need to count the max length of non special characters.\n#include \u0026lt;bits/stdc++.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int32_t main() { fastio int t; cin \u0026gt;\u0026gt; t; while (t--) { int n;cin \u0026gt;\u0026gt; n; string s; cin \u0026gt;\u0026gt; s; int k; cin \u0026gt;\u0026gt; k; vector\u0026lt;bool\u0026gt; c(26, false); for(int i=0;i\u0026lt;k;i++) { char tmp; cin \u0026gt;\u0026gt; tmp; c[tmp -\u0026#39;a\u0026#39;] = true; } int ans = 0; int cnt = 0; for(int i=0;i\u0026lt;s.size();i++){ if (c[s[i] - \u0026#39;a\u0026#39;] == 1) { ans = max(ans, cnt); cnt = 1; } else cnt ++; } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; } } ","date":"2022-05-06T00:00:00Z","permalink":"https://zongpitt.com/codeforces/788-div2/b/","section":"codeforces","tags":null,"title":"788B Dorms War"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1670/problem/C\nSolution For some position we don’t have any choice. It can only from array \\(a\\) or array \\(b\\). These position is numbers contains in \\(d\\) and related to \\(d\\).\nWe can build a graph from \\(a[i]\\) to \\(b[i]\\) for all \\(i\\). Then we remove the edge for those number already appeared in \\(d\\). Then we find the number of disconnect component \\(cnt\\). For each disconnect component we have 2 choice. So the result is \\(2^cnt\\).\n#include \u0026lt;bits/stdc++.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int32_t main() { fastio int t; cin \u0026gt;\u0026gt; t; while (t--) { int n; cin \u0026gt;\u0026gt; n; vector\u0026lt;int\u0026gt; a(n); vector\u0026lt;int\u0026gt; b(n); vector\u0026lt;int\u0026gt; d(n + 2, 0); for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; b[i]; set\u0026lt;int\u0026gt; s; for (int i = 0; i \u0026lt; n; i++) { int tmp; cin \u0026gt;\u0026gt; tmp; if (tmp!=0) d[tmp] = 1; } vector\u0026lt;bool\u0026gt; done(n + 1, false); vector\u0026lt;int\u0026gt; g(n + 1, 0); for (int i = 0; i \u0026lt; n; i++) { if (d[a[i]] != 1 \u0026amp;\u0026amp; d[b[i]] != 1 \u0026amp;\u0026amp; a[i] != b[i]) g[a[i]] = b[i]; } int cnt = 0; for (int i = 1; i \u0026lt;= n; i++) { int cur = i; if (done[cur] == true) continue ; while (g[cur] != 0 || done[g[cur]] == false) { cur = g[cur]; done[cur] = true; if (cur == i) { cnt++; break; } } done[g[i]] = true; } const int MOD = int(1e9 + 7); long long ans = 1; for (int i = 0; i \u0026lt; cnt; i++) { ans *= 2; ans %= MOD; } cout \u0026lt;\u0026lt; ans \u0026lt;\u0026lt; endl; } } ","date":"2022-05-06T00:00:00Z","permalink":"https://zongpitt.com/codeforces/788-div2/c/","section":"codeforces","tags":null,"title":"788C Where is the Pizza?"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1670/problem/D\nSolution There are 3 kinds for lines \\, /, —. When we add a new line to the graph it will across with two other different type of line. Each cross will increase 2 equilateral triangles.\n#include \u0026lt;bits/stdc++.h\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int32_t main() { fastio int t; cin \u0026gt;\u0026gt; t; int cnt = 0; vector\u0026lt;int\u0026gt; v; v.push_back(0); for (int i = 0; i \u0026lt; 1e9; i++) { v.push_back(v[v.size()-1] + i * 2 * 2); v.push_back(v[v.size() -1] + i * 4 + 2); v.push_back(v[v.size()-1] + i * 4 + 4); if (v[v.size()-1] \u0026gt;= int(1e9)) break; } while (t--) { int n; cin \u0026gt;\u0026gt; n; auto it = std::upper_bound(v.begin(), v.end(), n-1); cout \u0026lt;\u0026lt; it - v.begin() \u0026lt;\u0026lt; endl; } } ","date":"2022-05-06T00:00:00Z","permalink":"https://zongpitt.com/codeforces/788-div2/d/","section":"codeforces","tags":null,"title":"788D Very Suspicious"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1675/problem/D\nSolution The number of leaves is the number of paths. For each leaf, we have to build a path for it. So the leaf is the last element for each path.\nNow we start to build our path from the leaf. We will find its father and add it to the path. If the parent node is already added by another brother node, we should stop at the current node to avoid having the same node in two paths. So the time complexity is \\(O(n)\\).\nvector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; anss; for (int i = 0; i \u0026lt; leaf.size(); i++) { vector\u0026lt;int\u0026gt; cur_ans; int cur_node = leaf[i]; cur_ans.push_back(cur_node); done[cur_node] = true; while (done[p[cur_node]] == false) { done[p[cur_node]] = true; cur_node = p[cur_node]; cur_ans.push_back(cur_node); } std::reverse(cur_ans.begin(), cur_ans.end()); anss.push_back(cur_ans); } ","date":"2022-05-05T00:00:00Z","permalink":"https://zongpitt.com/codeforces/787-div3/d/","section":"codeforces","tags":null,"title":"787D Vertical Paths"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1675/problem/E\nTo get the minimum string, we need to optimize characters from left to right. The point we should minimize from the largest character which we can let it become a. For example, we can only need 2 steps to minimize string \\(abc\\) instead of 3.\nNow the string can divide into two parts. The first part is we can get all a within \\(k\\) step. In another part, we can only minimize the first character.\nSupport first part is \\(s1\\), the second part is \\(s2\\). the whole string is \\(s1 + s2\\). When we minimize the first part we should replace the character which ranges from \\(a\\) to \\(max(s1)\\) to \\(a\\). Because we perform from the largest element. For the second part, we just optimize for the first element. Suppose we have \\(k2\\) operations left. We need to replace the character range from \\(s2[0] - k2\\) to \\(s2[0]\\) to \\(s2[0] - k2\\).\n//#include \u0026lt;bits/stdc++.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;string\u0026gt; #define endl \u0026quot;\\n\u0026quot; using namespace std; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int32_t main() { fastio int t; cin \u0026gt;\u0026gt; t; while (t--) { int n, k;cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; string s; cin \u0026gt;\u0026gt; s; vector\u0026lt;int\u0026gt; diff(s.size(), 0); vector\u0026lt;int\u0026gt; min_k(s.size(), 0); for(int i=0;i\u0026lt;n;i++) { diff[i] = s[i] - \u0026#39;a\u0026#39;; if (i==0) min_k[i] = diff[i]; else min_k[i] = max(min_k[i-1], diff[i]); } int pos_k = upper_bound(min_k.begin(), min_k.end(), k) - min_k.begin(); if (pos_k == n || min_k[pos_k] \u0026lt;= k) { for(int i=0;i\u0026lt;s.size();i++) cout \u0026lt;\u0026lt; \u0026#39;a\u0026#39;; cout\u0026lt;\u0026lt;endl; continue; } if (pos_k == 0) { int up = s[0]; int down = s[0] - k; for(int i=0;i\u0026lt;n;i++) { if (s[i] \u0026lt;= up \u0026amp;\u0026amp; s[i] \u0026gt;= down) { s[i] = down; } } } else { pos_k --; int up = \u0026#39;a\u0026#39; + min_k[pos_k]; int rest = k - min_k[pos_k]; for(int i=0;i\u0026lt;n;i++){ if (s[i] \u0026lt;= up) { s[i] = \u0026#39;a\u0026#39;; } } up = diff[pos_k+1] + \u0026#39;a\u0026#39;; int down = up - rest; for(int i=0;i\u0026lt;n;i++) { if (s[i] \u0026lt;= up \u0026amp;\u0026amp; s[i] \u0026gt;= down) { s[i] = down; } } } cout \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl; } } ","date":"2022-05-05T00:00:00Z","permalink":"https://zongpitt.com/codeforces/787-div3/e/","section":"codeforces","tags":null,"title":"787E Replace With the Previous, Minimize"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1675/problem/F\nSolution Put \\(x\\) as root. Then we can find that to visit a house, not on the path \\(x\\rightarrow y\\) we need to walk on the same road twice. If we visit a house on the path \\(x \\rightarrow y\\) we only need to walk the road once. After we found this characteristic, the rest of things became easy. We can just use dfs to visit all nodes, if this node is in the path \\(x\\) to \\(y\\), we mark it as 2, else if it is in the path \\(x\\) to some houses, we mark it as 1, Otherwise mark it as 0. Now all nodes marked as 2 will contribute 1 to the answer, and those marked as 1 will contribute 2 to the answer.\n#include \u0026lt;bits/stdc++.h\u0026gt; //#define endl \u0026quot;\\n\u0026quot; using namespace std; int x, y; set\u0026lt;int\u0026gt; s; int ans; #define fastio cin.tie(0), cout.tie(0), ios_base::sync_with_stdio(0); int dfs(int father, int node, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp;g) { int val = 0; if (s.count(node)) val = 1; if (node == y) val = 2; for(int i = 0; i\u0026lt; g[node].size(); i++) { if (g[node][i] == father) continue; val = max(val, dfs(node, g[node][i], g)); } if (val == 1) ans += 2; else if (val == 2) ans += 1; return val; } int32_t main() { fastio int t; cin \u0026gt;\u0026gt; t; while (t--) { ans = 0; int k, n; // cin \u0026gt;\u0026gt; k \u0026gt;\u0026gt; n; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; k; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; x--, y--; s.clear(); vector\u0026lt;int\u0026gt; a(n); for (int i = 0; i \u0026lt; k; i++) cin \u0026gt;\u0026gt; a[i], s.insert(--a[i]); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g(n+1, vector\u0026lt;int\u0026gt;()); for (int i = 0; i \u0026lt; n - 1; i++) { int u, v; cin \u0026gt;\u0026gt; u \u0026gt;\u0026gt; v; u--,v--; g[u].push_back(v); g[v].push_back(u); } dfs(-1, x, g); cout \u0026lt;\u0026lt; ans-1 \u0026lt;\u0026lt; endl; } } ","date":"2022-05-05T00:00:00Z","permalink":"https://zongpitt.com/codeforces/787-div3/f/","section":"codeforces","tags":null,"title":"787F Vlad and Unfinished Business"},{"categories":null,"contents":"Problem link https://codeforces.com/contest/1736/problem/C2\nProblem Desciption This is the hard version of this problem. In this version, we have queries. Note that we do not have multiple test cases in this version. You can make hacks only if both versions of the problem are solved.\nAn array \\(b\\) of length \\(m\\) is good if for all \\(i\\) the \\(i\\)-th element is greater than or equal to \\(i\\). In other words, \\(b\\) is good if and only if \\(b_i \\geq i\\) for all \\(i\\) \\((1 \\leq i \\leq m)\\) You are given an array \\(a\\) consisting of \\(n\\) positive integers, and you are asked \\(q\\) queries. In each query, you are given two integers \\(p\\) and \\(x(1 \\leq p, x \\leq n)\\). You have to do \\(a_p:=x\\) (assign \\(x\\) to \\(\\left.a_p\\right)\\). In the updated array, find the number of pairs of indices \\((l, r)\\), where \\(1 \\leq l \\leq r \\leq n\\), such that the array \\(\\left[a_l, a_{l+1}, \\ldots, a_r\\right]\\) is good. Note that all queries are independent, which means after each query, the initial array \\(a\\) is restored.\nInput The first line contains a single integer \\(n\\left(1 \\leq n \\leq 2 \\cdot 10^5\\right)\\) The second line contains \\(n\\) integers \\(a_1, a_2, \\ldots, a_n\\left(1 \\leq a_i \\leq n\\right)\\) The third line contains an integer \\(q\\left(1 \\leq q \\leq 2 \\cdot 10^5\\right)-\\) the number of queries. Each of the next \\(q\\) lines contains two integers \\(p_j\\) and \\(x_j\\left(1 \\leq p_j, x_j \\leq n\\right)\\)-the description of the \\(j\\)-th query.\nOutput For each query, print the number of suitable pairs of indices after making the change.\nSolution solution for C1 Our index start from 1 instead of 0 in this post.\nWe use dynamic progromming to solve this problem. First we define our state. \\(dp[i]\\) stand for the number of pairs end with \\(i\\). Now we can introduce our state transfer function. It should be \\(dp[i] = min(dp[i-1] + 1, a[i])\\).\nTo understand this equaion we discuss two scenario.\n\\(a[i] \u0026gt; dp[i-1]+1\\). If \\(a[i] \u0026gt; dp[i-1]+1\\) that means all segment work for \\(i-1\\) are works for \\(i\\), amd it the segment \\((i-1,i)\\) also works. That is why we have \\(dp[i-1] + 1\\) \\(a[i] \u0026lt; dp[i-1] + 1\\). Now not all segment for \\(i-1\\) works for \\(i\\). Only \\(a[i]\\) element from left works for \\(i\\). This is because we do have \\(dp[i-1] + 1\\), so there are more element are good segemnt for \\(i\\). But \\(a[i]\\) not large enough to make all them good. The answer for C1 is sum of \\(dp\\) array.\nsolution for C2 Notice the test case are independent.\nLet’s continue the idea from C1. But now we need to precalculate some values.\ncalculate for track[i] \\(track[i]\\)is define as if \\(dp[i] == a[i]\\),\n\\[ track[i] = ∑_{j=i}^{n} dp[i] \\]\nThe track \\(i\\) means, if \\(dp[i] == a[i]\\) the sum of \\(dp\\) array from \\(i\\) to \\(n\\). notice Here \\(dp[i] == a[i]\\) not mean we only calcause the position where \\(dp[i] == a[i]\\). We calculate for \\(i\\) from 1 to \\(n\\) ans supporse \\(dp[i] == a[i]\\).\nFollowing code can calculate \\(track[i]\\) in time \\(O(n^2)\\).\nfor (int i = 1; i \u0026lt;= n; i++) { track[i] = 0; dp[i-1] = a[i]; for (int j=i; j \u0026lt;= n; j++) { dp[j] = min(dp[j-1], a[j]); track[i] += dp[j]; } } But \\(O(n^2)\\) method does not satisfy the constrain. Next we will develop \\(O(n \\log n)\\) method to calculate \\(track[i]\\).\ntrack[i] in \\(O(n \\log n)\\) suppose we are calculate \\(track[i]\\) from \\(n\\) to \\(1\\).\nfor (int i = n; i \u0026gt;= 1; i--) { track[i] = 0; dp[i-1] = a[i]; for (int j=i; j \u0026lt;= n; j++) { dp[j] = min(dp[j-1], a[j]); track[i] += dp[j]; } } We notice that when we calculate \\(dp[i]\\), there is some point \\(q\\), \\(q\u0026gt;p\\), \\(dp[q] = a[q]\\). After this point the array of \\(dp\\) does not change anymore.\nSo when we can use following fomula to calculate \\(track[i]\\)\n\\[ track[i] = dp[i] + dp[i+1] + … + dp[q-1] + track[q]$ \\]\nand notice \\(dp[i] = a[i]\\), \\(dp[i+1] = a[i] + 1\\), and so on.\nSo the equation for \\(track[i]\\) become\n\\[ track[i] = (a[i] + a[i] + (q-1-i)) * (q-i) / 2 + track[q] \\]\nIf we know where is the \\(q\\), we can calculate \\(track[i]\\) in \\(O(n)\\) time complexity.\nNext we are going to explain how to find \\(q\\)\nnotice we have \\(dp[q] = a[q]\\) and \\(a[q] \u0026lt; dp[i] + (q-i)\\)\n\\(a[q] - q \u0026lt; dp[i] - i\\) and notice \\(dp[i] - i\\) is constant for every \\(i\\).\nAnd we can precalculate \\(a[q] - q\\) since it only related to \\(q\\).\nNow we can use semgent tree to find the \\(q\\). We first query the min value in the range \\((i, n)\\). If we find min value less than \\(dp[i] - i\\) we then query \\((i, (n+i)/2)\\). and so on. Just use binary search to find \\(q\\).\nup to now, we get method to calculate \\(track[i]\\) and got the mothod to find \\(q\\).\nNext step is we apply these method again to solve the whole problem.\nSolution for final problem Suppose we change \\(a[p]\\) to \\(x\\). First if current \\(dp[p]= dp[p-1] + 1\\) and \\(x\u0026gt;=dp[p-1]+1\\) then there is nothing change. Other we will divide the array to three part. First part is array from \\(1-\u0026gt;p-1\\), in this part, nothing will be change. We defind a \\(q\\), \\(q\\) is a miminum value which satisfy following conditions, \\(q \u0026gt; p\\) and \\(dp[q] = a[q]\\). Now the second part is \\(p-\u0026gt;q-1\\). in this part, the \\(dp\\) arary is $dp[p], dp[p] + 1, … $. This is easy to calculate in \\(O(1)\\). That last part is \\(q-\u0026gt;n\\). The contribution of this part is \\(track[q]\\). So now the answer can be calculate using following equation.\n\\[ ans = (a[p] + a[p] + (q-1-p)) * (q-p) / 2 + track[q] + presum_dp[p-1] \\]\n","date":"2022-05-05T00:00:00Z","permalink":"https://zongpitt.com/codeforces/825-div2/c2/","section":"codeforces","tags":null,"title":"825 C2. Good Subarrays (Hard Version)"},{"categories":null,"contents":"Build Hugo The default Hugo release does not satisfy my requirement. I want it to support pandoc with TOC (table of content) and extensions. Because pandoc has better math formula support.\ngit clone https://github.com/chen-gz/hugo go build --tag extend We need Hugo to support pandoc because pandoc support math formula much better than default render.\nSecond, we need extensions. So we need to build huge with parameter go build --tag extend . This will make Hugo support extensions.\nDocker I deploy my blog in GitLab. So I need to set up docker for Hugo in order to automatically build the website.\nThe docker file is shown as the following block.\nFROM pandoc/ubuntu RUN apt update \u0026amp;\u0026amp; apt -y install hugo ENTRYPOINT [\u0026quot;\u0026quot;] Content Organize Just put an empty file _index.md in the folders under content folder.\n","date":"2022-05-05T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/hugo-setup/","section":"posts","tags":null,"title":"HUGO setup"},{"categories":["Algorithm"],"contents":"Graph The graph has 3 member variables. edges store all edges. To retrieve an edge the index of the edge is required. So there are other member variables g to store the index of the edges. The content in g[i] are the edges index which start from node i. The member variable n is number of node. The node index in this template is [0, n-1].\nThe edge have three attribution, from, to, cost. The cost is weight of the edge.\ntemplate \u0026lt;typename T\u0026gt; class graph { public: struct edge { int from; int to; T cost; }; vector\u0026lt;edge\u0026gt; edges; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g; int n; graph(int _n) : n(_n) { g.resize(n); } virtual int add(int from, int to, T cost) = 0; }; Directly acyclic graph In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG) is a directed graph with no directed cycles.1\nThe class digraphis inherit from graph. The function add in the parent class is virtual function. So digraph need to overwrite it. Every we call add function, a edge will be push in to the member variable edges. To retrieve the edge, we need the index of current edge. So we push the index of the edge to member variable g.\nThe function reverse will will change the direction for all edge and return an reversed diagraph.\ntemplate \u0026lt;typename T\u0026gt; class digraph : public graph\u0026lt;T\u0026gt; { public: using graph\u0026lt;T\u0026gt;::edges; using graph\u0026lt;T\u0026gt;::g; using graph\u0026lt;T\u0026gt;::n; digraph(int _n) : graph\u0026lt;T\u0026gt;(_n) {} int add(int from, int to, T cost = 1) { assert(0 \u0026lt;= from \u0026amp;\u0026amp; from \u0026lt; n \u0026amp;\u0026amp; 0 \u0026lt;= to \u0026amp;\u0026amp; to \u0026lt; n); int id = (int)edges.size(); g[from].push_back(id); edges.push_back({from, to, cost}); return id; } digraph\u0026lt;T\u0026gt; reverse() const { digraph\u0026lt;T\u0026gt; rev(n); for (auto \u0026amp;e : edges) { rev.add(e.to, e.from, e.cost); } return rev; } }; Topological Sort In computer science, a topological sort or topological ordering of directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. 2\nThe algorithm to find topological sort for DAG using a property of topological.\nProperty: The node with in-degree equal 0 does not rely on other node.\nBase on this property, we found out all node which in-degree is 0. Since these node are not reply on any other node. Next we decrease in-degree for all node which are connected to these node. Since the dependency is satisfied. When all dependency of a node are satisfied we can put it into the vector.\nIf we are able to find a topological sort, all node should be pushed in to vector. Otherwise there are some circular dependency which will not able the satisfied.\ntemplate \u0026lt;typename T\u0026gt; vector\u0026lt;int\u0026gt; find_topsort(const digraph\u0026lt;T\u0026gt; \u0026amp;g) { vector\u0026lt;int\u0026gt; deg(g.n, 0); for (int id = 0; id \u0026lt; (int)g.edges.size(); id++) { deg[g.edges[id].to]++; } vector\u0026lt;int\u0026gt; x; for (int i = 0; i \u0026lt; g.n; i++) { if (deg[i] == 0) { x.push_back(i); } } for (int ptr = 0; ptr \u0026lt; (int)x.size(); ptr++) { int i = x[ptr]; for (int id : g.g[i]) { auto \u0026amp;e = g.edges[id]; int to = e.to; if (--deg[to] == 0) { x.push_back(to); } } } if ((int)x.size() != g.n) { return vector\u0026lt;int\u0026gt;(); } return x; } Probelm set https://codeforces.com/contest/1672/problem/F2 Whole code 3 #include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; template \u0026lt;typename T\u0026gt; class graph { public: struct edge { int from; int to; T cost; }; vector\u0026lt;edge\u0026gt; edges; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g; int n; graph(int _n) : n(_n) { g.resize(n); } virtual int add(int from, int to, T cost) = 0; }; template \u0026lt;typename T\u0026gt; class digraph : public graph\u0026lt;T\u0026gt; { public: using graph\u0026lt;T\u0026gt;::edges; using graph\u0026lt;T\u0026gt;::g; using graph\u0026lt;T\u0026gt;::n; digraph(int _n) : graph\u0026lt;T\u0026gt;(_n) {} int add(int from, int to, T cost = 1) { assert(0 \u0026lt;= from \u0026amp;\u0026amp; from \u0026lt; n \u0026amp;\u0026amp; 0 \u0026lt;= to \u0026amp;\u0026amp; to \u0026lt; n); int id = (int)edges.size(); g[from].push_back(id); edges.push_back({from, to, cost}); return id; } digraph\u0026lt;T\u0026gt; reverse() const { digraph\u0026lt;T\u0026gt; rev(n); for (auto \u0026amp;e : edges) { rev.add(e.to, e.from, e.cost); } return rev; } }; template \u0026lt;typename T\u0026gt; vector\u0026lt;int\u0026gt; find_topsort(const digraph\u0026lt;T\u0026gt; \u0026amp;g) { vector\u0026lt;int\u0026gt; deg(g.n, 0); for (int id = 0; id \u0026lt; (int)g.edges.size(); id++) { deg[g.edges[id].to]++; } vector\u0026lt;int\u0026gt; x; for (int i = 0; i \u0026lt; g.n; i++) { if (deg[i] == 0) { x.push_back(i); } } for (int ptr = 0; ptr \u0026lt; (int)x.size(); ptr++) { int i = x[ptr]; for (int id : g.g[i]) { auto \u0026amp;e = g.edges[id]; int to = e.to; if (--deg[to] == 0) { x.push_back(to); } } } if ((int)x.size() != g.n) { return vector\u0026lt;int\u0026gt;(); } return x; } https://en.wikipedia.org/wiki/Directed_acyclic_graph↩︎\nhttps://en.wikipedia.org/wiki/Topological_sorting↩︎\ncode copy from Codeforce tourist’s submission↩︎\n","date":"2022-05-04T00:00:00Z","permalink":"https://zongpitt.com/algorithm/topology-sort/","section":"algorithm","tags":["Algorithm"],"title":"Topology Sort"},{"categories":["Misc"],"contents":"CMake and MathGL configuration Most of the content is from StackOverflow.\ncmake_minimum_required(VERSION 3.21) project(untitled) set(CMAKE_CXX_STANDARD 17) FIND_PATH(MathGL_INCLUDE_DIRS NAMES mgl2/mgl.h PATHS /opt/local/include /usr/include /usr/local/include ) FIND_LIBRARY(MathGL_LIB NAMES libmgl2.so PATHS /opt/local/lib /usr/local/lib /usr/lib ) FIND_LIBRARY(MathGL_QT_LIB NAMES libmgl2-qt5.so PATHS /opt/local/lib /usr/local/lib /usr/lib ) SET(MathGL_LIBRARIES ${MathGL_LIB} ${MathGL_QT_LIB}) message(${MathGL_LIBRARIES}) IF (MathGL_INCLUDE_DIRS AND MathGL_LIBRARIES) SET(MathGL_FOUND TRUE) MESSAGE(STATUS \u0026quot;MathGL found\u0026quot;) MESSAGE(STATUS \u0026quot;MathGL Include dirs:\u0026quot; ${MathGL_INCLUDE_DIRS}) MESSAGE(STATUS \u0026quot;MathGL Library:\u0026quot; ${MathGL_LIB}) ELSE (MathGL_INCLUDE_DIRS AND MathGL_LIBRARIES) MESSAGE(STATUS \u0026quot;MathGL was not found\u0026quot;) ENDIF(MathGL_INCLUDE_DIRS AND MathGL_LIBRARIES) #add_executable(untitled mathgl.cpp) add_executable(untitled mathgl.cpp) target_link_libraries(untitled ${MathGL_LIB}) target_link_libraries(untitled ${MathGL_QT_LIB}) ","date":"2022-01-04T11:36:00-04:00","permalink":"https://zongpitt.com/posts/2022-01-04-mathgl-cmake/","section":"posts","tags":["Misc"],"title":"CMake and MathGL configuration"},{"categories":["Tool"],"contents":"I am an embedded software developer. USART is a very common communication protocol for communication and debug. I was using Moserial Terminal and Cutecom. But I found several problems with those tools. So I designed this tool to help embedded software developers. This tool still works in progress. The function of this tool is limited for now.\nThe main thing in this tool is reliability. The tool will not crash and get correct data from the serial port. Display these data correctly and completely.\nUsage This tool support following commands. list, open, close, send, loop, loopend, clean, show, hide, set.\nlist: List all available ports in the log window\nopen \u0026lt;port_name\u0026gt;: Open port.\nclose: Close port.\nsend \u0026lt;data\u0026gt;: Send data to port.\nloop \u0026lt;data\u0026gt;: Send data periodic. The period is shown in the status bar.\nloopend: Stop sending data periodic.\nclear: Clear log and receive data windows.\nshow \u0026lt;window\u0026gt;: Show log or hex window. The parameter, window, should be log or hex.\nhide \u0026lt;window\u0026gt;” Inverse operation of show\nset \u0026lt;name\u0026gt; \u0026lt;value\u0026gt;: Change port parameter.\nSet command Supportted settings: baudrate, databit, parity, stopbit, flowcontrol, period.\nbaudrate: Change baudrate. Support any value. databit: Change databit. Support value: 5, 6, 7, 8. parity: Change parity. Support value: 0: None, 1: Odd, 2: Even. stopbit” Change stopbit. Support value: 1, 2. flowcontrol: Change flow control. Support value: hard, soft, none. period: Change loop send periodically. This will work after the port is open. Minimum value: 10ms for Linux, 15 ms for Windows. Because the operating system can not run programming in real-time, the value is not accurate. Feature list open port close port send ASCII to port view received data as hex change port settings send data periodic record output to file send hex to port send hex periodically ","date":"2021-10-08T11:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-10-08-serial_port_tool/","section":"posts","tags":["Tool"],"title":"Serial Communication Tool Description"},{"categories":["Math"],"contents":"manifold: By an \\(n\\)-dimensional manifold we mean a connected Hausdorff space \\(M\\) such that each point has a neighborhood which is homeomorphic to a ball in \\(R^n\\)​. We sometimes express this by saying that a manifold is a connected Hausdorff space which is locally Euclidean. (real analysis)\nHausdorff space: In topology and related branches of mathematics, a Hausdorff space, separated space or T2 space is a topological space where for any two distinct points there exist neighbourhoods of each which are disjoint from each other.(wiki)\nhomeomorphic: Continuous, one-to-one, in surjection, and having a continuous inverse. (Wolfram Mathworld)\n","date":"2021-07-21T11:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-07-21-real-analysis-manifolds/","section":"posts","tags":["Math"],"title":"Manifolds"},{"categories":["Algorithm"],"contents":"Most content of this post is from Introduction to Algorithms. I will explain idea in a simple way but without any prove.\nStrongly connected components The definition of strongly connected components of a directed graph \\(G= (V, E)\\) is amximal set of vertices\\(C\\subseteq V\\) such that for every pair of vertices \\(u\\) and \\(v\\) in \\(C\\), we have both \\(u \\leadsto v\\) and \\(v\\leadsto u\\); that is, verices \\(u\\) and \\(v\\) are reachable from each other.\nThe algorithm list as below.\nSTRONGLY-CONNECTED-COMPONENTS(G)\nCall \\(DFS(G)\\) to compute finishing time \\(u.f\\) for each vertex \\(u\\)\ncompute \\(G^T\\)\ncall \\(DFS(G^T)\\), but in the main loop of \\(DFS\\)consider the vertices in order of decreasing \\(u.f\\) (as computed in line 1)\noutput the vertices of each tree in the depth-first forest formed in line 3 as a separated strongly connected component.\nLemma\nLet \\(C\\) and \\(C\u0026#39;\\) be distinct strongly connected components in directed graph \\(G=(V,E)\\). Suppose that there is an edge \\((u,v) \\in E\\) where \\(u\\in C\\) and \\(v \\in C\u0026#39;\\). Then \\(f(C) \u0026gt; f(C\u0026#39;)\\).\nCorollary\nLet \\(C\\) and \\(C\u0026#39;\\) be distinct strongly connected components in directed graph \\(G=(V,E)\\). Suppose that there is an edge \\((u,v) \\in E^T\\) where \\(u\\in C\\) and \\(v \\in C\u0026#39;\\). Then \\(f(C) \u0026lt; f(C\u0026#39;)\\).\ngraph LR C1 --\u0026gt;|u.C1.f| C2 --\u0026gt;|u.C2.f| C3 when we consider backwards\ngraph RL C3 --\u0026gt;|u.C2.s| C2 --\u0026gt;|u.C1.s| C1 For example, we focus at get component \\(C2\\). This may not the only node it can go \\(C3\\), but it will no have other node can go back to \\(C2\\).\nTheorem\nThe STRONGLY-CONNECTED-COMPONENTS(G) procedure correctly computes the strongly connected components of directed graph \\(G\\) provided as its input.\n","date":"2021-07-20T11:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-07-20-dfs-scc/","section":"posts","tags":["Algorithm"],"title":"Strongly connected components"},{"categories":["ML"],"contents":"Nvidia post style GAN. This GAN structure can generate very high resolution image. this person does not exist shows a very good result to apply in human face generate.\nThere are two paper relate to style GAN. A style-Based Generator Architecture for Generative Adversarial Networks proposed the idea. Analyzing and Improving the Image Quality of StyleGAN improve the styleGAN.\nThe most important idea in style GAN is control different feature in different stage instead of put all of them in the first stage.\nDataset human face dataset, Flickr-Faces-HQ (FFHQ) is used in the paper.\nArchitecture Traditionally the latent code is provided to the generator through an input layer, i.e., the first layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure \\(1 \\mathrm{~b}\\), right).\nmapping network Mapping network \\(\\mathbf{f}\\) is compose by 8-layer MLP, it will map latent code \\(\\mathbf{z}\\) to in intermediate latent space \\(\\mathbf{w}\\).\nStyle control The style information is come from intermediate latent space. Learned affine transformations then specialize \\(\\mathbf{w}\\) to styles \\(\\mathbf{y}=\\left(\\mathbf{y}_{s}, \\mathbf{y}_{b}\\right)\\) that control adaptive instance normalization (AdaIN) [27, 17, 21, 16] operations after each convolution layer of the synthesis network \\(g\\). The AdaIN operation is \\[ \\operatorname{AdaIN}\\left(\\mathbf{x}_{i}, \\mathbf{y}\\right)=\\mathbf{y}_{s, i} \\frac{\\mathbf{x}_{i}-\\mu\\left(\\mathbf{x}_{i}\\right)}{\\sigma\\left(\\mathbf{x}_{i}\\right)}+\\mathbf{y}_{b, i} \\] where each feature map \\(\\mathbf{x}_{i}\\) is normalized separately, and then scaled and biased using the corresponding scalar components from style \\(\\mathrm{y}\\). Thus the dimensionality of \\(\\mathbf{y}\\) is twice the number of feature maps on that layer.\nStochastic details Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned per-feature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure \\(1 \\mathrm{~b}\\). The implications of adding the noise inputs are discussed in Sections \\(3.2\\) and \\(3.3\\).\nprogressive growing progressive is proposed in Progressive Growing of GANs for Improved Quality, Stability, and Variation\nThe following picture shows how progressive growing works.\n","date":"2021-07-19T10:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-07-19-style-gan/","section":"posts","tags":["GAN"],"title":"Style GAN"},{"categories":["ML"],"contents":"There are two paper to introduce Wasserstein GAN. Towards Principled Methods for Training Generative Adversarial Networks and Wasserstein GAN.\nFirst one present the issue in the GAN. It prove several theorem to get understanding why GAN is unstable and why it is hard to train. There are a lot resources in the internet. This post will take ever theorem and give intuitive. Intuition means, it is not rigorous prove like the paper did.\nThe general idea is cross entropy cost function will not give enough gradient to train the network.\nTheorem 2.1 (smooth optimal separate function for two distribution) If two distributions \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) have support contained on two disjoint compact subset \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) respectively, then there is a smooth optimal decimator \\(D^{*}: \\mathcal{X} \\rightarrow[0,1]\\) that has accuracy 1 and \\(\\nabla_{x} D^{*}(x)=0\\) for all \\(x \\in \\mathcal{M} \\cup \\mathcal{P} .\\)\nThis theorem is very obvious. If there are two disjoin distributions, then there is some interval between these two distributions. We can map this distribution to another space, to make the interval larger. In mapped space, we can easily find a smooth optimal discriminator. Then we can map this optimal discriminator back. The prove in the paper, is divide interval between two distributions tow 3 parts. Then can find a smooth function in the middle part. That means there is a smooth optimal discriminator to separate these distributions.\nDefinition 2.1 (transversally) We first need to recall the definition of transversally. Let \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) be two boundary free regular submanifolds of \\(\\mathcal{F}\\), which in our cases will simply be \\(\\mathcal{F}=\\mathbb{R}^{d}\\). Let \\(x \\in \\mathcal{M} \\cap \\mathcal{P}\\) be an intersection point of the two manifolds. We say that \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) intersect transversally in \\(x\\) if \\(T_{x} \\mathcal{M}+T_{x} \\mathcal{P}=T_{x} \\mathcal{F}\\), where \\(T_{x} \\mathcal{M}\\) means the tangent space of \\(\\mathcal{M}\\) around \\(x\\).\nThis is the definition of transversally. This is onsite of tangent space. This almost means they have different tangent space at certain point.\nDefinition 2.2 (perfectly align) We say that two manifolds without boundary \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) perfectly align if there is an \\(x \\in \\mathcal{M} \\cap \\mathcal{P}\\) such that \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) don’t intersect transversally in \\(x\\). We shall note the boundary and interior of a manifold \\(\\mathcal{M}\\) by \\(\\partial M\\) and Int \\(M\\) respectively. We say that two manifolds \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) (with or without boundary) perfectly align if any of the boundary free manifold pairs \\((\\operatorname{Int} \\mathcal{M}, \\operatorname{Int} \\mathcal{P}),(\\operatorname{Int} \\mathcal{M}, \\partial \\mathcal{P}),(\\partial \\mathcal{M}\\), Int \\(\\mathcal{P})\\) or \\((\\partial \\mathcal{M}, \\partial \\mathcal{P})\\) perfectly align.\nWe can simply regard \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) are same.\nLemma 2 (not perfectly align) Lemma 2. Let \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) be two regular submanifolds of \\(\\mathbb{R}^{d}\\) that don’t have full dimension. Let \\(\\eta, \\eta^{\\prime}\\) be arbitrary independent continuous random variables. We therefore define the perturbed manifolds as \\(\\tilde{\\mathcal{M}}=\\mathcal{M}+\\eta\\) and \\(\\tilde{\\mathcal{P}}=\\mathcal{P}+\\eta^{\\prime} .\\) Then\n\\(\\mathbb{P}_{\\eta, \\eta^{\\prime}}(\\tilde{\\mathcal{M}}\\) does not perfectly align with \\(\\tilde{\\mathcal{P}})=1\\)\nLemma 2 shows that as long as there exist noise, two submanifold never perfect align.\nThis will be use later to prove cross entropy function does not work well for GAN.\nLemma 3 (intersection is lower dimension and measure is 0) Lemma 3. Let \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) be two regular submanifolds of \\(\\mathbb{R}^{d}\\) that don’t perfectly align and don’t have full dimension. Let \\(\\mathcal{L}=\\mathcal{M} \\cap \\mathcal{P}\\). If \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) don’t have boundary, then \\(\\mathcal{L}\\) is also a manifold, and has strictly lower dimension than both the one of \\(\\mathcal{M}\\) and the one of \\(\\mathcal{P}\\). If they have boundary, \\(\\mathcal{L}\\) is a union of at most 4 strictly lower dimensional manifolds. In both cases, \\(\\mathcal{L}\\) has measure 0 in both \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\).\nThis lemma means \\(M\\) and \\(P\\) almost disjoin. Even they have intersection.\nTheorem 2.2 Theorem 2.2. Let \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) be two distributions that have support contained in two closed manifolds \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) that don’t perfectly align and don’t have full dimension. We further assume that \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) are continuous in their respective manifolds, meaning that if there is a set \\(A\\) with measure 0 in \\(\\mathcal{M}\\), then \\(\\mathbb{P}_{r}(A)=0\\) (and analogously for \\(\\mathbb{P}_{g}\\) ). Then, there exists an optimal discriminator \\(D^{*}: \\mathcal{X} \\rightarrow[0,1]\\) that has accuracy 1 and for almost any \\(x\\) in \\(\\mathcal{M}\\) or \\(\\mathcal{P}, D^{*}\\) is smooth in a neighborhood of \\(x\\) and \\(\\nabla_{x} D^{*}(x)=0 .\\)\nThis theorem shows that if two distribution is in two manifolds. These manifolds does not perfect align, then optimal discriminator exist and accuracy is 1.\nTheorem 2.3 Theorem 2.3. Let \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) be two distributions whose support lies in two manifolds \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) that don’t have full dimension and don’t perfectly align. We further assume that \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) are continuous in their respective manifolds. Then, \\[ \\begin{aligned} J S D\\left(\\mathbb{P}_{r} \\| \\mathbb{P}_{g}\\right) \u0026amp;=\\log 2 \\\\ K L\\left(\\mathbb{P}_{r} \\| \\mathbb{P}_{g}\\right) \u0026amp;=+\\infty \\\\ K L\\left(\\mathbb{P}_{g} \\| \\mathbb{P}_{r}\\right) \u0026amp;=+\\infty \\end{aligned} \\]\nThis Theorem tell us JSD and KL divergence will not give correct gradient for generator when we have a good discriminator, because they value does not depend on two distribution.\nTheorem 2.4 Vanishing gradients on the generator Theorem \\(2.4\\) (Vanishing gradients on the generator). Let \\(g_{\\theta}: \\mathcal{Z} \\rightarrow \\mathcal{X}\\) be a differentiable function that induces a distribution \\(\\mathbb{P}_{g}\\). Let \\(\\mathbb{P}_{r}\\) be the real data distribution. Let \\(D\\) be a differentiable discriminator. If the conditions of Theorems \\(2.1\\) or \\(2.2\\) are satisfied, \\(\\left\\|D-D^{*}\\right\\|\u0026lt;\\epsilon\\), and \\(\\mathbb{E}_{z \\sim p(z)}\\left[\\left\\|J_{\\theta} g_{\\theta}(z)\\right\\|_{2}^{2}\\right] \\leq M^{2}, 2\\) then \\[ \\left\\|\\nabla_{\\theta} \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D\\left(g_{\\theta}(z)\\right)\\right)\\right]\\right\\|_{2}\u0026lt;M \\frac{\\epsilon}{1-\\epsilon} \\]\nThis theorem give us upbound of generator gradient base on optimal discriminator. This gradient is really small and it make us unable to train GAN.\nCorollary 2.1 Corollary 2.1. Under the same assumptions of Theorem \\(2.4\\) \\[ \\lim _{\\left\\|D-D^{*}\\right\\| \\rightarrow 0} \\nabla_{\\theta} \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D\\left(g_{\\theta}(z)\\right)\\right)\\right]=0 \\]\nThis corollary shows that gradient almost 0 when we have optimal discriminator.\nThe \\(-logD\\) Alternative The original GAN paper post use \\(-logD(g_\\theta(z))\\) as cost function to train the GAN at early stage. But this step will cost GAN instability. Following theorem give the prove for this statement.\nTheorem 2.5 Let \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g_{\\theta}}\\) be two continuous distributions, with densities \\(P_{r}\\) and \\(P_{g_{\\theta}}\\) respectively. Let \\(D^{*}=\\frac{P_{r}}{P_{g_{\\theta}}+P_{r}}\\) be the optimal discriminator, fixed for a value \\(\\theta_{0}\\). Therefore, \\[ \\mathbb{E}_{z \\sim p(z)}\\left[-\\left.\\nabla_{\\theta} \\log D^{*}\\left(g_{\\theta}(z)\\right)\\right|_{\\theta=\\theta_{0}}\\right]=\\left.\\nabla_{\\theta}\\left[K L\\left(\\mathbb{P}_{g_{\\theta}} \\| \\mathbb{P}_{r}\\right)-2 J S D\\left(\\mathbb{P}_{g_{\\theta}} \\| \\mathbb{P}_{r}\\right)\\right]\\right|_{\\theta=\\theta_{0}} \\] This theorem shows that JSD seems like a fault update when using \\(-log D\\) alternative. Because for generator we wan \\(\\mathbb{P}_\\theta\\) close to \\(\\mathbb{P}_r\\). But \\(JSD\\) in opposite sign, it will push them away from each other.\nTheorem 2.6 (Instability of generator gradient updates) Let \\(g_{\\theta}: \\mathcal{Z} \\rightarrow \\mathcal{X}\\) be a differentiable function that induces a distribution \\(\\mathbb{P}_{g}\\). Let \\(\\mathbb{P}_{r}\\) be the real data distribution, with either conditions of Theorems 2.1 or \\(2.2\\) satisfied. Let \\(D\\) be a discriminator such that \\(D^{*}-D=\\epsilon\\) is a centered Gaussian process indexed by \\(x\\) and independent for every \\(x\\) (popularly known as white noise) and \\(\\nabla_{x} D^{*}-\\nabla_{x} D=r\\) another independent centered Gaussian process indexed by \\(x\\) and independent for every \\(x .\\) Then, each coordinate of \\[ \\mathbb{E}_{z \\sim p(z)}\\left[-\\nabla_{\\theta} \\log D\\left(g_{\\theta}(z)\\right)\\right] \\] is a centered Cauchy distribution with infinite expectation and variance.\nBecause Cauchy distribution is unstable. The gradient follows the Cauchy distribution, so the training state will be unstable.\nTowards Softer Metrics and Distributions The theorem in this section is aim to fix the instability and vanishing gradient issue.\nTheorem 3.1 If \\(X\\) has distribution \\(\\mathbb{P}_{X}\\) with support on \\(\\mathcal{M}\\) and \\(\\epsilon\\) is an absolutely continuous random variable with density \\(P_{\\epsilon}\\), then \\(\\mathbb{P}_{X+\\epsilon}\\) is absolutely continuous with density \\[ \\begin{aligned} P_{X+\\epsilon}(x) \u0026amp;=\\mathbb{E}_{y \\sim \\mathbb{P}_{X}}\\left[P_{\\epsilon}(x-y)\\right] \\\\ \u0026amp;=\\int_{\\mathcal{M}} P_{\\epsilon}(x-y) \\mathrm{d} \\mathbb{P}_{X}(y) \\end{aligned} \\]\nCorollary 3.1 If \\(\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2} I\\right)\\) then \\[ P_{X+c}(x)=\\frac{1}{Z} \\int_{\\mathcal{M}} e^{-\\frac{\\|y-x\\|^{2}}{2 \\sigma^{2}}} \\mathrm{~d} \\mathbb{P}_{X}(y) \\] If \\(c \\sim \\mathcal{N}(0, \\Sigma)\\) then \\[ P_{X+\\epsilon}(x)=\\frac{1}{Z} \\mathbb{E}_{y \\sim \\mathbf{P}_{X}}\\left[e^{-\\frac{1}{2}\\|y-x\\|_{\\Sigma-1}^{2}}\\right] \\] If \\(P_{\\epsilon}(x) \\propto \\frac{1}{\\|x\\|^{d+1}}\\) then \\[ P_{X+\\epsilon}(x)=\\frac{1}{Z} \\mathbb{E}_{y \\sim \\mathbb{P}_{X}}\\left[\\frac{1}{\\|x-y\\|^{d+1}}\\right] \\] Theorem 3.2 Theorem 3.2. Let \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) be two distributions with support on \\(\\mathcal{M}\\) and \\(\\mathcal{P}\\) respectively, with \\(\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2} I\\right) .\\) Then, the gradient passed to the generator has the form \\[ \\begin{aligned} \\mathbb{E}_{z \\sim p(z)} \u0026amp;\\left[\\nabla_{\\theta} \\log \\left(1-D^{*}\\left(g_{0}(z)\\right)\\right)\\right] \\\\ \u0026amp;=\\mathbb{E}_{z \\sim p(z)}\\left[a(z) \\int_{\\mathcal{M}} P_{\\epsilon}\\left(g_{\\theta}(z)-y\\right) \\nabla_{\\theta}\\left\\|g_{\\theta}(z)-y\\right\\|^{2} \\mathrm{~d} \\mathbb{P}_{r}(y)\\right.\\\\ \u0026amp;\\left.-b(z) \\int_{\\mathcal{P}} P_{\\epsilon}\\left(g_{\\theta}(z)-y\\right) \\nabla_{\\theta}\\left\\|g_{\\theta}(z)-y\\right\\|^{2} \\mathrm{~d} \\mathbb{P}_{g}(y)\\right] \\end{aligned} \\] where \\(a(z)\\) and \\(b(z)\\) are positive functions. Furthermore, \\(b\u0026gt;a\\) if and only if \\(P_{r+\\epsilon}\u0026gt;P_{g+\\epsilon}\\), and \\(b\u0026lt;a\\) if and only if \\(P_{r+\\epsilon}\u0026lt;P_{g+\\epsilon}\\).\nCorollary 3.2 Let \\(c, \\epsilon^{\\prime} \\sim \\mathcal{N}\\left(0, \\sigma^{2} I\\right)\\) and \\(\\tilde{g}_{\\theta}(z)=g_{\\theta}(z)+c^{\\prime}\\), then \\[ \\begin{aligned} \\mathbb{E}_{z \\sim p(z), \\epsilon^{\\prime}} \u0026amp;\\left[\\nabla_{\\theta} \\log \\left(1-D^{*}\\left(\\tilde{g}_{0}(z)\\right)\\right)\\right] \\\\ \u0026amp;=\\mathbb{E}_{z \\sim p(z), \\epsilon^{\\prime}}\\left[a(z) \\int_{\\mathcal{M}} P_{\\epsilon}\\left(\\tilde{g}_{0}(z)-y\\right) \\nabla_{\\theta}\\left\\|\\tilde{g}_{0}(z)-y\\right\\|^{2} \\mathrm{~d} \\mathbb{P}_{r}(y)\\right.\\\\ \u0026amp;\\left.-b(z) \\int_{\\mathcal{P}} P_{\\epsilon}\\left(\\tilde{g}_{\\theta}(z)-y\\right) \\nabla_{\\theta}\\left\\|\\tilde{g}_{\\theta}(z)-y\\right\\|^{2} \\mathrm{~d} \\mathbb{P}_{g}(y)\\right] \\\\ \u0026amp;=2 \\nabla_{\\theta} J S D\\left(\\mathbb{P}_{r+\\epsilon} \\| \\mathbb{P}_{g+\\epsilon}\\right) \\end{aligned} \\]\nDefinition 3.1 Definition 3.1. We recall the definition of the Wasserstein metric \\(W(P, Q)\\) for \\(P\\) and \\(Q\\) two distributions over \\(\\mathcal{X}\\). Namely, \\[ W(P, Q)=\\inf _{\\gamma \\in \\Gamma} \\int_{\\mathcal{X} \\times \\mathcal{X}}\\|x-y\\|_{2} d \\gamma(x, y) \\] where \\(\\Gamma\\) is the set of all possible joints on \\(\\mathcal{X} \\times \\mathcal{X}\\) that have marginals \\(P\\) and \\(Q\\).\nLemma 4. If \\(\\epsilon\\) is a random vector with mean 0, then we have \\[ W\\left(\\mathbb{P}_{X}, \\mathbb{P}_{X+\\epsilon}\\right) \\leq V^{\\frac{1}{2}} \\] where \\(V=\\mathbb{E}\\left[\\|c\\|_{2}^{2}\\right]\\) is the variance of \\(\\epsilon\\)\nTheorem 3.3. Let \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) be any two distributions, and \\(\\epsilon\\) be a random vector with mean 0 and variance \\(V\\). If \\(\\mathbb{P}_{r+c}\\) and \\(\\mathbb{P}_{g+c}\\) have support contained on a ball of diameter \\(C\\), then \\[ W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right) \\leq 2 V^{\\frac{1}{2}}+2 C \\sqrt{J S D\\left(\\mathbb{P}_{r+\\epsilon} \\| \\mathbb{P}_{g+\\epsilon}\\right)} \\]\n","date":"2021-07-17T10:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-07-17-wgan/","section":"posts","tags":["GAN"],"title":"Wasserstein GAN 1"},{"categories":["ML"],"contents":"There are two paper to introduce Wasserstein GAN. Towards Principled Methods for Training Generative Adversarial Networks and Wasserstein GAN.\nDifferent Distances The Total Variation (TV) distance \\[ \\delta\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)=\\sup _{A \\in \\Sigma}\\left|\\mathbb{P}_{r}(A)-\\mathbb{P}_{g}(A)\\right| \\] The Kullback-Leibler (KL) divergence \\[ K L\\left(\\mathbb{P}_{r} \\| \\mathbb{P}_{g}\\right)=\\int \\log \\left(\\frac{P_{r}(x)}{P_{g}(x)}\\right) P_{r}(x) d \\mu(x) \\]\nwhere both \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\) are assumed to be absolutely continuous, and therefore admit densities, with respect to a same measure \\(\\mu\\) defined on \\(\\mathcal{X} \\stackrel{2}{ }^{2}\\) The KL divergence is famously assymetric and possibly infinite when there are points such that \\(P_{g}(x)=0\\) and \\(P_{r}(x)\u0026gt;0\\)\nThe Jensen-Shannon (JS) divergence \\[ J S\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)=K L\\left(\\mathbb{P}_{r} \\| \\mathbb{P}_{m}\\right)+K L\\left(\\mathbb{P}_{g} \\| \\mathbb{P}_{m}\\right) \\] where \\(\\mathbb{P}_{m}\\) is the mixture \\(\\left(\\mathbb{P}_{r}+\\mathbb{P}_{g}\\right) / 2 .\\) This divergence is symmetrical and always defined because we can choose \\(\\mu=\\mathbb{P}_{m}\\).\nThe Earth-Mover (EM) distance or Wasserstein-1 \\[ W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)=\\inf _{\\gamma \\in \\Pi\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)} \\mathbb{E}_{(x, y) \\sim \\gamma}[\\|x-y\\|] \\]\nwhere \\(\\Pi\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)\\) denotes the set of all joint distributions \\(\\gamma(x, y)\\) whose marginals are respectively \\(\\mathbb{P}_{r}\\) and \\(\\mathbb{P}_{g}\\). Intuitively, \\(\\gamma(x, y)\\) indicates how much “mass” must be transported from \\(x\\) to \\(y\\) in order to transform the distributions \\(\\mathbb{P}_{r}\\) into the distribution \\(\\mathbb{P}_{g} .\\) The EM distance then is the “cost” of the optimal transport plan.\nThe GAN paper give an example for these distance. I just provide the result. When \\(P_r\\) and \\(P_g\\) does not perfect align. The distance show as follow.\n\\[J S\\left(\\mathbb{P}_{0}, \\mathbb{P}_{\\theta}\\right)= \\begin{cases}\\log 2 \u0026amp; \\text { if } \\theta \\neq 0 \\\\ 0 \u0026amp; \\text { if } \\theta=0\\end{cases} \\]\n\\[ W\\left(\\mathbb{P}_{0}, \\mathbb{P}_{\\theta}\\right)=|\\theta|\\]\n\\[K L\\left(\\mathbb{P}_{\\theta} \\| \\mathbb{P}_{0}\\right)=K L\\left(\\mathbb{P}_{0} \\| \\mathbb{P}_{\\theta}\\right)= \\begin{cases}+\\infty \u0026amp; \\text { if } \\theta \\neq 0 \\\\ 0 \u0026amp; \\text { if } \\theta=0\\end{cases}\\]\n\\[ \\text { and } \\delta\\left(\\mathbb{P}_{0}, \\mathbb{P}_{\\theta}\\right)= \\begin{cases}1 \u0026amp; \\text { if } \\theta \\neq 0 \\\\ 0 \u0026amp; \\text { if } \\theta=0\\end{cases} \\]\nAssumption 1 Let \\(g: \\mathcal{Z} \\times \\mathbb{R}^{d} \\rightarrow \\mathcal{X}\\) be locally Lipschitz between finite dimensional vector spaces. We will denote \\(g_{\\theta}(z)\\) it’s evaluation on coordinates \\((z, \\theta) .\\) We say that \\(g\\) satisfies assumption 1 for a certain probability distribution \\(p\\) over \\(\\mathcal{Z}\\) if there are local Lipschitz constants \\(L(\\theta, z)\\) such that \\[ \\mathbb{E}_{z \\sim p}[L(\\theta, z)]\u0026lt;+\\infty \\]\nTheorem 1. Let \\(\\mathbb{P}_{r}\\) be a fixed distribution over \\(\\mathcal{X} .\\) Let \\(Z\\) be a random variable (e.g Gaussian) over another space \\(\\mathcal{Z} .\\) Let \\(g: \\mathcal{Z} \\times \\mathbb{R}^{d} \\rightarrow \\mathcal{X}\\) be a function, that will be denoted \\(g_{\\theta}(z)\\) with \\(z\\) the first coordinate and \\(\\theta\\) the second. Let \\(\\mathbb{P}_{\\theta}\\) denote the distribution of \\(g_{\\theta}(Z) .\\) Then, 1. If \\(g\\) is continuous in \\(\\theta\\), so is \\(W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)\\). 2. If \\(g\\) is locally Lipschitz and satisfies regularity assumption 1, then \\(W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)\\) is continuous everywhere, and differentiable almost everywhere. 3. Statements 1-2 are false for the Jensen-Shannon divergence \\(J S\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)\\) and all the KLs.\nThis theorem shows Earth-Mover distance is continue but \\(JS\\), \\(KL\\) are not.\nCorollary 1. Let \\(g_{\\theta}\\) be any feedforward neural network \\(p(z)\\) a prior over \\(z\\) such that \\(\\mathbb{E}_{z \\sim p(z)}[\\|z\\|]\u0026lt;\\infty\\) (e.g. Gaussian, uniform, etc.). Then assumption 1 is satisfied and therefore \\(W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)\\) is continuous everywhere and differentiable almost everywhere.\nTheorem 2. Theorem 2. Let \\(\\mathbb{P}\\) be a distribution on a compact space \\(\\mathcal{X}\\) and \\(\\left(\\mathbb{P}_{n}\\right)_{n \\in \\mathbb{N}}\\) be a sequence of distributions on \\(\\mathcal{X} .\\) Then, considering all limits as \\(n \\rightarrow \\infty\\), 1. The following statements are equivalent - \\(\\delta\\left(\\mathbb{P}_{n}, \\mathbb{P}\\right) \\rightarrow 0\\) with \\(\\delta\\) the total variation distance. - \\(J S\\left(\\mathbb{P}_{n}, \\mathbb{P}\\right) \\rightarrow 0\\) with JS the Jensen-Shannon divergence. 2. The following statements are equivalent - \\(W\\left(\\mathbb{P}_{n}, \\mathbb{P}\\right) \\rightarrow 0\\) - \\(\\mathbb{P}_{n} \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathbb{P}\\) where \\(\\stackrel{\\mathcal{D}}{\\rightarrow}\\) represents convergence in distribution for random variables. 3. \\(K L\\left(\\mathbb{P}_{n} \\| \\mathbb{P}\\right) \\rightarrow 0\\) or \\(K L\\left(\\mathbb{P} \\| \\mathbb{P}_{n}\\right) \\rightarrow 0\\) imply the statements in (1). 4. The statements in (1) imply the statements in (2).\nWasserstein GAN Because use definition of Earth mover distance is highly intractable.\nKantorovich-Rubinstein duality tell us that \\[ W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)=\\sup _{\\|f\\|_{L} \\leq 1} \\mathbb{E}_{x \\sim \\mathbb{P}_{r}}[f(x)]-\\mathbb{E}_{x \\sim \\mathbb{P}_{\\theta}}[f(x)] \\] where the supremum is over all the 1-Lipschitz functions \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\). Note that if we replace \\(\\|f\\|_{L} \\leq 1\\) for \\(\\|f\\|_{L} \\leq K\\) (consider \\(K\\) -Lipschitz for some constant \\(\\left.K\\right)\\), then we end up with \\(K \\cdot W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{g}\\right)\\).\nIn original paper proposed using clip to ensure the 1-Lipschitz condition. The issue cause by this setting is describe in the paper. Another paper propose using gradient penalty to ensure 1-Lipschitz. It will give better result than weight clip. Adam will not work in weigh clip setting. RMSprop is used in this algorithm. WGAN-GP solve this issue.\nTheorem 3 Theorem 3. Let \\(\\mathbb{P}_{r}\\) be any distribution. Let \\(\\mathbb{P}_{\\theta}\\) be the distribution of \\(g_{\\theta}(Z)\\) with \\(Z\\) a random variable with density \\(p\\) and \\(g_{\\theta}\\) a function satisfying assumption 1. Then, there is a solution \\(f: \\mathcal{X} \\rightarrow \\mathbb{R}\\) to the problem \\[ \\max _{\\|f\\|_{L} \\leq 1} \\mathbb{E}_{x \\sim \\mathbb{P}_{r}}[f(x)]-\\mathbb{E}_{x \\sim \\mathbb{P}_{\\theta}}[f(x)] \\] and we have \\[ \\nabla_{\\theta} W\\left(\\mathbb{P}_{r}, \\mathbb{P}_{\\theta}\\right)=-\\mathbb{E}_{z \\sim p(z)}\\left[\\nabla_{\\theta} f\\left(g_{\\theta}(z)\\right)\\right] \\] when both terms are well-defined.\nAlgorithm ","date":"2021-07-17T10:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-07-18-wgan2/","section":"posts","tags":["GAN"],"title":"Wasserstein GAN 2"},{"categories":["Math"],"contents":"The singular value decomposition for a matrix A writes A as a product (hanger)(stretcher)(aligner).\nIt’s an amazing and useful fact that every m x n matrix has a singular value decomposition.\nThe following theorem goes two-thirds of the way to proving this fact:\nTwo-thirds Theorem For an \\(m \\times n\\) matrix \\(\\mathrm{A}: \\mathbb{R}^{\\mathrm{n}} \\rightarrow \\mathbb{R}^{\\mathbb{m}}\\) and any orthonormal basis \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\ldots, \\vec{a}_{n}\\right\\}\\) of \\(\\mathbb{R}^{n}\\), define \\(s_{i}=\\left\\|\\hat{a} \\vec{a}_{i}\\right\\|\\) and \\(\\vec{h}_{i}=\\left\\{\\begin{array}{cc}\\overrightarrow{0} \u0026amp; \\text { if } s_{i}=0 \\\\ \\frac{1}{s_{i}} A \\vec{a}_{i} \u0026amp; \\text { if } s_{i} \\neq 0\\end{array}\\right.\\). Then \\(A=\\left(\\begin{array}{llll}\\vec{h}_{1} \u0026amp; \\mid \\vec{h}_{2} \u0026amp; \\ldots \u0026amp; \\mid \\vec{h}_{n}\\end{array}\\right)\\left(\\begin{array}{cccc}s_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_{2} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; s_{n}\\end{array}\\right)\\left(\\begin{array}{c}\\vec{a}_{1} \\\\ \\frac{\\vec{a}_{2}}{\\vdots} \\\\ \\overrightarrow{a_{n}}\\end{array}\\right)\\).\nProof: Using first the row way and then the column way to multiply a matrix times a point, you see that the right hand side of the equation sends \\(\\vec{a}_{i}\\) to \\(s_{i} \\vec{h}_{i}=s_{i} \\frac{1}{s_{i}} A \\vec{a}_{i}=A \\vec{a}_{i}\\) Thus the two sides of the equation agree on the basis \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\ldots, \\vec{a}_{n}\\right\\}\\) and so must be equal.\nThe two-thirds theorem gets you two-thirds of the way to the SVD. It says that given any orthonormal basis \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\ldots, \\vec{a}_{n}\\right\\}\\) of \\(\\mathbb{R}^{n}\\) you can write \\[ \\begin{aligned} A=\u0026amp;\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\ldots \\mid \\vec{h}_{n}\\right)\\left(\\begin{array}{cccc} s_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_{2} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; s_{n} \\end{array}\\right)\\left(\\frac{\\vec{a}_{1}}{\\vec{a}_{2}}\\right) \\\\ \u0026amp;=\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\ldots \\mid \\vec{h}_{n}\\right) \\text { (stretcher) (aligner). } \\end{aligned} \\] So you’ve got the stretcher and the aligner - - if \\(\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\ldots \\mid \\vec{h}_{n}\\right)\\) were a hanger matrix then this would be a Singular Value Decomposition for \\(A\\). For \\(\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\ldots \\mid \\vec{h}_{n}\\right)\\) to be a hanger matrix requires that the columns \\(\\vec{h}_{i}=\\frac{1}{s_{i}} A \\vec{a}_{i}\\) be pairwise perpendicular.\nSo one challenge to finding an SVD for \\(A\\) is to find an orthonormal basis of \\(\\mathbb{R}^{n}\\), \\(\\left\\{a_{1}, \\ldots, a_{n}\\right\\}\\) so that for all \\(i \\neq j, A a_{i} \\cdot A a_{j}=0\\)\nTheorem: If \\(A\\) is an \\(\\mathrm{m} \\mathrm{x} \\mathrm{n}\\) matrix, then there is an orthonormal basis of \\(\\mathbb{R}^{n}\\), \\(\\left\\{a_{1}, \\ldots, a_{n}\\right\\}\\) so that for all \\(i \\neq j, A a_{i} \\cdot A a_{j}=0\\)\nProof 1: For \\(m \\times 2\\) matrices You can get 2D perpframes using \\(v_{1}(t)=\\left(\\begin{array}{c}\\cos (t) \\\\ \\sin (t)\\end{array}\\right)\\) and \\(v_{2}(t)=\\left(\\begin{array}{c}-\\sin (t) \\\\ \\cos (t)\\end{array}\\right)\\) and specifying an angle \\(0 \\leq t \\leq \\frac{\\pi}{2}\\) You’d like to show that for any \\(\\mathrm{m} \\times 2\\) matrix there’s an angle \\(\\mathrm{t}\\) so that \\(\\left(A v_{1}(t)\\right) \\cdot\\left(A v_{2}(t)\\right)\\) is zero. How do you know there is always such a \\(t\\) ?\nAnswer: Go with the example matrix \\(A=\\left(\\begin{array}{cc}1 \u0026amp; -1 \\\\ 2 \u0026amp; 1\\end{array}\\right)\\) and look at the following plot that shows the perpframe \\(v_{1}(t)\\) and \\(v_{2}(t)\\) before and after the hit with \\(\\mathrm{A}\\).\nNotice that when \\(\\mathrm{t}=0\\), the angle between \\(\\mathrm{A} \\mathbf{v}_{1}[\\mathrm{t}]\\) and \\(\\mathrm{A} \\mathbf{v}_{2}[\\mathrm{t}]\\) is greater than \\(\\frac{\\mathrm{pi}}{2}\\). By the time \\(t=\\frac{p i}{2}=1.57\\), the angle is less than \\(\\frac{\\mathrm{Pi}}{2}\\). That’s enough to guarantee that somewhere between \\(\\mathrm{t}=0\\) and \\(\\mathrm{t}=\\frac{\\mathrm{Pi}}{2}\\) there’s an angle where \\(\\mathrm{A} \\mathrm{v}_{1}[\\mathrm{t}]\\) and \\(\\mathrm{A} \\boldsymbol{v}_{2}[\\mathrm{t}]\\) are perpendicular. Most reasonable folks would accept the evidence given in the plots, but the doubters might want to look at the function: \\[ \\mathrm{f}[\\mathrm{t}]=\\left(\\mathrm{A} \\cdot\\left(\\begin{array}{c} \\cos [t] \\\\ \\sin [t] \\end{array}\\right]\\right) \\cdot\\left(\\mathrm{A} \\cdot\\left(\\begin{array}{c} -\\sin (t) \\\\ \\cos (t) \\end{array}\\right)\\right) \\] Then \\(f[0] \\stackrel{(1)}{=} A\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right) \\cdot A\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right) \\stackrel{(2)}{=}-A\\left(\\begin{array}{c}-1 \\\\ 0\\end{array}\\right) \\cdot A\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right) \\stackrel{(3)}{=}-f\\left[\\frac{\\pi}{2}\\right]\\) (1) The definition of \\(\\mathrm{f}\\). (2) Linearity of matrix hits. (3) The definition of \\(\\mathrm{f}\\). This equation says that \\(\\mathrm{f}[0]\\) and \\(\\mathrm{f}\\left[\\frac{\\pi}{2}\\right]\\) have opposite signs. This should convince even the doubters that there’s a \\(t\\) between 0 and \\(\\frac{\\pi}{2}\\) that makes \\(\\mathrm{f}[\\mathrm{t}]\\) zero. The same argument holds for any \\(\\mathrm{m}\\) by 2 matrix.\nProof 2: General case The proof above works well enough for \\(\\mathrm{m} \\times 2\\) matrices, but this proof shows that for all \\(\\mathrm{m} \\times \\mathrm{n}\\) matrices there’s is a perpendicular frame \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\vec{a}_{3}, \\ldots, \\vec{a}_{n}\\right\\}\\) of unit vectors so that \\(\\hat{a} \\widehat{a}_{i} \\cdot A \\vec{a}_{j}=0\\) for \\(i\u0026lt;j\\) Here’s one way to get such a perpframe. Start with a \\(\\mathrm{m} \\times \\mathrm{n}\\) matrix \\(A\\). Step \\(1:\\) Let \\(\\vec{a}_{1}\\) be a unit vector maximizing \\(|A \\vec{v}|^{2}\\). Step 2 : Let \\(s_{2}\\) be the space perpendicular to span \\(\\left\\{\\vec{a}_{1}\\right\\}\\) Let \\(\\vec{a}_{2}\\) be a unit vector in \\(s_{2}\\) maximizing \\(|A \\vec{v}|^{2}\\).\nStep 3: Let \\(\\mathrm{s}_{3}\\) be the space perpendicular to span \\(\\left\\{\\overrightarrow{\\mathrm{a}}_{1}, \\overrightarrow{\\mathrm{a}}_{2}\\right\\}\\) Let \\(\\overrightarrow{\\mathrm{a}}_{3}\\) be a unit vector in \\(\\mathrm{s}_{3}\\) maximizing \\(\\quad|\\mathrm{A} \\overrightarrow{\\mathrm{v}}|^{2}\\). Step \\(\\mathrm{n}\\) : Let \\(\\mathrm{s}_{\\mathrm{n}}\\) be the space perpendicular to span \\(\\left\\{\\overrightarrow{\\mathrm{a}}_{1}, \\overrightarrow{\\mathrm{a}}_{2}, \\ldots, \\overrightarrow{\\mathrm{a}}_{n-1}\\right\\}\\) Let \\(\\vec{a}_{n}\\) be a unit vector in \\(s_{n}\\) maximizing \\(|A \\vec{v}|^{2}\\).\nThis gets you a perpendicular frame of unit vectors \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\vec{a}_{3}, \\ldots, \\vec{a}_{n}\\right\\}\\), but how do you know that \\(\\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}=0\\) for \\(\\mathrm{i}\u0026lt;\\mathrm{j}\\) ? Look at \\(\\vec{v}[t]=\\cos [t] \\vec{a}_{i}+\\sin [t] \\vec{a}_{j}\\) and see that - no matter what \\(\\mathrm{t}\\) you choose \\(\\mathrm{v}[\\mathrm{t}]\\) is a unit vector in \\(\\mathrm{s}_{\\mathrm{i}}\\)\n\\(v[0]=\\operatorname{Cos}[0] \\vec{a}_{i}+\\sin [0] \\vec{a}_{j}=\\vec{a}_{i}\\) Since \\(\\overrightarrow{\\mathrm{a}}_{\\mathrm{i}}\\) is a unit vector in \\(s_{\\mathrm{i}}\\) maximizing \\(|\\mathrm{A} \\overrightarrow{\\mathrm{v}}|^{2}\\) and \\(\\vec{v}[\\mathrm{t}]\\) is in \\(s_{\\mathrm{i}}\\) for all \\(\\mathrm{t}\\), you know that \\(\\mathrm{f}[\\mathrm{t}]=|\\mathrm{A} \\vec{v}[\\mathrm{t}]|^{2}\\) has a maximum at \\(\\mathrm{t}=0\\) This tells you \\(0=\\mathrm{f}^{\\prime}[0]\\) Now compute: \\(\\mathrm{f}[\\mathrm{t}]=|\\mathrm{A} \\vec{v}[\\mathrm{t}]|^{2}\\) \\(=\\mathrm{A} \\vec{v}[\\mathrm{t}] \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{v}}[\\mathrm{t}]\\) \\(=A\\left(\\operatorname{Cos}[t] \\vec{a}_{i}+\\operatorname{Sin}[t] \\vec{a}_{j}\\right) \\cdot A\\left(\\operatorname{Cos}[t] \\vec{a}_{i}+\\sin [t] \\vec{a}_{j}\\right)\\) \\(=\\left(A \\vec{a}_{i} \\cdot A \\vec{a}_{i}\\right) \\operatorname{Cos}[t]^{2}+2\\left(A \\vec{a}_{i} \\cdot A \\vec{a}_{j}\\right) \\sin [t] \\operatorname{Cos}[t]+\\) \\(\\left(A \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right) \\sin [\\mathrm{t}]^{\\varepsilon}\\)\nWhen you remember \\(\\left(\\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}}\\right),\\left(\\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right)\\), and \\(\\left(\\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right)\\) are just numbers, you understand that its nothing more than tedious to compute: \\(\\mathrm{f}^{\\prime}[\\mathrm{t}]=-2\\left(\\mathrm{~A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}}\\right) \\operatorname{Cos}[\\mathrm{t}] \\sin (\\mathrm{t}]-2\\left(\\hat{\\mathrm{A}} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right) \\sin [\\mathrm{t}] \\sin [\\mathrm{t}\\) \\(+2\\left(\\mathrm{~A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\hat{\\mathrm{A}} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right) \\operatorname{Cos}[\\mathrm{t}] \\operatorname{Cos}[\\mathrm{t}]+2\\left(\\mathrm{~A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\right) \\sin [\\mathrm{t}] \\operatorname{Cos}[\\mathrm{t}]\\) Plugging in \\(\\mathrm{t}=0\\) gives you \\(\\mathrm{f}^{\\prime}[0]=2 \\mathrm{~A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{i}} \\cdot \\mathrm{A} \\overrightarrow{\\mathrm{a}}_{\\mathrm{j}}\\). But you already know that \\(f^{\\prime}[0]=0\\), so after canceling the 2 you get \\(0=A \\vec{a}_{i} \\cdot A \\vec{a}_{j}\\) which is just what you wanted.\nProof 3: Based on the spectral theorem This proof is slick IF YOU’VE ALREADY SEEN THE SPECTRAL THEOREM. If you haven’t seen the spectral theorem, then skip this proof. Given \\(\\mathrm{A}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{\\mathrm{m}}\\) and an orthonormal basis \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\ldots, \\vec{a}_{n}\\right\\}\\) of \\(\\mathbb{R}^{n}\\), \\(A \\vec{a}_{i} \\cdot A \\vec{a}_{j}=0\\) for \\(i \\neq j\\) iff \\(\\vec{a}_{i} \\cdot\\left(A^{t} A \\vec{a}_{j}\\right)=0\\) for \\(i \\neq j\\) iff \\(A^{t} A \\vec{a}_{j}=\\lambda \\vec{a}_{j}\\) iff \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\ldots, \\vec{a}_{n}\\right\\}\\) are all eigenvectors of \\(A^{t} A\\). Conclusion: The desired basis is guaranteed by spectral theorem since \\(A^{t} A\\) is symmetric.\nTheorem: Every matrix has a singular value decomposition. The theorem above almost gives you the SVD for any matrix. The only problem is that although the columns of the “hanger” matrix are pairwise perpendicular, they might not form a basis for \\(\\mathbb{R}^{m}\\). For example, suppose for a \\(5 x 4\\) matrix \\(A: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}^{5}\\) the procedure outlined above gives you: To complete the decomposition, let \\(\\left\\{\\overrightarrow{\\mathrm{h}}_{3}, \\overrightarrow{\\mathrm{h}}_{4}, \\overrightarrow{\\mathrm{h}}_{5}\\right\\}\\) be an orthonormal basis for the three dimensional subspace of \\(\\mathbb{R}^{5}\\) perpendicular to \\(\\operatorname{span}\\left\\{\\vec{h}_{1}, \\vec{h}_{2}\\right\\}\\).\nThen write \\[ \\begin{gathered} A=\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\overrightarrow{0} \\mid \\overrightarrow{0}\\right)\\left(\\begin{array}{llll} 3 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right)\\left(\\begin{array}{l} \\frac{\\vec{a}_{1}}{\\vec{a}_{\\hat{2}}} \\\\ \\frac{\\vec{a}_{3}}{\\vec{a}_{4}} \\end{array}\\right) \\\\ \\stackrel{(1)}{=}\\left(\\vec{h}_{1}\\left|\\vec{h}_{2}\\right| \\vec{h}_{3}\\left|\\vec{h}_{4}\\right| \\vec{h}_{5}\\right)\\left(\\begin{array}{llll} 3 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 2 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right)\\left(\\begin{array}{l} \\frac{\\vec{a}_{1}}{\\vec{a}_{2}} \\\\ \\frac{\\vec{a}_{3}}{\\vec{a}_{4}} \\end{array}\\right) \\end{gathered} \\] (1) The two sides agree on the basis \\(\\left\\{\\vec{a}_{1}, \\vec{a}_{2}, \\vec{a}_{3}, \\vec{a}_{4}\\right\\}\\). This, finally, is a singular value decomposition for \\(A\\).\nComments: - The diagonal entries of the stretcher matrix are called the “singular values of \\(A \u0026quot; .\\) - An extra row of zeros has been added to the stretcher matrix to produce the dimensions required for the multiplication. If \\(A\\) is \\(\\mathrm{m} \\mathrm{x} \\mathrm{n}\\) with \\(m\u0026gt;n\\), then rows will be deleted.\nIn either case, the dimensions of the stretcher matrix will always match the dimensions of \\(A\\) - The decomposition shows that the action of every matrix can be described as a rotation followed by a stretch followed by another rotation. - The proofs above are meant to show that every matrix has an SVD. You can compute SVD’s for \\(\\mathrm{mx} 2\\) matrices by hand, but you should use a machine to compute SVD’s for bigger matrices.\nExercises 1. Above, you saw that if \\(\\mathrm{A}\\) is a \\(5 \\mathrm{x} 4\\) matrix \\(A: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}^{5}\\) then the SVD for \\(\\mathrm{A}\\) has the form \\[ A=\\left(\\overrightarrow{\\mathrm{h}}_{1}\\left|\\overrightarrow{\\mathrm{h}}_{2}\\right| \\overrightarrow{\\mathrm{h}}_{3}\\left|\\overrightarrow{\\mathrm{h}}_{4}\\right| \\overrightarrow{\\mathrm{h}}_{5} \\text { ) }\\left(\\begin{array}{cccc} s_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; s_{2} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; s_{3} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; s_{4} \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right)\\left(\\frac{\\overrightarrow{\\mathrm{a}}_{1}}{\\overrightarrow{\\mathrm{a}_{2}}}\\right) .\\right. \\] Give the form for the SVD of a \\(4 \\times 5\\) matrix \\(A: \\mathbb{R}^{5} \\rightarrow \\mathbb{R}^{4}\\). (Hint: The matrix has five singular values, but \\(s_{5}=0\\) does not appear in the decomposition.)\nLet \\(A=\\left(\\begin{array}{ll}1 \u0026amp; 2 \\\\ 0 \u0026amp; 1\\end{array}\\right) .\\) Following the steps of Proof 1 above, define \\(\\nu_{1}=\\left(\\begin{array}{c}\\cos [t] \\\\ \\sin [t]\\end{array}\\right]\\) and \\(v_{2}=\\left(\\begin{array}{c}-\\sin (t) \\\\ \\cos (t)\\end{array}\\right)\\). Expand \\(\\left(A v_{1}\\right) \\cdot\\left(A v_{2}\\right)\\) After applying trig identities it turns out that \\(\\left(A v_{1}\\right) \\cdot\\left(A v_{2}\\right)=2(\\operatorname{Cos}[2 t]+\\operatorname{Sin}[2 t]]\\). Use this fact to find a \\(t\\) so that \\(\\left(A v_{1}\\right) \\cdot\\left(A v_{2}\\right)=0\\) Using part (b) as a start, give an SVD for \\(A\\). ","date":"2021-06-17T17:18:00-04:00","permalink":"https://zongpitt.com/posts/2021-06-17-svd5/","section":"posts","tags":["Svd"],"title":"Singular value decomposition 5"},{"categories":["ML"],"contents":"Auxiliary Classifier GAN Add additional classifier to help GAN generator better image. This is very similar to conditional GANs. The structure show as follow.\nAdversarial Autoencoders using GAN as regulator to help training autoencoder\nTwo training stage.\nreconstruction\nregularization\nLeast Square GAN different loss function, structure as same as original.\nCycle GAN used for image to image translation\nCondition GAN Add more information as condition for GAN structure\n","date":"2021-06-15T10:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-06-15-gans/","section":"posts","tags":["GAN"],"title":"5 GAN structures"},{"categories":["ML"],"contents":"GAN is very popular network now. It can be used to generate fake read good fake dataset, change image style, generate carton images. Anyway, it is a really good network. As we all know it is useful, let’s get into detail directly. Generative adversarial network usually contains two part, generative model \\(G\\) and discriminative model \\(D .\\) Figure 1 shows this GAN structure.\nIn GAN simultaneously train two models: a generative model \\(G\\) that capture the data distribution, and a discriminative model \\(D\\) that estimates the probability that a sample came from the training data rather than \\(G\\). The training procedure for \\(G\\) is to maximize the probability of \\(D\\) making a mistake. The training procedure for \\(D\\) is to minimize discrimiante error for fixed \\(G\\). This is a kind of two-player minimax game\nThe unique solution exist for this system, with \\(G\\) recovering the training data distribution adn \\(D\\) equal to \\(\\frac{1}{2}\\) everywhere.\nFormulation we will have following value function base on maximum likelihood estimation\n\\[ \\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))] \\]\nwhere \\(p_{g}(\\boldsymbol{x})\\) is generator’s distribution over data \\(\\boldsymbol{x} . p_{\\boldsymbol{z}}(\\boldsymbol{z})\\) is prior defined distribution base on noise variables \\(\\boldsymbol{z} . G(\\boldsymbol{z})\\) is mapping which will map input noise variable \\(z\\) to data space \\(G\\left(\\boldsymbol{z} ; \\theta_{g}\\right) . \\theta_{g}\\) is parameter of Generative model \\(G\\). To explain this equation more intuitive, following explanation will not very accuracy. But it will help for understanding. The output of discriminator \\(D\\) is 0 or \\(1 .\\) It is binary classifier. Consider generate form of binary log likelihood function. We will maximize likelihood function to get best estimation.\n\\[ \\max _{\\theta} L(\\theta)=\\sum_{i=1}^{n} y_{i} \\log \\eta\\left(x_{i} ; \\theta\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\eta\\left(x_{i} ; \\theta\\right)\\right) \\]\nwhere \\(y_{i}\\) is ground trues, \\(\\eta\\left(x_{i} ; \\theta\\right)\\) is estimator output based on parameter \\(\\theta\\). Let’s look Eq.1, it is very similar to binary log likely hood function if we fix \\(G\\) and multiply the number of sample. We will found it is same as out binary \\(\\log\\) likelihood function. What GAN did is max \\(D\\) base on fix \\(G\\), then \\(\\min G\\) base on fix \\(D\\). Definitely we don’t have very good \\(D\\) or \\(G\\) at the begin. So just repeat operation until it converge \\((\\max D\\) based on fix \\(G, \\min G\\) based on fix \\(D)\\).\nAlgorithm Algorithm is shows Algorithm \\(1 .\\) The network is trained by gradient descent. As we mentioned before, GAN will train discriminator \\(D\\) first then train generative model \\(G\\).\nTheoretical Results Global Optimality of \\(p_{g}=p_{\\text {data }}\\) Proposition 1. For \\(G\\) fixed, the optimal discriminator \\(D\\) is\n\\[ D_{G}^{*}(\\boldsymbol{x})=\\frac{p_{\\text {data }}(\\boldsymbol{x})}{p_{\\text {data }}(\\boldsymbol{x})+p_{g}(\\boldsymbol{x})} \\]\n\\(C(G)\\) is defined as \\(\\max _{D} V(G, D)\\) Theorem 1. The global minimum of the virtual training criterion \\(C(G)\\) is achieved if and only if \\(p_{g}=p_{\\text {data }}\\). At that point, \\(C(G)\\) achieves the value \\(-\\log 4\\).\nConvergence of Algorithm 1 Proposition 2. If \\(G\\) and \\(D\\) have enough capacity, and at each step of Algorithm 1 , the discriminator is allowed to reach its optimum given \\(G\\), and \\(p_{g}\\) is updated so as to improve the criterion\n\\[ \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}}\\left[\\log D_{G}^{*}(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{g}}\\left[\\log \\left(1-D_{G}^{*}(\\boldsymbol{x})\\right)\\right] \\]\nthen \\(p_{g}\\) converges to \\(p_{\\text {data }}\\)\nApplications style GAN - https://thispersondoesnotexist.com/ Image to image translation ","date":"2021-06-12T10:36:00-04:00","permalink":"https://zongpitt.com/posts/2021-06-12-gan/","section":"posts","tags":["GAN"],"title":"Generative Adversarial Nets"},{"categories":["Math"],"contents":"Projections Hitting with one matrix and then another Here’s a perpframe \\(p_{1}=\\left(\\begin{array}{c}\\cos (s) \\\\ \\sin (s)\\end{array}\\right)\\) and \\(p_{2}=\\left(\\begin{array}{r}-\\sin (s) \\\\ \\cos (s)\\end{array}\\right)\\) specified by angle \\(s=\\frac{\\pi}{3}\\).\nPlot an ellipse with a minor axis of \\(0.5\\) in the direction of \\(\\mathbf{p}_{1}\\) and major axis of \\(1.2\\) in the direction of \\(\\mathbf{p}_{2}\\).\nAnswer: You know how to plot the unit circle:\nParametricPlot \\(\\left[\\left(\\begin{array}{l}\\operatorname{Cos}[t] \\\\ \\operatorname{Sin}[t]\\end{array}\\right),\\{\\mathrm{t}, 0,2 \\pi\\}\\right]\\)\nYou know how to use an xy-stretcher to stretch the circle into an ellipse with the desired axes lengths:\nstretcher \\(=\\left(\\begin{array}{cc}0.5 \u0026amp; 0 \\\\ 0 \u0026amp; 1.2\\end{array}\\right)\\) ParametricPlot [stretcher \\(\\left.\\left(\\begin{array}{c}\\cos [t] \\\\ \\text { Sin[t] }\\end{array}\\right),\\{\\mathrm{t}, 0,2 \\pi\\}\\right]\\)\nThe ellipse you’re looking at now is given by \\(\\left(\\begin{array}{cc}0.5 \u0026amp; 0 \\\\ 0 \u0026amp; 1.2\\end{array}\\right)\\left(\\begin{array}{l}\\cos [\\mathrm{t}] \\\\ \\sin [\\mathrm{t}]\\end{array}\\right)\\).\nAll you need to do is hang the ellipse on the perpendicular frame. To do that you hit the curve \\(\\left(\\begin{array}{cc}0.5 \u0026amp; 0 \\\\ 0 \u0026amp; 1.2\\end{array}\\right)\\left(\\begin{array}{c}\\cos [t] \\\\ \\sin [t]\\end{array}\\right)\\) with the nanger matrix.\nhanger \\(=\\left(\\mathrm{p}_{1} \\mid \\mathrm{p}_{2}\\right)\\)\nParametricplot [hanger (stretcher \\(\\left(\\begin{array}{l}\\cos [t] \\\\ \\text { Sin[t] }\\end{array}\\right)\\) ) , {t, 0, 2}\nAn ellipse with a minor axis of \\(0.5\\) in the direction of \\(\\mathbf{p}_{1}\\) and major axis of \\(1.2\\) in the direction of \\(\\mathbf{p}_{2}\\).\nJust what the doctor ordered.\nIn the problem above you used two matrix hits to plot an ellipse.\nPlot the same ellipse again, but this time use only one matrix hit.\nAnswer: Above, you defined a stretcher matrix and a hanger matrix and then plotted the results of a hit with the stretcher followed by a hit with the hanger:\nParametricPlot [hanger (stretcher \\(\\left.\\left(\\begin{array}{l}\\cos [\\mathrm{t}] \\\\ \\text { Sin[t] }\\end{array}\\right)\\right\\},\\{\\mathrm{t}, 0,2\\) )\nIf you multiply the out (hanger) (stretcher) you get: \\[ \\text { product }=(\\text { hanger }) \\text { (stretcher) }=\\left(\\begin{array}{cc} 0.25 \u0026amp; -1.03923 \\\\ 0.433013 \u0026amp; 0.6 \\end{array}\\right) . \\] Now hit the unit circle with the product: ParametricPlot [product \\(\\left.\\left(\\begin{array}{l}\\cos [t] \\\\ \\operatorname{Sin}[t]\\end{array}\\right),\\{t, 0,2 \\pi\\}\\right]\\)\nApparently,\nthe action of the matrix product: product = (hanger) (stretcher)\nis the same as\nthe action of the stretcher followed by the action of the hanger.\nKey point: The action of a matrix product: A=B C is the same as the action of the C followed by the action of B.\nThe point on a line through {0,0} closest to P. Here’s a plot of the point \\(P=\\{2,1\\}\\).\nWhat point on the \\(x\\) -axis is closest to the point \\(\\{2,1\\} ?\\) Answer: It’s gotta be the point \\(\\{2,0\\}\\).\nWhat point on the \\(\\mathrm{y}\\) -axis is closest to the point \\(\\{2,1\\} ?\\) Answer: It’s gotta be the point \\(\\{0,1\\}\\).\nHow can you get these closest points with matrix hits? Answer: To get the closest point on the \\(\\mathrm{x}\\) -axis, hit \\(\\{2,1\\}\\) with the \\(\\mathrm{xy}\\) -stretcher that squashes the \\(\\mathrm{y}\\) -coordinate but leaves the x-coordinate unchanged. The \\(x y\\) -stretcher you want is \\(\\left(\\begin{array}{ll}1 \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{array}\\right)\\). Check: \\(\\left(\\begin{array}{ll}1 \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{array}\\right)\\left(\\begin{array}{l}2 \\\\ 1\\end{array}\\right)=\\left(\\begin{array}{l}2 \\\\ 0\\end{array}\\right)\\)\nTo get the closest point on the y-axis hit \\(\\{2,1\\}\\) with the \\(x y\\) -stretcher that squashes the \\(x\\) -coordinate but leaves the y-coordinate unchanged. The \\(x y\\) -stretcher you want is \\(\\left(\\begin{array}{ll}0 \u0026amp; 0 \\\\ 0 \u0026amp; 1\\end{array}\\right)\\). Check: \\(\\left(\\begin{array}{ll}0 \u0026amp; 0 \\\\ 0 \u0026amp; 1\\end{array}\\right)\\left(\\begin{array}{l}2 \\\\ 1\\end{array}\\right)=\\left(\\begin{array}{l}0 \\\\ 1\\end{array}\\right)\\)\nHere’s a plot of a perpframe and the point \\(\\mathrm{P}=\\{2,1\\}\\).\nThe plot also shows lines through \\(\\{0,0\\}\\) parallel to \\(\\mathbf{p}_{1}\\) and \\(\\mathbf{p}_{2}\\) What point on the green line is closest to the point \\(\\mathrm{P}=\\{2,1\\}\\) ?\nAnswer: This is a lot harder than asking what point on the \\(x\\) -axis is closest to \\(\\{2,1\\}\\). Look at the following plots.\n\\(\\mathrm{S}\\) is the point on the green line closest to \\(\\{2,1\\}\\). Explain how the \\(\\mathrm{S}\\) got where it did.\nAnswer: Look again at the first two plots.\nYou get from the first plot to the second plot by rotating the perpframe vectors \\(\\mathbf{p}_{1}\\) and \\(\\mathbf{p}_{2}\\) to the \\(\\mathrm{x}\\) and \\(\\mathrm{y}\\) axes. This rotates the green line to the \\(x\\) -axis and the point \\(p\\) to the new position 0 . You get this rotation with the aligner \\(=\\left(\\frac{\\mathbf{p}_{1}}{\\mathbf{p}_{2}}\\right)\\).\nLook at the second and third plots:\n\\(\\mathrm{R}\\) is the point on the \\(\\mathrm{x}\\) -axis closest to \\(\\mathrm{Q}\\). You can get \\(\\mathrm{R}\\) by hitting \\(\\mathrm{Q}\\) with the stretcher \\(=\\left(\\begin{array}{ll}1 \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{array}\\right)\\). This squashes Q’s y-coordinate, but leaves the x-coordinate unchanged.\nLook at the third and fourth plots:\nHere you rotate the xy-axis back to the perpframe vectors. The point \\(\\mathrm{R}\\) gets rotated to \\(\\mathrm{S}\\). This is a job for the hanger \\(=\\left(\\mathbf{p}_{1} \\mid \\mathbf{p}_{2}\\right)\\).\nSummary: You arrived at \\(\\mathrm{S}\\) by hitting \\(\\mathrm{P}\\), - first with aligner \\(=\\left(\\frac{\\mathbf{p}_{1}}{\\mathbf{p}_{2}}\\right)\\), - then with stretcher \\(=\\left(\\begin{array}{ll}1 \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{array}\\right)\\), - and finally with hanger \\(=\\left(\\mathbf{p}_{1} \\mid \\mathbf{p}_{2}\\right)\\).\nThis says you can get the coordinates of the point on the green line closest to \\(\\mathrm{p}\\) by computing (hanger) (strecher) (aligmer) \\(\\mathrm{P}\\).\nCompute this point and add it to the plot closestGreen = (hanger) \\(\\left(\\begin{array}{ll}1 \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{array}\\right)\\) (aligner) P \\(\\{0.600338,1.54416\\}\\)\nLooks good.\nTake a look at the original setup again. What point on the blue line is closest to the point \\(\\{2,1\\} ?\\)\nAnswer: This time you should try to imagine the picture for each step: - Use the aligner matrix to rotate the \\(\\mathrm{p}_{1}\\) and \\(\\mathrm{p}_{2}\\) to the \\(\\mathrm{xy}\\) -axis. This rotates \\(\\mathrm{P}\\) to a new position \\(\\mathrm{Q}\\). - Use a stretcher matrix to find the point on the y-axis (the new location of the blue line) closest to \\(\\mathrm{Q}\\). - Use the hanger matrix to rotate back to the original perpframe.\nThis says that (hanger) \\(\\left(\\begin{array}{ll}0 \u0026amp; 0 \\\\ 0 \u0026amp; 1\\end{array}\\right)\\) (aligner) \\(\\mathrm{P}\\) should give you the point on the blue line closest to \\(\\mathrm{P}\\). You use \\(\\left(\\begin{array}{ll}0 \u0026amp; 0 \\\\ 0 \u0026amp; 1\\end{array}\\right)\\) for the stretcher since you want to squash the \\(x\\) but leave the y unchanged. Compute closestBlue = (hanger) \\(\\left(\\begin{array}{ll}0 \u0026amp; 0 \\\\ 0\\end{array}\\right)\\) (aligner) \\(\\mathrm{P}\\) and add it to the plot.\nLooks good.\nIt’s good to be able to imagine all of the matrix action that leads from \\(\\mathrm{p}\\) to the blue dot, but you deserve to see it too.\nThe point on a plane through {0,0,0} closest to P. Here’s a plot of a point \\(\\mathrm{P}=\\{1,-1,1\\}\\) and a perpframe \\(\\left\\{p_{1}, p_{2}, p_{3}\\right\\} .\\)\nThe plot also shows the plane through \\(\\{0,0,0\\}\\) spanned by \\(\\mathbf{p}_{1}\\) and \\(\\mathbf{p}_{2}\\) What point on the plane is closest to the point \\(\\mathrm{P}=\\{1,-1,1\\} ?\\)\nAnswer this question in stages.\nFirst align the plane spanned by \\(p_1\\) and \\(p_2\\) with the xy-plane.\nDo this by hitting with the aligner \\(=\\left(\\frac{\\frac{p_{1}}{p_{z}}}{p_{3}}\\right)\\). \\[ \\mathrm{Q}=\\left(\\frac{\\frac{\\mathrm{p}_{1}}{\\mathrm{p}_{2}}}{\\mathrm{p}_{3}}\\right) \\mathrm{P} \\] The aligner has rotated the perpframe to xyz-axis.\nThe original plane has been rotated to the xy-plane.\nThe point P has been rotated to the point Q.\nThe distances from P to the original plane are the same as distances from Q to the xy-plane.\nBut finding the point on the xy-plane closest to Q is a snap.\nJust hit Q with a stretcher matrix that leaves the x and y coordinates unchanged, but squashes the z coordinate. \\[ R=\\left(\\begin{array}{lll} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) Q=\\left(\\begin{array}{lll} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right)\\left(\\frac{p_{1}}{p_{2}}\\right) P \\] Now hang the plane back in its original position to find the point closest to \\(\\mathrm{P}\\). Do this with the hanger matrix. \\[ S=\\left(p_{1}\\left|p_{2}\\right| p_{3}\\right) R=\\left(p_{1}\\left|p_{2}\\right| p_{3}\\right)\\left(\\begin{array}{lll} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right)\\left(\\frac{p_{1}}{p_{2}}\\right) P \\] Summary: You arrived at \\(\\mathrm{S}\\), the point on the plane closest to \\(\\mathrm{P}\\), by hitting \\(\\mathrm{P}\\), - first with aligner \\(=\\left(\\frac{\\underline{\\mathbf{p}} \\mathbf{1}}{\\underline{\\mathbf{p}}} \\mid\\right.\\), \\(\\bullet\\) then with stretcher \\(=\\left(\\begin{array}{lll}1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0\\end{array}\\right)\\), - and finally with hanger \\(=(\\mathbf{p} \\mathbf{1}|\\mathbf{p} 2| \\mathbf{p} \\mathbf{3})\\). This says you can get the coordinates of the point on the green line closest to \\(\\mathrm{p}\\) by computing (hanger) (stretcher) (aligner) \\(\\mathrm{P}\\)\nSimplified formulas. You’ll see later that the product ( \\(\\mathbf{p} \\mathbf{1}|\\mathbf{p} \\mathbf{2}| \\mathbf{p}\\) 3 ) \\(\\left(\\begin{array}{lll}1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0\\end{array}\\right)\\left(\\frac{\\mathbf{p} \\mathbf{1}}{\\mathbf{p}^{2}}\\right)\\) simplifies to just \\(\\left(\\mathbf{p} \\mathbf{1} \\mid \\mathbf{p}^{2}\\right)\\left(\\frac{\\mathbf{p} \\mathbf{1}}{\\mathbf{p} 2}\\right)\\). More generally, (but without the pictures), if - \\(\\left\\{\\vec{p}_{1}, \\vec{p}_{2}, \\ldots, \\vec{p}_{m}\\right\\}\\) is an orthonormal basis for \\(\\mathbb{R}^{m}\\)\nand \\(S\\) is the subspace spanned by \\(\\left\\{\\vec{p}_{1}, \\vec{p}_{2}, \\ldots, \\vec{p}_{r}\\right\\}, \\mathrm{r}\u0026lt;\\mathrm{m}\\). Then the point in \\(\\mathrm{S}\\) closest to \\(\\mathrm{Q}\\) is given by\n\\[ S=\\left(\\vec{p}_{1}\\left|\\vec{p}_{2}\\right| \\cdots \\mid \\vec{p}_{\\gamma}\\right)\\left(\\frac{\\frac{\\vec{p}_{1}}{\\vec{p}_{2}}}{\\frac{\\vdots}{\\vec{p}_{y}}}\\right) Q \\]\n","date":"2021-06-04T17:18:00-04:00","permalink":"https://zongpitt.com/posts/2021-06-04-svd4/","section":"posts","tags":["Svd"],"title":"Singular value decomposition 4"},{"categories":["Math"],"contents":"Coordinates From standard coordinates to perpframe coordinates. Here’s a perpframe together with the point \\(P=\\{1, \\frac{1}{2}\\}\\).\nUse a matrix hit to find numbers a and \\(\\mathrm{b}\\) so that \\(P=a v_{1}+b v_{2}\\).\n(The numbers a and b are the perpframe coordinates of the point P.)\nWhy does that work?\nAnswer: Here’s what you get when you hit everything in the plot above with the aligner matrix:\nThe location of the rotated point aligner. P relative to the xy-axis is the same as the location of the original point P relative to the perpframe.\nBut in this picture it’s clear that the numbers a and b should be the x and y coordinates of the point aligner.P.\nFrom perpframe coordinates to standard coordinates. For the perpframe shown below, use a matrix hit to find the xy-coordinates of the point P that has perpframe coordinates {-2,3}.\nAnswer: The column method for matrix multiplication tells you that the standard \\(x y\\) -coordinates of the point \\(P\\) are given by \\(-2 v_{1}+3 v_{2}=\\left(v_{1} \\mid v_{2}\\right)\\left(\\begin{array}{c}-2 \\\\ 3\\end{array}\\right)\\)\nSummary A hit with the aligner matrix takes standard coordinates to perpframe coordinates. A hit with the hanger matrix takes perpframe coordinates to standard coordinates. Exercises 1. Estimate by eye the \\(\\{v_{1}, v_{2}\\}\\) coordinates of the point \\(\\mathrm{P}\\) shown in the plot below. 2. In terms of \\(v_{1}\\) and \\(v_{2}\\) express a matrix \\(\\mathrm{M}\\) so that \\(M P\\) gives the coor\\(\\{v_{1}, v_{2}\\}\\) dinates of the point \\(\\mathrm{P}\\).\n","date":"2021-06-04T10:56:00-04:00","permalink":"https://zongpitt.com/posts/2021-06-04-svd3/","section":"posts","tags":["Svd"],"title":"Singular value decomposition 3"},{"categories":["ML"],"contents":"Introduction The content is from Visualizing Data using t-SNE which proposed by Lauren and Hinton. This method is mainly focus on visualizing data. It used a none lienar method to map high dimension data to low dimension data. Then it will be easy for human to tell the relationship between data and verify our algorithm. This post will only introduce he general idea of t-SNE. The trick and implement detail may not explain in this post.\nBasic idea The t-SNE is a variation of Stochastic Neighbor Embedding(SNE). The basic idea is using propability to decribe the dataset. When dataset map from high dimensino to low dimension, the probability of the dataset will keep same. How to use probability to describe the data set will expalin later. After create probability description for dataset, the aim is keep the description same when map high dimension dataset to low dimension dataset. Keep the description same can be done by minimize the cost function, the cost function used in SNE is Kullback-Leibler divergence. That’s all!\nNext let’s go to detain one by one.\npropobility (similarity) description Let’s consider 3 points, \\(x\\), \\(y\\), \\(z\\). Suppose \\(x\\) and \\(y\\) are close to each other and far away from \\(z\\). From intuition, we will say \\(x\\) and \\(y\\) are very similar, \\(x\\) and \\(z\\) is differnt. This intuition is based Euclid distance. For SNE, it difine probabity to descripe this intuition instead using distance.\nLet’s look most simple version of similarity. For given a two point \\(i\\) and \\(j\\), we can tell similarity between two point by using Gaussian distribution.\n\\[ p(i|j) = p(j|i) = e^{-\\frac{\\|x\\_i-y\\_j\\|^2}{\\sigma^2}} \\]\nwhere \\(\\sigma\\) is variance of Gaussian distribution.\nBut there is a problem here, \\(\\sum\\_{j} p(j|i) \\neq 1\\) this is not accept for a probability model. So if we are using probabilty model to describe the dataset, this problem should be fixed. Base on this, we updata our definition. For this time, we only define \\(p(j|i)\\).\n\\[ p(j|i) = \\frac{e^{-\\frac{\\|x_i-x_j\\|^2}{\\sigma^2}}}{\\sum_{k \\neq i} e^{-\\frac{\\|x_k - x_i\\|^2}{\\sigma^2}}} \\]\nNow satisfy \\(\\sum_j p(j|i) = 1\\).\nUp to now, I explian how to define probility of dataset.\nThe t-SNE use different definition, it will explain later.\nDistance description The idae of map from high dimension to low dimension is keep probability description same. For example, in high dimension we have probabilty description for point \\(i\\) and \\(j\\), \\(p(j|i)\\) and in low dimension we have probabilty description for point \\(i\\) and \\(j\\), \\(q(j|i)\\). What we want is they are close to each other. So the problem become minimize the diverge between \\(p(j|i)\\) and \\(q(j|i)\\). In SNE, kullback-leibler divergence used to description the diverge between \\(p(j|i)\\) and \\(q(j|i)\\).\nNow, we have the cost function\n\\[ C = \\sum\\_{i} KL(P\\_i||Q\\_i) = \\sum\\_i \\sum\\_j p_{j|i} \\log \\frac{p\\_{j|i}}{q\\_{j|i}} \\]\nminimize this cost function. Everything is done here.\nProblems of SNE The mothod description above does not work very well sometime. That’s why it has been improve.\nthe probability of data does not symmetric. This means \\(p(i|j) \\neq p(j|i)\\). This does not fit intuition and it does not work well some time. And it also will make some data much more important to other. Crowding Problem. Can be solve by using t-distribution. Procedure of t-SNE (simplified version) ","date":"2021-05-18T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-05-18-t-sne/","section":"posts","tags":null,"title":"t-SNE"},{"categories":["Math"],"contents":"The reason I copy this information from website is to archive this information. The original website is no longer valid. I don’t want this really good tutorial of SVD disappeared.\nIntroduction The Singular Value Decomposition (SVD) is a topic rarely reached in undergraduate linear algebra courses and often skipped over in graduate courses.\nConsequently relatively few mathematicians are familiar with what M.I.T. Professor Gilbert Strang calls “absolutely a high point of linear algebra.”\nThese pages are a brief introduction to SVD suitable for inclusion in a standard sophomore level linear algebra course.\nThe lessons on coordinates, projections, matrix subspaces, and linear systems are standard linear algebra topics presented via the SVD. The later lessons give example applications of SVD.\nMuch of the terminology and many of the ideas presented in these pages come from the text Matrices, Geometry \u0026amp; Mathematica, by Bill Davis and Jerry Uhl.\nPlease give feedback or request answer keys to the exercises via email by contacting will.todd@uwlax.edu(I am not sure this still valid when I copy from the website.)\nMatrix Action Matrix Hits When you hit a point with a matrix you get another point.\nWhen the matrix \\(( \\[\\begin{array}{cc}1 \u0026amp; 3 \\\\\\ -3 \u0026amp; 2\\end{array}\\] )\\) hits \\(( \\[\\begin{array}{l}3 \\\\\\ 1\\end{array}\\] )\\) you get \\(( \\[\\begin{array}{cc}1 \u0026amp; 3 \\\\\\ -3 \u0026amp; 2\\end{array}\\] )( \\[\\begin{array}{l}3 \\\\\\ 1\\end{array}\\] )=( \\[\\begin{array}{c}6 \\\\\\ -7\\end{array}\\] )\\).\nTwo different ways to compute a matrix times a point.\nCOLUMN WAY to compute \\(A \\) : Take a linear combination of the COLUMNS of A using the weights from \\(\\). E.g. \\[\\left(\\begin{array}{c|c}1 \u0026amp; 3 \\\\\\ -3 \u0026amp; 2\\end{array}\\right)\\left(\\begin{array}{l}3 \\\\\\ 1\\end{array}\\right)=3\\left(\\begin{array}{c}1 \\\\\\ -3\\end{array}\\right)+1\\left(\\begin{array}{l}3 \\\\\\ 2\\end{array}\\right)=\\left(\\begin{array}{c}3 \\\\\\ -9\\end{array}\\right)+\\left(\\begin{array}{l}3 \\\\\\ 2\\end{array}\\right)=\\left(\\begin{array}{c}6 \\\\\\ -7\\end{array}\\right) \\]\nROW WAY to compute \\(A :\\) Dot each ROW of \\(A\\) with \\(\\).\n2D Matrix Action The plot below shows some color coded points on the unit circle. Move your mouse over the plot to see what happens when the matrix \\(( \\[\\begin{array}{cc}1 \u0026amp; 3 \\\\\\ -3 \u0026amp; 2\\end{array}\\] )\\) hits the points on the unit circle.\nYou can see that the matrix stretches and rotates the unit circle–that’s Matrix Action.\nCheck out a few more. Put your cursor over a matrix to see the matrix hit the unit circle.\n\\[ \\left(\\begin{array}{cc} 0.26 \u0026amp; 0.68 \\\\ 0.68 \u0026amp; 1.7 \\end{array}\\right)\\left(\\begin{array}{cc} 1.6 \u0026amp; -0.65 \\\\ -0.65 \u0026amp; -0.3 \\end{array}\\right)\\left(\\begin{array}{cc} 1.4 \u0026amp; -0.62 \\\\ -1.1 \u0026amp; -1.7 \\end{array}\\right) \\]\nYou can hit other curves too Check out the same three matrices hitting a damped sine wave.\n\\[ \\left(\\begin{array}{cc} 0.26 \u0026amp; 0.68 \\\\ 0.68 \u0026amp; 1.7 \\end{array}\\right)-\\left(\\begin{array}{cc} 1.6 \u0026amp; -0.65 \\\\ -0.65 \u0026amp; -0.3 \\end{array}\\right)-\\left(\\begin{array}{cc} 1.4 \u0026amp; -0.62 \\\\ -1.1 \u0026amp; -1.7 \\end{array}\\right) \\]\n3D Matrix Action If you have a 3 by 3 matrix you can hit 3D surfaces.\nCheck out the matrix \\(( \\[\\begin{array}{ccc}2 . \u0026amp; -0.48 \u0026amp; -0.56 \\\\\\ -0.072 \u0026amp; 0.74 \u0026amp; -2 . \\\\\\ 1.1 \u0026amp; 0.45 \u0026amp; -1.5\\end{array}\\] )\\) hitting this surface.\nThe matrix appears to be rotating and stretching the surface. That’s 3D matrix action.\nPerpframes, Aligner and Hangers 2D perpframes In 2D you need to perpendicular unit vectors to form a perpframe.\nOne way to get a perpframe is to specify an angle s and use the vectors\n\\[ \\nu_{1}=\\left(\\begin{array}{c} \\operatorname{Cos}[s] \\\\ \\operatorname{Sin}[s] \\end{array}\\right) \\text { and } \\nu_{2}=\\left(\\begin{array}{c} -\\operatorname{Sin}[s] \\\\ \\operatorname{Cos}[s] \\end{array}\\right) \\text { . } \\]\nHere are perpframes for some choices of s.\n3D perpframes A 3D perpframe consists of three mutually perpendicular unit vectors.\nComing up with a 3D perpframe is a little trickier, but you can look at some examples in the plot below.\nLots of folks call perpframes by the name “orthonormal basis”.\nAligners You get an aligner matrix by loading the vectors from a perpframe into the rows of the matrix.\n\\[ \\text { aligner }=\\left(\\frac{\\overrightarrow{\\mathrm{p}}_{1}}{\\overrightarrow{\\mathrm{p}}_{2}}\\right) \\]\nThe perpframe below consists of \\(_{1}=( \\[\\begin{array}{c}0.36 \\\\\\ 0.93\\end{array}\\] ) _{2}=( \\[\\begin{array}{c}-0.93 \\\\\\ 0.36\\end{array}\\] )\\). The aligner matrix you get from this perpframe is \\(( \\[\\begin{array}{cc}0.36 \u0026amp; 0.93 \\\\\\ -0.93 \u0026amp; 0.36\\end{array}\\] )\\).\nMouse over the plot to check out the action of this aligner matrix on the unit circle.\nThe aligner matrix gets its name since it aligns the perpframe to the xy-axis.\nSpecifically the aligner matrix \\(()\\) sends \\(_{1}\\) to \\(( \\[\\begin{array}{l}1 \\\\\\ 0\\end{array}\\] )\\) and sends \\(_{2}\\) to \\(( \\[\\begin{array}{l}0 \\\\\\ 1\\end{array}\\] )\\).\nYou can verify this by hand\n\\[ \\begin{array}{l} \\left(\\frac{\\vec{p}_{1}}{\\vec{p}_{2}}\\right) \\vec{p}_{1} \\stackrel{(1)}{=}\\left(\\begin{array}{l} \\vec{p}_{1} \\cdot \\vec{p}_{1} \\\\ \\vec{p}_{2} \\cdot \\vec{p}_{1} \\end{array}\\right) \\stackrel{(2)}{=}\\left(\\begin{array}{l} 1 \\\\ 0 \\end{array}\\right) \\\\ \\left(\\begin{array}{l} \\vec{p}_{1} \\\\ \\vec{p}_{z} \\end{array}\\right) \\vec{p}_{2} \\stackrel{(1)}{=}\\left(\\begin{array}{l} \\vec{p}_{1} \\cdot \\vec{p}_{2} \\\\ \\vec{p}_{z} \\cdot \\vec{p}_{2} \\end{array}\\right) \\stackrel{(2)}{=}\\left(\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right) \\end{array} \\]\nuses the row way to multiply a matrix times a vector. since \\(_{1}\\) and \\(_{2}\\) are a perpframe \\(_{1} _{2}=0\\) and \\(_{1} _{1}=_{} _{}=1\\). Hangers You get a hanger matrix by loading the vectors from a perpframe into the columns of the matrix.\n\\[ \\text { hanger }=\\left(\\overrightarrow{\\mathrm{p}}_{1} \\mid \\overrightarrow{\\mathrm{p}}_{2}\\right) \\]\nStay with the same perpframe from above \\(_{1}=( \\[\\begin{array}{c}0.36 \\\\\\ 0.93\\end{array}\\] )\\) and \\(_{2}=( \\[\\begin{array}{c}-0.93 \\\\\\ 0.36\\end{array}\\] )\\). The hanger matrix you get from this perpframe is \\(( \\[\\begin{array}{c|c}0.36 \u0026amp; -0.93 \\\\\\ 0.93 \u0026amp; 0.36\\end{array}\\] )\\).\nMouse over the plot to check out the action of this hanger matrix on the unit circle. The hanger matrix gets its name since it hangs the xy-axis onto the perpframe. Specifically the hanger matrix \\((_{1} _{2})\\) sends \\(( \\[\\begin{array}{l}1 \\\\\\ 0\\end{array}\\] )\\) to \\(_{1}\\) and sends \\(( \\[\\begin{array}{l}0 \\\\\\ 1\\end{array}\\] )\\) to \\(_{2}\\). You can verify this by hand\n\\[ \\begin{array}{l} \\left(\\overrightarrow{\\mathrm{p}}_{1} \\mid \\overrightarrow{\\mathrm{p}}_{2}\\right)\\left(\\begin{array}{l} 1 \\\\ 0 \\end{array}\\right) \\stackrel{(1)}{=} 1 \\overrightarrow{\\mathrm{p}}_{1}+0 \\overrightarrow{\\mathrm{p}}_{2}=\\overrightarrow{\\mathrm{p}}_{1} \\\\ \\left(\\overrightarrow{\\mathrm{p}}_{1} \\mid \\overrightarrow{\\mathrm{p}}_{2}\\right)\\left(\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right) \\stackrel{(1)}{=} 0 \\overrightarrow{\\mathrm{p}}_{1}+1 \\overrightarrow{\\mathrm{p}}_{2}=\\overrightarrow{\\mathrm{p}}_{2} \\end{array} \\]\nThis is the COLUMN WAY to multiply a matrix times a vector. Hitting curves with aligners and hangers. The plot shows the perpframe \\(_{1}, _{z}\\) and a damped sine curve lined up on the xy-axis. See what happens when you hit the curve with the hanger matrix \\((_{1} _{2})\\).\nThe hanger matrix hangs the curve on the perpframe.\nThe next plot shows an ellipse skewered on the perpframe \\(_{1}, _{2}\\) See what happens when you hit the ellipse with the aligner matrix \\(()\\).\nThe aligner matrix aligns the ellipse on the xy-axis.\n","date":"2021-05-08T22:00:00-04:00","permalink":"https://zongpitt.com/posts/2021-04-21-svd1/","section":"posts","tags":["Svd"],"title":"Singular value decomposition 1"},{"categories":["Math"],"contents":"Stretchers Look at the action of \\((\\begin{array}{ll}3 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2\\end{array})\\).\nwhen you look at taht action you can see why it’s natural to call a diagonal matrix a “stretcher” matrix.\nThe diagonal matrix \\(\\left(\\begin{array}{ll}a \u0026amp; 0 \\\\\\ 0 \u0026amp; b\\end{array}\\right)\\) stretches in the \\(x\\) direction by a factor of “a” and in the y direction by a factor of “b”.\nYou can verify this by hand using the column way to multiply a matrix times a vector: \\[ \\left(\\begin{array}{ll} a \u0026amp; 0 \\\\\\ 0 \u0026amp; b \\end{array}\\right)\\left(\\begin{array}{l} x \\\\\\ y \\end{array}\\right)=x\\left(\\begin{array}{l} a \\\\\\ 0 \\end{array}\\right)+y\\left(\\begin{array}{l} 0 \\\\\\ b \\end{array}\\right)=\\left(\\begin{array}{l} a x \\\\\\ b y \\end{array}\\right) \\]\nCheck out a few more stretchers Matrixifovie \\(\\left[\\left(\\begin{array}{cc} 2 \u0026amp; 0 \\\\\\ 0 \u0026amp; 1 / 2 \\end{array}\\right)\\right]\\)\nStretching by 1/2 squashes the circle in the y direction. Matrixifovie \\(\\left[\\left(\\begin{array}{ll}2 \u0026amp; 0 \\\\\\ 0 \u0026amp; 0\\end{array}\\right)\\right]\\)\nStretching by 0 in the y direction squashes the circle onto the x-axis. uatrixifovie \\(\\left[\\left(\\begin{array}{cc}-2 \u0026amp; 0 \\\\\\ 0 \u0026amp; 3\\end{array}\\right)\\right];\\)\nStretching by -2 in the x-direction, means flipping across the y-axis as well as stretching. Changing dimensions Both \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2\\end{array}\\right)\\) and \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2 \\\\\\ 0 \u0026amp; 0\\end{array}\\right)\\) are stretcher matrices since their non-diagonal entries are zero.\nThe matrix \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2\\end{array}\\right)\\) sends \\(\\left(\\begin{array}{l}x \\\\\\ y\\end{array}\\right)\\) to \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2\\end{array}\\right)\\left(\\begin{array}{l}x \\\\\\ y\\end{array}\\right)=\\left(\\begin{array}{l}5 x \\\\\\ 2 y\\end{array}\\right)\\). But \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2 \\\\\\ 0 \u0026amp; 0\\end{array}\\right)\\) sends \\(\\left(\\begin{array}{l}x \\\\\\ y\\end{array}\\right)\\) to \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2 \\\\\\ 0 \u0026amp; 0\\end{array}\\right)\\left(\\begin{array}{l}x \\\\\\ y\\end{array}\\right)=\\left(\\begin{array}{c}5 x \\\\\\ 2 y \\\\\\ 0\\end{array}\\right)\\).\nCheck out the action of each of these stretchers.\nMatrixifovie \\(\\left[\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2\\end{array}\\right)\\right]\\)\nA stretch by a factor of 5 in the x direction and a factor of 2 in the y direction. Matrixurovie \\(\\left[\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2 \\\\\\ 0 \u0026amp; 0\\end{array}\\right]\\);\nA stretch by a factor of 5 in the x direction and a factor of 2 in the y direction.\nBut note how the stretcher matrix \\(\\left(\\begin{array}{ll}5 \u0026amp; 0 \\\\\\ 0 \u0026amp; 2 \\\\\\ 0 \u0026amp; 0\\end{array}\\right)\\) not only stretches the \\(2 \\mathrm{D}\\) circle but also embeds the ellipse into 3 dimensional space.\n","date":"2021-05-08T22:00:00-04:00","permalink":"https://zongpitt.com/posts/2021-05-08-svd2/","section":"posts","tags":["Svd"],"title":"Singular value decomposition 2"},{"categories":["Machine Learning"],"contents":"papper link\ngithub link\ngithub link python version\nRegion Proposal Networks (RPNs) share convolutional layers with state-of-the-art object detection networks.\nmain contribution.\nusing region Proposal networks to make fast R-CNN faster.\nRegion Proposal Network is an a full connection neural network\nAttention mechanisms\n","date":"2021-04-26T19:21:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-28-faster-rcnn/","section":"posts","tags":["ML","RCNN"],"title":"Faster R-CNN paper notes"},{"categories":["Machine Learning"],"contents":"There is already a post about Faster R-CNN paper. Previous one is R-CNN note just for myself.\nIn this post I will explain the idea of Faster R-CNN and make it very easy to understand.\npaper link\nThe most idea alrady post on Fast R-CNN. The different is Faster R-CNN using RPN instead of selective search. The main idea of RPN is using sliding window and fast R-CNN sibling output layer to do region proposal.\n","date":"2021-04-21T20:00:00-04:00","permalink":"https://zongpitt.com/posts/2021-04-21-faster-r-cnn/","section":"posts","tags":["ML","RCNN"],"title":"Faster R-CNN"},{"categories":["Machine Learning"],"contents":"There is already a post about Fast R-CNN paper. Previous one is Fast R-CNN note just for myself.\nIn this post I will explain the idea of Fast R-CNN and make it very easy to understand.\npapper link\nThe idea of previous version of R-CNN is very simple. We can easily understand the idea. It is multiple stage network. It have very clear work flow for R-CNN. Proposed regions –\u0026gt; feed to CNN –\u0026gt; classify.\nIn fast R-CNN some trig is been introduced. First list the advantage or fast R-CNN.\nfast training end-to-end. any size of input image. higher accuracy. First take a glance of Fast R-CNN architecture.\nLet’s explain this architecture from left to right.\nAs same as R-CNN, the first step of Fast R-CNN is to propose about 2000 regions using selective search. Then feed the whole picture to the CNN feature extractor. The difference between R-CNN and Fast R-CNN is R-CNN only feeds regions to CNN feature extractor, while Fast R-CNN feeds the whole picture. The reason for the R-CNN only feed region is a fully connected layer or SVM requires fixed-size input. Fast R-CNN using RoI pooling to generate fixed output size. That’s why it can feed the whole image to the CNN feature extractor. Then Fast R-CNN map the region proposal from the original image to the output of the CNN feature extractor. Because of share compute when computing CNN features, fast R-CNN is much faster than origin R-CNN.\nThe very important thing to explain is the RoI pooling layer. How it produces fixed-size output. Actually, it is really easy. It just divides a picture using an SxS grid. (S can be any number here). Then do max pooling in each grid. The output of RoI pooling is an SxS pixel image.\nAfter the RoI pooling layer is fully connected layers. The trick is the last two sibling output layers.\nLet’s explain these two layers a little bit more. This is multiple tasks architecture. A network has two different outputs and these outputs can help each other to improve the network performance. Definitely, they can harm each other also. So a good loss function is important in this network.\nThe first output will determine the class, and the second output is the bounding box. The bounding box contains 4 values and it only makes sense when the classifier output is not the background. So the following loss function is used by the author.\n\\[ L = L_{cls} + \\lambda[u \\geq 1] L_{loc} \\]\n\\(\\) is a tunable parameter. \\(u \\) means only first part output as same as ground true, second part will add to whole loss function.\nThe total loss combine of two loss function. That’s the idea of multi-tasks loss function. The details will not explain here.\n","date":"2021-04-21T16:00:00-04:00","permalink":"https://zongpitt.com/posts/2021-04-21-fast-r-cnn/","section":"posts","tags":["ML","RCNN"],"title":"Fast R-CNN"},{"categories":["Machine Learning"],"contents":"There is already a post about R-CNN paper. Previous one is R-CNN note just for myself.\nIn this post I will explain the idea of R-CNN and make it very easy to understand.\npaper link\nObject detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVMs. The system contains 3 parts. The first part is the region proposal algorithm. R-CNN adopts selective search as its region proposal algorithm. I did not dig through this algorithm yet. The output of selective search is 2000 proposals. This proposal will be used later.\nThe second part is the CNN feature extractor. R-CNN using VGG-16 as a feature extractor. It only applies to 5 stages in VGG16. It does not contain a fully connected layer. The result from the last convolution layer is the image features.\nThe proposed region can be any size. But a fully connected layer or SVM requires fixed input. So wrap the region is required before feeding the region into the CNN feature extractor. After warping the region, the size of the input of CNN will be fixed. For VGG16 the dimension of the input image is 227x227. The author discusses several methods in the paper Appendix. I will not explain more here.\nThe output of the CNN features extractor is 4000 features for each region. The last part of the system is SVM classifier. As we know SVM generally works as a binary classifier. If we want it to become multiple classifiers, we need to build multiple binary classifiers. Then use all of these to determine the class. For example, we have an SVM classifier that can distinguish where the picture is aeroplane. We can use this classifier to tell where the region is aeroplane or not. So the final layer is SVMs as the classifier.\nUp to here, the basic idea has already been explained. The original paper introduces a lot of tricks and gives explanations why they did that.\n","date":"2021-04-21T16:00:00-04:00","permalink":"https://zongpitt.com/posts/2021-04-21-r-cnn/","section":"posts","tags":["ML","RCNN"],"title":"R-CNN"},{"categories":["Misc"],"contents":"from the stackoverflow[https://stackoverflow.com/questions/37435369/matplotlib-how-to-draw-a-rectangle-on-image]\nimport matplotlib.pyplot as plt import matplotlib.patches as patches from PIL import Image im = Image.open(\u0026#39;stinkbug.png\u0026#39;) # Create figure and axes fig, ax = plt.subplots() # Display the image ax.imshow(im) # Create a Rectangle patch rect = patches.Rectangle((50, 100), 40, 30, linewidth=1, edgecolor=\u0026#39;r\u0026#39;, facecolor=\u0026#39;none\u0026#39;) # Add the patch to the Axes ax.add_patch(rect) plt.show() ","date":"2021-04-18T13:28:00-04:00","permalink":"https://zongpitt.com/posts/2021-04-18-draw-a-rectangle-on-image/","section":"posts","tags":["matplot"],"title":"Draw a rectangle on image"},{"categories":["Machine Learning"],"contents":"read the post https://www.deeplearning.ai/ai-notes/initialization/index.html and https://cs230.stanford.edu/section/4/\nThis two post is enough to understand the Xavier initialization.\nThe Newer initialization is proposed by Kaiming He, A modified version of Xavier.\nHere is the link of the paper. https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html\nFor more information of initializer refer to TensorFlow document. https://www.tensorflow.org/api_docs/python/tf/keras/initializers\n","date":"2021-04-03T15:54:00-05:00","permalink":"https://zongpitt.com/posts/2021-04-03-initializing-neural-networks/","section":"posts","tags":["ML"],"title":"Initializing neural networks"},{"categories":["Machine Learning"],"contents":"Paper available from https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf and https://dl.acm.org/doi/10.5555/2627435.2670313\nThe idea of dropout is eliminate some node randomly and then training the network.\nInverted dropout technique makes the parameter remain same.\nhttps://machinelearning.wtf/terms/inverted-dropout/ https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\nIntuition: Can’t rely on any one feature, so have to spread out weights.\nDifferent dropout ratio for different layer base on the number of the neuron number will help to improve the accuracy.\nFollowing is copy from link https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/ the author is Paolo Galeone. The reason I copy to here is keep the resource available for me.\nAnalysis of Dropout Overfitting is a problem in Deep Neural Networks (DNN): the model learns to classify only the training set, adapting itself to the training examples instead of learning decision boundaries capable of classifying generic instances. Many solutions to the overfitting problem have been presented during these years; one of them have overwhelmed the others due to its simplicity and its empirical good results: Dropout.\nThe idea behind Dropout is to train an ensemble of DNNs and average the results of the whole ensemble instead of train a single DNN.\nThe DNNs are built dropping out neurons with \\(p\\) probability, therefore keeping the others on with probability \\(q=1−p\\). When a neuron is dropped out, its output is set to zero, no matter what the input or the associated learned parameter is.\nThe dropped neurons do not contribute to the training phase in both the forward and backward phases of back-propagation: for this reason every time a single neuron is dropped out it’s like the training phase is done on a new network.\nquoting the authors: \u0026gt; In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, Dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes.\nIn short: Dropout works well in practice because it prevents the co-adaption of neurons during the training phase.\nNow that we got an intuitive idea behind Dropout, let’s analyze it in depth.\nHow Dropout works As said before, Dropout turns off neurons with probability \\(p\\) and therefore let the others turned on with probability \\(q=1−p\\).\nEvery single neuron has the same probability of being turned off. This means that:\nGiven\n\\(h(x)=x W+b\\) a linear projection of a \\(d_{i}-\\) dimensional input \\(x\\) in a \\(d_{h}\\)-dimensional output space. \\(a(h)\\) an activation function. It’s possible to model the application of Dropout, in the training phase only, to the given projection as a modified activation function:\n\\[ f(h)=D \\odot a(h) \\]\nWhere \\(D=(X_{1}, , X_{d_{h}})\\) is a \\(d_{h}\\) -dimensional vector of Bernoulli variables \\(X_{i}\\).\nA Bernoulli random variable has the following probability mass distribution: \\[ f(k ; p)=\\left\\{\\begin{array}{ll} p \u0026amp; \\text { if } \\quad k=1 \\\\ 1-p \u0026amp; \\text { if } \\quad k=0 \\end{array}\\right. \\] Where \\(k\\) are the possible outcomes.\nIt’s evident that this random variable perfectly models the Dropout application on a single neuron. In fact, the neuron is turned off with probability \\(p=P(k=1)\\) and kept on otherwise.\nIt can be useful to see the application of Dropout on a generic \\(\\) -th neuron: \\[ O_{i}=X_{i} a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right)=\\left\\{\\begin{array}{ll} a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right) \u0026amp; \\text { if } \\quad X_{i}=1 \\\\ 0 \u0026amp; \\text { if } \\quad X_{i}=0 \\end{array}\\right. \\] where \\(P(X_{i}=0)=p\\). Since during train phase a neuron is kept on with probability \\(q\\), during the testing phase we have to emulate the behavior of the ensemble of networks used in the training phase.\nTo do this, the authors suggest scaling the activation function by a factor of q during the test phase in order to use the expected output produced in the training phase as the single output required in the test phase. Thus:\n\\[\\begin{array}{l} \\text { Train phase: } O_{i}=X_{i} a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right) \\\\ \\text { Test phase: } O_{i}=q a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right) \\end{array}\\] Inverted Dropout A slightly different approach is to use Inverted Dropout. This approach consists in the scaling of the activations during the training phase, leaving the test phase untouched.\n=, \\[\\begin{array}{l} \\text { Train phase: } O_{i}=\\frac{1}{q} X_{i} a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right) \\\\ \\text { Test phase: } O_{i}=a\\left(\\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\\right) \\end{array}\\] Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks because it helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model.\nDirect Dropout, instead, force you to modify the network during the test phase because if you don’t multiply by \\(q\\) the output the neuron will produce values that are higher respect to the one expected by the successive neurons (thus the following neurons can saturate or explode): that’s why Inverted Dropout is the more common implementation.\nDropout of a set of neurons It can be easily noticed that a layer h with n neurons, in a single train step, can be seen as an ensemble of \\(n\\) Bernoulli experiments, each one with a probability of success equals to \\(p\\).\nThus, the output of the layer \\(h\\) have a number of dropped neurons equals to:\n\\[ Y=\\sum_{i=1}^{d_{h}}\\left(1-X_{i}\\right) \\]\nSince every neuron is now modeled as a Bernoulli random variable and all these random variables are independent and identically distributed, the total number of dropped neuron is a random variable too, called Binomial:\n\\[ Y \\sim B i\\left(d_{h}, p\\right) \\]\nWhere the probability of getting exactly \\(k\\) success in \\(n\\) trials is given by the probability mass distribution:\n\\[ f(k ; n, p)=\\left(\\begin{array}{l} n \\\\ k \\end{array}\\right) p^{k}(1-p)^{n-k} \\]\nThis formula can be easily explained: - \\(p{k}(1-p){n-k}\\) is the probability of getting a single sequence of \\(k\\) successes on \\(n\\) trials and therefore \\(n-k\\) failures. \\(( \\[\\begin{array}{l}n \\\\ k\\end{array}\\] )\\) is the binomial coefficient used to calculate the number of possible sequence of success.\nWe can now use this distribution to analyze the probability of dropping a specified number of neurons.\nWhen using Dropout, we define a fixed Dropout probability \\(p\\) for a chosen layer and we expect that a proportional number of neurons are dropped from it.\nFor example, if the layer we apply Dropout to has \\(n=1024\\) neurons and \\(p=0.5,\\) we expect that 512 get dropped. Let’s verify this statement:\n\\[ \\begin{aligned} Y \u0026amp;=\\sum_{i=1}^{1024} X_{i} \\sim B i(1024,0.5) \\\\ P(Y=512) \u0026amp;=\\left(\\begin{array}{c} 1024 \\\\ 512 \\end{array}\\right) 0.5^{512}(1-0.5)^{1024-512} \\approx 0.025 \\end{aligned} \\]\nThus, the probability of dropping out exactly \\(n p=512\\) neurons is of only \\(0.025 !\\) A python 3 script can help us to visualize how neurons are dropped for different values of \\(p\\) and a fixed value of \\(n\\). The code is commented.\nimport matplotlib.pyplot as plt from scipy.stats import binom import numpy as np # number of neurons n = 1024 # number of tests (input examples) size = 500 # histogram bin width, for data visualization binwidth = 5 for p in range(1, 10): # per layer probability prob = p / 10 # generate size values from a bi(n, prob) rnd_values = binom.rvs(n, prob, size=size) # draw histogram of rnd values plt.hist( rnd_values, bins=[x for x in range(0, n, binwidth)], # normalize = extract the probabilities normed=1, # pick a random color color=np.random.rand(3, 1), # label the histogram with its probability label=str(prob)) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.show() As we can see from the image above no matter what the \\(p\\) value is, the number of neurons dropped on average is proportional to \\(n p\\), in fact: \\[ E[B i(n, p)]=n p \\] Moreover, we can notice that the distribution of values is almost symmetric around \\(p=0.5\\) and the probability of dropping \\(n p\\) neurons increase as the distance from \\(p=0.5\\) increase. The scaling factor has been added by the authors to compensate the activation values, because they expect that during the training phase only a percentage of \\(1-p\\) neurons have been kept. During the testing phase, instead, the \\(100 %\\) of neurons are kept on, thus the value should be scaled down accordingly.\nDropout \u0026amp; other regularizers Dropout is often used with L2 normalization and other parameter constraint techniques (such as Max Norm \\(\\), this is not a case. Normalizations help to keep model parameters value low, in this way a parameter can’t grow too much. In brief, the L2 normalization (for example) is an additional term to the loss, where \\(\\) is an hyper-parameter called regularization strength, \\(F(W ; x)\\) is the model and \\(\\) is the error function between the real \\(y\\) and the predicted \\(\\) value.\n\\[ \\mathcal{L}(y, \\hat{y})=\\mathcal{E}(y, F(W ; x))+\\frac{\\lambda}{2} W^{2} \\]\nIt’s easy to understand that this additional term, when doing back-propagation via gradient descent, reduces the update amount. If \\(\\) is the learning rate, the update amount of the parameter \\(w W\\) is\n\\[ w \\leftarrow w-\\eta\\left(\\frac{\\partial F(W ; x)}{\\partial w}+\\lambda w\\right) \\]\nDropout alone, instead, does not have any way to prevent parameter values from becoming too large during this update phase. Moreover, the inverted implementation leads the update steps to become bigger, as showed below.\nInverted Dropout and other regularizers Since Dropout does not prevent parameters from growing and overwhelming each other, applying L2 regularization (or any other regularization technique that constraints the parameter values) can help. Making explicit the scaling factor, the previous equation becomes:\n\\[ w \\leftarrow w-\\eta\\left(\\frac{1}{q} \\frac{\\partial F(W ; x)}{\\partial w}+\\lambda w\\right) \\]\nIt can be easily seen that when using Inverted Dropout, the learning rate is scaled by a factor of \\(q\\). Since \\(q\\) has values in \\(] 0,1]\\) the ratio between \\(\\) and \\(q\\) can vary between:\n\\[ r(q)=\\frac{\\eta}{q} \\in\\left[\\eta=\\lim _{q \\rightarrow 1} r(q),+\\infty=\\lim _{q \\rightarrow 0} r(q)\\right] \\]\nFor this reason, from now on we’ll call \\(q\\) boosting factor because it boosts the learning rate. Moreover, we’ll call \\(r(q)\\) the effective learning rate. The effective learning rate, thus, is higher respect to the learning rate chosen: for this reason normalizations that constrain the parameter values can help to simplify the learning rate selection process.\nSummary Dropout exists in two versions: direct (not commonly implemented) and inverted Dropout on a single neuron can be modeled using a Bernoulli random variable Dropout on a set of neurons can be modeled using a Binomial random variable Even if the probability of dropping exactly \\(n p\\) neurons is low, \\(n p\\) neurons are dropped on average on a layer of \\(n\\) neurons. Inverted Dropout boost the learning rate Inverted Dropout should be using together with other normalization techniques that constrain the parameter values in order to simplify the learning rate selection procedure Dropout helps to prevent overfitting in deep neural networks. Max Norm impose a constraint to the parameters size. Chosen a value for the hyper-parameter \\(c\\) it impose the constraint \\(|w| c . \\). ","date":"2021-04-02T19:06:00-05:00","permalink":"https://zongpitt.com/posts/2021-04-02-dropout-regularization/","section":"posts","tags":["ML"],"title":"Dropout Regularization"},{"categories":["Machine Learning"],"contents":"If we treated the growth function as an effective number of hypotheses, and replaced \\(M\\) in the generalization bound (2.1) with \\(m_{}(N),\\) the resulting bound would be\n\\[ E_{\\mathrm{out}}(g) \\stackrel{?}{\\leq} E_{\\mathrm{in}}(g)+\\sqrt{\\frac{1}{2 N} \\ln \\frac{2 m_{\\mathcal{H}}(N)}{\\delta}} \\]\nIt turns out that this is not exactly the form that will hold. The quantities in red need to be technically modified to make (2.11) true. The correct bound, which is called the VC generalization bound, is given in the following theorem; it holds for any binary target function \\(f,\\) any hypothesis set \\(,\\) any learning algorithm \\(,\\) and any input probability distribution \\(P\\).\nTheorem 2.5 (VC generalization bound). For any tolerance \\(\u0026gt;0\\),\n\\[ E_{\\text {out }}(g) \\leq E_{\\text {in }}(g)+\\sqrt{\\frac{8}{N} \\ln \\frac{4 m_{\\mathcal{H}}(2 N)}{\\delta}} \\]\nwith probability \\(-\\).\nSample Complexity The sample complexity denotes how many training examples \\(N\\) are needed to achieve a certain generalization performance. The performance is specified by two parameters, \\(\\) and \\(.\\) The error tolerance \\(\\) determines the allowed generalization error, and the confidence parameter \\(\\) determines how of ten the error tolerance \\(\\) is violated. How fast \\(N\\) grows as \\(\\) and \\(\\) become smaller \\(^{4}\\) indicates how much data is needed to get good generalization.\nWe can use the VC bound to estimate the sample complexity for a given learning model. Fix \\(\u0026gt;0,\\) and suppose we want the generalization error to be at most \\(\\). From Equation (2.12), the generalization error is bounded by \\(,\\) and so it suffices to make \\( .\\) It follows that\n\\[ N \\geq \\frac{8}{\\epsilon^{2}} \\ln \\left(\\frac{4 m_{\\mathcal{H}}(2 N)}{\\delta}\\right) \\]\nsuffices to obtain generalization error at most \\(\\) (with probability at least \\(1-)\\). This gives an implicit bound for the sample complexity \\(N,\\) since \\(N\\) appears on both sides of the inequality. If we replace \\(m_{}(2 N)\\) in (2.12) by its polynomial upper bound in (2.10) which is based on the the VC dimension, we get a similar bound\n\\[ N \\geq \\frac{8}{\\epsilon^{2}} \\ln \\left(\\frac{4\\left((2 N)^{d_{\\mathrm{vc}}}+1\\right)}{\\delta}\\right) \\]\nwhich is again implicit in \\(N\\). We can obtain a numerical value for \\(N\\) using simple iterative methods.\nPenalty for Model Complexity Sample complexity fixes the performance parameters \\(\\) (generalization error) and \\(\\) (confidence parameter) and estimates how many examples \\(N\\) are needed. In most practical situations, however, we are given a fixed data set \\(,\\) so \\(N\\) is also fixed. In this case, the relevant question is what performance can we expect given this particular \\(N\\). The bound in (2.12) answers this question: with probability at least \\(1-\\),\n\\[ E_{\\mathrm{out}}(g) \\leq E_{\\mathrm{in}}(g)+\\sqrt{\\frac{8}{N} \\ln \\left(\\frac{4 m_{\\mathcal{H}}(2 N)}{\\delta}\\right)} \\]\nIf we use the polynomial bound based on \\(d_{}\\) instead of \\(m_{7 t}(2 N),\\) we get another valid bound on the out-of-sample error,\n\\[ E_{\\mathrm{out}}(g) \\leq E_{\\mathrm{in}}(g)+\\sqrt{\\frac{8}{N} \\ln \\left(\\frac{4\\left((2 N)^{d_{\\mathrm{vc}}+1}\\right)}{\\delta}\\right)} \\]\n","date":"2021-03-04T12:46:00-05:00","permalink":"https://zongpitt.com/posts/2021-03-04-vc-generaliztion-bound/","section":"posts","tags":["ML"],"title":"VC generalization bound"},{"categories":["Misc"],"contents":"using softwareopengl to avoid frozen when using matlab\nmatlab -softwareopengl ","date":"2021-03-03T10:44:00-05:00","permalink":"https://zongpitt.com/posts/2021-03-03-linux-matlab-avoid-frozen/","section":"posts","tags":["Linux","Matlab"],"title":"Linux Matlab Avoid Frozen"},{"categories":["Machine Learning"],"contents":"papper link\ngithub link\nsummary Basic idea is get CNN feature one time instead 2000 times.\nIn R-CNN it will propose 2000 region, then do classification using CNN.\nIt will run about 2000 times CNN. This is very slow proceesion. So in fast R-CNN it only run one time CNN and map the region to feature at the same time. Finally put these region into classification network.\nThere is a lot detail different between R-CNN and fast R-CNN. Just a summary here and following is the key point in the fast R-CNN paper.\npaper notes Introduction summary of R-CNN \u0026gt; Training is a multi-stage pipeline. R-CNN first finetunes a ConvNet on object proposals using log loss. Then, it fits SVMs to ConvNet features. These SVMs act as object detectors, replacing the softmax classifier learnt by fine-tuning. In the third training stage, bounding-box regressors are learned.\nProblem of R-CNN (Training is expensive and object detection is slow) \u0026gt; Training is expensive in space and time. For SVM and bounding-box regressor training, features are extracted from each object proposal in each image and written to disk. With very deep networks, such as VGG16, this process takes 2.5 GPU-days for the $5\nObject detection is slow. At test-time, features are extracted from each object proposal in each test image. Detection with VGG16 takes \\(47 \\mathrm{~s}\\) / image (on a GPU).\nWHY R-CNN so slow \u0026gt; R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation.\nguangzong comments\nThe differnet betweent between SPPnet and fast R-CNN is SPPnet have spatial pyramid pooling, but fast R-CNN jave RoI poolinglayer.\nBoth of these is trying to solve full connection layer require same size of input.\nend of comments\nFast R-CNN architecture and training.\nFig. 1 illustrates the Fast R-CNN architecture. A Fast R-CNN network takes as input an entire image and a set of object proposals. The network first processes the whole image with several convolutional ( conv) and max pooling layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected (fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over \\(K\\) object classes plus a catch-all “background” class and another layer that outputs four real-valued numbers for each of the \\(K\\) object classes. Each set of 4 values encodes refined bounding-box positions for one of the \\(K\\) classes.\nmost important part\nThe idea just divide RoI to serveral pieces (consttant number) and do max poolling in these pieces.\nThe RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of \\(H \\times W(e . g ., 7 \\times 7)\\) where \\(H\\) and \\(W\\) are layer hyper-parameters that are independent of any particular RoI. In this paper, an RoI is a rectangular window into a conv feature map. Each RoI is defined by a four-tuple \\((r, c, h, w)\\) that specifies its top-left corner \\((r, c)\\) and its height and width \\((h, w)\\).\nThe RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets in which there is only one pyramid level. We use the pooling sub-window calculation given in 11.\nrest of paper are details of implementation and discussion read the paper to understand author’s idea\n2.2 Initializing from pre-trained networks\n2.3 Fine-tuning for detection\nWe propose a more efficient training method that takes advantage of feature sharing during training. In Fast RCNN training, stochastic gradient descent (SGD) minibatches are sampled hierarchically, first by sampling \\(N\\) images and then by sampling \\(R / N\\) RoIs from each image.\nMulti-task loss\nrest should read from paper\nMini-batch sampling\nBack-propagation through RoI pooling layers.\nSGF hyper-parameters\nScale invariance\nTruncated SVD for faster detection\n","date":"2021-02-28T15:03:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-28-fast-rcnn/","section":"posts","tags":["ML","RCNN"],"title":"Fast R-CNN paper notes"},{"categories":["Misc"],"contents":"create file \u0026lt;service_name\u0026gt;.service at /etc/systemd/system/multi-user.target.wants/\nExample file\n[Unit] Description=unmount network disks before shutdown [Service] Type=oneshot RemainAfterExit=true ExecStop=/usr/local/bin/un_mount_script.sh [Install] WantedBy=multi-user.target then do systemctl enable \u0026lt;service_name\u0026gt; --now\n","date":"2021-02-28T14:27:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-28-linux-shutdown-execute-script/","section":"posts","tags":["Linux"],"title":"Linux shutdown execute script"},{"categories":["Machine Learning"],"contents":"excellent material from Andrew Ng\nhttps://github.com/chen-gz/picBed/blob/master/PCA.pdf\nI add intuition here.\nThe intuition of PCA for me comes from linear algebra’s base. (Orthogonal unit vector) form a vector space. All vectors can form from these vectors. Some bases only provide little information, so they can be ignored.\nPCA will keep the features with maximum variance.\nProcedure Normalization the data There are \\(N\\) samples, each sample is \\(x^i \\in \\mathbb{R}^n\\).\nCalculate mean value. \\(\\mu = \\sum_{i=0}^N x^i\\) Remove bias. For \\(i\\) from \\(1\\) to \\(N\\), \\(x^i \\leftarrow x^i - \\mu\\) Calculate Variance for all features. \\(\\sigma^2_j = \\frac{1}{m} \\sum_{i}\\left(x_{j}^{(i)}\\right)^{2}\\) Unit Variance. \\(x_j^i \\leftarrow x_{j}^{(i)} / \\sigma_{j}\\) step 4 should calculate for each coordinate.\nPCA Calculate Covariance Matrix. \\(\\Sigma = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} x^{(i)^{T}}\\)\nFind eigenvalue and eigen vector for \\(\\Sigma\\).\nSort eigenvalue.\\(\\lambda_1 \u0026gt; \\lambda_2 \u0026gt; \\dots \u0026gt; \\lambda_n\\), correspond eigenvector \\(u_{1}, \\dots, u_{k}\\) which satisfy \\(\\|u_i\\| = 1\\).\nThe vector \\(u_{1}, \\dots, u_{k}\\) are called the first k principal components of the data.\nDimension reduction. Determine the reduced dimension \\(K\\).\nRepresent data in \\(\\mathbb{R}^k\\).\n\\[ y^{(i)}=\\left[\\begin{array}{c} u_{1}^{T} x^{(i)} \\\\ u_{2}^{T} x^{(i)} \\\\ \\vdots \\\\ u_{k}^{T} x^{(i)} \\end{array}\\right] \\in \\mathbb{R}^{k} \\]\nWe can put \\(u\\) together to form a matrix \\(W\\). Then we can represent our equation as \\(y^i = Wx^i\\).\nWhere \\(W = \\begin{bmatrix}u_1 \u0026amp; \\dots \u0026amp; u_k\\end{bmatrix}^T\\). The dimension of \\(W\\) id \\(k\\times n\\).\nAnd using \\(\\hat{x} = W^T\\)\nExample for PCA # data dimension is not as same as document # need to be fix import numpy as np import matplotlib.pyplot as plt # generate random data points aline y = x + 1 x = np.random.rand(10, 2) # Doing PCA base on these data points. # normalize x x = x - np.mean(x, axis=0) x = x / np.std(x, axis=0) plt.figure(figsize=(2, 1)) plt.subplot(2, 1, 1) plt.xlim([-2, 2]) plt.ylim([-2, 2]) plt.plot(x[:, 0], x[:, 1], \u0026#39;o\u0026#39;) plt.title(\u0026#39;Original Data\u0026#39;) # 1. calculate the covariance matrix cov = np.cov(x.T) print(cov) # 2. calculate the eigenvalues and eigenvectors eigvals, eigvecs = np.linalg.eig(cov) # 3. sort the eigenvalues and eigenvectors idx = eigvals.argsort()[::-1] egivals = eigvals[idx] eigvecs = eigvecs[:,idx] # print(f\u0026quot;eigvals: {egivals}\u0026quot;) # print(f\u0026quot;eigvec: {eigvecs}\u0026quot;) # 4. calculate the principal components W = eigvecs[:, 0].T W.resize(1, 2) print(f\u0026quot;W: {W}\u0026quot;) print(f\u0026quot;x: {x}\u0026quot;) print(f\u0026quot;x.shape: {x.shape}\u0026quot;) print(f\u0026quot;W.shape: {W.shape}\u0026quot;) y = W.dot(x.T) # expand y from 1 dimension to 2 dimension # y = np.expand_dims(y, axis=1) print(f\u0026quot;y.shape: {y.shape}\u0026quot;) rec_x = W.T.dot(y) print(f\u0026quot;rec_x.shape: {rec_x.shape}\u0026quot;) # plot rec_x plt.subplot(2, 1, 2) plt.xlim([-2, 2]) plt.ylim([-2, 2]) plt.plot(rec_x[0,:], rec_x[1,:], \u0026#39;o\u0026#39;) plt.title(\u0026#39;Reconstructed Data\u0026#39;) plt.show() Probability PCA PCA is a special case of pPCA. pPCA suppose there is some noise in the data. Then present a probability model to handle the noise. There is an assumption. The noise follows the Gaussian distribution.\nWe will use different terminology in the following discussion.\n\\(y^i \\in \\mathbb{R}^d\\) is the observed variable. \\(z^i \\in \\mathbb{R}^q\\) is the latent variable. \\(q \u0026lt; d\\).\nFrom factor analysis, the latent variable and observed variable have the following relationship.\nThe observed variable \\(y \\sim Wx + \\mu + \\epsilon\\) . The latent variables \\(z \\sim N(0, I)\\). Noise is \\(\\epsilon \\sim N(0, \\Phi)\\). Location term (mean) is \\(\\mu\\).\nIn Probabilistic PCA: Noise variances contrained to be equal (\\(\\Phi = \\sigma^2\\)). Error: \\(\\epsilon \\sim N(0, \\Phi I)\\). Then \\(y|x \\sim N(Wx + \\mu + \\epsilon, \\Phi I)\\). and \\(y \\sim N(\\mu , C_y)\\), where \\(C_y = WW^T + \\sigma^2 I\\) (where \\(C_y\\) is the covariance matrix of the observed data \\(y\\)).\n","date":"2021-02-26T13:50:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-26-principal-component-analysis/","section":"posts","tags":["ML","PCA"],"title":"Principal component analysis"},{"categories":["Misc"],"contents":"set -x PATH $PATH /home/zong/.gem/ruby/2.7.0/bin ","date":"2021-02-26T09:45:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-26-fish-add-path/","section":"posts","tags":["Linux","fish"],"title":"Fish Add Path"},{"categories":["Misc"],"contents":" ","date":"2021-02-26T09:21:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-26-xterm-settings/","section":"posts","tags":["Linux","xterm"],"title":"Xterm settings"},{"categories":["Misc"],"contents":"\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;redirecting\u0026gt;\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script language=\u0026#39;javascript\u0026#39;\u0026gt;document.location = \u0026#39;https://giogio.club\u0026#39;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","date":"2021-02-24T22:17:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-24-html-redirecting/","section":"posts","tags":["Html"],"title":"Html redirecting"},{"categories":["Machine Learning"],"contents":"The neural network should initialization with very small and random value.\nThere are two reason.\nif initialization with same value, the same layer neural will have some value no matter how much iteration. because of sigmoid function, it is not sensitive when value is large. So neural should be initialized with small value. ","date":"2021-02-24T15:17:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-24-neural-network-random-initialization/","section":"posts","tags":["ML","NN"],"title":"Neural Network Random Initialization"},{"categories":["Misc"],"contents":"the content is from following link\nhttps://www.geeksforgeeks.org/android-how-to-request-permissions-in-android-application/\npackage org.geeksforgeeks.requestPermission; import android.Manifest; import android.content.pm.PackageManager; import android.support.annotation.NonNull; import android.support.v4.app.ActivityCompat; import android.support.v4.content.ContextCompat; import android.support.v7.app.AppCompatActivity; import android.os.Bundle; import android.view.View; import android.widget.Button; import android.widget.Toast; public class MainActivity extends AppCompatActivity { // Defining Buttons private Button storage, camera; // Defining Permission codes. // We can give any value // but unique for each permission. private static final int CAMERA_PERMISSION_CODE = 100; private static final int STORAGE_PERMISSION_CODE = 101; @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); storage = findViewById(R.id.storage); camera = findViewById(R.id.camera); // Set Buttons on Click Listeners storage.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { checkPermission( Manifest.permission.WRITE_EXTERNAL_STORAGE, STORAGE_PERMISSION_CODE); } }); camera.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { checkPermission(Manifest.permission.CAMERA, CAMERA_PERMISSION_CODE); } }); } // Function to check and request permission. public void checkPermission(String permission, int requestCode) { if (ContextCompat.checkSelfPermission(MainActivity.this, permission) == PackageManager.PERMISSION_DENIED) { // Requesting the permission ActivityCompat.requestPermissions(MainActivity.this, new String[] { permission }, requestCode); } else { Toast.makeText(MainActivity.this, \u0026quot;Permission already granted\u0026quot;, Toast.LENGTH_SHORT) .show(); } } // This function is called when the user accepts or decline the permission. // Request Code is used to check which permission called this function. // This request code is provided when the user is prompt for permission. @Override public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) { super .onRequestPermissionsResult(requestCode, permissions, grantResults); if (requestCode == CAMERA_PERMISSION_CODE) { if (grantResults.length \u0026gt; 0 \u0026amp;\u0026amp; grantResults[0] == PackageManager.PERMISSION_GRANTED) { Toast.makeText(MainActivity.this, \u0026quot;Camera Permission Granted\u0026quot;, Toast.LENGTH_SHORT) .show(); } else { Toast.makeText(MainActivity.this, \u0026quot;Camera Permission Denied\u0026quot;, Toast.LENGTH_SHORT) .show(); } } else if (requestCode == STORAGE_PERMISSION_CODE) { if (grantResults.length \u0026gt; 0 \u0026amp;\u0026amp; grantResults[0] == PackageManager.PERMISSION_GRANTED) { Toast.makeText(MainActivity.this, \u0026quot;Storage Permission Granted\u0026quot;, Toast.LENGTH_SHORT) .show(); } else { Toast.makeText(MainActivity.this, \u0026quot;Storage Permission Denied\u0026quot;, Toast.LENGTH_SHORT) .show(); } } } } ","date":"2021-02-24T12:00:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-24-android-permission/","section":"posts","tags":["Java","Android"],"title":"Android Permission"},{"categories":["Misc"],"contents":"java get time from system\nimport java.time.format.DateTimeFormatter; import java.time.LocalDateTime; public class CurrentDateTimeExample1 { public static void main(String[] args) { DateTimeFormatter dtf = DateTimeFormatter.ofPattern(\u0026quot;yyyy/MM/dd HH:mm:ss\u0026quot;); LocalDateTime now = LocalDateTime.now(); System.out.println(dtf.format(now)); } } ","date":"2021-02-22T20:14:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-22-java-date-and-time/","section":"posts","tags":["Java"],"title":"Java Date and Time"},{"categories":["Misc"],"contents":"Test file file exist by exists() method\nFile tempFile = new File(\u0026quot;c:/temp/temp.txt\u0026quot;); boolean exists = tempFile.exists(); java read file by Scanner (code from w3school)\nimport java.io.File; // Import the File class import java.io.FileNotFoundException; // Import this class to handle errors import java.util.Scanner; // Import the Scanner class to read text files public class ReadFile { public static void main(String[] args) { try { File myObj = new File(\u0026quot;filename.txt\u0026quot;); Scanner myReader = new Scanner(myObj); while (myReader.hasNextLine()) { String data = myReader.nextLine(); System.out.println(data); } myReader.close(); } catch (FileNotFoundException e) { System.out.println(\u0026quot;An error occurred.\u0026quot;); e.printStackTrace(); } } } java write file\nimport java.io.FileWriter; // Import the FileWriter class import java.io.IOException; // Import the IOException class to handle errors public class WriteToFile { public static void main(String[] args) { try { FileWriter myWriter = new FileWriter(\u0026quot;filename.txt\u0026quot;); myWriter.write(\u0026quot;Files in Java might be tricky, but it is fun enough!\u0026quot;); myWriter.close(); System.out.println(\u0026quot;Successfully wrote to the file.\u0026quot;); } catch (IOException e) { System.out.println(\u0026quot;An error occurred.\u0026quot;); e.printStackTrace(); } } } java append content to a file\npublic static void usingPath() throws IOException { String textToAppend = \u0026quot;\\r\\n Happy Learning !!\u0026quot;; //new line in content Path path = Paths.get(\u0026quot;c:/temp/samplefile.txt\u0026quot;); Files.write(path, textToAppend.getBytes(), StandardOpenOption.APPEND); //Append mode } or\npublic static void usingBufferedWritter() throws IOException { String textToAppend = \u0026quot;Happy Learning !!\u0026quot;; //Set true for append mode BufferedWriter writer = new BufferedWriter( new FileWriter(\u0026quot;c:/temp/samplefile.txt\u0026quot;, true)); writer.write(textToAppend); writer.close(); } ","date":"2021-02-22T18:54:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-22-java-file/","section":"posts","tags":["Java"],"title":"Java file"},{"categories":["Machine Learning"],"contents":"This material is from learning from data.\n","date":"2021-02-20T22:07:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-20-growth-function/","section":"posts","tags":["ML"],"title":"Growth Function"},{"categories":["Math"],"contents":"This post is a notation for myself.\nline function\n\\[ ax + by + c = 0 \\]\nvector (a,b) is normal vector for this line, c is the bias for this line.\nand more we can regard it as a plane.\n\\[ ax + by + cz = d \\] where z = 1 and d = 0.\nvector (a, b, c) is normal vector for the plane.\nwe can calculate distance between point and line easily by following equation.\n\\[ \\frac{|ax_1 + by_1 + c|}{\\|(a, b)\\|} \\]\nthis can conduct from a simple graph by using projection.\nI found following link might be useful it reader knowing nothing about liner function.\nhttp://sites.math.washington.edu/~king/coursedir/m445w04/notes/vector/normals-planes.html\n","date":"2021-02-20T21:56:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-20-linear-function/","section":"posts","tags":["Math"],"title":"linear function"},{"categories":["Machine Learning"],"contents":"paper link\nObject detection system overview Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVMs.\nGeneral Idea and Reason (what and why) extent CNN to object detection \u0026gt; To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?\nsliding window for face and pedestrians detection but not for object detection (RCNN) \u0026gt; An alternative is to build a sliding-window detector. CNNs have been used in this way for at least two decades, typically on constrained object cat- egories, such as faces [32, 40] and pedestrians [35]. In order to maintain high spatial resolution, these CNNs typically only have two convolutional and pooling layers. We also considered adopting a sliding-window approach. However, units high up in our network, which has five convolutional layers, have very large receptive fields (195 × 195 pixels) and strides (32×32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge.\nrecognition using regions (propose 2000 region) \u0026gt; Instead, we solve the CNN localization problem by oper- ating within the “recognition using regions” paradigm [21], which has been successful for both object detection [39] and semantic segmentation [5].\nreshpae the region then put into linear SVMs and why call it R-CNN \u0026gt; with category-specific linear SVMs. We use a simple tech- nique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region’s shape. Figure 1 presents an overview of our method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN features.\nlabel data scare, huge CNN training dataset and scare detection training data \u0026gt; A second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by su- pervised fine-tuning (e.g., [35]). The second principle con- tribution of this paper is to show that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain- specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. In our experiments, fine-tuning for detection improves mAP performance by 8 percentage points. After\nDetail Feature extraction and CNN structure \u0026gt;We extract a 4096-dimensional fea- ture vector from each region proposal using the Caffe [24] implementation of the CNN described by Krizhevsky et al. [25]. Features are computed by forward propagating a mean-subtracted 227 × 227 RGB image through five con- volutional layers and two fully connected layers. We refer readers to [24, 25] for more network architecture details.\nwrapping the proposed region \u0026gt; Prior to warping, we dilate the tight bounding box so that at the warped size there are ex- actly p pixels of warped image context around the original box (we use p = 16). The Details are in Appendix A.\nCNN \u0026gt; training by ILSVRC2012 classification suing image-level annotations only.\nfine tunning (SGD = 0.001 1/10th of te initail pre-training rate) \u0026gt; Domain-specific fine-tuning. To adapt our CNN to the new task (detection) and the new domain (warped proposal windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals. Aside from replacing the CNN’s ImageNet- specific 1000-way classification layer with a randomly ini- tialized (N + 1)-way classification layer (where N is the number of object classes, plus 1 for background), the CNN architecture is unchanged. For VOC, N = 20 and for ILSVRC2013, N = 200.\noverlap (IoU threshold is important!)\nLess clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives. The overlap threshold, 0.3, was selected by a grid search over {0, 0.1, . . . , 0.5} on a validation set. We found that selecting this threshold care- fully is important. Setting it to 0.5, as in [39], decreased mAP by 5 points. Similarly, setting it to 0 decreased mAP\nabbreviations\nmAP mean Average Precision\nSIFT Scale-invariant feature transform (2004) D. Lowe.Distinctive image features from scale-invariantkeypoints. IJCV, 2004\nHOG Histogram of oriented gradients\nIoU intersection over Union\nTODO - [ ] Scale-invariant feature transform. D. Lowe.Distinctive image features from scale-invariantkeypoints. IJCV, 2004\n- [ ] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1 - [ ] Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position - [ ] C. Gu, J. J. Lim, P. Arbel´aez, and J. Malik. Recognition using regions. In CVPR, 2009. 2 - [ ] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, 2013. 2\n","date":"2021-02-18T19:49:00-05:00","permalink":"https://zongpitt.com/posts/2021-02-18-r-cnn-paper-notes/","section":"posts","tags":["ML","RCNN"],"title":"R-CNN paper notes"},{"categories":["Misc"],"contents":"iframe can use include other html.\n\u0026lt;iframe src=\u0026quot;souce_path\u0026quot; width=\u0026quot;100%\u0026quot; height=\u0026quot;700px\u0026quot;\u0026gt;\u0026lt;/iframe\u0026gt; ","date":"2021-02-16T14:00:52-05:00","permalink":"https://zongpitt.com/posts/2021-02-16-iframe-include-html/","section":"posts","tags":["Html"],"title":"Include Other HTML File by iframe"},{"categories":["Embedded System"],"contents":"This code is not written by me. It is copy from website.\nI put it here just for easy to find.\nI change a little bit of this code to make it work. (It post 6 years ago, when i found it, it doesn’t work.)\n/* * This is a simple example to communicate with a CDC-ACM USB device * using libusb. */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;libusb-1.0/libusb.h\u0026gt; /* You may want to change the VENDOR_ID and PRODUCT_ID * depending on your device. */ #define VENDOR_ID 1155 #define PRODUCT_ID 22336 #define ACM_CTRL_DTR 0x01 #define ACM_CTRL_RTS 0x02 /* We use a global variable to keep the device handle */ static struct libusb_device_handle *devh = NULL; /* The Endpoint address are hard coded. You should use lsusb -v to find * the values corresponding to your device. */ static int ep_in_addr = 0x83; static int ep_out_addr = 0x02; void write_char(unsigned char c) { /* To send a char to the device simply initiate a bulk_transfer to the * Endpoint with address ep_out_addr. */ int actual_length; if (libusb_bulk_transfer(devh, ep_out_addr, \u0026amp;c, 1, \u0026amp;actual_length, 0) \u0026lt; 0) { fprintf(stderr, \u0026quot;Error while sending char\\n\u0026quot;); } } int read_chars(unsigned char * data, int size) { /* To receive characters from the device initiate a bulk_transfer to the * Endpoint with address ep_in_addr. */ int actual_length; int rc = libusb_bulk_transfer(devh, ep_in_addr, data, size, \u0026amp;actual_length, 1000); if (rc == LIBUSB_ERROR_TIMEOUT) { printf(\u0026quot;timeout (%d)\\n\u0026quot;, actual_length); return -1; } else if (rc \u0026lt; 0) { fprintf(stderr, \u0026quot;Error while waiting for char\\n\u0026quot;); return -1; } return actual_length; } int main(int argc, char **argv) { printf(\u0026quot;hello world!2\\n\u0026quot;); int rc; /* Initialize libusb */ rc = libusb_init(NULL); if (rc \u0026lt; 0) { fprintf(stderr, \u0026quot;Error initializing libusb: %s\\n\u0026quot;, libusb_error_name(rc)); exit(1); } /* Set debugging output to max level. */ libusb_set_debug(NULL, 3); /* Look for a specific device and open it. */ devh = libusb_ope_device_with_vid_pid(NULL, VENDOR_ID, PRODUCT_ID); if (!devh) { fprintf(stderr, \u0026quot;Error finding USB device\\n\u0026quot;); if (devh) libusb_close(devh); libusb_exit(NULL); } /* As we are dealing with a CDC-ACM device, it\u0026#39;s highly probable that * Linux already attached the cdc-acm driver to this device. * We need to detach the drivers from all the USB interfaces. The CDC-ACM * Class defines two interfaces: the Control interface and the * Data interface. */ for (int if_num = 0; if_num \u0026lt; 2; if_num++) { if (libusb_kernel_driver_active(devh, if_num)) { libusb_detach_kernel_driver(devh, if_num); } rc = libusb_claim_interface(devh, if_num); if (rc \u0026lt; 0) { fprintf(stderr, \u0026quot;Error claiming interface: %s\\n\u0026quot;, libusb_error_name(rc)); if (devh) libusb_close(devh); libusb_exit(NULL); } } /* Start configuring the device: * - set line state */ rc = libusb_control_transfer(devh, 0x21, 0x22, ACM_CTRL_DTR | ACM_CTRL_RTS, 0, NULL, 0, 0); if (rc \u0026lt; 0) { fprintf(stderr, \u0026quot;Error during control transfer: %s\\n\u0026quot;, libusb_error_name(rc)); } /* - set line encoding: here 9600 8N1 * 9600 = 0x2580 ~\u0026gt; 0x80, 0x25 in little endian */ unsigned char encoding[] = { 0x80, 0x25, 0x00, 0x00, 0x00, 0x00, 0x08 }; rc = libusb_control_transfer(devh, 0x21, 0x20, 0, 0, encoding, sizeof(encoding), 0); if (rc \u0026lt; 0) { fprintf(stderr, \u0026quot;Error during control transfer: %s\\n\u0026quot;, libusb_error_name(rc)); } /* We can now start sending or receiving data to the device */ unsigned char buf[65]; int len; printf(\u0026quot;hello world!2\\n\u0026quot;); while(1) { // write_char(\u0026#39;t\u0026#39;); len = read_chars(buf, 64); // buf[len] = 0; fprintf(stdout, \u0026quot;Received: \\\u0026quot;%s\\\u0026quot;\\n\u0026quot;, buf); sleep(1); } libusb_release_interface(devh, 0); return rc; } ","date":"2021-02-15T23:08:52-05:00","permalink":"https://zongpitt.com/posts/2021-02-15-usb-cdc-linux/","section":"posts","tags":["USB","CDC"],"title":"USB Communications Device Class"},{"categories":["Machine Learning"],"contents":" my Matlab implementation\nclc; clear all; load(\u0026#39;/home/zong/Downloads/dataLDA-1/dataLDA/trainTrain.mat\u0026#39;) load(\u0026#39;/home/zong/Downloads/dataLDA-1/dataLDA/testLDA.mat\u0026#39;) % X and Y here is training data Y = Ytrain; X = Xtrain; [featrue_size, sample] = size(Xtrain); pi_0 = length(find(Y==0))/size(Y,2); pi_1 = length(find(Y==1))/size(Y,2); % mean u_0 = mean(X(:,find(Y==0)),2); u_1 = mean(X(:,find(Y==1)),2); % covariance by my code (psudo, not exactly convariance) covv = ((X(:,find(Y==0)) - u_0)*(X(:,find(Y==0)) - u_0)\u0026#39; + (X(:,find(Y==1)) - u_1)*(X(:,find(Y==1)) - u_1)\u0026#39;)/200; a = inv(covv) * (u_0 - u_1); b = -0.5*u_0\u0026#39;*inv(covv)*u_0 + 0.5*u_1\u0026#39;*inv(covv)*u_1 + log(pi_0/pi_1); % predict result classifierOutput = a\u0026#39; * Xtest + b; classifierOutput(find(classifierOutput\u0026gt;0)) = 0; classifierOutput(find(classifierOutput\u0026lt;0)) = 1; classperf(Ytest,classifierOutput) %replace covariance 2!!!!!!!!!!!!!!!!!!!!!!!!!!!! covv = trace(covv) / featrue_size * eye(featrue_size); a = inv(covv) * (u_0 - u_1); b = -0.5*u_0\u0026#39;*inv(covv)*u_0 + 0.5*u_1\u0026#39;*inv(covv)*u_1 + log(pi_0/pi_1); disp(\u0026quot; \u0026quot;) % predict result classifierOutput = a\u0026#39; * Xtest + b; classifierOutput(find(classifierOutput\u0026gt;0)) = 0; classifierOutput(find(classifierOutput\u0026lt;0)) = 1; classperf(Ytest,classifierOutput) Materials from Dr. Can\n","date":"2021-02-10T05:12:52-05:00","permalink":"https://zongpitt.com/posts/2021-02-10-linear-discriminant-analysis/","section":"posts","tags":["LDA","ML"],"title":"Linear Discriminant Analysis"},{"categories":["Linux"],"contents":"folloing scrip is used to monitor ag it folder by using inotify-tool\n#!/bin/bash src=\u0026quot;/home/git/hugo.git\u0026quot; /usr/bin/inotifywait -mq --timefmt \u0026#39;%d/%m/%y %H:%M\u0026#39; --format \u0026#39;%T %w%f%e\u0026#39; --event delete,modify,create,attrib $src | while read file do sh ./publish.sh done ","date":"2021-01-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-01-24-monitor-folder/","section":"posts","tags":["Linux"],"title":"monitor folder"},{"categories":["Linux"],"contents":"install udisks2\nunmount udisksctl unmout -b /dev/sd****\nsafe remove udisksctl power-off -b /dev/sdb*\n","date":"2021-01-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-01-24-safe-remove-drive-command/","section":"posts","tags":["Linux"],"title":"safe remove drive command"},{"categories":["Misc"],"contents":"my clang format config file.\nshow details --- Language: Cpp # BasedOnStyle: LLVM AccessModifierOffset: -2 AlignAfterOpenBracket: Align AlignArrayOfStructures: None AlignConsecutiveMacros: None AlignConsecutiveAssignments: None AlignConsecutiveBitFields: None AlignConsecutiveDeclarations: None AlignEscapedNewlines: Right AlignOperands: Align AlignTrailingComments: true AllowAllArgumentsOnNextLine: true AllowAllParametersOfDeclarationOnNextLine: true AllowShortEnumsOnASingleLine: true AllowShortBlocksOnASingleLine: Never AllowShortCaseLabelsOnASingleLine: false AllowShortFunctionsOnASingleLine: All AllowShortLambdasOnASingleLine: All AllowShortIfStatementsOnASingleLine: Never AllowShortLoopsOnASingleLine: false AlwaysBreakAfterDefinitionReturnType: None AlwaysBreakAfterReturnType: None AlwaysBreakBeforeMultilineStrings: false AlwaysBreakTemplateDeclarations: MultiLine AttributeMacros: - __capability BinPackArguments: true BinPackParameters: true BraceWrapping: AfterCaseLabel: false AfterClass: false AfterControlStatement: Never AfterEnum: false AfterFunction: false AfterNamespace: false AfterObjCDeclaration: false AfterStruct: false AfterUnion: false AfterExternBlock: false BeforeCatch: false BeforeElse: false BeforeLambdaBody: false BeforeWhile: false IndentBraces: false SplitEmptyFunction: true SplitEmptyRecord: true SplitEmptyNamespace: true BreakBeforeBinaryOperators: None BreakBeforeConceptDeclarations: true BreakBeforeBraces: Attach BreakBeforeInheritanceComma: false BreakInheritanceList: BeforeColon BreakBeforeTernaryOperators: true BreakConstructorInitializersBeforeComma: false BreakConstructorInitializers: BeforeColon BreakAfterJavaFieldAnnotations: false BreakStringLiterals: true ColumnLimit: 80 CommentPragmas: \u0026#39;^ IWYU pragma:\u0026#39; QualifierAlignment: Leave CompactNamespaces: false ConstructorInitializerIndentWidth: 4 ContinuationIndentWidth: 4 Cpp11BracedListStyle: true DeriveLineEnding: true DerivePointerAlignment: false DisableFormat: false EmptyLineAfterAccessModifier: Never EmptyLineBeforeAccessModifier: LogicalBlock ExperimentalAutoDetectBinPacking: false PackConstructorInitializers: BinPack BasedOnStyle: \u0026#39;\u0026#39; ConstructorInitializerAllOnOneLineOrOnePerLine: false AllowAllConstructorInitializersOnNextLine: true FixNamespaceComments: true ForEachMacros: - foreach - Q_FOREACH - BOOST_FOREACH IfMacros: - KJ_IF_MAYBE IncludeBlocks: Preserve IncludeCategories: - Regex: \u0026#39;^\u0026quot;(llvm|llvm-c|clang|clang-c)/\u0026#39; Priority: 2 SortPriority: 0 CaseSensitive: false - Regex: \u0026#39;^(\u0026lt;|\u0026quot;(gtest|gmock|isl|json)/)\u0026#39; Priority: 3 SortPriority: 0 CaseSensitive: false - Regex: \u0026#39;.*\u0026#39; Priority: 1 SortPriority: 0 CaseSensitive: false IncludeIsMainRegex: \u0026#39;(Test)?$\u0026#39; IncludeIsMainSourceRegex: \u0026#39;\u0026#39; IndentAccessModifiers: false IndentCaseLabels: false IndentCaseBlocks: false IndentGotoLabels: true IndentPPDirectives: None IndentExternBlock: AfterExternBlock IndentRequires: false IndentWidth: 4 IndentWrappedFunctionNames: false InsertTrailingCommas: None JavaScriptQuotes: Leave JavaScriptWrapImports: true KeepEmptyLinesAtTheStartOfBlocks: true LambdaBodyIndentation: Signature MacroBlockBegin: \u0026#39;\u0026#39; MacroBlockEnd: \u0026#39;\u0026#39; MaxEmptyLinesToKeep: 1 NamespaceIndentation: None ObjCBinPackProtocolList: Auto ObjCBlockIndentWidth: 4 ObjCBreakBeforeNestedBlockParam: true ObjCSpaceAfterProperty: false ObjCSpaceBeforeProtocolList: true PenaltyBreakAssignment: 2 PenaltyBreakBeforeFirstCallParameter: 19 PenaltyBreakComment: 300 PenaltyBreakFirstLessLess: 120 PenaltyBreakOpenParenthesis: 0 PenaltyBreakString: 1000 PenaltyBreakTemplateDeclaration: 10 PenaltyExcessCharacter: 1000000 PenaltyReturnTypeOnItsOwnLine: 60 PenaltyIndentedWhitespace: 0 PointerAlignment: Right PPIndentWidth: -1 ReferenceAlignment: Pointer ReflowComments: true RemoveBracesLLVM: false SeparateDefinitionBlocks: Leave ShortNamespaceLines: 1 SortIncludes: CaseSensitive SortJavaStaticImport: Before SortUsingDeclarations: true SpaceAfterCStyleCast: false SpaceAfterLogicalNot: false SpaceAfterTemplateKeyword: true SpaceBeforeAssignmentOperators: true SpaceBeforeCaseColon: false SpaceBeforeCpp11BracedList: false SpaceBeforeCtorInitializerColon: true SpaceBeforeInheritanceColon: true SpaceBeforeParens: ControlStatements SpaceBeforeParensOptions: AfterControlStatements: true AfterForeachMacros: true AfterFunctionDefinitionName: false AfterFunctionDeclarationName: false AfterIfMacros: true AfterOverloadedOperator: false BeforeNonEmptyParentheses: false SpaceAroundPointerQualifiers: Default SpaceBeforeRangeBasedForLoopColon: true SpaceInEmptyBlock: false SpaceInEmptyParentheses: false SpacesBeforeTrailingComments: 1 SpacesInAngles: Never SpacesInConditionalStatement: false SpacesInContainerLiterals: true SpacesInCStyleCastParentheses: false SpacesInLineCommentPrefix: Minimum: 1 Maximum: -1 SpacesInParentheses: false SpacesInSquareBrackets: false SpaceBeforeSquareBrackets: false BitFieldColonSpacing: Both Standard: Latest StatementAttributeLikeMacros: - Q_EMIT StatementMacros: - Q_UNUSED - QT_REQUIRE_VERSION TabWidth: 4 UseCRLF: false UseTab: Never WhitespaceSensitiveMacros: - STRINGIZE - PP_STRINGIZE - BOOST_PP_STRINGIZE - NS_SWIFT_NAME - CF_SWIFT_NAME ... ","date":"2021-01-05T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/2021-01-05-c-language-clang-format-config-file/","section":"posts","tags":["c language"],"title":"C language clang format config file"},{"categories":["Misc"],"contents":"https://seujxh.wordpress.com/2018/09/25/%e6%90%ad%e5%bb%ba%e5%9c%a8%e8%99%9a%e6%8b%9f%e6%9c%bacentos-7%e4%b8%8a%e7%9a%84cadence-ic617%e5%a5%97%e4%bb%b6/\nmkdir -p /opt/cadence/installscape mkdir -p /opt/mentor/Calibre2015\nshould install lib32-gcc-libs\n127.0.0.1 主机名 主机别名\n#127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1 localhost localhost.localdomain localhost6 localhost6.localdomain6\n","date":"2021-01-05T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-01-05-cadence-ic6-install/","section":"posts","tags":["Cadence"],"title":"cadence IC6 install"},{"categories":["Misc"],"contents":"for JavaFx in idea we need add VM option\nlink https://openjfx.io/openjfx-docs/\n","date":"2021-01-03T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-01-03-javafx-in-idea/","section":"posts","tags":["Program"],"title":"JavaFX in IDEA"},{"categories":["Misc"],"contents":"the way to make two column synchoronze.\nchinese test\n\\documentclass{article} \\usepackage[UTF8]{ctex} \\begin{document} 你好，这是一个测试文档。 \\end{document} two column synchoronize template\n\\documentclass{article} \\usepackage[a4paper,top=1cm,bottom=2.1cm,left=2.1cm,right=1.8cm,% includehead,includefoot]{geometry} % \\usepackage[UTF8]{ctex} \\usepackage{parallel} \\begin{document} \\begin{Parallel} \\ParallelLText{ % 左侧文字1 test } \\ParallelRText{ % 右侧文字1 test2 } \\ParallelPar \\ParallelLText{ % 左侧文字2 } \\ParallelRText{ % 右侧文字2 } \\ParallelPar \\end{Parallel} \\end{document} ","date":"2020-11-29T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-29-latex-synchronize-column/","section":"posts","tags":["Latex","Double Column"],"title":"Latex synchronize column"},{"categories":["Program"],"contents":"unique Removes all but the first element from every consecutive group of equivalent elements in the range [first,last).\nreturn value: An iterator to the element that follows the last element not removed.\nExample\nint n; vector\u0026lt;int\u0026gt; a; // suppose there are some value in a n = unique(a.begin(), a.end()) - a.begin(); // n is the number the unique item. a.resize(n); // remove repeated item. resize Resizes the container so that it contains n elements.\nIf n is smaller than the current container size, the content is reduced to its first n elements, removing those beyond (and destroying them).\nIf n is greater than the current container size, the content is expanded by inserting at the end as many elements as needed to reach a size of n. If val is specified, the new elements are initialized as copies of val, otherwise, they are value-initialized.\nIf n is also greater than the current container capacity, an automatic reallocation of the allocated storage space takes place.\nNotice that this function changes the actual content of the container by inserting or erasing elements from it.\n","date":"2020-11-27T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-27-c++-std-library/","section":"posts","tags":["C","Std"],"title":"C++ std library"},{"categories":["Misc"],"contents":"change image caption to center\nhttps://tex.stackexchange.com/questions/95207/how-to-center-a-specific-caption\n\\captionsetup{justification=centering} Fix image position issue\nhttps://tex.stackexchange.com/questions/494694/how-can-i-get-my-table-or-figure-to-stay-where-they-are-instead-of-going-to-the\n\\documentclass[a4paper,man,floatsintext,natbib]{apa6} how to insert image\n\\begin{figure}[!h] \\centering % 图片居中 \\includegraphics[width = 15cm]{result1.png} \\caption{Adversarial images} \\label{Fig3} \\end{figure} ","date":"2020-11-23T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-23-latex-usage/","section":"posts","tags":["Latex"],"title":"Latex usage"},{"categories":["Misc"],"contents":"change point orgin point to left-bottom corner.\nplt.xlim(0, 1) plt.ylim(0, 1) ","date":"2020-11-23T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-23-matplot-origin-point/","section":"posts","tags":["Latex"],"title":"Matplot origin point"},{"categories":["Misc"],"contents":"今天早上的梦， 今天梦中的场景 有两个阶段，第一个阶段是我回到家里并且妹妹喊着要抱抱。\n第二个阶段是因为寒冷，梦到有人给我盖了两床被子，再两个梦中间，我醒来过一次。 然后将梦的内容说了一下才继续睡觉。\n人的记忆是非常短暂的，即使在当时我说出了梦境的内容，早上起来梦境还是快速的消失了。像是两个平行宇宙。 在这一次的梦境里，所有的东西都非常的清晰，非常的真实，甚至连情绪上的表现也有。\n人的视觉处理和记忆是分开。记忆和视觉无关。但是处理图像的方法几乎所有人都差不多。\n记忆是再不断训练神经元的过程而不是一直改变原有的神经元。 原有的记忆会不断的减弱。大部分的只是依靠现有的知识去想象和重建。而非真的有以前的记忆。\n人的记忆是高度压缩的，也是快速遗忘的。 人不会记住没有用的信息。冗余信息。信息越多当然越好，但是人脑的功率只有20多W。在有限资源的情况下需要高度压缩所有的内容。\n及时反馈，人对信息的处理不是到了最后一层才通过BP逐层往回传递。而是可能在连个细胞之间有信息传递的通道了。\n","date":"2020-11-19T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-19-%E6%A2%A6-%E8%AE%B0%E5%BD%95-11-19/","section":"posts","tags":["Idea"],"title":"梦-记录-11-19"},{"categories":["Math"],"contents":"梦中梦发生时，脑子在想什么？ 陈光宗在上optimization课时，我在睡午觉（第二顿饭后都觉难道不就是午觉吗？）。在我快醒来的时间里，发生了梦中梦。\n在梦中梦里，我大概在经历什么危险的事，整个人的状态都很不安，我挣扎着想要起来，但是怎样都睁不开眼，眼皮好像有千斤重。一旦我挣扎着睁眼又没有成功，就会有很严重的耳鸣，让我感觉脑袋都要炸了。梦里强烈的耳鸣也不是第一次了，而这是在我生病后才新出现的状况。好不容易从梦中梦里醒来了，但是还是困得不得了，随时都会再次睡下。这时，在梦里，陈光宗来了，在我床边坐了一会就准备离开。我慌了，好不容易来了个能叫醒我的人，就这么走了我可怎么办。于是我抓住他的手，想让他叫醒我。奈何陈光宗悟性有限，没能理解我的意思，还是靠我自己挣扎着才醒来的。虽然是在梦里醒来，但是周围环境和现实里基本一致，我也没觉得有什么区别，黑漆漆的一片。到这里，梦就真的醒了，我在一片黑暗中，看见陈光宗推门而入，把灯打开了，啪的一声，很快啊。\n靠着记忆的连贯性，我确定自己是完全醒了，而不是另一层套娃梦。那么问题来了，梦中梦的我想要醒来时，为什么那么艰难？这种体验陈光宗也有过。梦中梦的机制会不会和神经网络一样，是嵌套着的？还有，梦中梦的我醒来时，为什么不会意识到自己还在梦里？现实中醒来后，为什么我可以立刻意识到我醒了？区别在哪？体验感更真实吗？如果每次睡觉醒来后第一件事就是确认自己记忆的连贯性，有没有可能意识到自己还在梦里？\n盗梦空间里，区分现实和梦的关键道具是陀螺。如果我们也这么做，可以分辨梦和现实吗？\n","date":"2020-11-16T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-16-%E6%A2%A6%E4%B8%AD%E6%A2%A6%E7%9A%84%E5%8F%91%E7%94%9F%E5%8F%AF%E8%83%BD%E6%80%A7/","section":"posts","tags":["Idea"],"title":"梦中梦的发生可能性"},{"categories":["Math"],"contents":"参考资料 kalman-filter 前言 卡尔曼滤波器是一个非常神奇的滤波器。卡尔曼滤波器就是一个状态估计器，与状态观测器的最大区别不大。 可以认为卡尔曼滤波器就是一个变系数的状态估计器。这里的系数成为卡尔曼增益。以纪念卡尔曼在1960年发表的文章中所作出的突出贡献。\n卡尔曼滤波器是在最小均方根误差意义下对状态变量的最优估计。在不同的参考资料中会给出不同的推导。\n本人看过两个不同的推导。 先对这两种方法的思路给出简单的描述 1. 直接给出均方根的表达式，然后对卡尔曼增益求导，这样就可以得到均方根取得极值时，最优的卡尔曼增益。 2. 使用函数逼近的想法，因为在过去时刻测量的数据具有无关性，因此使用新息的线性组合来表示状态变量，为了达到均方根最小的目的，就是选择合适系数，在这个想法的基础上，把公式变成递推公式就得到了卡尔曼滤波的公式。\n结论 在这里先直接的给出卡尔曼滤波公式。 这里的模型是最简单的状态空间模型 \\[\\begin{equation} \\pmb{x}_{n} = \\pmb{A}\\pmb{x}_{n-1} + \\pmb{\\omega}_{n-1} \\\\\\ \\pmb{y}_{n} = \\pmb{B}\\pmb{x}_{n} + \\pmb{v}_{n} \\end{equation}\\]{n} + {n} \\end{equation} 其中\\(\\pmb{A}\\)是\\(\\pmb{x}_{n-1}\\)到\\(\\pmb{x}_n\\)的过渡矩阵,\\(\\pmb{B}_n\\)是测量矩阵。动态噪声\\(\\pmb{\\omega}\\)和测量噪声\\(\\pmb{v}\\)满足均值为0的高斯分布，且相互独立。 \\[\\begin{equation} \\begin{aligned} Q_{\\omega} \u0026amp;=E\\left[w_{k} w_{k}^{T}\\right] \\\\\\ Q_{v} \u0026amp;=E\\left[v_{k} v_{k}^{T}\\right] \\end{aligned} \\end{equation}\\]\\end{equation}\n卡尔曼滤波方程如下\n卡尔曼增益 \\[\\begin{equation} K_{k}=P_{k|k-1} B^T (B P_{k|k-1} B^T+Q_v)^{-1} \\end{equation}\\] 滤波方程 \\[\\begin{equation} \\hat{x}_{k|k}=\\hat{x}_{k|k-1}+K_{k} (y_{k}-B \\hat{x}_{k|k-1}) \\end{equation}\\] 预测误差协方差矩阵 \\[\\begin{equation} P_{k|k}=(I-K_{k} B) P_{k|k-1} \\end{equation}\\] 预报方程 \\[\\begin{equation} \\begin{array}{c} \\hat{x}_{k+1|k}=A \\hat{x}_{k|k} \\\\\\ P_{k+1|k}=A P_{k|k} A^T +Q_{\\omega} \\end{array} \\end{equation}\\]\\end{equation} 初始条件 1. \\(\\hat{x}_{0|0} = E[x_1]\\) 2. \\(P_{0|0} = Q_{v}\\)\n补充 1. 预测误差协方差矩阵P的定义 \\[\\begin{equation} P_{k}=E[e_{k} e_{k}^{T}]=E[(x_{k}-\\hat{x}_{k|k-1})(x_{k}-\\hat{x}_{k|k-1})^{T}] \\end{equation}\\] 2. 如果有控制变量的输入，就将上面模型中的\\(A\\)变成一个动态的变量而不作为一个常量，在\\(X\\)中添加1的维度，控制变量写在矩阵\\(A\\)的最后一列。也就是齐次向量的形式。\n代码 在这个代码中设定了一个\\(x\\)的变化，然后用卡尔曼滤波对\\(x\\)进行估计，估计得到的结果和原来\\(x\\)的值显示在同一张图上。\n推导过程 啥时候有需要了在补。\n","date":"2020-11-12T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-12-kalman-filter/","section":"posts","tags":null,"title":"Kalman Filter 卡尔曼滤波"},{"categories":["Math"],"contents":"凸规划是指在凸函数上的寻找最小值或者最大值。\n一元函数的最值和极值 凸规划的基本想法来源于使用函数导数求函数极值。在一元函数中我们通过通过寻找导数值为0的点来寻找极大值和极小值。 \\[ \\frac{df(x)}{dx} = 0 \\] 导数值为0的点可以确定为极值，但无法确定是极大值还是极小值。 因此通过二阶导数正负来确定是哪一种。 \\[ \\text{在极大值处有 } \\frac{d^2f(x)}{dx^2} \u0026lt; 0 \\\\ \\text{在极小值处有 }\\frac{d^2f(x)}{dx^2} \u0026gt; 0 \\\\ \\text{驻点 非极值点} \\frac{d^2f(x)}{dx^2} = 0 \\] 在凸函数上，极值将退化为最值。\n关于一元函数的极值可以理解为：函数f(x)在x方向上的变化量为0的点为极值点（可能是驻点）。\n多元函数的最值和极值 多元函数的极值和最值可以理解为一元函数的推广：多元函数的极值点在所有方向的上的变化量都为0.。在函数连续可微的情况下，这个条件可以退化成为：在基向量方向上，函数的变化量为0；\n例如，二元函数 \\(f(x,y)\\) 有一点\\((x_0, y_0)\\) 在方向\\((0,1)\\)和方向\\((1,0)\\)上的变化量为都0，可以推出函数\\(f(x,y)\\)在点\\((x_0, y_0)\\)上的所有方向变化量都为0。\n表达式为 \\[ \\frac{df(\\mathbf{x})}{d\\mathbf{x^T}} = 0 \\]\n\\[ \\left[\\begin{array}{c} \\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_{n}} \\end{array}\\right]=0 \\]\n这就是所谓的一阶必要条件(first order necessary condition)。\n和一元函数类似，如果函数\\(f(x,y)\\) 有一点\\((x_0,y_0)\\)的hessian矩阵是positive defined（正定的），该点为极小值点；反之，为极大值点。此为二阶必要条件。如果既不是正定也不是负定，那该点为驻点。\ndraft 凸规划得定理大多数可以有一元函数得全导数扩展。\n一元函数集值点的导数为0。 如果二次导数大于0 则为最小值点。二次导数小于0则为最大值点。\n这两条规则的扩展成为了凸规划中的first order necessary condition（一阶必要条件）和 二阶必要条件（hessian 矩阵为 positive defined(正定)\n上述的凸规划是没有边界的。对于变量没有任何的限制。\n当对变量有限制的时候。拉格朗日乘数，使有限制的问题转换成为没有限制的凸规划问题。拉格朗日乘数法的基本想法就是在不改变最优值的情况下。改变原函数， 使得可以原来的方法求解，并且同时满足了限制条件。 （没写出我想要的东西）\n引入拉格朗日乘数之后，原函数的极值点发生了移动。 但是由于限制条件的存在，最优值是不变的。也就是说拉格朗日乘数为原来函数增加的了自由度. 换句话说就是增加了极值点。 然后通过限制条件来选出符合要求的极值点。\n对偶。 以后再说\n注： 正定这个词的翻译并不易理解。 英文更容易理解。 可以简单理解为这个矩阵总是正的。 正定矩阵\\(A\\) 对任意的向量\\(x\\) 有\\(x^{T} A x \u0026gt; 0\\).\n","date":"2020-11-08T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-08-%E5%85%B3%E4%BA%8E%E5%87%B8%E8%A7%84%E5%88%92%E7%9A%84%E6%83%B3%E6%B3%95/","section":"posts","tags":["Math","Linear Programming"],"title":"凸规划的一些想法"},{"categories":["Math"],"contents":"TANGENT PLANE Definition. A point \\(x^{*}\\) satisfying the constraint \\(\\mathbf{h}\\left(\\mathbf{x}^{*}\\right)=\\mathbf{0}\\) is said to be a regular point of the constraint if the gradient vectors \\(\\nabla h_{1}\\left(\\mathbf{x}^{*}\\right), \\nabla h_{2}\\left(\\mathbf{x}^{*}\\right), \\ldots, \\nabla h_{m}\\left(\\mathbf{x}^{*}\\right)\\) are linearly independent.\nTheorem. At a regular point \\(\\mathbf{x}^{*}\\) of the surface \\(S\\) defined by \\(\\mathbf{h}(x)=0\\) the tangent plane is equal to \\[ M=\\left\\{y: \\nabla h\\left(x^{*}\\right) y=0\\right\\} \\]\nFIRST-ORDER NECESSARY CONDITIONS (EQUALITY CONSTRAINTS) Lemma. Let \\(\\mathbf{x}^{*}\\) be a regular point of the constraints \\(\\mathbf{h}(\\boldsymbol{x})=\\boldsymbol{0}\\) and a local extreme point ( \\(a\\) minimum or maximum) of \\(f\\) subject to these constraints. Then all \\(\\mathbf{y} \\in \\mathbf{E}^{n}\\) satisfying \\[ \\nabla \\mathbf{h}\\left(\\mathbf{x}^{*}\\right) \\mathbf{y}=\\mathbf{0} \\] must also satisfy \\[ \\nabla f\\left(\\mathbf{x}^{*}\\right) \\mathbf{y}=0 \\] Theorem. Let \\(\\mathbf{x}^{*}\\) be a local extreme point of \\(f\\) subject to the constraints \\(\\mathbf{h}(x)=0 .\\) Assume further that \\(\\mathbf{x}^{*}\\) is a regular point of these constraints. Then there is \\(a \\lambda \\in E^{m}\\) such that \\[ \\nabla f\\left(x^{*}\\right)+\\lambda^{T} \\nabla h\\left(x^{*}\\right)=0 \\]\nSECOND-ORDER CONDITIONS Second-Order Necessary Conditions. Suppose that \\(\\mathbf{x}^{*}\\) is a local minimum of \\(f\\) subject to \\(\\mathbf{h}(\\mathbf{x})=0\\) and that \\(\\mathbf{x}^{*}\\) is a regular point of these constraints. Then there is \\(a \\lambda \\in E^{\\prime \\prime \\prime}\\) such that \\[ \\nabla f\\left(\\mathbf{x}^{*}\\right)+\\lambda^{T} \\nabla h\\left(\\mathbf{x}^{*}\\right)=\\mathbf{0} \\] If we denote by \\(M\\) the tangent plane \\(M=\\left\\{\\mathbf{y}: \\nabla \\mathbf{h}\\left(\\mathbf{x}^{*}\\right) \\mathbf{y}=\\mathbf{0}\\right\\},\\) then the matrix \\[ \\mathbf{L}\\left(\\mathbf{x}^{*}\\right)=\\mathbf{F}\\left(\\mathbf{x}^{*}\\right)+\\lambda^{T} \\mathbf{H}\\left(\\mathbf{x}^{*}\\right) \\] is positive semidefinite on \\(M,\\) that is, \\(\\mathbf{y}^{T} \\mathbf{L}\\left(\\mathbf{x}^{*}\\right) \\mathbf{y} \\geqslant \\mathbf{0}\\) for all \\(\\mathbf{y} \\in M\\)\nSecond-Order Sufficiency Conditions. Suppose there is a point \\(\\mathbf{x}^{*}\\) satisfying \\(\\mathbf{h}\\left(\\mathbf{x}^{*}\\right)=\\mathbf{0},\\) and \\(a \\lambda \\in E^{m}\\) such that \\[ \\nabla f\\left(\\mathbf{x}^{*}\\right)+\\lambda^{T} \\nabla h\\left(\\mathbf{x}^{*}\\right)=\\mathbf{0} \\] Suppose also that the matrix \\(\\mathbf{L}\\left(\\mathbf{x}^{*}\\right)=\\mathbf{F}\\left(\\mathbf{x}^{*}\\right)+\\lambda^{T} \\mathbf{H}\\left(\\mathbf{x}^{*}\\right)\\) is positive definite on \\(M=\\{\\mathbf{y}: \\left.\\nabla \\mathbf{h}\\left(\\mathbf{x}^{*}\\right) \\mathbf{y}=\\mathbf{0}\\right\\},\\) that is for \\(\\mathbf{y} \\in M, \\mathbf{y} \\neq \\mathbf{0}\\) there holds \\(\\mathbf{y}^{T} \\mathbf{L}\\left(\\mathbf{x}^{*}\\right) \\mathbf{y}\u0026gt;\\mathbf{0} .\\) Then \\(\\mathbf{x}^{*}\\) is a strict local minimum of \\(f\\) subject to \\(\\mathbf{h}(\\mathbf{x})=\\mathbf{0} .\\)\nInequality constraints Definition. Let \\(\\mathbf{x}^{*}\\) be a point satisfying the constraints \\[ \\mathbf{h}\\left(\\mathbf{x}^{*}\\right)=\\mathbf{0}, \\mathbf{g}\\left(\\mathbf{x}^{*}\\right) \\leqslant \\mathbf{0} \\] and let \\(J\\) be the set of indices \\(j\\) for which \\(g_{j}\\left(\\mathbf{x}^{*}\\right)=0 .\\) Then \\(\\mathbf{x}^{*}\\) is said to be a regular point of the constraints ( 33 ) if the gradient vectors \\(\\nabla h_{i}\\left(\\mathbf{x}^{*}\\right), \\nabla g_{i}\\left(\\mathbf{x}^{*}\\right), 1 \\leqslant i \\leqslant m, j \\in J\\) are linearly independent.\nKarush-Kuhn-Tucker Conditions\nLet \\(\\mathbf{x}^{*}\\) be a relative minimum point for the problem \\[ \\begin{array}{ll} \\operatorname{minimize} \u0026amp; f(\\mathbf{x}) \\\\ \\text { subject to } \u0026amp; \\mathbf{h}(\\mathbf{x})=\\mathbf{0}, \\mathbf{g}(\\mathbf{x}) \\leqslant \\mathbf{0} \\end{array} \\] and suppose \\(\\mathbf{x}^{*}\\) is a regular point for the constraints. Then there is a vector \\(\\lambda \\in E^{m}\\) and \\(a\\) vector \\(\\mu \\in E^{p}\\) with \\(\\mu \\geqslant 0\\) such that \\[ \\begin{aligned} \\boldsymbol{\\nabla} f\\left(\\mathbf{x}^{*}\\right)+\\lambda^{T} \\nabla \\mathbf{h}\\left(\\mathbf{x}^{*}\\right)+\\boldsymbol{\\mu}^{T} \\nabla \\mathbf{g}\\left(\\mathbf{x}^{*}\\right) \u0026amp;=\\mathbf{0} \\\\ \\boldsymbol{\\mu}^{T} \\mathbf{g}\\left(\\mathbf{x}^{*}\\right) \u0026amp;=0 \\end{aligned} \\]\n","date":"2020-11-07T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions-constain/","section":"posts","tags":["Math","Linear Programming"],"title":"constrain optimization"},{"categories":["Math"],"contents":"CONJUGATE DIRECTIONS Definition. Given a symmetric matrix \\(\\mathbf{Q}\\), two vectors \\(\\mathbf{d}_{1}\\) and \\(\\mathbf{d}_{2}\\) are said to be \\(\\mathbf{Q}\\) -orthogonal, or conjugate with respect to \\(\\mathbf{Q},\\) if \\(\\mathbf{d}_{1}^{T} \\mathbf{Q} \\mathbf{d}_{2}=0 .\\)\nProposition. If \\(\\mathbf{Q}\\) is positive definite and the set of nonzero vectors \\(\\mathrm{d}_{0}, \\mathrm{d}_{1}, \\mathrm{d}_{2}, \\ldots, \\mathrm{d}_{k}\\) are \\(\\mathbf{Q}\\)-orthogonal, then these vectors are linearly independent.\nConjugate Direction Theorem Let \\(\\left\\{\\mathbf{d}_{i}\\right\\}_{i=0}^{n-1}\\) be a set of nonzero \\(\\mathbf{Q}\\) -orthogonal vectors. For any \\(\\mathbf{x}_{0} \\in E^{n}\\) the sequence \\(\\left\\{\\mathbf{x}_{\\mathbf{k}}\\right\\}\\) generated according to \\[ \\begin{equation} \\mathbf{x}_{k+1}=\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{d}_{k}, k \\geqslant 0 \\end{equation} \\] with \\[ \\begin{equation} \\alpha_{k}=-\\frac{\\mathbf{g}_{k}^{T} \\mathbf{d}_{k}}{\\mathbf{d}_{k}^{T} \\mathbf{Q} \\mathbf{d}_{\\mathbf{k}}} \\end{equation} \\] and \\[ \\mathrm{g}_{k}=\\mathbf{Q} \\mathbf{x}_{k}-\\mathbf{b} \\] converges to the unique solution, \\(\\mathbf{x}^{*},\\) of \\(\\mathbf{Q} \\mathbf{x}=\\mathbf{b}\\) after \\(n\\) steps, that is, \\(\\mathbf{x}_{n}=\\mathbf{x}^{*}\\)\nExpanding Subspace Theorem We define \\(\\mathcal{B}_{k}\\) as the subspace of \\(E^{n}\\) spanned by \\(\\left\\{\\mathbf{d}_{0}, \\mathbf{d}_{1}, \\ldots, \\mathbf{d}_{k-1}\\right\\} .\\) We shall show that as the method of conjugate directions progresses each \\(\\mathbf{x}_{k}\\) minimizes the objective over the \\(k\\) -dimensional linear variety \\(\\mathbf{x}_{0}+\\mathcal{B}_{k}\\).\nLet \\(\\left\\{\\mathbf{d}_{i}\\right\\}_{i=0}^{n-1}\\) be a sequence of nonzero \\(\\mathbf{Q}\\) -orthogonal vectors in \\(E^{n} .\\) Then for any \\(\\mathbf{x}_{0} \\in E^{n}\\) the sequence \\(\\left\\{\\mathbf{x}_{k}\\right\\}\\) generated according to \\[ \\begin{aligned} \\mathbf{x}_{k+1} \u0026amp;=\\mathbf{x}_{k}+\\alpha_{k} \\mathbf{d}_{k} \\\\ \\alpha_{k} \u0026amp;=-\\frac{\\mathbf{g}_{k}^{T} \\mathbf{d}_{k}}{\\mathbf{d}_{k}^{T} \\mathbf{Q} \\mathbf{d}_{k}} \\end{aligned} \\] has the property that \\(\\mathbf{x}_{k}\\) minimizes \\(f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^{T} \\mathbf{Q} \\mathbf{x}-\\mathbf{b}^{T} \\mathbf{x}\\) on the line \\(\\mathbf{x}=\\mathbf{x}_{k-1}+\\alpha \\mathbf{d}_{k-1},-\\infty\u0026lt;\\) \\(\\alpha\u0026lt;\\infty,\\) as well as on the linear variety \\(\\mathbf{x}_{0}+\\mathcal{B}_{k}\\)\nCorollary. In the method of conjugate directions the gradients \\(\\mathrm{g}_{k}, k=0,1, \\ldots, n\\) satisfy \\[ \\mathbf{g}_{k}^{T} \\mathbf{d}_{i}=0 \\text { for } i\u0026lt;k \\]\nConjugate Gradient Theorem. The conjugate gradient algorithm \\((17)-(20)\\) is a conjugate direction method. If it does not terminate at \\(\\mathbf{x}_{k},\\) then a) \\(\\left[\\mathrm{g}_{0}, \\mathrm{g}_{1}, \\ldots, \\mathrm{g}_{k}\\right]=\\left[\\mathrm{g}_{0}, \\mathrm{Qg}_{0}, \\ldots, \\mathrm{Q}^{k} \\mathrm{g}_{0}\\right]\\) b) \\(\\left[\\mathbf{d}_{0}, \\mathbf{d}_{1}, \\ldots, \\mathbf{d}_{k}\\right]=\\left[\\mathbf{g}_{0}, \\mathbf{Q} \\mathbf{g}_{0}, \\ldots, \\mathbf{Q}^{k} \\mathbf{g}_{0}\\right]\\) c) \\(\\mathbf{d}_{k}^{T} \\mathbf{Q} \\mathbf{d}_{i}=0\\) for \\(i \\leqslant k-1\\) d) \\(\\alpha_{k}=\\mathbf{g}_{k}^{T} \\mathbf{g}_{k} / \\mathbf{d}_{k}^{T} \\mathbf{Q} \\mathbf{d}_{k}\\) e) \\(\\beta_{k}=\\mathbf{g}_{k+1}^{T} \\mathbf{g}_{k+1} / \\mathbf{g}_{k}^{T} \\mathbf{g}_{k}\\)\n","date":"2020-11-02T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-02-conjugate-direction-method/","section":"posts","tags":["Math","Linear Programming"],"title":"Conjugate Direction Method"},{"categories":["Misc"],"contents":"IPSec gateway vpn.pitt.edu IPSec ID guc25 IPSec secret ooL6ohho Xauth username guc25 ","date":"2020-11-02T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-17-vpnc-configure/","section":"posts","tags":["Vpn"],"title":"vpnc configuration"},{"categories":["Misc"],"contents":"深夜睡不着觉的夜谈会part1\n经过半年的隔离，我总算等到了外卖平台大打折。10月30日，Kongfu Tea半价，不买不是人。终于，在十一月到来之前，我们喝到了奶茶店的奶茶，两杯加布丁，两杯加珍珠。\n但是我和陈光宗万万没想到，这奶茶的劲是真的足。凌晨两点，两个人一点睡意都没有。不仅没有睡意，还能侃大山。从人生哲学谈到宇宙物理，基本上能想到的都扯了一遍。本次日记为夜谈有趣部分总结。\n蛋壳宇宙：人在死后会去天堂，遇见上帝。而上帝会督促你去“投胎”，即转生开始体验下一个生命旅程。这个旅程是所有时空所有生命中的任意一个，只要是你尚未经历过的，都有可能。包括上帝，也是你。而所有生命旅程都体验完了，这个宇宙也就迎来了终结。这么一想，颇有种一为全，全为一的感觉。看来我杀我自己是真的。\n万物皆可薛定谔：薛定谔的猫是个老物理故事了，真要解释起来涉及的知识还挺深奥。但是用通俗的话来说，其实可以很简单。打个比方，我和小表弟分别后，在下一次见面之前，小表弟都有一定概率遭遇不测。所以在我见到小表弟之前，我是不能确定小表弟的状态的。所以小表弟处于又死又活的叠加态。故，万物皆可薛定谔。所以，只要我不看B站，我关注的UP主就会处于更新和拖更的叠加态。直到我打开了B站，我才能确定状态。所以我逛B站是有理由的。\n陈光宗自学能力培养之谜：陈光宗以前也是个会乖乖听课的小孩，但是他总忍不住在学习新知识的过程中联想以前学过的东西。等他回国神来，老师已经在讲其他的内容了。如果老师的进度不快，陈光宗有一定概率跟上老师进度，继续学习。如果老师讲的很快，他就跟不上了，最后只能自己看书弄懂。从小学到大学，老师讲课的速度是单调递增的，所以陈光宗的自学能力稳步提升。用我的话说，就是这小孩上课总走神，所以只能自学。还好脑子好，要不然大专又要多一个学生。\n陈嘉诚换女朋友规律总结：陈嘉诚可能比咱俩更容易适应新环境，证据如下：第一，陈嘉诚每逢寒暑假有大概率换女朋友。第二，在换了女朋友后三个月内，下一个女朋友预备将会出现在陈嘉诚的生活中，并迅速感情升温，连游戏都可以不打。第三，这家伙来美国后考了驾照，还有车。至于前两条规律，还需要更多样本来证实，如果将来没有更多样本……那就恭喜陈嘉诚终于快踏进婚姻的坟墓了。\nPS：最后我三点多才睡着。但是第二天的奶茶照喝不误。\n","date":"2020-11-02T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-11-02-%E6%B7%B1%E5%A4%9C%E7%9D%A1%E4%B8%8D%E7%9D%80%E8%A7%89%E7%9A%84%E5%A4%9C%E8%B0%88%E4%BC%9Apart1/","section":"posts","tags":["Idea"],"title":"深夜睡不着觉的夜谈会Part1"},{"categories":["Program"],"contents":"This may happen in windows although you have permission.\nThe way to solve the problem is change properity of file in windows.\nfile-\u0026gt;properties-\u0026gt;General\nuncheck Hidden and archived\n","date":"2020-10-29T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-29-python-errno13-permission-denied/","section":"posts","tags":["Python","Errno13"],"title":"Python Permission Denied"},{"categories":["Linux"],"contents":"enable windows user to change the permission\ndos filemod = yes fix time synchronize issue\ntime server = yes dos filetimes = yes fake directory create times = yes dos filetime resolution = yes ","date":"2020-10-29T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-29-samba-settings/","section":"posts","tags":["Linux"],"title":"Some Usefull Samba Settings"},{"categories":["Productive"],"contents":"mermaid is a very good tool to draw simple diagram.\nofficial site\nmost example can be found in the official website.\nThe most simple example of flowchat.\ngraph TD; A--\u0026gt;B; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D; graph LR A[Square Rect] -- Link text --\u0026gt; B((Circle)) A --\u0026gt; C(Round Rect) B --\u0026gt; D{Rhombus} C --\u0026gt; D ","date":"2020-10-28T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-28-mermaid/","section":"posts","tags":["Mermaid"],"title":"Mermaid"},{"categories":["Misc"],"contents":"2020.10.27\n一早上没有喝水的陈光宗找不到杯子了。要喝水时找不到杯子已经不是第一次了，一般来说，找找厨房或者卧室就马上能找到。很奇怪的是我俩找遍了家里所有地方，连冰箱都看了，就是找不到。直到在厨房洗旧杯子时，看到微波炉的我才突然想起来，昨天陈光宗用微波炉热咖啡忘记拿出来了。\n杯子的事真相大白了，这让我俩关于人类记忆机制产生了一个猜想：记忆的开启需要一个关键物。比如找杯子时，微波炉就是开启关于杯子的记忆的关键物。但是只有关键物也不准确，因为在想起来之前，其实我们已经看到过微波炉很多遍了。而日常中的经验也是这样的规律，直到找到了物品，才想起来曾经自己有意无意把它放在这了。奇怪的是，突然想起来的记忆又非常清楚，当时的情景也十分鲜活。\n其实这不是我们俩第一次讨论关于记忆产生的机制了。上一次讨论是在找三笔画的字的时候。字与字之间基本没有逻辑关系可言，那为什么有的字就会突然从脑袋里冒出来呢？这种突然出现又毫无逻辑的记忆，真的没有规律可循吗？也许本质上是非常简单的机制，但是在想到之前，它就是琢磨不透。这么一想，简直就和找东西本质一样嘛（笑）。\n可能人的大部分记忆是基于某个场景重构的。利用脑海里元素拼接而成。而非真正的记住物体和历史。\n","date":"2020-10-27T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-27-%E5%85%B3%E4%BA%8E%E8%AE%B0%E7%9A%84%E6%9C%BA%E5%88%B6%E7%9A%84%E7%8C%9C%E6%83%B3part1/","section":"posts","tags":["Idea"],"title":"关于记的机制的猜想Part1"},{"categories":["Misc"],"contents":"vim skill vimgrep is same as vim.\nsearch a string in current folder :vim /\u0026lt;pattern\u0026gt;/gj * search a string in current folder and all subfolder :vim /\u0026lt;pattern\u0026gt;/gj ** search a string in current folder and all subfolder only search in source file :vim /\u0026lt;pattern\u0026gt;/gj */**.c open file with line number vim +\u0026lt;line_number\u0026gt; \u0026lt;filename\u0026gt; remove space at the end of line %s/\\s\\+$//g 可以删除行尾多余的空格和tab符号。\n%s/ *$//g /和*中间有空格，只能删除行尾空格，对于tab不能删除\n","date":"2020-10-26T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-26-vim-skill/","section":"posts","tags":["Vim"],"title":"Vim Skill"},{"categories":["Program"],"contents":"notice: this post used gcc, clang, pelle C, to test code. including C99, C17 standard\nI didn’t change integer length in printf function except long long(64 bits)\nConclusion Do not trust C when doing compare using integer with overflow case. It is very stupid.\nExpression C use complement to store numbers wikipedia\nuse following code will easy to find out how numbers store in C.\nprintf(\u0026quot;%x\\n\u0026quot;,1); // 1 printf(\u0026quot;%x\\n\u0026quot;,2); // 2 printf(\u0026quot;%x\\n\u0026quot;,-1); // ffffffff printf(\u0026quot;%x\\n\u0026quot;,-2); // fffffffe printf(\u0026quot;%x\\n\u0026quot;,0); // 0 printf(\u0026quot;%x\\n\u0026quot;,-0); // 0 printf(\u0026quot;%d\\n\u0026quot;,0x80000000); // the minium value -2147483648 printf(\u0026quot;%d\\n\u0026quot;,0xffffffff); // -1 printf(\u0026quot;%d\\n\u0026quot;,0x7fffffff); // largest value 2147483647 examination: why 0x8000 0000 is minimum value the first one means this number is negative value. Use complement number low, negative value should be computer as ~0x8000 0000 +1 so it is 0x8000 0000 = 2147483648. So in C the integer from -2147483648 ~ 2147483647\nOverflow Will show what happen when plus overflow in expression with cast and without cast.\nPlus Overflow uint8_t use following code to see what will happen if overflow\nuint8_t a, b; a = 2; b = 0xff; printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, (uint8_t)(a+b)); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, (uint8_t)(a+b)); printf(\u0026quot;a = %u, b = %u, a + b \u0026gt; b : %d\\n\u0026quot;, a, b, a + b \u0026gt; b); printf(\u0026quot;a = %u, b = %u, a + b \u0026gt; b : %d\\n\u0026quot;, a, b, (uint8_t)(a + b) \u0026gt; b); result\na = 0x2, b = 0xff, a+b = 0x101 a = 2, b = 255, a+b = 257 a = 0x2, b = 0xff, a+b = 0x1 a = 2, b = 255, a+b = 1 a = 2, b = 255, a + b \u0026gt; b : 1 a = 2, b = 255, a + b \u0026gt; b : 0 notice: result will be different in this case\nfrom the code and result, Overflows generated during the calculation process will be convert to 32 bits. If cast it to uint8_t then it will retain last 8 bits.\nuint32_t next, that we see what will happen when overflow happen in uint32_t. Because machine always 32 bits or 64 bits. So this may be a little different.\nuint32_t a, b; a = 2; b = 0xffffffff; printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, (uint32_t)(a+b)); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, (uint32_t)(a+b)); printf(\u0026quot;a = %u, b = %u, a + b \u0026gt; b : %d\\n\u0026quot;, a, b, a + b \u0026gt; b); printf(\u0026quot;a = %u, b = %u, a + b \u0026gt; b : %d\\n\u0026quot;, a, b, (uint32_t)(a + b) \u0026gt; b); result\na = 0x2, b = 0xffffffff, a+b = 0x1 a = 2, b = 4294967295, a+b = 1 a = 0x2, b = 0xffffffff, a+b = 0x1 a = 2, b = 4294967295, a+b = 1 a = 2, b = 4294967295, a + b \u0026gt; b : 0 a = 2, b = 4294967295, a + b \u0026gt; b : 0 Automatically cast to 32 bits.\nuint64_t uint64_t a, b; a = 2; b = 0xffffffffffffffff; printf(\u0026quot;a = 0x%llx, b = 0x%llx, a+b = 0x%llx\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = %llu, b = %llu, a+b = %llu\\n\u0026quot;,a, b, a+b); printf(\u0026quot;a = 0x%llx, b = 0x%llx, a+b = 0x%llx\\n\u0026quot;,a, b, (uint64_t)(a+b)); printf(\u0026quot;a = %llu, b = %llu, a+b = %llu\\n\u0026quot;,a, b, (uint64_t)(a+b)); printf(\u0026quot;a = %llu, b = %llu, a - b \u0026gt; b : %d\\n\u0026quot;, a, b, a + b \u0026gt; b); printf(\u0026quot;a = %llu, b = %llu, a - b \u0026gt; b : %d\\n\u0026quot;, a, b, (uint64_t)(a + b) \u0026gt; b); result\na = 0x2, b = 0xffffffffffffffff, a+b = 0x1 a = 2, b = 18446744073709551615, a+b = 1 a = 0x2, b = 0xffffffffffffffff, a+b = 0x1 a = 2, b = 18446744073709551615, a+b = 1 a = 2, b = 18446744073709551615, a - b \u0026gt; b : 0 a = 2, b = 18446744073709551615, a - b \u0026gt; b : 0 Automatically cast to 64 bits.\nMinus Overflow uint8_t uint8_t a, b; a = 2; b = 0xff; printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, a-b); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, a-b); printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, (uint8_t)(a-b)); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, (uint8_t)(a-b)); printf(\u0026quot;a = %u, b = %u, a - b \u0026gt; 0 : %d\\n\u0026quot;, a, b, a - b \u0026gt; 0); printf(\u0026quot;a = %u, b = %u, a - b \u0026gt; 0 : %d\\n\u0026quot;, a, b, (uint8_t)(a - b) \u0026gt; 0); result\na = 0x2, b = 0xff, a+b = 0xffffff03 a = 2, b = 255, a+b = 4294967043 a = 0x2, b = 0xff, a+b = 0x3 a = 2, b = 255, a+b = 3 a = 2, b = 255, a - b \u0026gt; 0 : 0 a = 2, b = 255, a - b \u0026gt; 0 : 1 notice: different in last two expression\nuint32_t uint32_t a, b; a = 2; b = 0xff; printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, a-b); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, a-b); printf(\u0026quot;a = 0x%x, b = 0x%x, a+b = 0x%x\\n\u0026quot;,a, b, (uint32_t)(a-b)); printf(\u0026quot;a = %u, b = %u, a+b = %u\\n\u0026quot;,a, b, (uint32_t)(a-b)); printf(\u0026quot;a = %u, b = %u, a - b \u0026gt; 0 : %d\\n\u0026quot;, a, b, a - b \u0026gt; 0); printf(\u0026quot;a = %u, b = %u, a - b \u0026gt; 0 : %d\\n\u0026quot;, a, b, (uint32_t)(a - b) \u0026gt; 0); result\na = 0x2, b = 0xff, a+b = 0xffffff03 a = 2, b = 255, a+b = 4294967043 a = 0x2, b = 0xff, a+b = 0xffffff03 a = 2, b = 255, a+b = 4294967043 a = 2, b = 255, a - b \u0026gt; 0 : 1 a = 2, b = 255, a - b \u0026gt; 0 : 1 ","date":"2020-10-25T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-25-c-integer-overflow-and-compare/","section":"posts","tags":["C","Programming"],"title":"C Integer Overflow And Compare"},{"categories":["Math"],"contents":"Convex definition Definition. A function \\(f\\) defined on a convex set \\(\\Omega\\) is said to be convex if, for every \\(\\mathbf{x}_{1}, \\mathbf{x}_{2} \\in\\) \\(\\Omega\\) and every \\(\\alpha, 0 \\leqslant \\alpha \\leqslant 1,\\) there holds\n\\[ \\begin{equation} f\\left(\\alpha \\mathbf{x}_{1}+(1-\\alpha) \\mathbf{x}_{2}\\right) \\leqslant \\alpha f\\left(\\mathbf{x}_{1}\\right)+(1-\\alpha) f\\left(\\mathbf{x}_{2}\\right) \\end{equation} \\]\nIf, for every \\(\\alpha, 0\u0026lt;\\alpha\u0026lt;1,\\) and \\(\\mathbf{x}_{1} \\neq \\mathbf{x}_{2},\\) there holds\n\\[ \\begin{equation} f\\left(\\alpha \\mathbf{x}_{1}+(1-\\alpha) \\mathbf{x}_{2}\\right)\u0026lt;\\alpha f\\left(\\mathbf{x}_{1}\\right)+(1-\\alpha) f\\left(\\mathbf{x}_{2}\\right) \\end{equation} \\]\nthen \\(f\\) is said to be strictly convex.\nConcave definition Definition. A function \\(g\\) defined on a convex set \\(\\Omega\\) is said to be concave if the function \\(f=-g\\) is convex. The function \\(g\\) is strictly concave if \\(-g\\) is strictly convex.\nCombinations of Convex Functions Proposition 1. Let \\(f_{1}\\) and \\(f_{2}\\) be convex functions on the convex set \\(\\Omega .\\) Then the function \\(f_{1}+f_{2}\\) is convex on \\(\\Omega\\)\nProposition 2. Let \\(f\\) be a convex function over the convex set \\(\\Omega .\\) Then the function af is convex for any \\(a \\geqslant 0 .\\)\nProposition \\(3 .\\) Let \\(f\\) be a convex function on a convex set \\(\\Omega\\). The set \\(\\Gamma_{c}=\\{\\mathbf{x}: \\mathbf{x} \\in\\) \\(\\Omega, f(\\mathbf{x}) \\leqslant c\\}\\) is convex for every real number \\(c .\\)\nProposition 4. Let \\(f \\in C^{1} .\\) Then \\(f\\) is convex over a convex set \\(\\Omega\\) if and only if \\[ f(\\mathbf{y}) \\geqslant f(\\mathbf{x})+\\mathbf{\\nabla} f(\\mathbf{x})(\\mathbf{y}-\\mathbf{x}) \\] for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbf{\\Omega}\\)\nProposition \\(5 .\\) Let \\(f \\in C^{2} .\\) Then \\(f\\) is convex over a convex set \\(\\Omega\\) containing an interior point if and only if the Hessian matrix \\(\\mathbf{F}\\) of \\(f\\) is positive semidefinite throughout \\(\\Omega\\).\nTheorem 1. Let \\(f\\) be a convex function defined on the convex set \\(\\Omega\\). Then the set \\(\\Gamma\\) where \\(f\\) achieves its minimum is convex, and any relative minimum of \\(f\\) is a global minimum.\n","date":"2020-10-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-24-convex-and-concave-function/","section":"posts","tags":["Math","Linear Programming"],"title":"Convex And Concave Functions"},{"categories":["Math"],"contents":"Proposition 1. (First-order necessary conditions). Let \\(\\Omega\\) be a subset of \\(E^{n}\\) and let \\(f \\in C^{1}\\) be a function on \\(\\Omega\\). If \\(\\mathbf{x}^{*}\\) is a relative minimum point of \\(f\\) over \\(\\Omega,\\) then for any \\(\\mathbf{d} \\in E^{n}\\) that is a feasible direction at \\(\\mathbf{x}^{*}\\), we have \\(\\nabla f\\left(\\mathbf{x}^{*}\\right) \\mathbf{d} \\geqslant 0 .\\)\nCorollary. (Unconstrained case). Let \\(\\Omega\\) be a subset of \\(E^{n},\\) and let \\(f \\in C^{1}\\) be function on \\(\\Omega\\). If \\(\\mathbf{x}^{*}\\) is a relative minimum point of \\(f\\) over \\(\\Omega\\) and if \\(\\mathbf{x}^{*}\\) is an interior point of \\(\\Omega,\\) then \\(\\nabla f\\left(\\mathbf{x}^{*}\\right)=0\\)\n","date":"2020-10-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions/","section":"posts","tags":["Math","Linear Programming"],"title":"First Order necessary condition"},{"categories":["Productive"],"contents":"windeployqt\nThis tool will help deploy qt problem.\nusage:\nwindeployqt \u0026lt;execute file name\u0026gt; and using enigma Virtual box to package files to one file.\nIf deploy as a folder upx can be used to shrink the size.\nenigma Virtual box\nupx\n","date":"2020-10-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-24-qt-deploy/","section":"posts","tags":["Qt","Compress"],"title":"Qt Deploy And Compress"},{"categories":["Math"],"contents":"Proposition 1. (Second-order necessary conditions). Let \\(\\Omega\\) be a subset of \\(E^{n}\\) and let \\(f \\in C^{2}\\) be a function on \\(\\Omega\\). If \\(\\mathbf{x}^{*}\\) is a relative minimum point of \\(f\\) over \\(\\Omega,\\) then for any \\(\\mathbf{d} \\in E^{n}\\) that is a feasible direction at \\(\\mathbf{x}^{*}\\) we have\n\\(\\nabla f\\left(\\mathbf{x}^{*}\\right) \\mathbf{d} \\geqslant 0\\) if \\(\\nabla f\\left(\\mathbf{x}^{*}\\right) \\mathbf{d}=0,\\) then \\(\\mathbf{d}^{T} \\nabla^{2} f\\left(\\mathbf{x}^{*}\\right) \\mathbf{d} \\geqslant 0\\) Proposition 2. (Second-order necessary conditions-unconstrained case). Let \\(\\mathbf{x}^{*}\\) be an interior point of the set \\(\\Omega,\\) and suppose \\(\\mathbf{x}^{*}\\) is a relative minimum point over \\(\\Omega\\) of the function \\(f \\in C^{2} .\\) Then\n\\(\\nabla f\\left(\\mathbf{x}^{*}\\right)=0\\) for all \\(\\mathbf{d}, \\mathbf{d}^{T} \\nabla^{2} f\\left(\\mathbf{x}^{*}\\right) \\mathbf{d} \\geqslant 0\\) Proposition \\(3 .\\) (Second-order sufficient conditions-unconstrained case). Let \\(f \\in C^{2}\\) be function defined on a region in which the point \\(\\mathbf{x}^{*}\\) is an interior point. Suppose in addition that\n\\(\\nabla f\\left(x^{*}\\right)=0\\) \\(\\mathbf{F}\\left(\\mathbf{x}^{*}\\right)\\) is positive definite Then \\(\\mathbf{x}^{*}\\) is a strict relative minimum point of \\(f\\) ","date":"2020-10-24T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-24-second-order-necessary-conditions/","section":"posts","tags":["Math","Linear Programming"],"title":"Second Order necessary condition"},{"categories":["Machine Learning"],"contents":"Receiver Operating Characteristic contingency table or confusion matrix, image-20201022185345033 \\[ recall = TPR = \\frac{T P}{T P+F N} \\]\n\\[ \\mathrm{FPR}=\\frac{\\mathrm{FP}}{\\mathrm{FP}+\\mathrm{TN}} \\]\nreceiver operating example img Linear Regression using linear function to minimize the cost function.\nSeems like linear kernel SVM. The difference is SVM only consider support vector, linear regression will consider all samples.\nExperiment Result Food No Food 723 62 No 838 2265 for threshold = 0.1\nResult distribution (Histogram will be better) image-20201022192311836 ROC curve image-20201022192432434 ","date":"2020-10-22T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-22-linear-regression-and-roc/","section":"posts","tags":["ML"],"title":"Linear Regression And Roc"},{"categories":["Productive"],"contents":"File -\u0026gt; Preferences -\u0026gt; Image -\u0026gt; upload images\n{ \u0026quot;picBed\u0026quot;: { \u0026quot;current\u0026quot;: \u0026quot;github\u0026quot;, \u0026quot;github\u0026quot;: { \u0026quot;repo\u0026quot;: \u0026quot;chen-gz/picBed\u0026quot;, \u0026quot;branch\u0026quot;: \u0026quot;master\u0026quot;, \u0026quot;token\u0026quot;: \u0026quot;35823da939bd7717afde1449c852a25a26d73bfe\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;customUrl\u0026quot;: \u0026quot;\u0026quot; } }, \u0026quot;picgoPlugins\u0026quot;: {} } ","date":"2020-10-19T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-19-typora-picgo-setup/","section":"posts","tags":["Productive"],"title":"Typora PicGo Setup"},{"categories":["Misc"],"contents":" Memory_L01 Memory_L02 Memory_L03 Memory_L04_PCM all ECE 2263 Memory_L05_RRAM 1 ECE 2263 Memory_L06_RRAM II ECE 2263 Memory_L07_v3_Neuromorphic Intro(1) ECE 2263 Memory_L08_Neuromorphic Learning ECE 2263 Memory_L09_Synaptic Electronics ","date":"2020-10-17T11:00:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-17-emerging-memory/","section":"posts","tags":["Memory"],"title":"Emerging Memory Technology"},{"categories":["Algorithm"],"contents":"Problem Description After graduation, you decide to join ride sharing company Uber. An important problem of Uber is to assign drivers to different locations in order to maximize the sum of all driver’s payoffs, also called the welfare. Suppose there are n drivers to be assigned to l locations. How much payoff a driver can receive at location k will depend on how many drivers in total are at k since they compete with each other. Formally, any driver will receive payoff \\(r_k(n_k) \u0026gt; 0\\) at location \\(k\\) if location \\(k\\) is assigned \\(n_k\\) drivers in total, where \\(n_k\\) may equal \\(0, 1, 2,..., n\\). A driver assignment can be represented as \\((n1; · · · ; nl)\\) where \\(\\sum_{k=1}^l n_k \\leq n\\) and \\(n_k ≥ 0\\) is the number of drivers assigned to location \\(k\\).\nGiven input \\(r_{k}(i)\\ k=1, \\cdots, l , i=0, \\cdots, n\\) you are asked to design an \\(O(n^{2} l)\\) time algorithm to compute the driver assignment to maximize the total welfare \\(\\sum_{k=1}^{l} r_{k}(n_{k}).\\) Please describe the pseudo code, and prove the correctness and running time of your algorithm.\nGuideline Formula the problem first.\n\\[ \\max \\sum_{k=0}^{l} r_{k}\\left(n_{k}\\right) \\\\ n_{1}+n_{2}+\\cdots+n_{l} \\leqslant n \\\\ n_{i}\\geqslant0 \\text { for } i=0,1, \\cdots, l \\]\nThen we define \\(f_{i} (n)\\)\n\\[ f_{i} (n)=\\max \\sum_{k=0}^{i} r_{k}(n_{k}) \\\\ \\sum_{k=0}^{i} n_{k} =n \\]\nWe can easily find max value in function \\(f_{i}\\) by time complexity \\(O(n)\\).\nIn order to find the final result. we need to find \\(f_{i}\\), \\(i\\) from 1 to \\(l\\).\nFinally The answer is max value in function \\(f_l\\).\nTo find a \\(f_i\\) we need time \\(O(n^2)\\). Totally we have number \\(l\\) of \\(f_i\\) need to find. So the time complexity is \\(O(n^2l)\\). To get the answer we need \\(O(n)\\). Totally complexity is \\(O(n^2l + n ) = O(n^2l)\\)\n","date":"2020-10-16T22:32:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-16-welfare-optimal-driver-assignment-in-fide-sharing/","section":"posts","tags":["Algorithm"],"title":"Welfare-Optimal Driver Assignment in Ride Sharing"},{"categories":["Linux"],"contents":" Get CPU information\ncat /proc/cpuinfo change default terminal\ngsettings set org.cinnamon.desktop.default-applications.terminal exec kitty modify system PATH\nexport PATH=$PATH:/place/with/the/file View all network socket and connect status\nnetstat -na system sys TCP syn cookies status(default is on)\nsysctl -a | grep cookie (Display the SYN cookie flag) sysctl -w net.ipv4.tcp_syncookies=0 (turn off SYN cookie) sysctl -w net.ipv4.tcp_syncookies=1 (turn on SYN cookie) Netwox Tool 76\nimage-20201017151944946 Netwox Tool 78\nimage-20201017152400707 synchronize time\ntimedatectl set-ntp true ","date":"2020-10-17T00:00:00Z","permalink":"https://zongpitt.com/posts/2020-10-17-useful-linux-command/","section":"posts","tags":["Linux"],"title":"Useful Linux Command"},{"categories":["Algorithm"],"contents":"Problem Description After graduation, you decide to join Google. As a leading search engine company, an important problem Google faces is to evaluate the quality of a ranking of web pages. Many measures can capture this quality. In this question, we will consider the number of inverse pairs. Concretely, suppose you are asked o evaluate the ranking of \\(n\\) web pages. Web page \\(i\\) is ranked at position \\(i\\) on a search outcome page by Google. You are given \\(t_i\\) as the number of times that web page \\(i\\) was visited in the past. For any pair of pages \\(i, j\\), we call it an inverse pair if \\(i \u0026lt; j\\) but \\(t_i \u0026lt; t_j\\), i.e., page \\(i\\) is ranked before page j but actually was visited less in the past.\nGiven any input sequence \\(t_1, t_2, · · · , t_n\\) (non-negative integers), you are asked to design an \\(O(n \\log n)\\) time algorithm to correctly output the number of inverse pairs in this sequence.\nGuideline If using basic idea to compare all pair, \\((1,2)\\), \\((1,3)\\), …, \\((1,n)\\), (2,3), …,\\((2,n)\\),…. The time will \\(O(n^2)\\). Somehow we need to improvement the time complexity.\nWhen we saw the algorithm time complexity is \\(O(n \\log n )\\) that means we can sort the array first. Base on \\(t\\). Then we know the rank exactly position.\norigin rank 1 2 3 4 5 visit 3 2 4 1 5 new rank 3 4 2 5 1 consider another format\nnew rank 1 2 3 4 5 origin rank 5 3 1 2 4 sweep left to right.\nencounter 5 that means 4 inverse pair.\nthe table become\nnew rank 1 2 3 4 origin rank 3 1 2 4 encounter 3 that means 2 inverse pair.\nthe table become\nnew rank 1 2 3 origin rank 1 2 3 encounter 1 that means (0) inverse pair.\nnew rank 1 2 origin rank 1 2 encounter 1 that means (0) inverse pair.\nnew rank 1 origin rank 1 encounter 1 that means (0) inverse pair.\nUsing C++ set, then the insert, find only \\(O(\\log n)\\).\n","date":"2020-10-16T18:39:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-16-evaluation-of-page-ranking/","section":"posts","tags":["Algorithm"],"title":"Evaluation of Page Ranking"},{"categories":["Linux"],"contents":"We don’t need using root to execute the file.\nInstall Location Create an folder in /opt/ and change owner to normal user\nmkdir Mathematica chown zong:users mathematica provide following position for Mathematica\n/opt/Mathematica/11.3 create directory type y\nit will start installing\nStart Script in order to start Mathematica and change nothing in our system.\nCreate a folder bin under Mathematica folder.\ncd /opt/Mathematica/11.3 mkdir bin chown zong:users bin type following command in Mathematica install terminal\n/opt/Mathematica/11.3/bin Export Mathematica Bin Path I am using xinit to start the system. So I add it to xinitrc\ncd /etc/X11/xinit/ vim xinitc export PATH=$PATH:\u0026#39;/opt/Mathematica/11.3/bin/\u0026#39; restart the system. it will work.\nSome problem if encounter problem, some symbol is undefined. Do following step\ncd /opt/Mathematica/11.3/SystemFiles/Libraries/Linux-x86-64 mv libz.so.1 libz.so.1.bak mv libfreetype.so.6 libfreetype.so.6.bak mv libfreetype.so libfreetype.so.bak Register Mathematica Buy a license from Wolfram.\n","date":"2020-10-11T20:20:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-11-mathematica-install/","section":"posts","tags":["Linux","Mathematics"],"title":"Mathematica Install"},{"categories":["Misc"],"contents":"Because I am using nvim as git different tool, following configuration is required.\n[user] email = guangzong@pitt.edu name = Guangzong Chen [merge] tool = vimdiff [mergetool] keepBackup = false [mergetool \u0026quot;vimdiff\u0026quot;] cmd = nvim -d $LOCAL $REMOTE $MERGED -c \u0026#39;$wincmd w\u0026#39; -c \u0026#39;wincmd J\u0026#39; [diff] tool = vimdiff [difftool] keepBackup = false [difftool \u0026quot;vimdiff\u0026quot;] cmd = nvim -d $LOCAL $REMOTE -c \u0026#39;$wincmd w\u0026#39; ","date":"2020-10-10T23:49:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-10-git-configuration/","section":"posts","tags":["Linux","Git","Vim"],"title":"Git Configuration"},{"categories":["Linux"],"contents":"install zsh and plugins in order to keep system clear and I don’t need too much functions. So I only using the package which already in archlinux package.\npacman -Syu zsh zhs-syntax-highlighting zsh-theme-powerlevel10k` pacman -Syu zsh-autosuggestings After these we need zshrc to configure zsh.\nFollowing command to get my zsh configuration\nwget guanngzong.xyz ./assets/linux/conf/zsh for zsh-theme-powerlevel10k theme, we just using following command to start configuration\n","date":"2020-10-08T23:54:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-08-setup-zsh/","section":"posts","tags":["Linux","Zsh","Conf"],"title":"Setup ZSH"},{"categories":["Misc"],"contents":"install neovim install neovim and python-pynvim. Install python-pynvim in order to support python\nbasic settings show line number\nset number show invisible number\nset list set listchars=tab:\u0026gt;- share system clipboard. in Archlinux package xclip is required\nset clipboard=unnamedplus convert tab to space\nset expandtab spell check\nset spell spelllang=en_us plug-in plug-in manager vim-plug\nI am using neovim, so following command used to install vim-plug.\nsh -c \u0026#39;curl -fLo \u0026quot;${XDG_DATA_HOME:-$HOME/.local/share}\u0026quot;/nvim/site/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\u0026#39; put plug-in name between begin and end\ncall plug#begin(\u0026#39;~/.config/nvim/plugin\u0026#39;) call plug#end() theme to make eye more comfortable Plug \u0026#39;morhetz/gruvbox\u0026#39; colorscheme gruvbox set background=dark powerful completion Plug \u0026#39;zxqfl/tabnine-vim\u0026#39; reformation document. If using python, package autopep8 is required. Plug \u0026#39;Chiel92/vim-autoformat\u0026#39; transparent vim background highlight Normal guibg=NONE ctermbg=None For sidebar, font nerd-fonts-hack is required. Plug \u0026#39;Shougo/defx.nvim\u0026#39;, { \u0026#39;do\u0026#39;: \u0026#39;:UpdateRemotePlugins\u0026#39; } Plug \u0026#39;kristijanhusak/defx-git\u0026#39; Plug \u0026#39;kristijanhusak/defx-icons\u0026#39; call defx#custom#option(\u0026#39;_\u0026#39;, { \\ \u0026#39;winwidth\u0026#39;: 30, \\ \u0026#39;split\u0026#39;: \u0026#39;vertical\u0026#39;, \\ \u0026#39;show_ignored_files\u0026#39;: 0, \\ \u0026#39;buffer_name\u0026#39;: \u0026#39;defxplorer\u0026#39;, \\ \u0026#39;toggle\u0026#39;: 1, \\ \u0026#39;resume\u0026#39;: 1 \\ }) \u0026quot; Toggle Defx using Ctrl + Space map \u0026lt;C-space\u0026gt; :Defx\u0026lt;CR\u0026gt; autocmd FileType defx call s:defx_my_settings() function! s:defx_my_settings() abort \u0026quot; Define mappings nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; \u0026lt;CR\u0026gt; defx#do_action(\u0026#39;drop\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; c defx#do_action(\u0026#39;copy\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; m defx#do_action(\u0026#39;move\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; p defx#do_action(\u0026#39;paste\u0026#39;) \u0026quot; nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; l defx#do_action(\u0026#39;open\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; v defx#do_action(\u0026#39;open\u0026#39;, \u0026#39;vsplit\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; P defx#do_action(\u0026#39;preview\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; o defx#do_action(\u0026#39;open_or_close_tree\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; K defx#do_action(\u0026#39;new_directory\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; N defx#do_action(\u0026#39;new_file\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; M defx#do_action(\u0026#39;new_multiple_files\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; C defx#do_action(\u0026#39;toggle_columns\u0026#39;, \u0026#39;mark:indent:icon:filename:type:size:time\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; S \\ defx#do_action(\u0026#39;toggle_sort\u0026#39;, \u0026#39;time\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; d \\ defx#do_action(\u0026#39;remove\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; r defx#do_action(\u0026#39;rename\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; ! defx#do_action(\u0026#39;execute_command\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; x defx#do_action(\u0026#39;execute_system\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; yy defx#do_action(\u0026#39;yank_path\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; . defx#do_action(\u0026#39;toggle_ignored_files\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; ; defx#do_action(\u0026#39;repeat\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; h defx#do_action(\u0026#39;cd\u0026#39;, [\u0026#39;..\u0026#39;]) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; ~ defx#do_action(\u0026#39;cd\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; q defx#do_action(\u0026#39;quit\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; \u0026lt;Space\u0026gt; defx#do_action(\u0026#39;toggle_select\u0026#39;) . \u0026#39;j\u0026#39; nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; * defx#do_action(\u0026#39;toggle_select_all\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; j line(\u0026#39;.\u0026#39;) == line(\u0026#39;$\u0026#39;) ? \u0026#39;gg\u0026#39; : \u0026#39;j\u0026#39; nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; k line(\u0026#39;.\u0026#39;) == 1 ? \u0026#39;G\u0026#39; : \u0026#39;k\u0026#39; nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; \u0026lt;C-l\u0026gt; defx#do_action(\u0026#39;redraw\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; \u0026lt;C-g\u0026gt; defx#do_action(\u0026#39;print\u0026#39;) nnoremap \u0026lt;silent\u0026gt;\u0026lt;buffer\u0026gt;\u0026lt;expr\u0026gt; cd defx#do_action(\u0026#39;change_vim_cwd\u0026#39;) endfunction tabsize set tabstop=4 set shiftwidth=4 set expandtab setup mouse\nset mouse=a setup encoding\nset encoding=UTF-8 terminal use Esc to return normal mode\ntnoremap \u0026lt;Esc\u0026gt; \u0026lt;C-\\\u0026gt;\u0026lt;C-n\u0026gt; setup invisible character\nset list set listchars=tab:\u0026gt;-,space:· leader e to open file menu\nnmap \u0026lt;silent\u0026gt; \u0026lt;Leader\u0026gt;e :Defx -columns=icons:indent:filename:type \u0026lt;cr\u0026gt; configure file\n","date":"2020-10-08T23:54:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-10-vim-configuration/","section":"posts","tags":["Linux","Vim","Conf"],"title":"Vim Configuration"},{"categories":null,"contents":"install neovim install neovim and python-pynvim. Install python-pynvim in order to support python\nvim-shortcut default collection and have the same dicimal keycode. Becareful.\nZZ save and quit Z = \u0026lt;S-z\u0026gt; za toggle fold :windo diffthis to show the differnt between two buffer. :windo diffoff turn off the differnt view. ","date":"2020-10-08T23:54:00-04:00","permalink":"https://zongpitt.com/posts/misc/vim-configuration/","section":"posts","tags":["Linux","vim"],"title":"Vim Configuration"},{"categories":["Linux"],"contents":"i3 pacman -Syu i3-gap xorg-xint we are using startx to start i3. So change /etc/X11/xinit/xinitrc uncomment last four lines and add follwing lines\nexport GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx exec i3 Before we start i3, we need some configuration. Otherwise we can no do anything aferter i3 been started.\nThe configuration file for i3 is here. Follwing command can be use to set up configuration.\nmkdir -p ~/.config/i3 wget https://gist.githubusercontent.com/chen-gz/83eade557baf20a4021af321d973d8fe/raw/d0f0fdcfa03f32f9497d08689dad14c7fcbf54ec/i3_config -o ~/.config/i3/config ROFI First, i am using rofi as window switch. So we need install and configuration rofi.\npacman -Syu rofi Polybar Second I am using polybar instread of i3status. polybar is from AUR. Because of polybar require font siji so we need package siji-git. siji-git is from AUR.\nhttps://aur.archlinux.org/polybar.git https://aur.archlinux.org/siji-git.git after install siji need to refresh the font(using root user), otherwise the fond still doesn’t work.\nfc-cache -f -v Polybar need configuration file. you can use following command to copy this configuration to you computer.\nmkdir -p ~/.config/polybar wget guangzong.xyz/asset/linux/conf/polybar/config -o ~/.config/polybar/config wget guangzong.xyz/asset/linux/conf/polybar/launch.sh -o ~/.config/polybar/launch.sh feh feh is for background.\npacman -Syu feh xterm cd ~ wget guangzong.xyz/asset/linux/conf/xterm/Xresources -o ~/.Xresources make it works\nxrdb .Xresources ","date":"2020-10-08T10:12:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-08-setup-i3/","section":"posts","tags":["Linux","I3"],"title":"Setup i3"},{"categories":["Linux"],"contents":"Install pacman -Syu vpnc Configuration copy follwing configuration to /etc/vpnc/\u0026lt;name\u0026gt;.conf\nIPSec gateway vpn.pitt.edu IPSec ID sam_users IPSec secret ooL6ohho Xauth username guc25 ","date":"2020-10-06T17:58:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-06-setup-vpnc/","section":"posts","tags":["Linux","Vpnc"],"title":"Setup VPNC"},{"categories":["Linux"],"contents":"Refer to archlinux wiki and archlinux wiki - goldendict\nInstall pacman -Syu Sdcv Once you have the appropriate files you can extract them into /usr/share/stardict/dic.\nFollowing command can be used.\nmkdir -p /usr/share/stardict/dic \u0026amp;\u0026amp; cd /usr/share/stardict/dic wget \u0026quot;https://github.com/chen-gz/picBed/raw/master/langdao-ec-gb.dict.dz\u0026quot; \u0026amp;\u0026amp; wget \u0026quot;https://raw.githubusercontent.com/chen-gz/picBed/master/langdao-ec-gb.idx\u0026quot; \u0026amp;\u0026amp; wget \u0026quot;https://raw.githubusercontent.com/chen-gz/picBed/master/langdao-ec-gb.ifo\u0026quot; Dictionary files\ndict index info ","date":"2020-10-06T14:57:00-04:00","permalink":"https://zongpitt.com/posts/misc/2020-10-06-setup-star-dictionary/","section":"posts","tags":["Linux","Software"],"title":"Setup Star and GoldenDict Dictionary"},{"categories":["Machine Learning"],"contents":"For each image k have 200 tag and confidence level.\nimage tag1 confidence level … tag200 confidence level 1 tag1_1_name p1_2 tag200_1_name p1_200 2 tag1_2_name p2_1 tag200_2_name p2_200 … n tag1_n_name pn_1 tag200_n_name pn_200 Totally, 3641 tag appeared in all picture. Pict the tag with the highest frequency.\nconstruct the vector\nimage tag1 tag2 … tag310 tag311 1 p1_1 p1_2 p1_310 p1_310 2 p2_1 p2_2 p2_310 p2_310 … n pn_1 pn_2 pn_310 pn_310 e.g. P_k_i = P(tag1 | image k) which is given by clairifai.\nNew idae, instead of using condence level from clairifai. we using \\(p_{i}^{k}\\) as svm input\n\\(P(food = 1 \\| image\\ k)\\) only consider tag \\(i\\).\n\\[ p_{i}^{k} = P(food = 1, tag\\ i = 1| image\\ k) + P(food = 1, tag\\ i = 0| image\\ k) \\]\nApproximately equal to\n\\[ p = p(i|k) * p(food = 1, tag = 1) + (1-p(k|i) * p_food \\]\nThe code I am using\ntmp_x[useful_tag.index(tag_tmp)] = float(row[i+1].strip()) * tag_food_correlation_dict[tag_tmp] + (1-float(row[i+1].strip())) * p_food result using \\[p_{i}^{k}\\] as svm input\nresult1 previous result\n","date":"2020-10-06T09:44:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-06-food-clarify/","section":"posts","tags":["classification"],"title":"Food Clarify"},{"categories":["Linux"],"contents":"caddy configure file Install caddy for now caddy2 not in archlinux official repository, should using aur to install\nfollwing key is require when i was install (data: 2020-10-02)\ngpg --keyserver pool.sks-keyservers.net --receive-keys 29D0817A67156E4F25DC24782A349DD577D586A5 After install we need to setup caddy. configure file locate in /etc/caddy/Caddyfile.\nMy configure show as follow.\nguangzong.xyz:443 { # i am only using 443 port tls chen-gz@outlook.com root * path_to_you_website_file file_server } For me, my file are locate in user account. So go to /etc/lib/systemd/system/caddy.service change ProtectHome to read-only\n","date":"2020-10-02T16:02:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-02-setup-caddy/","section":"posts","tags":["Linux","Caddy"],"title":"Setup Caddy"},{"categories":["Linux"],"contents":"login logs using command last and lastb to check the login information.\nlast will show the last login information. The log file locate in /var/wtmp.\nlast will show the the user who trying to login information. The log file locate in /var/btmp.\n","date":"2020-10-02T15:15:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-02-linux-system-log/","section":"posts","tags":["Linux"],"title":"linux system log"},{"categories":["Linux"],"contents":"disable password login generate ssh key and put ssh publick to server.\nthe location of server ssh key\n${USER_HOME}/.ssh/authorized_keys open /etc/ssh/sshd_config as root user.\nuncomment foloowing lines and becareful about the option should be no\nAuthorizedKeysFile .ssh/authorized_keys PasswordAuthentication no disable using root login the system in file /etc/ssh/sshd_config the option PermitRootLogin should be no\nPermitRootLogin no change login port to protect nmap scan in /etc/ssh/sshd_config uncomment Port 22 change to another port like Port 2020\nwhen loging the system should use\nssh usename@hostname -p portnumber ","date":"2020-10-02T14:43:00-04:00","permalink":"https://zongpitt.com/posts/2020-10-02-protect-ssh-port/","section":"posts","tags":["Linux","Security"],"title":"Protect SSH Port"},{"categories":["Linux"],"contents":"Add following content to /etc/systemd/network/10-static.network\nThe service systemd-networkd is required.\n[https://wiki.archlinux.org/index.php/systemd-networkd]\n[Match] Name=enp37s0 # may be wlan0 for wireless. Using ip addr to check [Network] DNS=0.0.0.0 Address=192.168.0.200/24 Gateway=192.168.0.255 ","date":"2020-09-29T22:01:00-04:00","permalink":"https://zongpitt.com/posts/2020-09-29-setup-static-ip-address/","section":"posts","tags":["Linux","Network"],"title":"Setup Static Ip Address"},{"categories":["Linux"],"contents":"Assume already install Arch Linux. guide is here\nNetwork manager pacman -Syu networkmanager network-manager-applet dhcpcd dhclient systemctl enable NetworkManager touchpad To enable touchpad tap to click\ncopy configure file to /etc/X11/xorg.conf.d/\ncongifure file\nbacklight pacman -S acpilight and add user to video group\nreference arch wiki\ngpasswd -a user group sound install pulseaudio to support sound\npacman -S pulseaudio pavucontrol Chinese font pacman -Syu wqy-microhei Browser firefox and chrome both are good choise in linux system. I am using brave.\ngit clone https://aur.archlinux.org/brave-bin.git cd brave-bin makepkg -si plugins * vimium * Dark Mode\nDesktop I recommand i3, awesome, xfce4. Gnome and KDE are too heavy to me.\npacman -Syu i3-gaps i3status configuration for i3, i3 config\nfile should be put in ~/.config/i3/\nfile should be put in ~/.config/i3status/\nsoftware for i3\nfeh (wallpaper) dmenu (run program) picom (transparent) (fork from compton) transset-df (transparent patch) Chinese input method fcitx will be used. (ibus also a good choise)\npacman -S fcitx fcitx-configtool fcitx-googlepinyin fcitx-im recommand change Configure-\u0026gt;Global Config-\u0026gt;Trigger Input Method to shift+Space. Other keys are conflit with vim or i3.\nconfigure file need add to ~/.xprofile\nexport GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx Xterm pacman -S xterm configure file, xterm\nfile should be put in ~/\nxrdb .Xresources Vim use vim-plug to manager plugs install command\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim configuration in vim post\nYMC configuration prepare software\ncmake clang python-setuptools cd .vim/plugged/YouCompleteMe ./install.py --clangd-completer lightdm install\npacman -Syu lightdm lightdm-gtk-greeter-settings autologin\n/etc/lightdm/lightdm.conf ---- [Seat:*] autologin-user=username groupadd -r autologin gpasswd -a username autologin Basic software recommand just use follow command\npacmcan -S package name packages\nmupdf-gl (pdf viwer) gimp (like photoshop) doublecmd (file manager) flameshot (screen capture) sdcv (translation)(Arch wiki) udisks2 (mount external disk automatically) mendeley-bundle (from Aur) eog zsh install zsh\npacman -Syu zsh set zsh as default\nchsh -s /bin/zsh configure file and oh-my-zsh (will drop)\nbattery save power\npacman -Syu tlp file management recommand using nemo as file manager\npacman -Syu nemo ","date":"2020-09-22T14:30:00-04:00","permalink":"https://zongpitt.com/posts/2020-09-22-how-to-configure-archlinux-desktop/","section":"posts","tags":["Linux"],"title":"How to Configure Arch Linux desktop"},{"categories":["Linux"],"contents":"Setup network In new version of installer image, wifi-menu instead by iwctl.\nusing iwctl get into iwctl command window scan the network\nstation wlan0 scan get the network\nstation wlan0 get-networks connect to the network\nstation wlan0 connect \u0026lt;ssid\u0026gt; after connect wireless network, use ping to test network\nping google.com Synchronised time timedatectl set-ntp true Change mirrorlist choose one close to you\nvim /etc/pacman.d/mirrorlist Partition use lsblk to check disks and decide which one want to be installed archlinux system.\nuse fdisk to partion\nCreate partition table enter partition tool\nfdisk /dev/sda create gpt partition table\ng\u0026lt;Enter\u0026gt; creat partiton n\u0026lt;Enter\u0026gt; create new partion\n+200M determine the size of partion same as above.\nReformat partition and mount reformat format EFI partition\nmkfs.fat -F32 /dev/sda1 format normal partition\nmkfs.ext4 /dev/sda2 mount mount /dev/sda2 /mnt mkdir -p /mnt/boot/EFI mount /dev/sda1 /mnt/boot/EFI lsblk to check result\nInstall basic operator system pacstrap -i /mnt base base-devel linux linux-firmware Configure basic system fstab genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab to check, it should have 2 or 3 disk information\ncat /mnt/etc/fstab change to new system arch-chroot /mnt /bin/bash in this mode everything should be configure correctly one time, otherwice you will get trouble after reboot.\ninstall vim first, need an editor\npacman -Syu vim language setting select language in locale.gen\nvim /etc/locale.gen locale-gen echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf Timezone ln -sf /usr/share/zoneinfo/...... /etc/localtime set hardware time\nhwclock --systohc --utc boot and grub install boot tools\npacman -S dosfstools grub efibootmgr install grub and generator grub configure\ngrub-install --target=x86_64-efi --efi-directory=/boot/EFI --recheck grub-mkconfig -o /boot/grub/grub.cfg If grub command not working, you should try arch forum\ngrub-install --target=x86_64-efi --bootloader-id=GRUB --efi-directory=/boot/efi --no-nvram --removable set root password and add user set root password\npasswd add new user username should be name you want\nuseradd -m -g users -s /bin/bash username set new user password\npasswd username before reboot install wireless support\npacman -Syu iwd dhcpcd install xorg\npacam -S xrog ","date":"2020-09-11T15:30:00-04:00","permalink":"https://zongpitt.com/posts/2020-09-11-how-to-install-archlinux/","section":"posts","tags":["Linux"],"title":"How To Install Archlinux"},{"categories":["Electric"],"contents":"Basic Concept MOS stand for metal oxide semiconductor. FET stand for field effects transistor.\nMOSFET has four terminal, gate, source , drain and body. Source and drain actually they are same, they only been determined when there are external connection.\nGate is used for control MOSFET. Source is the emit the resources. Drain is the receive the resources. Body is main part of MOSFET. Support and provide PN node to cut off the circuit.\nResources refer to electron and hole. The resources in N type node is electron, in P type node is hole. Most case we will see the arrow in MOSFET, arrow indicate the current direction.\nThere exis capacitor in MOSEFT. there exist an insulate between gate and body. So body and gate can from and capacitor. Because of this capacitor we can control the performance of MOSFET.\nN-channel MOSFET Most time we say NMOS refer to N-channel MOS. The channel refer to the channel between two N type node.\nthe structure of N MOS Turn On N-MOS The way to turn on N-MOS is create N channel between source and drain. The idea to create N channel between source and drain is attract electron to form a channel. Under this idea, we apply \\(V_{GB} \u0026gt; 0\\) to gate and body. As mentioned before, gate and body can regard as an capacitor. So electron will be attract between source and gain. So now the current can flow between source and drain. At most time we will connect body to ground, and apply positive voltage in gate to open the NMOS.\nSource And Drain So far, source and drain are no difference. The resources in NMOS in electron. So the source is the source of electron. The electron flow from source to drain. So \\(V_{DS} \u0026gt; 0\\). After we apply the the voltage between source and drain, they are being determined. Actually, the structure of source and drain might be a little different. So just follow the data-sheet. This idea just help to understand what is source and drain.\nP-channel MOSFET The structure of P channel MOSFET as some as N channel MOSFET, just exchange N type and P type material.\nThe method to analysis the P-MOS as same as N-MOS.\nReferences https://www.electronics-tutorials.ws/transistor/tran_6.html ","date":"2020-08-31T17:35:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-30-mosfet-structure/","section":"posts","tags":["Mosfet"],"title":"MOSFET Structure"},{"categories":["Math"],"contents":"Reference mathworld Fourier Transform Willard Miller - Fourier Transform)(The pdf from website) Paul Heckbert - Fourier Transforms and the Fast Fourier Transform (FFT) Algorithm Fourier Series Concept The Fourier transform is a generalization of the complex Fourier series in the limit as \\(L \\to \\infty\\). Replace the discrete \\(A_n\\) with the continuous \\(F(k)dk\\) while letting \\(n/L\\to k\\). Then change the sum to an integral, and the equations become\n\\[ f(x)= \\int_{-\\infty}^\\infty F(k) e^{2\\pi ikx}dk \\\\ F(k)=\\int_{-\\infty}^\\infty f(x)e^{-2\\pi ikx}dx \\]\nNote that some authors (especially physicists) prefer to write the transform in terms of angular frequency \\(\\omega=2\\pi nv\\) instead of the oscillation frequency \\(v\\). However, this destroys the symmetry, resulting in the transform pair\n\\[ F(\\omega)=\\int_{-\\infty}^{\\infty} f(x) e^{-i \\omega x} dx \\\\\\ f(x)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} F(\\omega) e^{i \\omega x} d \\omega \\]\nSince any function can be split up into even and odd portions \\(E(x)\\) and \\(O(x)\\),\n\\[ \\begin{aligned} f(x) \u0026amp;= \\frac{1}{2}[f(x)+f(-x)]+\\frac{1}{2}[f(x)-f(-x)] \\\\\\ \u0026amp;=E(x)+O(x), \\end{aligned} \\]\nTo restore the symmetry of the transforms, the convention\n\\[ \\begin{aligned} g(y) \u0026amp;=\\mathcal{F}[f(t)] \\\\\\ \u0026amp;=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} f(t) e^{-i y t} d t \\\\\\ f(t) \u0026amp;=\\mathcal{F}^{-1}[g(y)] \\\\\\ \u0026amp;=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} g(y) e^{j y t} d y \\end{aligned} \\]\nis sometimes used.\nDirichlet condition In mathematics, the Dirichlet conditions are sufficient conditions for a real-valued, periodic function \\(f\\) to be equal to the sum of its Fourier series at each point where \\(f\\) is continuous. Moreover, the behavior of the Fourier series at points of discontinuity is determined as well (it is the midpoint of the values of the discontinuity). These conditions are named after Peter Gustav Lejeune Dirichlet.\nThe conditions are:\n\\(f\\) must be absolutely integrable over a period. \\(f\\) must be of bounded variation in any given bounded interval. \\(f\\) must have a finite number of discontinuities in any given bounded interval, and the discontinuities cannot be infinite. from wikipedia - Dirichlet conditions\nProperties see the references.\nWillard Miller - Fourier Transform\nI will write it here in the future.\n","date":"2020-08-30T11:51:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-30-fourier-transform/","section":"posts","tags":["Math"],"title":"Fourier Transform"},{"categories":["Math"],"contents":"Reference mathworld Discrete Fourier Transform Paul Heckbert - Fourier Transforms and the Fast Fourier Transform (FFT) Algorithm Fourier Series Fourier Transform Concept When a signal is discrete and periodic, we don’t need the continuous Fourier transform. Instead we use the discrete Fourier transform, or DFT. Suppose our signal is a n for \\(n =0 . . . N − 1\\), and \\(a_n = a_n+ jN\\) for all \\(n\\) and \\(j\\). The discrete Fourier transform of \\(a\\)\n\\[ A_{k}=\\sum_{n=0}^{N-1} e^{-i \\frac{2 \\pi}{N} k n} a_{n} \\]\nThis is more commonly written:\n\\[ A_{k}=\\sum_{n=0}^{N-1} W_{N}^{k n} a_{n} \\]\nwhere\n\\[ W_{N}=e^{-i \\frac{2 \\pi}{N}} \\]\nand \\(W_N^k\\) for \\(k = 0 . . . N − 1\\) are called the Nth roots of unity.\nThe sequence \\(a_n\\) is the inverse discrete Fourier transform of the sequence \\(A_k\\).The formula for the inverse DFT is\n\\[ a_{n}=\\frac{1}{N} \\sum_{k=0}^{N-1} W_{N}^{-k n} A_{k} \\]\nThe formula is identical except that \\(a\\) and \\(A\\) have exchanged roles, as have \\(k\\) and \\(n\\). Also,the exponent of \\(W\\) is negated, and there is a \\(\\frac{1}{N}\\) normalization in front.\n","date":"2020-08-30T11:49:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-30-discrete-fourier-transfrom/","section":"posts","tags":["Math"],"title":"Discrete Fourier Transform"},{"categories":["Math"],"contents":"Reference mathworld Fourier Series Willard Miller - Fourier Series (The pdf from website) Concept Any set of functions that form a complete orthogonal system have a corresponding generalized Fourier series analogous to the Fourier series. For example, using orthogonality of the roots of a Bessel function of the first kind gives a so-called Fourier-Bessel series.\nFourier orthogonal function system:\n\\[ 1,cosx,sinx,...,coskx,sinkx,\\dots. \\]\nthese functions that form a complete orthogonal system over \\([-\\pi,\\pi]\\) in \\(L^2[-\\pi,\\pi]\\) space.\n\\(f(x)\\) and \\(g(x)\\) orthogonality in \\(L^2[-\\pi,\\pi]\\) space means \\(\\int_{-\\pi}^{\\pi}f(x)g(x)dx =0\\).\nthe Fourier series of a function \\(f(x)\\) is given by\n\\[ f(x)=\\frac{1}{2} a_{0} + \\sum_{n=1}^{\\infty} a_n cos(nx)+\\sum_{n=1}^{\\infty} b_n sin(nx) \\]\nwhere\n\\[ a_0 =\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi}f(x)dx \\\\ a_n =\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi}f(x) \\cos(nx)dx \\\\ b_n =\\frac{1}{\\pi} \\int_{-\\pi}^{\\pi}f(x) \\sin(nx)dx \\]\nFor a function \\(f(x)\\) periodic on an interval \\([-L,L]\\) instead of \\([-\\pi,\\pi]\\), a simple change of variables can be used to transform the interval of integration from \\([-\\pi,\\pi]\\) to \\([-L,L]\\). Let\n\\[ x = \\frac{\\pi x\u0026#39;}{L} \\\\ dx = \\frac{\\pi d x\u0026#39;}{L}. \\]\nSolving for \\(x\u0026#39;\\) gives \\(x\u0026#39;=\\frac{Lx}{\\pi}\\), and plugging this in gives\n\\[ f(x\u0026#39;)=\\frac{1}{2}a_0+\\sum_{n=1}^{\\infty} a_n\\cos{\\frac{n\\pi x\u0026#39;}{L}}+\\sum_{n=1}^{\\infty}b_n\\sin{\\frac{n\\pi x\u0026#39;}{L}}. \\]\nTherefore,\n\\[ a_0 =\\frac{1}{L}\\int_{-L}^L f(x\u0026#39;)dx\u0026#39; \\\\ a_n =\\frac{1}{L}\\int_{-L}^L f(x\u0026#39;)\\cos{\\frac{n\\pi x\u0026#39;}{L}}dx\u0026#39; \\\\ b_n =\\frac{1}{L}\\int_{-L}^L f(x\u0026#39;)\\sin{\\frac{n\\pi x\u0026#39;}{L}}dx\u0026#39; \\]\nThe notion of a Fourier series can also be extended to complex coefficients.\nConsider a real-valued function \\(f(x)\\). Write\n\\[ f(x)=\\sum_{n=-\\infty}^{\\infty}A_ne^{inx}. \\]\nwhere\n\\[ A_n=\\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}f(x)e^{-inx}dx. \\]\nThe coefficients can be expressed in terms of those in the Fourier series\n\\[ \\begin{aligned} A_n \u0026amp;=\\frac{1}{2\\pi}\\int_{-\\pi}^\\pi f(x)[\\cos(nx)-i\\sin(nx)]dx \\\\ \u0026amp;= \\begin{cases} \\frac{1}{2}(a_n + i b_n) \u0026amp; n \u0026lt; 0\\\\ \\frac{1}{2}a_0 \u0026amp; n = 0\\\\ \\frac{1}{2}(a_n - i b_n) \u0026amp; n \u0026gt; 0\\\\ \\end{cases} \\end{aligned} \\]\nFor a function periodic in \\([-L/2,L/2]\\), these become\n\\[ f(x)=\\sum_{n=-\\infty}^{\\infty}A_n e^{i\\frac{2\\pi nx}{L}} \\]\nwhere\n\\[ A_n=\\frac{1}{L}\\int_{-L/2}^{L/2}f(x) e^{-i\\frac{2\\pi nx}{L}}dx \\]\nThese equations are the basis for the extremely important Fourier transform, which is obtained by transforming \\(A_n\\) from a discrete variable to a continuous one as the length \\(L\\to \\infty\\).\n","date":"2020-08-30T11:47:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-30-fourier-series/","section":"posts","tags":["Math"],"title":"Fourier Seriers"},{"categories":["Math"],"contents":"Problem from “Linear and Nonlinear Programing” problem 2.9\nLinear programming Standard Form The standard from of linear programming is\n\\[ \\begin{equation} \\begin{array}{cl} \\operatorname{minimize} \u0026amp; \\mathbf{c}_{1}^{T} \\mathbf{x}\\\\ \\text { subject to } \u0026amp; \\mathbf{Ax} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\geq 0 \\end{array} \\end{equation} \\]\nDescription of Problem A class of piecewise linear functions can be represented as \\(f(x)=\\) Maximum \\((\\mathbf{c}_{1}^{T} \\mathbf{x}+ d_{1}, \\mathbf{c}_{2}^{T} \\mathbf{x}+d_{2}, \\ldots, \\mathbf{c}_{p}^{T} \\mathbf{x}+d_{p}).\\) For such a function \\(f\\), consider the problem\n\\[ \\begin{array}{cl} \\operatorname{minimize} \u0026amp; f(\\mathbf{x}) \\\\ \\operatorname{subject to} \u0026amp; \\mathbf{A x}=\\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\geq \\mathbf{0} \\end{array} \\]\nSolution This function \\(f\\) should be expand to contains.\nThe simple idea is let \\(f\\) be a value. Let \\(y = f\\). Then we have following equations\n\\[ \\mathbf{c_i}^T \\mathbf{x} + d_i \\leq y \\]\nadd slack variables, then we have stand form of linear programming problem.\n\\[ \\begin{array}{cl} \\operatorname{minimize} \u0026amp; y \\\\ \\text { subject to } \u0026amp; \\mathbf{c}_{i}^{T} \\mathbf{x}+d_{i}+r_{i}=y, i=1,2, \\ldots, p \\\\ \u0026amp; \\mathbf{Ax} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\geq 0, \\mathbf{r} \\geq 0 \\end{array} \\]\nBut this form still not the stand form, because of \\(y\\) do not have any constrain.\nSo convert to stand form we should eliminate symbol \\(y\\). Using an of constrains.\n\\[ \\begin{array}{cl} \\operatorname{minimize} \u0026amp; \\mathbf{c}_{1}^{T} \\mathbf{x} + d_1 + r_1 \\\\ \\text { subject to } \u0026amp; \\mathbf{c}_{i}^{T} \\mathbf{x} - \\mathbf{c}_{1}^{T} \\mathbf{x} + r_{i} - r_{1} = d_1 - d_i, i=2,3 \\ldots, p \\\\ \u0026amp; \\mathbf{Ax} = \\mathbf{b} \\\\ \u0026amp; \\mathbf{x} \\geq 0, \\mathbf{r} \\geq 0 \\end{array} \\]\n","date":"2020-08-29T17:43:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-29-a-linear-programming-standard-form-problem/","section":"posts","tags":["Math","Linear Programming"],"title":"A linear Programming Standard Form Problem"},{"categories":["Misc"],"contents":"Purpose The reason I writing this post is because none of documente give right process to bulid gtk in Windows. I spent a lot to build and simple gtk problem in Windows.\nTwo Useful Link https://github.com/rust-lang/rustup/issues/846 https://www.reddit.com/r/rust/comments/86kmhu/compiling_rust_windows_gtk_stepbystep/ Installation And Build Project step to step using rust and gtk in Windows\nInstall rust using default setting. Rust Downlaod Link. Go to http://www.msys2.org/ and install msys (x86_64). Downlaod Link Run these commands inside MSYS (not Windows cmd!) pacman -S mingw-w64-x86_64-gtk3 pacman -S mingw-w64-x86_64-toolchain Make the needed environment variable linkings inside Windows cmd SET GTK_LIB_DIR=C:\\msys64\\mingw64\\lib SET PATH=%PATH%;C:\\msys64\\mingw64\\bin SETX GTK_LIB_DIR %GTK_LIB_DIR% SETX PATH %PATH% Add gnu/gtk compatible target to rustup in Windows cmd rustup target add x86_64-pc-windows-gnu Install rust gnu toolchain. rustup toolchain install stable-x86_64-pc-windows-gnu set rust toolchain to GNU rustup set default-host x86_64-pc-windows-gnu Build your porject. cargo build Remove command line window in gui application. add following line to the top of your main.rs #![windows_subsystem = \u0026quot;windows\u0026quot;] ","date":"2020-08-28T18:30:00-04:00","permalink":"https://zongpitt.com/posts/2020-08-28-using-rust-and-gtk-in-windows/","section":"posts","tags":["Rust","Gtk"],"title":"Using RUST and GTK In Windows"},{"categories":null,"contents":"We still confine ourselves to real functions in this section. We shall show that integration and differentiation are, in a certain sense, inverse operations.\n6.20 Theorem Let \\(f \\in \\mathscr{R}\\) on \\([a, b]\\). For \\(a \\leq x \\leq b\\), put \\[ F(x)=\\int_{a}^{x} f(t) d t . \\]\nThen \\(F\\) is continuous on \\([a, b]\\); furthermore, if \\(f\\) is continuous at a point \\(x_{0}\\) of \\([a, b]\\), then \\(F\\) is differentiable at \\(x_{0}\\), and\n\\[ F^{\\prime}\\left(x_{0}\\right)=f\\left(x_{0}\\right) . \\]\nProof Since \\(f \\in \\mathscr{R}, f\\) is bounded. Suppose \\(|f(t)| \\leq M\\) for \\(a \\leq t \\leq b\\). If \\(a \\leq x\u0026lt;y \\leq b\\), then\n\\[ |F(y)-F(x)|=\\left|\\int_{x}^{y} f(t) d t\\right| \\leq M(y-x), \\]\nby Theorem \\(6.12(c)\\) and \\((d)\\). Given \\(\\varepsilon\u0026gt;0\\), we see that\n\\[ |F(y)-F(x)|\u0026lt;\\varepsilon \\text {, } \\] provided that \\(|y-x|\u0026lt;\\varepsilon / M\\). This proves continuity (and, in fact, uniform continuity) of \\(F\\).\nNow suppose \\(f\\) is continuous at \\(x_{0}\\). Given \\(\\varepsilon\u0026gt;0\\), choose \\(\\delta\u0026gt;0\\) such that\n\\[ \\left|f(t)-f\\left(x_{0}\\right)\\right|\u0026lt;\\varepsilon \\]\nif \\(\\left|t-x_{0}\\right|\u0026lt;\\delta\\), and \\(a \\leq t \\leq b\\). Hence, if\n\\[ x_{0}-\\delta\u0026lt;s \\leq x_{0} \\leq t\u0026lt;x_{0}+\\delta \\quad \\text { and } \\quad a \\leq s\u0026lt;t \\leq b, \\]\nwe have, by Theorem \\(6.12(d)\\),\n\\[ \\left|\\frac{F(t)-F(s)}{t-s}-f\\left(x_{0}\\right)\\right|=\\left|\\frac{1}{t-s} \\int_{s}^{t}\\left[f(u)-f\\left(x_{0}\\right)\\right] d u\\right|\u0026lt;\\varepsilon . \\]\nIt follows that \\(F^{\\prime}\\left(x_{0}\\right)=f\\left(x_{0}\\right)\\).\n6.21 The fundamental theorem of calculus If \\(f \\in \\mathscr{R}\\) on \\([a, b]\\) and if there is a differentiable function \\(F\\) on \\([a, b]\\) such that \\(F^{\\prime}=f\\), then \\[ \\int_{a}^{b} f(x) d x=F(b)-F(a) . \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given. Choose a partition \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\) so that \\(U(P, f)-L(P, f)\u0026lt;\\varepsilon\\). The mean value theorem furnishes points \\(t_{i} \\in\\left[x_{i-1}, x_{i}\\right]\\) such that\n\\[ F\\left(x_{i}\\right)-F\\left(x_{i-1}\\right)=f\\left(t_{i}\\right) \\Delta x_{i} \\]\nfor \\(i=1, \\ldots, n\\). Thus\n\\[ \\sum_{i=1}^{n} f\\left(t_{i}\\right) \\Delta x_{i}=F(b)-F(a) . \\]\nIt now follows from Theorem \\(6.7(c)\\) that\n\\[ \\left|F(b)-F(a)-\\int_{a}^{b} f(x) d x\\right|\u0026lt;\\varepsilon . \\]\nSince this holds for every \\(\\varepsilon\u0026gt;0\\), the proof is complete.\n6.22 Theorem (integration by parts) Suppose \\(F\\) and \\(G\\) are differentiable functions on \\([a, b], F^{\\prime}=f \\in \\mathscr{R}\\), and \\(G^{\\prime}=g \\in \\mathscr{R}\\). Then \\[ \\int_{a}^{b} F(x) g(x) d x=F(b) G(b)-F(a) G(a)-\\int_{a}^{b} f(x) G(x) d x . \\]\nProof Put \\(H(x)=F(x) G(x)\\) and apply Theorem \\(6.21\\) to \\(H\\) and its derivative. Note that \\(H^{\\prime} \\in \\mathscr{R}\\), by Theorem \\(6.13 .\\)\n","date":"2020-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/3-integration-and-differentiation/","section":"baby rudin","tags":null,"title":"INTEGRATION AND DIFFERENTIATION"},{"categories":null,"contents":"6.23 Definition Let \\(f_{1}, \\ldots, f_{k}\\) be real functions on \\([a, b]\\), and let \\(f=\\left(f_{1}, \\ldots, f_{k}\\right)\\) be the corresponding mapping of \\([a, b]\\) into \\(R^{k}\\). If \\(\\alpha\\) increases monotonically on \\([a, b]\\), to say that \\(f \\in \\mathscr{R}(\\alpha)\\) means that \\(f_{j} \\in \\mathscr{R}(\\alpha)\\) for \\(j=1, \\ldots, k\\). If this is the case, we define \\[ \\int_{a}^{b} \\mathrm{f} d \\alpha=\\left(\\int_{a}^{b} f_{1} d \\alpha, \\ldots, \\int_{a}^{b} f_{k} d \\alpha\\right) . \\]\nIn other words, \\(\\int f d \\alpha\\) is the point in \\(R^{k}\\) whose \\(j\\) th coordinate is \\(\\int f_{J} d \\alpha\\).\nIt is clear that parts \\((a),(c)\\), and \\((e)\\) of Theorem \\(6.12\\) are valid for these vector-valued integrals; we simply apply the earlier results to each coordinate. The same is true of Theorems \\(6.17,6.20\\), and 6.21. To illustrate, we state the analogue of Theorem \\(6.21\\).\n6.24 Theorem If \\(\\mathbf{f}\\) and \\(\\mathbf{F}\\) map \\([a, b]\\) into \\(R^{k}\\), if \\(\\mathrm{f} \\in \\mathscr{R}\\) on \\([a, b]\\), and if \\(\\mathbf{F}^{\\prime}=\\mathbf{f}\\), then \\[ \\int_{a}^{b} \\mathrm{f}(t) d t=\\mathbf{F}(b)-\\mathbf{F}(a) . \\]\nThe analogue of Theorem 6.13(b) offers some new features, however, at least in its proof.\n6.25 Theorem If \\(\\mathrm{f}\\) maps \\([a, b]\\) into \\(R^{k}\\) and if \\(\\mathrm{f} \\in \\mathscr{R}(\\alpha)\\) for some monotonically increasing function \\(\\alpha\\) on \\([a, b]\\), then \\(|\\mathbf{f}| \\in \\mathscr{R}(\\alpha)\\), and \\[ \\left|\\int_{a}^{b} \\mathbf{f} d \\alpha\\right| \\leq \\int_{a}^{b}|\\mathbf{f}| d \\alpha . \\]\nProof If \\(f_{1}, \\ldots, f_{k}\\) are the components of \\(f\\), then\n\\[ |\\mathbf{f}|=\\left(f_{1}^{2}+\\cdots+f_{k}^{2}\\right)^{1 / 2} . \\]\nBy Theorem 6.11, each of the functions \\(f_{i}^{2}\\) belongs to \\(\\mathscr{R}(\\alpha)\\); hence so does their sum. Since \\(x^{2}\\) is a continuous function of \\(x\\), Theorem \\(4.17\\) shows that the square-root function is continuous on \\([0, M]\\), for every real \\(M\\). If we apply Theorem \\(6.11\\) once more, (41) shows that \\(|f| \\in \\mathscr{R}(\\alpha)\\).\nTo prove (40), put \\(\\mathbf{y}=\\left(y_{1}, \\ldots, y_{k}\\right)\\), where \\(y_{j}=\\int f_{j} d \\alpha\\). Then we have \\(y=\\int f d \\alpha\\), and\n\\[ |\\mathbf{y}|^{2}=\\sum y_{i}^{2}=\\sum y_{j} \\int f_{j} d \\alpha=\\int\\left(\\sum y_{j} f_{j}\\right) d \\alpha . \\]\nBy the Schwarz inequality,\n\\[ \\sum y_{j} f_{j}(t) \\leq|\\mathbf{y}||\\mathbf{f}(t)| \\quad(a \\leq t \\leq b) ; \\]\nhence Theorem \\(6.12(b)\\) implies\n\\[ |\\mathbf{y}|^{2} \\leq|\\mathbf{y}| \\int|\\mathbf{f}| d \\alpha . \\]\nIf \\(\\mathbf{y}=\\mathbf{0},(40)\\) is trivial. If \\(\\mathbf{y} \\neq \\mathbf{0}\\), division of (43) by \\(|\\mathbf{y}|\\) gives (40).\n","date":"2020-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/4-integration-of-vector-valued-functions/","section":"baby rudin","tags":null,"title":"INTEGRATION OF VECTOR-VALUED FUNCTIONS"},{"categories":null,"contents":"6.12 Theorem\nIf \\(f_{1} \\in \\mathscr{R}(\\alpha)\\) and \\(f_{2} \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\), then \\[ f_{1}+f_{2} \\in \\mathscr{R}(\\alpha), \\]\n\\(c f \\in \\mathscr{R}(\\alpha)\\) for every constant \\(c\\), and\n\\[ \\begin{gathered} \\int_{a}^{b}\\left(f_{1}+f_{2}\\right) d \\alpha=\\int_{a}^{b} f_{1} d \\alpha+\\int_{a}^{b} f_{2} d \\alpha, \\\\ \\int_{a}^{b} c f d \\alpha=c \\int_{a}^{b} f d \\alpha . \\end{gathered} \\]\nIf \\(f_{1}(x) \\leq f_{2}(x)\\) on \\([a, b]\\), then \\[ \\int_{a}^{b} f_{1} d \\alpha \\leq \\int_{a}^{b} f_{2} d \\alpha . \\]\nIf \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\) and if \\(a\u0026lt;c\u0026lt;b\\), then \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, c]\\) and on \\([c, b]\\), and \\[ \\int_{a}^{c} f d \\alpha+\\int_{c}^{b} f d \\alpha=\\int_{a}^{b} f d \\alpha . \\]\nIf \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\) and if \\(|f(x)| \\leq M\\) on \\([a, b]\\), then \\[ \\left|\\int_{a}^{b} f d \\alpha\\right| \\leq M[\\alpha(b)-\\alpha(a)] \\text {. } \\]\nIf \\(f \\in \\mathscr{R}\\left(\\alpha_{1}\\right)\\) and \\(f \\in \\mathscr{R}\\left(\\alpha_{2}\\right)\\), then \\(f \\in \\mathscr{R}\\left(\\alpha_{1}+\\alpha_{2}\\right)\\) and \\[ \\int_{a}^{b} f d\\left(\\alpha_{1}+\\alpha_{2}\\right)=\\int_{a}^{b} f d \\alpha_{1}+\\int_{a}^{b} f d \\alpha_{2} ; \\]\nif \\(f \\in \\mathscr{R}(\\alpha)\\) and \\(c\\) is a positive constant, then \\(f \\in \\mathscr{R}(c \\alpha)\\) and\n\\[ \\int_{a}^{b} f d(c \\alpha)=c \\int_{a}^{b} f d \\alpha . \\]\nProof If \\(f=f_{1}+f_{2}\\) and \\(P\\) is any partition of \\([a, b]\\), we have\n\\[ \\begin{aligned} L\\left(P, f_{1}, \\alpha\\right)+L\\left(P, f_{2}, \\alpha\\right) \\leq L(P, f, \\alpha) \\\\ \u0026amp; \\leq U(P, f, \\alpha) \\leq U\\left(P, f_{1}, \\alpha\\right)+U\\left(P, f_{2}, \\alpha\\right) . \\end{aligned} \\]\nIf \\(f_{1} \\in \\mathscr{R}(\\alpha)\\) and \\(f_{2} \\in \\mathscr{R}(\\alpha)\\), let \\(\\varepsilon\u0026gt;0\\) be given. There are partitions \\(P_{j}\\) \\((j=1,2)\\) such that\n\\[ U\\left(P_{j}, f_{j}, \\alpha\\right)-L\\left(P_{j}, f_{j}, \\alpha\\right)\u0026lt;\\varepsilon \\]\nThese inequalities persist if \\(P_{1}\\) and \\(P_{2}\\) are replaced by their common refinement \\(P\\). Then (20) implies\n\\[ U(P, f, \\alpha)-L(P, f, \\alpha)\u0026lt;2 \\varepsilon, \\]\nwhich proves that \\(f \\in \\mathscr{R}(\\alpha)\\).\nWith this same \\(P\\) we have\n\\[ U\\left(P, f_{J}, \\alpha\\right)\u0026lt;\\int f_{j} d \\alpha+\\varepsilon \\quad(j=1,2) ; \\]\nhence (20) implies\n\\[ \\int f d \\alpha \\leq U(P, f, \\alpha)\u0026lt;\\int f_{1} d \\alpha+\\int f_{2} d \\alpha+2 \\varepsilon . \\]\nSince \\(\\varepsilon\\) was arbitrary, we conclude that\n\\[ \\int f d \\alpha \\leq \\int f_{1} d \\alpha+\\int f_{2} d \\alpha . \\]\nIf we replace \\(f_{1}\\) and \\(f_{2}\\) in (21) by \\(-f_{1}\\) and \\(-f_{2}\\), the inequality is reversed, and the equality is proved.\nThe proofs of the other assertions of Theorem \\(6.12\\) are so similar that we omit the details. In part \\((c)\\) the point is that (by passing to refinements) we may restrict ourselves to partitions which contain the point \\(c\\), in approximating \\(\\int f d \\alpha\\).\n6.13 Theorem If \\(f \\in \\mathscr{R}(\\alpha)\\) and \\(g \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\), then\n\\(f g \\in \\mathscr{R}(\\alpha)\\);\n\\(|f| \\in \\mathscr{R}(\\alpha)\\) and \\(\\left|\\int_{a}^{b} f d \\alpha\\right| \\leq \\int_{a}^{b}|f| d \\alpha\\).\nProof If we take \\(\\phi(t)=t^{2}\\), Theorem \\(6.11\\) shows that \\(f^{2} \\in \\mathscr{R}(\\alpha)\\) if \\(f \\in \\mathscr{R}(a)\\). The identity\n\\[ 4 f g=(f+g)^{2}-(f-g)^{2} \\]\ncompletes the proof of \\((a)\\).\nIf we take \\(\\phi(t)=|t|\\), Theorem 6.11 shows similarly that \\(|f| \\in \\mathscr{R}(\\alpha)\\). Choose \\(c=\\pm 1\\), so that\n\\[ c \\int f d \\alpha \\geq 0 \\]\nThen\n\\[ \\left|\\int f d \\alpha\\right|=c \\int f d \\alpha=\\int c f d \\alpha \\leq-\\int|f| d \\alpha, \\]\nsince \\(c f \\leq|f|\\).\n6.14 Definition The unit step function \\(I\\) is defined by \\[ I(x)= \\begin{cases}0 \u0026amp; (x \\leq 0) \\\\ 1 \u0026amp; (x\u0026gt;0)\\end{cases} \\]\n6.15 Theorem If \\(a\u0026lt;s\u0026lt;b, f\\) is bounded on \\([a, b], f\\) is continuous at \\(s\\), and \\(\\alpha(x)=I(x-s)\\), then \\[ \\int_{a}^{b} f d \\alpha=f(s) . \\]\nProof Consider partitions \\(P=\\left\\{x_{0}, x_{1}, x_{2}, x_{3}\\right\\}\\), where \\(x_{0}=a\\), and \\(x_{1}=s\u0026lt;x_{2}\u0026lt;x_{3}=b\\). Then\n\\[ U(P, f, \\alpha)=M_{2}, \\quad L(P, f, \\alpha)=m_{2} . \\]\nSince \\(f\\) is continuous at \\(s\\), we see that \\(M_{2}\\) and \\(m_{2}\\) converge to \\(f(s)\\) as \\(x_{2} \\rightarrow s\\).\n6.16 Theorem Suppose \\(c_{n} \\geq 0\\) for \\(1,2,3, \\ldots, \\Sigma c_{n}\\) converges, \\(\\left\\{s_{n}\\right\\}\\) is a sequence of distinct points in \\((a, b)\\), and \\[ \\alpha(x)=\\sum_{n=1}^{\\infty} c_{n} I\\left(x-s_{n}\\right) . \\]\nLet \\(f\\) be continuous on \\([a, b]\\). Then\n\\[ \\int_{a}^{b} f d \\alpha=\\sum_{n=1}^{\\infty} c_{n} f\\left(s_{n}\\right) . \\]\nProof The comparison test shows that the series (22) converges for every \\(x\\). Its \\(\\operatorname{sum} \\alpha(x)\\) is evidently monotonic, and \\(\\alpha(a)=0, \\alpha(b)=\\Sigma c_{n}\\). (This is the type of function that occurred in Remark 4.31.)\nLet \\(\\varepsilon\u0026gt;0\\) be given, and choose \\(N\\) so that\n\\[ \\sum_{N+1}^{\\infty} c_{n}\u0026lt;\\varepsilon . \\]\nPut\n\\[ \\alpha_{1}(x)=\\sum_{n=1}^{N} c_{n} I\\left(x-s_{n}\\right), \\quad \\alpha_{2}(x)=\\sum_{N+1}^{\\infty} c_{n} I\\left(x-s_{n}\\right) . \\]\nBy Theorems \\(6.12\\) and \\(6.15\\),\n\\[ \\int_{a}^{b} f d \\alpha_{1}=\\sum_{i=1}^{N} c_{n} f\\left(s_{n}\\right) . \\]\nSince \\(\\alpha_{2}(b)-\\alpha_{2}(a)\u0026lt;\\varepsilon\\)\n\\[ \\left|\\int_{a}^{b} f d \\alpha_{2}\\right| \\leq M \\varepsilon, \\]\nwhere \\(M=\\sup |f(x)|\\). Since \\(\\alpha=\\alpha_{1}+\\alpha_{2}\\), it follows from (24) and (25) that\n\\[ \\left|\\int_{a}^{b} f d \\alpha-\\sum_{i=1}^{N} c_{n} f\\left(s_{n}\\right)\\right| \\leq M \\varepsilon . \\]\nIf we let \\(N \\rightarrow \\infty\\), we obtain (23).\n6.17 Theorem Assume \\(\\alpha\\) increases monotonically and \\(\\alpha^{\\prime} \\in \\mathscr{R}\\) on \\([a, b]\\). Let \\(f\\) be a bounded real function on \\([a, b]\\).\nThen \\(f \\in \\mathscr{R}(\\alpha)\\) if and only if \\(f \\alpha^{\\prime} \\in \\mathscr{R}\\). In that case\n\\[ \\int_{a}^{b} f d \\alpha=\\int_{a}^{b} f(x) \\alpha^{\\prime}(x) d x . \\]\nProof Let \\(\\varepsilon\u0026gt;0\\) be given and apply Theorem \\(6.6\\) to \\(\\alpha^{\\prime}\\) : There is a partition \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\) such that\n\\[ U\\left(P, \\alpha^{\\prime}\\right)-L\\left(P, \\alpha^{\\prime}\\right)\u0026lt;\\varepsilon . \\]\nThe mean value theorem furnishes points \\(t_{i} \\in\\left[x_{i-1}, x_{i}\\right]\\) such that\n\\[ \\Delta \\alpha_{i}=\\alpha^{\\prime}\\left(t_{i}\\right) \\Delta x_{i} \\]\nfor \\(i=1, \\ldots, n\\). If \\(s_{i} \\in\\left[x_{i-1}, x_{i}\\right]\\), then\n\\[ \\sum_{i=1}^{n}\\left|\\alpha^{\\prime}\\left(s_{i}\\right)-\\alpha^{\\prime}\\left(t_{i}\\right)\\right| \\Delta x_{i}\u0026lt;\\varepsilon, \\]\nby (28) and Theorem 6.7(b). Put \\(M=\\sup |f(x)|\\). Since\n\\[ \\sum_{i=1}^{n} f\\left(s_{i}\\right) \\Delta \\alpha_{i}=\\sum_{i=1}^{n} f\\left(s_{i}\\right) \\alpha^{\\prime}\\left(t_{i}\\right) \\Delta x_{i} \\]\nit follows from (29) that\n\\[ \\left|\\sum_{i=1}^{n} f\\left(s_{i}\\right) \\Delta \\alpha_{i}-\\sum_{i=1}^{n} f\\left(s_{i}\\right) \\alpha^{\\prime}\\left(s_{i}\\right) \\Delta x_{i}\\right| \\leq M \\varepsilon . \\]\nIn particular,\n\\[ \\sum_{i=1}^{n} f\\left(s_{i}\\right) \\Delta \\alpha_{i} \\leq U\\left(P, f \\alpha^{\\prime}\\right)+M \\varepsilon, \\]\nfor all choices of \\(s_{i} \\in\\left[x_{i-1}, x_{i}\\right]\\), so that\n\\[ U(P, f, \\alpha) \\leq U\\left(P, f \\alpha^{\\prime}\\right)+M \\varepsilon . \\]\nThe same argument leads from (30) to\n\\[ U\\left(P, f \\alpha^{\\prime}\\right) \\leq U(P, f, \\alpha)+M \\varepsilon . \\]\nThus\n\\[ \\left|U(P, f, \\alpha)-U\\left(P, f \\alpha^{\\prime}\\right)\\right| \\leq M \\varepsilon . \\]\nNow note that (28) remains true if \\(P\\) is replaced by any refinement. Hence (31) also remains true. We conclude that\n\\[ \\left|\\int_{a}^{b} f d \\alpha-\\bar{\\int}_{a}^{b} f(x) \\alpha^{\\prime}(x) d x\\right| \\leq M \\varepsilon . \\]\nBut \\(\\varepsilon\\) is arbitrary. Hence\n\\[ \\int_{a}^{b} f d \\alpha=\\bar{\\int}_{a}^{b} f(x) \\alpha^{\\prime}(x) d x, \\]\nfor any bounded \\(f\\). The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows.\n6.18 Remark The two preceding theorems illustrate the generality and flexibility which are inherent in the Stieltjes process of integration. If \\(\\alpha\\) is a pure step function [this is the name often given to functions of the form (22)], the integral reduces to a finite or infinite series. If \\(\\alpha\\) has an integrable derivative, the integral reduces to an ordinary Riemann integral. This makes it possible in many cases to study series and integrals simultaneously, rather than separately.\nTo illustrate this point, consider a physical example. The moment of inertia of a straight wire of unit length, about an axis through an endpoint, at right angles to the wire, is\n\\[ \\int_{0}^{1} x^{2} d m \\]\nwhere \\(m(x)\\) is the mass contained in the interval \\([0, x]\\). If the wire is regarded as having a continuous density \\(\\rho\\), that is, if \\(m^{\\prime}(x)=\\rho(x)\\), then (33) turns into\n\\[ \\int_{0}^{1} x^{2} \\rho(x) d x . \\]\nOn the other hand, if the wire is composed of masses \\(m_{i}\\) concentrated at points \\(x_{i},(33)\\) becomes\n\\[ \\sum_{i} x_{i}^{2} m_{i} \\text {. } \\]\nThus (33) contains (34) and (35) as special cases, but it contains much more; for instance, the case in which \\(m\\) is continuous but not everywhere differentiable.\n6.19 Theorem (change of variable) Suppose \\(\\varphi\\) is a strictly increasing continuous function that maps an interval \\([A, B]\\) onto \\([a, b]\\). Suppose \\(\\alpha\\) is monotonically increasing on \\([a, b]\\) and \\(f \\in \\mathscr{R}(\\alpha)\\) on \\([a, b]\\). Define \\(\\beta\\) and \\(g\\) on \\([A, B]\\) by \\[ \\beta(y)=\\alpha(\\varphi(y)), \\quad g(y)=f(\\varphi(y)) . \\]\n\\[ \\int_{A}^{B} g d \\beta=\\int_{a}^{b} f d \\alpha \\]\nProof To each partition \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\) corresponds a partition \\(Q=\\left\\{y_{0}, \\ldots, y_{n}\\right\\}\\) of \\([A, B]\\), so that \\(x_{i}=\\varphi\\left(y_{i}\\right)\\). All partitions of \\([A, B]\\) are obtained in this way. Since the values taken by \\(f\\) on \\(\\left[x_{i-1}, x_{i}\\right]\\) are exactly the same as those taken by \\(g\\) on \\(\\left[y_{i-1}, y_{i}\\right]\\), we see that\n\\[ U(Q, g, \\beta)=U(P, f, \\alpha), \\quad L(Q, g, \\beta)=L(P, f, \\alpha) . \\]\nSince \\(f \\in \\mathscr{R}(\\alpha), P\\) can be chosen so that both \\(U(P, f, \\alpha)\\) and \\(L(P, f, \\alpha)\\) are close to \\(\\int f d \\alpha\\). Hence (38), combined with Theorem 6.6, shows that \\(g \\in \\mathscr{R}(\\beta)\\) and that (37) holds. This completes the proof.\nLet us note the following special case:\nTake \\(\\alpha(x)=x\\). Then \\(\\beta=\\varphi\\). Assume \\(\\varphi^{\\prime} \\in \\mathscr{R}\\) on \\([A, B]\\). If Theorem \\(6.17\\) is applied to the left side of (37), we obtain\n\\[ \\int_{a}^{b} f(x) d x=\\int_{A}^{B} f(\\varphi(y)) \\varphi^{\\prime}(y) d y . \\]\n","date":"2020-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/2-properties-of-the-integral/","section":"baby rudin","tags":null,"title":"PROPERTIES OF THE INTEGRAL"},{"categories":null,"contents":"We conclude this chapter with a topic of geometric interest which provides an application of some of the preceding theory. The case \\(k=2\\) (i.e., the case of plane curves) is of considerable importance in the study of analytic functions of a complex variable.\n6.26 Definition A continuous mapping \\(\\gamma\\) of an interval \\([a, b]\\) into \\(R^{k}\\) is called a curve in \\(R^{k}\\). To emphasize the parameter interval \\([a, b]\\), we may also say that \\(\\gamma\\) is a curve on \\([a, b]\\).\nIf \\(\\gamma\\) is one-to-one, \\(\\gamma\\) is called an arc.\nIf \\(\\gamma(a)=\\gamma(b), \\gamma\\) is said to be a closed curve.\nIt should be noted that we define a curve to be a mapping, not a point set. Of course, with each curve \\(\\gamma\\) in \\(R^{k}\\) there is associated a subset of \\(R^{k}\\), namely the range of \\(\\gamma\\), but different curves may have the same range.\nWe associate to each partition \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) of \\([a, b]\\) and to each curve \\(\\gamma\\) on \\([a, b]\\) the number\n\\[ \\Lambda(P, \\gamma)=\\sum_{i=1}^{n}\\left|\\gamma\\left(x_{i}\\right)-\\gamma\\left(x_{i-1}\\right)\\right| . \\]\nThe ith term in this sum is the distance (in \\(\\left.R^{k}\\right)\\) between the points \\(\\gamma\\left(x_{i-1}\\right)\\) and \\(\\gamma\\left(x_{i}\\right)\\). Hence \\(\\Lambda(P, \\gamma)\\) is the length of a polygonal path with vertices at \\(\\gamma\\left(x_{0}\\right)\\), \\(\\gamma\\left(x_{1}\\right), \\ldots, \\gamma\\left(x_{n}\\right)\\), in this order. As our partition becomes finer and finer, this polygon approaches the range of \\(\\gamma\\) more and more closely. This makes it seem reasonable to define the length of \\(\\gamma\\) as\n\\[ \\Lambda(\\gamma)=\\sup \\Lambda(P, \\gamma), \\]\nwhere the supremum is taken over all partitions of \\([a, b]\\).\nIf \\(\\Lambda(\\gamma)\u0026lt;\\infty\\), we say that \\(\\gamma\\) is rectifiable.\nIn certain cases, \\(\\Lambda(\\gamma)\\) is given by a Riemann integral. We shall prove this for continuously differentiable curves, i.e., for curves \\(\\gamma\\) whose derivative \\(\\gamma^{\\prime}\\) is continuous.\n6.27 Theorem If \\(\\gamma^{\\prime}\\) is continuous on \\([a, b]\\), then \\(\\gamma\\) is rectifiable, and \\[ \\Lambda(\\gamma)=\\int_{a}^{b}\\left|\\gamma^{\\prime}(t)\\right| d t . \\]\nProof If \\(a \\leq x_{i-1}\u0026lt;x_{i} \\leq b\\), then\n\\[ \\left|\\gamma\\left(x_{i}\\right)-\\gamma\\left(x_{i-1}\\right)\\right|=\\left|\\int_{x_{t-1}}^{x_{i}} \\gamma^{\\prime}(t) d t\\right| \\leq \\int_{x_{i-1}}^{x_{i}}\\left|\\gamma^{\\prime}(t)\\right| d t . \\]\nHence\n\\[ \\Lambda(P, \\gamma) \\leq \\int_{a}^{b}\\left|\\gamma^{\\prime}(t)\\right| d t \\]\nfor every partition \\(P\\) of \\([a, b]\\). Consequently,\n\\[ \\Lambda(\\gamma) \\leq \\int_{a}^{b}\\left|\\gamma^{\\prime}(t)\\right| d t . \\]\nTo prove the opposite inequality, let \\(\\varepsilon\u0026gt;0\\) be given. Since \\(\\gamma^{\\prime}\\) is uniformly continuous on \\([a, b]\\), there exists \\(\\delta\u0026gt;0\\) such that\n\\[ \\left|\\gamma^{\\prime}(s)-\\gamma^{\\prime}(t)\\right|\u0026lt;\\varepsilon \\quad \\text { if }|s-t|\u0026lt;\\delta . \\]\nLet \\(P=\\left\\{x_{0}, \\ldots, x_{n}\\right\\}\\) be a partition of \\([a, b]\\), with \\(\\Delta x_{i}\u0026lt;\\delta\\) for all \\(i\\). If \\(x_{i-1} \\leq t \\leq x_{i}\\), it follows that\n\\[ \\left|\\gamma^{\\prime}(t)\\right| \\leq\\left|\\gamma^{\\prime}\\left(x_{i}\\right)\\right|+\\varepsilon . \\]\nHence\n\\[ \\begin{aligned} \\int_{x_{i-1}}^{x_{i}}\\left|\\gamma^{\\prime}(t)\\right| d t \u0026amp; \\leq\\left|\\gamma^{\\prime}\\left(x_{i}\\right)\\right| \\Delta x_{i}+\\varepsilon \\Delta x_{i} \\\\ \u0026amp;=\\left|\\int_{x_{i-1}}^{x_{i}}\\left[\\gamma^{\\prime}(t)+\\gamma^{\\prime}\\left(x_{i}\\right)-\\gamma^{\\prime}(t)\\right] d t\\right|+\\varepsilon \\Delta x_{i} \\\\ \u0026amp; \\leq\\left|\\int_{x_{i-1}}^{x_{i}} \\gamma^{\\prime}(t) d t\\right|+\\left|\\int_{x_{i-1}}^{x_{i}}\\left[\\gamma^{\\prime}\\left(x_{i}\\right)-\\gamma^{\\prime}(t)\\right] d t\\right|+\\varepsilon \\Delta x_{i} \\\\ \u0026amp; \\leq\\left|\\gamma\\left(x_{i}\\right)-\\gamma\\left(x_{i-1}\\right)\\right|+2 \\varepsilon \\Delta x_{i} . \\end{aligned} \\]\nIf we add these inequalities, we obtain\n\\[ \\begin{aligned} \\int_{a}^{b}\\left|\\gamma^{\\prime}(t)\\right| d t \u0026amp; \\leq \\Lambda(P, \\gamma)+2 \\varepsilon(b-a) \\\\ \u0026amp; \\leq \\Lambda(\\gamma)+2 \\varepsilon(b-a) . \\end{aligned} \\]\nSince \\(\\varepsilon\\) was arbitrary,\n\\[ \\int_{a}^{b}\\left|\\gamma^{\\prime}(t)\\right| d t \\leq \\Lambda(\\gamma) \\]\nThis completes the proof.\n","date":"2020-07-30T00:00:00Z","permalink":"https://zongpitt.com/baby-rudin/6-the-riemann-stieltjes-integral/5-rectifiable-curves/","section":"baby rudin","tags":null,"title":"RECTIFIABLE CURVES"},{"categories":null,"contents":"This post summary two papers, DatasetGAN, EditGAN.\nDataset GAN Dataset GAN proposed use GAN to generate segmentation data.\nAdvance: Only few label image (40) can training the segmentation branch.\nDrawback: Only for those image which can be modeled by GAN.\nNetwork structure This network is based on styleGAN, it add a classification branch for segmentation.\nThe key insight of DATASETGAN is that generative models such as GANs that are trained to synthesize highly realistic images must acquire semantic knowledge in their high dimensional latent space.\nimage-20220728203854910 EditGAN\nimage-20220728205509349 ","date":"2020-07-28T00:00:00Z","permalink":"https://zongpitt.com/posts/misc/2020-07-28-editgan/","section":"posts","tags":null,"title":"EditGAN"},{"categories":["Misc"],"contents":"Following content are from pandoc document\npandoc math.text -s --mathjax -o mathMathJax.html ","date":"2020-02-16T14:00:52-05:00","permalink":"https://zongpitt.com/posts/2021-02-16-pandoc-convert-tex-html/","section":"posts","tags":["Pandoc"],"title":"Pandoc Convert Latex to Html"},{"categories":["Machine Learning"],"contents":" For many logistic regression note they will put cost function as\n\\[ J = - \\frac{1}{N} l(\\theta) \\]\nI am not doing this here, basicaly we can use gradien ascent to to maximized cost function in the notes instread of gradient decent. Ther are almost the same. I alos want to comment \\(1/m\\) here, for gradient descent algorithm, what we care about is gradient descent direction. We will choose learning rate to change gradient descent speed. If \\(1/m\\) did put in the cost function, it will as same as learn rate \\(alpha/m\\). So for algorithm actually they are same.\nIf we using library to implement this algorithm, maybe we need to read the documnt to distinguish how they implement this algorithm in order to choose proper learning rate.\nmy Matlab implementation\nalpha = 0.5; % learning rate feature_size = 2; iter = 1000; theta = zeros(feature_size, 1); theta = [1 1]\u0026#39;; x = [0 1; 1 1]; y = [0 1]\u0026#39;; costs = zeros(1, iter); for i = 1:iter costs(i) = my_cost( x, y, theta); partical_cost = my_part_cost(x, y, theta); theta = theta - alpha .* partical_cost; end plot(costs) function y = my_sigmoid(x) y = 1 ./(1 + exp(-x)); end function cost = my_cost( x, y, theta) cost = y\u0026#39; .* (theta\u0026#39; * x) - log(1 + exp(theta\u0026#39; * x)); cost = sum(cost); % this one is for maximize cost = -cost; % for minimize end function part_cost = my_part_cost(x, y, theta) part_cost = sum(x .* (y\u0026#39; - my_sigmoid(theta\u0026#39; * x)),2); % for maximize cost function part_cost = -part_cost; % for minimize cost function end And also provide other algorithm to implement logistic regression. Here is two mathod to implement logistic regreesion, Newton Method and Stochastic gridient descent.\nNewton method is based on hessian metric, Hessian method actually is second order derivation. Newton method will converge much faster than gradient descent. But the probelm of Newton method is it need to compute invert of hessian matrix. So if the feature size is really large it will require lot of computation.(computation is slow)\nHessian max is how as bellow\n\\[ hessian = \\frac{\\nabla^{2} !(\\theta)}{\\partial \\theta}=-\\sum_{i=1}^{n} \\tilde{x}_{i} \\tilde{x}_{i}^{T} g\\left(\\theta^{T} \\tilde{x}_{i}\\right)\\left(1-g\\left(\\theta^{T} \\tilde{x}_{i}\\right)\\right) \\]\nupdate rule \\[ x^{j+1}=x^{j}-\\left.\\left(\\nabla^{2} f(x)\\right)^{-1} \\nabla f(x)\\right|_{x=x^{j}} \\] Code for Newton method. Just replace line 19-24 by following code section.\nfor i = 1:iter costs(i) = my_cost( x, y, theta); hessian = x .* my_sigmoid((theta\u0026#39; * x)) * x\u0026#39;; partical_cost = my_part_cost(x, y, theta); theta = theta - inv(hessian) * partical_cost; end Stochastic gradient is useful when batch is too large. But Stochastic will take much more iteration to converge. I would not privide stochastic gradient decent code here. It is easy. Just randomly pick an sample and do gradient decent base on this sample.\nMaterials from Dr. Can\n","date":"2020-02-10T05:12:52-05:00","permalink":"https://zongpitt.com/posts/2021-02-11-logistic-regression/","section":"posts","tags":["LR","ML"],"title":"Logistic Regression"},{"categories":["Math"],"contents":"Exponential Random Variable Continuous version of the discrete geometric RV. Models the inter-arrival time for the Poisson process.\n\\(X \\) Exponential \\(\\)\n\\[ f_{X}(x)=\\left\\{\\begin{array}{ll}{\\lambda e^{-\\lambda x}} \u0026amp; {\\text { if } x \\geq 0} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\n\\[ F_{X}(x)=\\left\\{\\begin{array}{ll}{0} \u0026amp; {\\text { if } x\u0026lt;0} \\\\ {1-e^{-\\lambda x}} \u0026amp; {\\text { if } x \\geq 0}\\end{array}\\right. \\]\nErlang Random Variable Continuous version of the discrete Pascal Random Variable. Models the total arrival time for \\(n\\) arrivals of the Poisson process\nThe wait time for the \\(n\\) th bus time to emision of the nth particle from a radioactive source \\(X (n, \\)\n\\[ f_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{\\lambda^{n} x^{n-1} e^{-\\lambda x}}{(n-1) !}} \u0026amp; {\\text { if } x \\geq 0} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\nGamma Random Variable Generalization of Erlang and Exponential RVs. \\(X (, )\\):\n\\[ f_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{\\lambda(\\lambda x)^{\\alpha-1} e^{-\\lambda x}}{\\Gamma(\\alpha)}} \u0026amp; {\\text { if } x \\geq 0} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\nwhere \\((z)=_{0}^{} x^{z-1} e^{-x} x z\u0026gt;0\\( Exponential distribution with parameter \\(lambda\\) if \\(alpha=1\\) Erlang distribution with parameters \\(n\\) and \\(lambda\\) if \\(alpha=n\\) In general there is no closed form for the CDF\n\\(Gamma(1 / 2)=\\sqrt{\\) \\(Gamma(z+1)=z (z\\) for \\(z\u0026gt;0\\) \\(Gamma(m+1)=m \\) for \\(m\\) a nonnegative integer. Beta Random Variable \\(X (a, b) a\u0026gt;0 b\u0026gt;0\\)\n\\[ f_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{x^{a-1}(1-x)^{b-1}}{\\beta(a, b)}} \u0026amp; {\\text { if } x \\in(0,1)} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\n\\(text { where } (a, b)=_{0}^{1} x{a-1}(1-x){b-1} x z\u0026gt;0\\) If \\(a=b=1\\) then \\(X\\) is a standard uniform random variable. Useful to model a variety of behaviors for random variables that range over finite intervals. Gaussian Random Variable Also called the normal random variable because of its prevalence.\n\\(X \\) Gaussian \\(mu, ) \\)\n\\[ f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{\\frac{-(x-\\mu)^{2}}{2 \\sigma^{2}}} \\]\nLaplacian Random Variable \\[ \\begin{array}{l}{S_{X}=(-\\infty, \\infty)} \\\\ {f_{X}(x)=\\frac{\\alpha}{2} e^{-\\alpha|x|} \\quad-\\infty\u0026lt;x\u0026lt;+\\infty \\quad \\text { and } \\quad \\alpha\u0026gt;0} \\\\ {E[X]=0 \\quad \\operatorname{VAR}[X]=2 / \\alpha^{2} \\quad \\Phi_{X}(\\omega)=\\frac{\\alpha^{2}}{\\omega^{2}+\\alpha^{2}}}\\end{array} \\]\nThe Cauchy Random Variable The Cauchy random variable \\(X\\) assumes values over the entire real line and has pdf:\n\\[ f_{X}(x)=\\frac{1 / \\pi}{1+x^{2}} \\]\nIt is easy to verify that this pdf integrates to \\(1 \\) However, \\(X\\) does not have any moments since the associated integrals do not converge. The Cauchy random variable arises as the tangent of a uniform random variable in the unit interval.\nThe Pareto Random Variable The Pareto random variable arises in the study of the distribution of wealth where it has been found to model the tendency for a small portion of the population to own a large portion of the wealth. Recently the Pareto distribution has been found to cap- ture the behavior of many quantities of interest in the study of Internet behavior, e.g., sizes of files, packet delays, audio and video title preferences, session times in peer-to-peer networks, etc.The Pareto random variable can be viewed as a continuous version of the Zipf discrete random variable. The Pareto random variable X takes on values in the range where is a positive real number. X has complementary cdf with shape parameter given by:\n\\[ P[X\u0026gt;x]=\\left\\{\\begin{array}{ll}{1} \u0026amp; {x\u0026lt;x_{m}} \\\\ {\\frac{x_{m}^{\\alpha}}{x^{\\alpha}}} \u0026amp; {x \\geq x_{m}}\\end{array}\\right. \\]\nThe tail of \\(X\\) decays algebraically with x which is rather slower in comparison to the exponential and Gaussian random variables. The Pareto random variable is the most\nprominent example of random variables with “long tails.” The cdf and pdf of \\(X\\) are\n\\[ \\begin{aligned}F_{X}(x)=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x\u0026lt;x_{m}} \\\\ {1-\\frac{x_{m}^{\\alpha}}{x^{\\alpha}}} \u0026amp; {x \\geq x_{m}}\\end{array}\\right. \\end{aligned} \\]\nBecause of its long tail, the cdf of \\(X\\) approaches 1 rather slowly as \\(x\\) increases.\n\\[ f_{X}(x)=\\left\\{\\begin{array}{ll}{0} \u0026amp; {x\u0026lt;x_{m}} \\\\ {\\alpha \\frac{x_{m}^{\\alpha}}{x^{\\alpha+1}}} \u0026amp; {x \\geq x_{m}}\\end{array}\\right. \\]\nmean and variance Find the mean and variance of the Pareto random variable.\n\\[ E[X]=\\int_{x_{m}}^{\\infty} t \\alpha \\frac{x_{m}^{\\alpha}}{t^{\\alpha+1}} d t=\\int_{x_{m}}^{\\infty} \\alpha \\frac{x_{m}^{\\alpha}}{t^{\\alpha}} d t=\\frac{\\alpha}{\\alpha-1} \\frac{x_{m}^{\\alpha}}{x_{m}^{\\alpha-1}}=\\frac{\\alpha x_{m}}{\\alpha-1} \\quad \\text { for } \\alpha\u0026gt;1 \\]\nwhere the integral is defined for \\(alpha\u0026gt;1\\) and\n\\[ E\\left[X^{2}\\right]=\\int_{x_{m}}^{\\infty} t^{2} \\alpha \\frac{x_{m}^{\\alpha}}{t^{\\alpha+1}} d t=\\int_{x_{m}}^{\\infty} \\alpha \\frac{x_{m}^{\\alpha}}{t^{\\alpha-1}} d t=\\frac{\\alpha}{\\alpha-2} \\frac{x_{m}^{\\alpha}}{x_{m}^{\\alpha-2}}=\\frac{\\alpha x_{m}^{2}}{\\alpha-2} \\quad \\text { for } \\alpha\u0026gt;2 \\]\nwhere the second moment is defined for \\(alpha\u0026gt;2 \\)\n\\[ \\begin{aligned} \\text { The variance of } X \u0026amp; \\text { is then: } \\\\ \\operatorname{VAR}[X] \u0026amp;=\\frac{\\alpha x_{m}^{2}}{\\alpha-2}-\\left(\\frac{\\alpha x_{m}^{2}}{\\alpha-1}\\right)^{2}=\\frac{\\alpha x_{m}^{2}}{(\\alpha-2)(\\alpha-1)^{2}} \u0026amp; \\text { for } \\alpha\u0026gt;2 \\end{aligned} \\]\n","date":"2019-10-03T00:00:00Z","permalink":"https://zongpitt.com/posts/2019-10-03-type-of-continuous-random-variable/","section":"posts","tags":["Probability","Math","Random Variable"],"title":"Type of Continuous Random Variable"},{"categories":["Math"],"contents":"Uniform Random Variable For each \\(k\\) in \\(S_{X},\\) we have \\(p_{X}(k)=\\frac{1}{M}\\).\n\\(X \\sim\\) Uniform \\((a, b),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a+1}} \u0026amp; {\\text { if } x=a, a+1, \\cdots, b} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\n\\(E[X] = \\frac{a+b}{2}\\) \\(\\operatorname{Var}[X]=\\frac{(b-a+1)^{2}-1}{12}=\\frac{(b-a)(b-a+2)}{12}\\) Bernoulli Random Variable Used for experiments with two outcomes of interest: The success or failure of an experiment\n\\(X \\sim\\) Bernoulli \\((p),\\) if its PMF has the following form:\n\\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{1-p} \u0026amp; {\\text { if } x=0} \\\\ {p} \u0026amp; {\\text { if } x=1} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\n\\(S_X = \\{0,1\\}\\) Mean: \\(E[X] = p\\) \\(VAR[X]=p(1-p)\\) Binomial Random Variable Used for experiments that involve \\(n\\) independent trials of the Bernoulli experiment:\n\\(X \\sim\\) Binomial \\((n, p)\\), if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\left(\\begin{array}{l}{n} \\\\ {x}\\end{array}\\right) p^{x}(1-p)^{n-x}} \u0026amp; {\\text { if } x=\\{0,1, \\cdots, n\\}} \\\\ {0} \u0026amp; {\\text { otherwise. }}\\end{array}\\right. \\]\nMean: \\(E[X] = np\\)\nVariance: \\(Var[X] = np(1-p)\\)\nGeometric Random Variable \\(X \\sim\\) Geometric \\((p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{p(1-p)^{x-1}} \u0026amp; {\\text { if } x=1,2,3, \\cdots} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\]\nMean: \\(E[x] = \\frac{1}{p}\\) Variance: \\(Var[X] = \\frac{1-p}{p^2}\\) notice that if \\(x\\) begin from \\(0\\), the mean is \\(E(x) = \\frac{1-p}{p}\\). Pascal Random Variable(Negtive binomial Distribution) For an infinite sequence of Benoulli trials each with probability \\(p\\), the Pascal random variable is the number of trials up to and including the \\(k\\)th success.\nTo derive the Pascal \\(PMF\\), we consider the scenario of finding the \\(k\\) th success in the \\(x\\) th trial: there are exactly \\((k-1)\\) successes in the previous \\((x-1)\\) trials, and the \\(k\\) th success occurs in the \\(x\\) th trial. The probability of this event is:\n\\[ P\\{X=x\\}=\\left(\\begin{array}{l}{x-1} \\\\ {k-1}\\end{array}\\right) p^{k-1}(1-p)^{(x-1)-(k-1)} \\cdot p \\]\n\\(X \\sim\\) Pascal \\((k, p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\left(\\begin{array}{c}{x-1} \\\\ {k-1}\\end{array}\\right) p^{k}(1-p)^{(x-1)-(k-1)} \\text { if } x=k, k+1, k+2, \\cdots} \\\\ {0} \u0026amp; {\\text { otherwise. }}\\end{array}\\right. \\]\nMean: \\(E[X]=\\frac{k}{p}\\) Variance: \\(\\operatorname{Var}[X]=\\frac{k(1-p)}{p^{2}}\\). Poisson Random Variable Used to describe phenomenon that occur randomly in time. While the time of each occurrence is completely random, there is a known average number of occurrences per unit time.\nIn many applications, we are interested in counting the number of occurrences of an event in a certain time period or in a certain region in space. The Poisson random variable arises in situations where the events occur “completely at random” in time or space.\n\\(X \\sim\\) Poisson \\((\\alpha),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{\\alpha^{x}}{x !} e^{-\\alpha}} \u0026amp; {\\text { if } x=0,1,2, \\cdots} \\\\ {0} \u0026amp; {\\text { otherwise. }}\\end{array}\\right. \\] where the parameter \\(\\alpha\u0026gt;0\\).\nWhen dealing with problems involving time \\(t\\) with an arrival rate of \\(\\lambda\\) for an event, the Poisson probability of \\(x\\) arrivals in time \\(t\\) is found by setting the Poisson parameter \\(\\alpha=\\lambda t\\)\n\\(E[X] = \\alpha\\) \\(Var[X] = \\alpha\\) Zipf Random Variable Used to account for the relative popularity of a few members of a population and the relative uncertainty of other members of a population:\n\\(X \\sim \\operatorname{Zipf}(\\alpha, L),\\) if its \\(PMF\\) has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{c_{L}} \\frac{1}{x^{\\alpha}}} \u0026amp; {\\text { if } x=0,1,2, \\cdots L} \\\\ {0} \u0026amp; {\\text { otherwise }}\\end{array}\\right. \\] where the parameter \\(c_{L}\\) is a normalization factor, and \\(\\alpha \\geq 0\\)\nFor \\(\\alpha=1,\\) the Zipf random variable has\n\\(\\text { Mean: } E[X]=\\frac{L}{c_{L}}\\) \\(\\text { Variance: } \\operatorname{Var}[X]=\\frac{L(L+1)}{2 c_{L}}-\\frac{L^{2}}{c_{L}^{2}}\\) ","date":"2019-10-03T00:00:00Z","permalink":"https://zongpitt.com/posts/2019-10-03-type-of-discrete-random-variable/","section":"posts","tags":["Probability","Math","Random Variable"],"title":"Type of Discrete Random Variable"},{"categories":null,"contents":"bubble sort Time Complex \\(O(n^2)\\).\nchoose two value to compare each time.\n‘-’ present small one ‘+’ present large one\n-+****** *-+***** **-+**** ... The fellow code write using python\na = [10, 203, 2, 5, 444, 333, 333, 666, 777] for i in range(len(a)): for j in range(i,len(a)): if(a[i] \u0026gt; a[j]): tmp = a[j] a[j] = a[i] a[i] = tmp print(a) choose sort time complexity \\(O(n^2)\\)， choose minumum value in the list each time.\na = [10, 203, 2, 5, 444, 333, 333, 666, 777] for i in range(len(a)): mina = i for j in range(i, len(a)): if a[j] \u0026lt; a[mina]: mina = j tmp = a[mina] a[mina] = a[i] a[i] = tmp print(a) quick sort 快速排序二分的思想，随即的选出一个数，数字大的放右边，小的放左边，然后进行合并，就完成了排序的步骤。\n平均复杂度是\\(O(nlgn)\\)\nimport random def quick_sort(a): b, c = [], [] rand_num = random.randint(0, len(a)-1) for i in a: if i \u0026gt; a[rand_num]: c.append(i) else: b.append(i) same_b, same_c = True, True for i in range(len(b)-1): if b[i] != b[i+1]: same_b = False break for i in range(len(c)-1): if c[i] != c[i+1]: same_c = False break if len(b) \u0026gt; 1 and not same_b: b = quick_sort(b) if len(c) \u0026gt; 1 and not same_c: c = quick_sort(c) return b+c a = [10, 203, 2, 5, 444, 333, 333, 666, 777] a = quick_sort(a) print(a) 归并排序 分为两组，两组分别排序，然后合并，复杂度\\(O(nlgn)\\)\ndef conbine(a, b): c = [] cnt_a, cnt_b = 0, 0 while len(c) \u0026lt; len(a) + len(b): if cnt_a == len(a): for i in range(cnt_b, len(b)): c.append(b[i]) break if cnt_b == len(b): for i in range(cnt_a, len(a)): c.append(a[i]) break if a[cnt_a] \u0026lt; b[cnt_b]: c.append(a[cnt_a]) cnt_a += 1 else: c.append(b[cnt_b]) cnt_b += 1 return c def merge_sort(a): b, c = [], [] if len(a) \u0026lt;= 1: return a i = 0 while i \u0026lt; len(a) / 2: b.append(a[i]) i += 1 while i \u0026lt; len(a): c.append(a[i]) i += 1 return conbine(merge_sort(b), merge_sort(c)) a = [10, 203, 2, 5, 444, 333, 333, 666, 777] a = merge_sort(a) print(a) ","date":"2019-07-14T00:00:00Z","permalink":"https://zongpitt.com/algorithm/sort/","section":"algorithm","tags":null,"title":"sort"},{"categories":null,"contents":"Background wikipedia \u0026gt; 模反元素也称为模倒数，或者模逆元。\n\\(ax \\equiv 1 (\\mod m)\\) \\(x \\equiv a^{-1} (\\mod m)\\) \\(a\\) 与 \\(x\\) 互为乘法逆元\n####扩展欧几里得算法\n欧几里得算法(GCD)\n\\(ax \\equiv 1\\ (mod m) \\Rightarrow ax + my = 1\\) 求出上述方程整数解即可,如果解出\\(x\\)为负数,\\(a(x+m) +m(y-a) = 1\\)也成立,即\\(x+m\\)同样满足方程的解\n用欧几里得算法(辗转相除法)计算\\(a,m\\)的最大公因素\\((a,m)= 1\\)保留中间计算的过程.最后进行叠加.\npython\ndef ext_euclid(a, b): b == 0: return 1, 0, a else: x, y, q = ext_euclid(b, a % b) # q = gcd(a, b) = gcd(b, a%b) print(str(x)+\u0026quot; \u0026quot;+str(y)+\u0026quot; \u0026quot;+str(q)) x, y = y, (x - (a // b) * y) return x, y, q ext_euclid(3,7) // this function will return gcd, x, y are return as reference parameter. int gcdEx(int a, int b, int \u0026amp; x, int \u0026amp; y) { if(b == 0){ x = 1, y = 0; return a; } else{ int r = gcdEx(b, a % b, x, y); /* r = GCD(a, b) = GCD(b, a%b) */ int t = x; x = y; y = t - a / b * y; return r; } } ","date":"2019-02-18T00:00:00Z","permalink":"https://zongpitt.com/algorithm/inverse_modulo/","section":"algorithm","tags":null,"title":"inverse gcd"},{"categories":null,"contents":"问题描述 有\\(m\\)种物品，第\\(i\\)种物品有\\(x_i\\)个,重量为\\(w_i\\),背包大小为\\(n\\). 问有多少种装满背包的方法。\n动态规划 对于每种物体有\\(x_i\\)种策略 \\(dp[j] = dp[j-w_i] + dp[j-2*w_i]+...\\)\ncnt = [3, 2, 1, 0, 0] # 个数 w = [1, 2, 3, 4, 5] # 重量 dp = [0 for i in range(int(20))] dp[0] = 1 # dp[i] 代表背包大小为i时有多少种取法 for i in range(len(cnt)): for j in range(len(dp) - 1, w[i] - 1, -1): k = 1 while k \u0026lt;= cnt[i]: if j \u0026gt;= k * w[i]: dp[j] += dp[j - k * w[i]] k += 1 ","date":"2019-02-17T00:00:00Z","permalink":"https://zongpitt.com/algorithm/package/","section":"algorithm","tags":null,"title":"package question"},{"categories":null,"contents":"通过二分的思想快速求幂\\(x^{n}\\)\n将\\(n\\)分解成\\(2^{0},2^{1},2^{2},2^{3},...,2^{m}\\)的组合(相当于将\\(n\\)用二进制表示)\n说明:python库函数用的是快速幂\ndef mod_pow(x, n, mod): ans = 1 while n != 0: if n \u0026amp; 1 != 0: ans = (ans * x) % mod x = x*x % mod n \u0026gt;\u0026gt;= 1 return ans C++\nll poww(ll base, ll exp, ll mod) { ll ans = 1; while (exp) { if (exp \u0026amp; 1) ans = ans * base % mod; base = base * base % mod; exp \u0026gt;\u0026gt;= 1; } return ans; } ","date":"2019-02-17T00:00:00Z","permalink":"https://zongpitt.com/algorithm/power/","section":"algorithm","tags":null,"title":"power"},{"categories":null,"contents":"定义 子序列\n在数学中，某个序列的子序列是从最初序列通过去除某些元素但不破坏余下元素的相对位置（在前或在后）而形成的新序列。\n最大公共子序列(LCS)\n一个数列\\(S\\)，如果分别是两个或多个已知数列的子序列，且是所有符合此条件序列中最长的，则\\(S\\)称为已知序列的最长公共子序列。\n动态规划 \\(X = {x_1,x_2,...,x_m}\\)\n\\(Y = {y_1,y_2,...,y_n}\\)\n\\(Z = {z_1,z_2,...,z_k}\\)\n4 假设\\(Z\\)是\\(X,Y\\)的最大公共子序列.\n* 如果\\(x_m = y_n\\), 则\\(z_k=x_m=y_n\\),且\\(Z_k-1\\) 是\\(X_m-1\\)和\\(Y_n-1\\)的\\(LCS\\) * 如果\\(x_m \\not= y_n\\),则\\(Z\\)是\\(X_m-1\\)和\\(Y_n\\)的\\(LCS\\)或是\\(X_m\\)和\\(Y_n-1\\)的\\(LCS\\)\n\\(c[i][j]\\)表示\\(\\{x_1,x_2,...,x_i\\}\\)和\\(\\{y1,y_2,...,y_j\\}\\)的最大公共子序列的元素个数\n\\[ c[i,j]= \\left \\{ \\begin{array}{l} 0 \u0026amp; i=0\\ or\\ j=0 \\\\ c[i-1,j-1] + 1 \u0026amp; i,j\u0026gt;0\\ and\\ x_i = y_j \\\\ max(c[i,j-1],c[i-1,j]) \u0026amp; i,j\u0026gt;0\\ and\\ x_i \\not= y_j \\\\ \\end{array} \\right. \\]\ndef lcs(a, b): c = [[0 for i in range(len(a)+1)] for i in range(len(b)+1)] for i in range(len(a)+1): for j in range(len(b)+1): if i == 0 or j == 0: c[i][j] = 0 elif a[i-1] == b[j-1]: c[i][j] = c[i-1][j-1] + 1 else: c[i][j] = max(c[i-1][j], c[i][j-1]) return c[len(a)][len(b)] public static int lcs(String str1, String str2) { int len1 = str1.length(); int len2 = str2.length(); int c[][] = new int[len1+1][len2+1]; for (int i = 0; i \u0026lt;= len1; i++) { for( int j = 0; j \u0026lt;= len2; j++) { if(i == 0 || j == 0) { c[i][j] = 0; } else if (str1.charAt(i-1) == str2.charAt(j-1)) { c[i][j] = c[i-1][j-1] + 1; } else { c[i][j] = max(c[i - 1][j], c[i][j - 1]); } } } return c[len1][len2]; } ","date":"2019-02-16T00:00:00Z","permalink":"https://zongpitt.com/algorithm/longest-common-subsequence/","section":"algorithm","tags":null,"title":"Longest Common Subsequence"},{"categories":null,"contents":"int gcd(int a, int b){ if (b == 0) return a; return gcd(b, a%b); } ","date":"2019-02-15T00:00:00Z","permalink":"https://zongpitt.com/algorithm/greatest-common-divisor/","section":"algorithm","tags":null,"title":"Greatest Common Divisor"},{"categories":null,"contents":"#include \u0026lt;cstring\u0026gt; #define maxn 555555 //素数保存在prime数组 int mark[maxn], prime[maxn], res; void get_prime(int n) { //从2----n的素数 memset(mark, 0, sizeof(mark)); res = 0; for (int i = 2; i \u0026lt;= n; i++) { if (!mark[i]) mark[i] = prime[res++] = i; for (int j = 0; j \u0026lt; res \u0026amp;\u0026amp; prime[j] * i \u0026lt;= n; j++) { mark[i*prime[j]] = prime[j]; if (i%prime[j] == 0) break; } } } ","date":"2019-02-15T00:00:00Z","permalink":"https://zongpitt.com/algorithm/primes/","section":"algorithm","tags":null,"title":"prime numbers"},{"categories":["Misc"],"contents":"This blog will not going update anymore. I am going to seperate all topic and migrate to hugo.\nFollowing are new link for the new blog website.\nAlgorithm Book – Collection of algorithms and data structures. Real analysis – permission required Paper notes – My Notes and comment of ML learning papers ","date":"0001-01-01T00:00:00Z","permalink":"https://zongpitt.com/posts/2022-02-05-segment-tree/","section":"posts","tags":["Misc"],"title":"Announcement of this blog"},{"categories":null,"contents":"5.5 The manner in which the completeness of Banach spaces is frequently exploited depends on the following theorem about complete metric spaces, which also has many applications in other parts of mathematics. It implies two of the three most important theorems which make Banach spaces useful tools in analysis, the Banach-Steinhaus theorem and the open mapping theorem. The third is the Hahn-Banach extension theorem, in which completeness plays no role.\n5.6 Baire’s Theorem If \\(X\\) is a complete metric space, the intersection of every countable collection of dense open subsets of \\(X\\) is dense in \\(X\\).\nIn particular (except in the trivial case \\(X=\\varnothing\\) ), the intersection is not empty. This is often the principal significance of the theorem.\nProof Suppose \\(V_1, V_2, V_3, \\ldots\\) are dense and open in \\(X\\). Let \\(W\\) be any open set in \\(X\\). We have to show that \\(\\bigcap V_n\\) has a point in \\(W\\) if \\(W \\neq \\varnothing\\). Let \\(\\rho\\) be the metric of \\(X\\); let us write \\[ S(x, r)=\\{y \\in X: \\rho(x, y)\u0026lt;r\\} \\] and let \\(\\bar{S}(x, r)\\) be the closure of \\(S(x, r)\\). [Note: There exist situations in which \\(\\bar{S}(x, r)\\) does not contain all \\(y\\) with \\(\\rho(x, y) \\leq r !]\\)\nSince \\(V_1\\) is dense, \\(W \\cap V_1\\) is a nonempty open set, and we can therefore find \\(x_1\\) and \\(r_1\\) such that \\[ \\bar{S}\\left(x_1, r_1\\right) \\subset W \\cap V_1 \\text { and } 0\u0026lt;r_1\u0026lt;1 . \\] If \\(n \\geq 2\\) and \\(x_{n-1}\\) and \\(r_{n-1}\\) are chosen, the denseness of \\(V_n\\) shows that \\(V_n \\cap\\) \\(S\\left(x_{n-1}, r_{n-1}\\right)\\) is not empty, and we can therefore find \\(x_n\\) and \\(r_n\\) such that \\[ \\bar{S}\\left(x_n, r_n\\right) \\subset V_n \\cap S\\left(x_{n-1}, r_{n-1}\\right) \\text { and } 0\u0026lt;r_n\u0026lt;\\frac{1}{n} . \\]\nBy induction, this process produces a sequence \\(\\left\\{x_n\\right\\}\\) in \\(X\\). If \\(i\u0026gt;n\\) and \\(j\u0026gt;n\\), the construction shows that \\(x_i\\) and \\(x_j\\) both lie in \\(S\\left(x_n, r_n\\right)\\), so that \\(\\rho\\left(x_i, x_j\\right)\u0026lt;2 r_n\u0026lt;2 / n\\), and hence \\(\\left\\{x_n\\right\\}\\) is a Cauchy sequence. Since \\(X\\) is complete, there is a point \\(x \\in X\\) such that \\(x=\\lim _{n \\rightarrow \\infty} x_n\\).\nSince \\(x_i\\) lies in the closed set \\(\\bar{S}\\left(x_n, r_n\\right)\\) if \\(i\u0026gt;n\\), it follows that \\(x\\) lies in each \\(\\bar{S}\\left(x_n, r_n\\right)\\), and (3) shows that \\(x\\) lies in each \\(V_n\\). By (2), \\(x \\in W\\). This completes the proof.\nCorollary In a complete metric space, the intersection of any countable collection of dense \\(G_\\delta\\) ’s is again a dense \\(G_\\delta\\).\nThis follows from the theorem, since every \\(G_\\delta\\) is the intersection of a countable collection of open sets, and since the union of countably many countable sets is countable.\n5.7 Baire’s theorem is sometimes called the category theorem, for the following reason.\nCall a set \\(E \\subset X\\) nowhere dense if its closure \\(\\bar{E}\\) contains no nonempty open subset of \\(X\\). Any countable union of nowhere dense sets is called a set of the first category; all other subsets of \\(X\\) are of the second category (Baire’s terminology). Theorem \\(5.6\\) is equivalent to the statement that no complete metric space is of the first category. To see this, just take complements in the stateent of Theorem 5.6.\n5.8 The Banach-Steinhaus Theorem Suppose \\(X\\) is a Banach space, \\(Y\\) is a normed linear space, and \\(\\left\\{\\Lambda_\\alpha\\right\\}\\) is a collection of bounded linear transformations of \\(X\\) into \\(Y\\), where \\(\\alpha\\) ranges over some index set \\(A\\). Then either there exists an \\(M\u0026lt;\\infty\\) such that\n\\[ \\left\\|\\Lambda_\\alpha\\right\\| \\leq M \\]\nfor every \\(\\alpha \\in A\\), or\n\\[ \\sup _{\\alpha \\in A}\\left\\|\\Lambda_\\alpha x\\right\\|=\\infty \\]\nfor all \\(x\\) belonging to some dense \\(G_\\delta\\) in \\(X\\).\nIn geometric terminology, the alternatives are as follows: Either there is a ball \\(B\\) in \\(Y\\) (with radius \\(M\\) and center at 0 ) such that every \\(\\Lambda_\\alpha\\) maps the unit ball of \\(X\\) into \\(B\\), or there exist \\(x \\in X\\) (in fact, a whole dense \\(G_\\delta\\) of them) such that no ball in \\(Y\\) contains \\(\\Lambda_\\alpha x\\) for all \\(\\alpha\\).\nThe theorem is sometimes referred to as the uniform boundedness principle.\nProof Put\n\\[ \\varphi(x)=\\sup _{\\alpha \\in A}\\left\\|\\Lambda_\\alpha x\\right\\| \\quad(x \\in X) \\]\nand let\n\\[ V_n=\\{x: \\varphi(x)\u0026gt;n\\} \\quad(n=1,2,3, \\ldots) . \\]\nSince each \\(\\Lambda_\\alpha\\) is continuous and since the norm of \\(Y\\) is a continuous function on \\(Y\\) (an immediate consequence of the triangle inequality, as in the proof of Theorem 4.6), each function \\(x \\rightarrow\\left\\|\\Lambda_\\alpha x\\right\\|\\) is continuous on \\(X\\). Hence \\(\\varphi\\) is lower semicontinuous, and each \\(V_n\\) is open.\nIf one of these sets, say \\(V_N\\), fails to be dense in \\(X\\), then there exist an \\(x_0 \\in X\\) and an \\(r\u0026gt;0\\) such that \\(\\|x\\| \\leq r\\) implies \\(x_0+x \\notin V_N\\); this means that \\(\\varphi\\left(x_0+x\\right) \\leq N\\), or\n\\[ \\left\\|\\Lambda_\\alpha\\left(x_0+x\\right)\\right\\| \\leq N \\]\nfor all \\(\\alpha \\in A\\) and all \\(x\\) with \\(\\|x\\| \\leq r\\). Since \\(x=\\left(x_0+x\\right)-x_0\\), we then have\n\\[ \\left\\|\\Lambda_\\alpha x\\right\\| \\leq\\left\\|\\Lambda_\\alpha\\left(x_0+x\\right)\\right\\|+\\left\\|\\Lambda_\\alpha x_0\\right\\| \\leq 2 N, \\]\nand it follows that (1) holds with \\(M=2 N / r\\).\nThe other possibility is that every \\(V_n\\) is dense in \\(X\\). In that case, \\(\\bigcap V_n\\) is a dense \\(G_\\delta\\) in \\(X\\), by Baire’s theorem; and since \\(\\varphi(x)=\\infty\\) for every \\(x \\in \\bigcap V_n\\), the proof is complete.\n5.9 The Open Mapping Theorem Let \\(U\\) and \\(V\\) be the open unit balls of the \\(B a n a c h\\) spaces \\(X\\) and \\(Y\\). To every bounded linear transformation \\(\\Lambda\\) of \\(X\\) onto \\(Y\\) there corresponds a \\(\\delta\u0026gt;0\\) so that\n\\[ \\Lambda(U) \\supset \\delta V . \\]\nNote the word “onto” in the hypothesis. The symbol \\(\\delta V\\) stands for the set \\(y: y \\in V\\}\\), i.e., the set of all \\(y \\in Y\\) with \\(\\|y\\|\u0026lt;\\delta\\).\nIt follows from (1) and the linearity of \\(\\Lambda\\) that the image of every open ball in with center at \\(x_0\\), say, contains an open ball in \\(Y\\) with center at \\(\\Lambda x_0\\). Hence e image of every open set is open. This explains the name of the theorem. Here is another way of stating (1): To every \\(y\\) with \\(\\|y\\|\u0026lt;\\delta\\) there corresponds \\(x\\) with \\(\\|x\\|\u0026lt;1\\) so that \\(\\Lambda x=y\\).\nProof Given \\(y \\in Y\\), there exists an \\(x \\in X\\) such that \\(\\Lambda x=y\\); if \\(\\|x\\|\u0026lt;k\\), it follows that \\(y \\in \\Lambda(k U)\\). Hence \\(Y\\) is the union of the sets \\(\\Lambda(k U)\\), for \\(k=1,2,3, \\ldots\\). Since \\(Y\\) is complete, the Baire theorem implies that there is a nonempty open set \\(W\\) in the closure of some \\(\\Lambda(k U)\\). This means that every point of \\(W\\) is the limit of a sequence \\(\\left\\{\\Lambda x_i\\right\\}\\), where \\(x_i \\in k U\\); from now on, \\(k\\) and \\(W\\) are fixed.\nChoose \\(y_0 \\in W\\), and choose \\(\\eta\u0026gt;0\\) so that \\(y_0+y \\in W\\) if \\(\\|y\\|\u0026lt;\\eta\\). For any such \\(y\\) there are sequences \\(\\left\\{x_i^{\\prime}\\right\\},\\left\\{x_i^{\\prime \\prime}\\right\\}\\) in \\(k U\\) such that \\[ \\Lambda x_i^{\\prime} \\rightarrow y_0, \\quad \\Lambda x_i^{\\prime \\prime} \\rightarrow y_0+y \\quad(i \\rightarrow \\infty) . \\] Setting \\(x_i=x_i^{\\prime \\prime}-x_i^{\\prime}\\), we have \\(\\left\\|x_i\\right\\|\u0026lt;2 k\\) and \\(\\Lambda x_i \\rightarrow y\\). Since this holds for every \\(y\\) with \\(\\|y\\|\u0026lt;\\eta\\), the linearity of \\(\\Lambda\\) shows that the following is true, if \\(\\delta=\\eta / 2 k\\) To each \\(y \\in Y\\) and to each \\(\\epsilon\u0026gt;0\\) there corresponds an \\(x \\in X\\) such that \\[ \\|x\\| \\leq \\delta^{-1}\\|y\\| \\quad \\text { and }\\|y-\\Lambda x\\|\u0026lt;\\epsilon \\text {. } \\] This is almost the desired conclusion, as stated just before the start of the proof, except that there we had \\(\\epsilon=0\\).\nFix \\(y \\in \\delta V\\), and fix \\(\\epsilon\u0026gt;0\\). By (3) there exists an \\(x_1\\) with \\(\\left\\|x_1\\right\\|\u0026lt;1\\) and \\[ \\left\\|y-\\Lambda x_1\\right\\|\u0026lt;\\frac{1}{2} \\delta \\epsilon . \\] Suppose \\(x_1, \\ldots, x_n\\) are chosen so that \\[ \\left\\|y-\\Lambda x_1-\\cdots-\\Lambda x_n\\right\\|\u0026lt;2^{-n} \\delta \\epsilon . \\] Use (3), with \\(y\\) replaced by the vector on the left side of (5), to obtain an \\(x_{n+1}\\) so that (5) holds with \\(n+1\\) in place of \\(n\\), and \\[ \\left\\|x_{n+1}\\right\\|\u0026lt;2^{-n} \\epsilon \\quad(n=1,2,3, \\ldots) . \\] If we set \\(s_n=x_1+\\cdots+x_n\\), (6) shows that \\(\\left\\{s_n\\right\\}\\) is a Cauchy sequence in \\(X\\). Since \\(X\\) is complete, there exists an \\(x \\in X\\) so that \\(s_n \\rightarrow x\\). The inequality \\(\\left\\|x_1\\right\\|\u0026lt;1\\), together with (6), shows that \\(\\|x\\|\u0026lt;1+\\epsilon\\). Since \\(\\Lambda\\) is continuous, \\(\\Lambda s_n \\rightarrow \\Lambda x\\). By (5), \\(\\Lambda s_n \\rightarrow y\\). Hence \\(\\Lambda x=y\\).\nWe have now proved that \\[ \\Lambda((1+\\epsilon) U) \\supset \\delta V \\] or \\[ \\Lambda(U) \\supset(1+\\epsilon)^{-1} \\delta V, \\] for every \\(\\epsilon\u0026gt;0\\). The union of the sets on the right of (8), taken over all \\(\\epsilon\u0026gt;0\\), is \\(\\delta V\\). This proves (1).\n5.10 Theorem If \\(X\\) and \\(Y\\) are Banach spaces and if \\(\\Lambda\\) is a bounded linear transformation of \\(X\\) onto \\(Y\\) which is also one-to-one, then there is a \\(\\delta\u0026gt;0\\) such that\n\\[ \\|\\Lambda x\\| \\geq \\delta\\|x\\| \\quad(x \\in X) . \\]\nIn other words, \\(\\Lambda^{-1}\\) is a bounded linear transformation of \\(Y\\) onto \\(X\\).\nProof If \\(\\delta\\) is chosen as in the statement of Theorem 5.9, the conclusion of that theorem, combined with the fact that \\(\\Lambda\\) is now one-to-one, shows that \\(\\|\\Lambda x\\|\u0026lt;\\delta\\) implies \\(\\|x\\|\u0026lt;1\\). Hence \\(\\|x\\| \\geq 1\\) implies \\(\\|\\Lambda x\\| \\geq \\delta\\), and (1) is proved.\nThe transformation \\(\\Lambda^{-1}\\) is defined on \\(Y\\) by the requirement that \\(\\Lambda^{-1} y=x\\) if \\(y=\\Lambda x\\). A trivial verification shows that \\(\\Lambda^{-1}\\) is linear, and (1) implies that \\(\\left\\|\\Lambda^{-1}\\right\\| \\leq 1 / \\delta\\).\n","date":"0001-01-01T00:00:00Z","permalink":"https://zongpitt.com/papa-rudin/ch-5/2-consequences-of-baires-theorem/","section":"papa rudin","tags":null,"title":"Consequences of Baire's Theorem"},{"categories":null,"contents":"MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent. The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions.\ntime steps, \\(t=0,1,2,3, \\ldots\\) At each time step \\(t\\), the agent receives some representation of the environment’s state, \\(S_{t} \\in \\mathcal{S}\\), and on that basis selects an action, \\(A_{t} \\in \\mathcal{A}(s) .{ }\\) One time step later, in part as a consequence of its action, the agent receives a numerical reward, \\(R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}\\), and finds itself in a new state, \\(S_{t+1} \\cdot\\)\nThat is, for particular values of these random variables, \\(s^{\\prime} \\in \\mathcal{S}\\) and \\(r \\in \\mathcal{R}\\), there is a probability of those values occurring at time \\(t\\), given particular values of the preceding state and action:\n\\[ p\\left(s^{\\prime}, r \\mid s, a\\right) \\doteq \\operatorname{Pr}\\left\\{S_{t}=s^{\\prime}, R_{t}=r \\mid S_{t-1}=s, A_{t-1}=a\\right\\} \\]\nfor all \\(s^{\\prime}, s \\in \\mathcal{S}, r \\in \\mathcal{R}\\), and \\(a \\in \\mathcal{A}(s)\\). The function \\(p\\) defines the dynamics of the MDP.\nFrom the four-argument dynamics function, \\(p\\), one can compute anything else one might want to know about the environment, such as the state-transition probabilities (which we denote, with a slight abuse of notation, as a three-argument function \\(p: \\mathcal{S} \\times \\mathcal{S} \\times \\mathcal{A} \\rightarrow[0,1]\\),\n\\[ p\\left(s^{\\prime} \\mid s, a\\right) \\doteq \\operatorname{Pr}\\left\\{S_{t}=s^{\\prime} \\mid S_{t-1}=s, A_{t-1}=a\\right\\}=\\sum_{r \\in \\mathcal{R}} p\\left(s^{\\prime}, r \\mid s, a\\right) . \\]\nWe can also compute the expected rewards for state-action pairs as a two-argument function \\(r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\) :\n\\[ r(s, a) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a\\right]=\\sum_{r \\in \\mathcal{R}} r \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime}, r \\mid s, a\\right), \\]\nand the expected rewards for state-action-next-state triples as a three-argument function \\(r: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\)\n\\[ r\\left(s, a, s^{\\prime}\\right) \\doteq \\mathbb{E}\\left[R_{t} \\mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\\prime}\\right]=\\sum_{r \\in \\mathcal{R}} r \\frac{p\\left(s^{\\prime}, r \\mid s, a\\right)}{p\\left(s^{\\prime} \\mid s, a\\right)} . \\]\nReturn and Episodes In general, we seek to maximize the expected return, where the return, denoted \\(G_{t}\\), is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\n\\[ G_{t} \\doteq R_{t+1}+R_{t+2}+R_{t+3}+\\cdots+R_{T}, \\]\nwhere \\(T\\) is a final time step. This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states.\nPolicies and Value Functions Almost all reinforcement learning algorithms involve estimating value functions – functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).\nFormally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy \\(\\pi\\) at time \\(t\\), then \\(\\pi(a \\mid s)\\) is the probability that \\(A_{t}=a\\) if \\(S_{t}=s .\\) Like \\(p, \\pi\\) is an ordinary function; the “|” in the middle of \\(\\pi(a \\mid s)\\) merely reminds us that it defines a probability distribution over \\(a \\in \\mathcal{A}(s)\\) for each \\(s \\in \\mathcal{S}\\). Reinforcement learning methods specify how the agent’s policy is changed as a result of its experience.\nThe value function of a state \\(s\\) under a policy \\(\\pi\\), denoted \\(v_{\\pi}(s)\\), is the expected return when starting in \\(s\\) and following \\(\\pi\\) thereafter. For MDPs, we can define \\(v_{\\pi}\\) formally by\n\\[ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right], \\text { for all } s \\in \\mathcal{S} \\]\nWe call the function \\(v_{\\pi}\\) the state-value function for policy \\(\\pi\\).\nQ-function\nSimilarly, we define the value of taking action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\), denoted \\(q_{\\pi}(s, a)\\), as the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\) :\n\\[ q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s, A_{t}=a\\right] . \\]\nWe call \\(q_{\\pi}\\) the action-value function for policy \\(\\pi\\).\n","date":"0001-01-01T00:00:00Z","permalink":"https://zongpitt.com/posts/2021-12-22-refinforcement-learning/","section":"posts","tags":null,"title":"Reinforcement Learning"}]