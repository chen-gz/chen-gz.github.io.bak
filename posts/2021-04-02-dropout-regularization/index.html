<!DOCTYPE html>
<html><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
        class="dark-mode-native-dark-original" />
    
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
     
    <link rel="stylesheet" href="https://zongpitt.com/sass/main.06935202011a97ba473ebeeb0cb6788daa9fb5a9fff471a4e85850293d1c5ac4.css" />
    <title>
          Guangzong Blog | Dropout Regularization
    </title>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>
    <script src=" https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

    <script
        src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>
    <script type="text/javascript" src="/js/commons/back-to-top.js"></script>
    
    
    
    <script type="text/javascript" src="/js/commons/topbar-switch.js"></script>
    <script type="text/javascript" src="/js/commons/sidebar.js"></script>
    <script type="text/javascript" src="/js/utils/category-collapse.js"></script>
    <script type="text/javascript" src="/js/utils/checkbox.js"></script>
    <script type="text/javascript" src="/js/utils/clipboard.js"></script>
    <script type="text/javascript" src="/js/utils/img-extra.js"></script>
    <script type="text/javascript" src="/js/utils/pageviews.js"></script>
    
    <script type="text/javascript" src="/js/utils/timeago.js"></script>
    
    
</head>
<body data-spy="scroll" data-target="#toc" data-topbar-visible="true" class="d-flex"><div id="sidebar" class="d-flex flex-column align-items-end">
    <div class="profile-wrapper text-center">

        <div id="avatar">
            <a href="/" alt="avatar" class="mx-auto"> <img src=https://github.com/chen-gz/chen-gz/blob/main/mine_square.jpg?raw&#61;truehttps://raw.githubusercontent.com/chen-gz/chen-gz/blob/main/mine_square.jpg alt="avatar"
                    onerror="this.style.display='none'"> </a>
        </div>

        <div class="site-title mt-3"> <a href="/">Guangzong</a> </div>
        <div class="site-subtitle font-italic">Welcome to my blog</div>
    </div>

    <ul>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/">
                <span>
                    Home
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/archive/">
                <span>
                    Archive
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/tags/">
                <span>
                    Tags
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/about/">
                <span>
                    About
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/categories/">
                <span>
                    Categories
                </span>
            </a>
        </li>
        
    </ul>

    <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">
        <button class="mode-toggle btn" aria-label="Switch Mode"> 
            <i class="fas fa-adjust"></i> 
        </button>

        <span class="icon-border"></span>
        <a href="https://github.com/chen-gz" aria-label="github" target="_blank" rel="noopener">
            <i class="fab fa-github"></i>
        </a>

        <a href=" javascript:location.href = 'mailto:' + ['chen-gz','outlook.com'].join('@')" aria-label="email">
            <i class="fas fa-envelope"></i>
        </a>

        <a href="/feed.xml" aria-label="rss"> <i class="fas fa-rss"></i> </a>
    </div>

    
    
</div>
<div id="all-wrapper" class="d-flex flex-column"><div id="topbar-wrapper">
    <div id="topbar" class="d-flex">
        
        <span id="breadcrumb" class="d-flex">
              
               
            
            
            <div><a href='https://zongpitt.com/'>Home</a></div>
              
              
            
            <div><a href="https://zongpitt.com/posts">posts</a></div>
               
                 
                
        </span>
        <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>
        <div id="topbar-title">Guangzong</div>
        <i id="search-trigger" class="fas fa-search fa-fw"></i>
        <span id="search-wrapper" class="align-items-center">
            <i class="fas fa-search fa-fw"></i>
            <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off"
                placeholder="Search..." />
            <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
        </span>
    </div>
</div>
<div id="main-wrapper" class="d-flex mt-3">
            <div id="core-wrapper" class="d-flex flex-column">
                <div id="main" class="d-flex flex-column">
                    
<div class="post pl-1 pr-1 pl-md-2 pr-md-2">
    
    <div id="post-content">
        <h1 style="text-align: center;">Dropout Regularization</h1>
        <br>
        
        <p>Paper available from <a
href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
class="uri">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a>
and <a href="https://dl.acm.org/doi/10.5555/2627435.2670313"
class="uri">https://dl.acm.org/doi/10.5555/2627435.2670313</a></p>
<p>The idea of dropout is eliminate some node randomly and then training
the network.</p>
<p>Inverted dropout technique makes the parameter remain same.</p>
<p><a href="https://machinelearning.wtf/terms/inverted-dropout/"
class="uri">https://machinelearning.wtf/terms/inverted-dropout/</a> <a
href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/"
class="uri">https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/</a></p>
<p>Intuition: Can’t rely on any one feature, so have to spread out
weights.</p>
<p>Different dropout ratio for different layer base on the number of the
neuron number will help to improve the accuracy.</p>
<p>Following is copy from link <a
href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/"
class="uri">https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/</a>
the author is Paolo Galeone. The reason I copy to here is keep the
resource available for me.</p>
<h2 id="analysis-of-dropout">Analysis of Dropout</h2>
<p>Overfitting is a problem in Deep Neural Networks (DNN): the model
learns to classify only the training set, adapting itself to the
training examples instead of learning decision boundaries capable of
classifying generic instances. Many solutions to the overfitting problem
have been presented during these years; one of them have overwhelmed the
others due to its simplicity and its empirical good results:
Dropout.</p>
<p><img
src="https://raw.githubusercontent.com/chen-gz/picBed/master/20210403123330.png" /></p>
<p>The idea behind Dropout is to train an ensemble of DNNs and average
the results of the whole ensemble instead of train a single DNN.</p>
<p>The DNNs are built dropping out neurons with \(p\) probability,
therefore keeping the others on with probability \(q=1−p\). When a
neuron is dropped out, its output is set to zero, no matter what the
input or the associated learned parameter is.</p>
<p>The dropped neurons do not contribute to the training phase in both
the forward and backward phases of back-propagation: for this reason
every time a single neuron is dropped out it’s like the training phase
is done on a new network.</p>
<p>quoting the authors: &gt; In a standard neural network, the
derivative received by each parameter tells it how it should change so
the final loss function is reduced, given what all other units are
doing. Therefore, units may change in a way that they fix up the
mistakes of the other units. This may lead to complex co-adaptations.
This in turn leads to overfitting because these co-adaptations do not
generalize to unseen data. We hypothesize that for each hidden unit,
Dropout prevents co-adaptation by making the presence of other hidden
units unreliable. Therefore, a hidden unit cannot rely on other specific
units to correct its mistakes.</p>
<p>In short: Dropout works well in practice because it prevents the
co-adaption of neurons during the training phase.</p>
<p>Now that we got an intuitive idea behind Dropout, let’s analyze it in
depth.</p>
<h3 id="how-dropout-works">How Dropout works</h3>
<p>As said before, Dropout turns off neurons with probability \(p\) and
therefore let the others turned on with probability \(q=1−p\).</p>
<p><strong>Every single neuron has the same probability of being turned
off.</strong> This means that:</p>
<p>Given</p>
<ul>
<li>\(h(x)=x W+b\) a linear projection of a \(d_{i}-\) dimensional input
\(x\) in a \(d_{h}\)-dimensional output space.</li>
<li>\(a(h)\) an activation function.</li>
</ul>
<p>It’s possible to model the application of Dropout, in the training
phase only, to the given projection as a modified activation
function:</p>
<p><span class="math display">\[
f(h)=D \odot a(h)
\]</span></p>
<p>Where \(D=(X_{1}, , X_{d_{h}})\) is a \(d_{h}\) -dimensional vector
of Bernoulli variables \(X_{i}\).</p>
<p>A Bernoulli random variable has the following probability mass
distribution: <span class="math display">\[
f(k ; p)=\left\{\begin{array}{ll}
p &amp; \text { if } \quad k=1 \\
1-p &amp; \text { if } \quad k=0
\end{array}\right.
\]</span> Where \(k\) are the possible outcomes.</p>
<p>It’s evident that this random variable perfectly models the Dropout
application on a single neuron. In fact, the neuron is turned off with
probability \(p=P(k=1)\) and kept on otherwise.</p>
<p>It can be useful to see the application of Dropout on a generic \(\)
-th neuron: <span class="math display">\[
O_{i}=X_{i} a\left(\sum_{k=1}^{d_{i}} w_{k}
x_{k}+b\right)=\left\{\begin{array}{ll}
a\left(\sum_{k=1}^{d_{i}} w_{k} x_{k}+b\right) &amp; \text { if } \quad
X_{i}=1 \\
0 &amp; \text { if } \quad X_{i}=0
\end{array}\right.
\]</span> where \(P(X_{i}=0)=p\). Since during train phase a neuron is
kept on with probability \(q\), during the testing phase we have to
emulate the behavior of the ensemble of networks used in the training
phase.</p>
<p>To do this, the authors suggest scaling the activation function by a
factor of q during the test phase in order to use the expected output
produced in the training phase as the single output required in the test
phase. Thus:</p>
<span class="math display">\[\begin{array}{l}
\text { Train phase: } O_{i}=X_{i} a\left(\sum_{k=1}^{d_{i}} w_{k}
x_{k}+b\right) \\
\text { Test phase: } O_{i}=q a\left(\sum_{k=1}^{d_{i}} w_{k}
x_{k}+b\right)
\end{array}\]</span>
<h3 id="inverted-dropout">Inverted Dropout</h3>
<p>A slightly different approach is to use <strong>Inverted
Dropout</strong>. This approach consists in the scaling of the
activations during the training phase, leaving the test phase
untouched.</p>
=,
<span class="math display">\[\begin{array}{l}
\text { Train phase: } O_{i}=\frac{1}{q} X_{i} a\left(\sum_{k=1}^{d_{i}}
w_{k} x_{k}+b\right) \\
\text { Test phase: } O_{i}=a\left(\sum_{k=1}^{d_{i}} w_{k}
x_{k}+b\right)
\end{array}\]</span>
<p>Inverted Dropout is how Dropout is implemented in practice in the
various deep learning frameworks because it helps to define the model
once and just change a parameter (the keep/drop probability) to run
train and test on the same model.</p>
<p>Direct Dropout, instead, force you to modify the network during the
test phase because if you don’t multiply by \(q\) the output the neuron
will produce values that are higher respect to the one expected by the
successive neurons (thus the following neurons can saturate or explode):
that’s why Inverted Dropout is the more common implementation.</p>
<h3 id="dropout-of-a-set-of-neurons">Dropout of a set of neurons</h3>
<p>It can be easily noticed that a layer h with n neurons, in a single
train step, can be seen as an ensemble of \(n\) Bernoulli experiments,
each one with a probability of success equals to \(p\).</p>
<p>Thus, the output of the layer \(h\) have a number of dropped neurons
equals to:</p>
<p><span class="math display">\[
Y=\sum_{i=1}^{d_{h}}\left(1-X_{i}\right)
\]</span></p>
<p>Since every neuron is now modeled as a Bernoulli random variable and
all these random variables are independent and identically distributed,
the total number of dropped neuron is a random variable too, called
Binomial:</p>
<p><span class="math display">\[
Y \sim B i\left(d_{h}, p\right)
\]</span></p>
<p>Where the probability of getting exactly \(k\) success in \(n\)
trials is given by the probability mass distribution:</p>
<p><span class="math display">\[
f(k ; n, p)=\left(\begin{array}{l}
n \\ k
\end{array}\right) p^{k}(1-p)^{n-k}
\]</span></p>
This formula can be easily explained: - \(p<sup>{k}(1-p)</sup>{n-k}\) is
the probability of getting a single sequence of \(k\) successes on \(n\)
trials and therefore \(n-k\) failures. \((
<span class="math display">\[\begin{array}{l}n \\ k\end{array}\]</span>
<p>)\) is the binomial coefficient used to calculate the number of
possible sequence of success.</p>
<p>We can now use this distribution to analyze the probability of
dropping a specified number of neurons.</p>
<p>When using Dropout, we define a fixed Dropout probability \(p\) for a
chosen layer and we expect that a proportional number of neurons are
dropped from it.</p>
<p>For example, if the layer we apply Dropout to has \(n=1024\) neurons
and \(p=0.5,\) we expect that 512 get dropped. Let’s verify this
statement:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\sum_{i=1}^{1024} X_{i} \sim B i(1024,0.5) \\
P(Y=512) &amp;=\left(\begin{array}{c}
1024 \\
512
\end{array}\right) 0.5^{512}(1-0.5)^{1024-512} \approx 0.025
\end{aligned}
\]</span></p>
<p>Thus, the probability of dropping out exactly \(n p=512\) neurons is
of only \(0.025 !\) A python 3 script can help us to visualize how
neurons are dropped for different values of \(p\) and a fixed value of
\(n\). The code is commented.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># number of neurons</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># number of tests (input examples)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># histogram bin width, for data visualization</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>binwidth <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># per layer probability</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> p <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate size values from a bi(n, prob)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    rnd_values <span class="op">=</span> binom.rvs(n, prob, size<span class="op">=</span>size)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># draw histogram of rnd values</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    plt.hist(</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        rnd_values,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        bins<span class="op">=</span>[x <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n, binwidth)],</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># normalize = extract the probabilities</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        normed<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pick a random color</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span>np.random.rand(<span class="dv">3</span>, <span class="dv">1</span>),</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># label the histogram with its probability</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="bu">str</span>(prob))</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">&#39;upper right&#39;</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img
src="https://raw.githubusercontent.com/chen-gz/picBed/master/20210403130415.png" /></p>
<p>As we can see from the image above no matter what the \(p\) value is,
the number of neurons dropped on average is proportional to \(n p\), in
fact: <span class="math display">\[
E[B i(n, p)]=n p
\]</span> Moreover, we can notice that the distribution of values is
almost symmetric around \(p=0.5\) and the probability of dropping \(n
p\) neurons increase as the distance from \(p=0.5\) increase. The
scaling factor has been added by the authors to compensate the
activation values, because they expect that during the training phase
only a percentage of \(1-p\) neurons have been kept. During the testing
phase, instead, the \(100 %\) of neurons are kept on, thus the value
should be scaled down accordingly.</p>
<h3 id="dropout-other-regularizers">Dropout &amp; other
regularizers</h3>
<p>Dropout is often used with L2 normalization and other parameter
constraint techniques (such as Max Norm \(\), this is not a case.
Normalizations help to keep model parameters value low, in this way a
parameter can’t grow too much. In brief, the L2 normalization (for
example) is an additional term to the loss, where \(\) is an
hyper-parameter called regularization strength, \(F(W ; x)\) is the
model and \(\) is the error function between the real \(y\) and the
predicted \(\) value.</p>
<p><span class="math display">\[
\mathcal{L}(y, \hat{y})=\mathcal{E}(y, F(W ; x))+\frac{\lambda}{2} W^{2}
\]</span></p>
<p>It’s easy to understand that this additional term, when doing
back-propagation via gradient descent, reduces the update amount. If
\(\) is the learning rate, the update amount of the parameter \(w W\)
is</p>
<p><span class="math display">\[
w \leftarrow w-\eta\left(\frac{\partial F(W ; x)}{\partial w}+\lambda
w\right)
\]</span></p>
<p>Dropout alone, instead, does not have any way to prevent parameter
values from becoming too large during this update phase. Moreover, the
inverted implementation leads the update steps to become bigger, as
showed below.</p>
<h3 id="inverted-dropout-and-other-regularizers">Inverted Dropout and
other regularizers</h3>
<p>Since Dropout does not prevent parameters from growing and
overwhelming each other, applying L2 regularization (or any other
regularization technique that constraints the parameter values) can
help. Making explicit the scaling factor, the previous equation
becomes:</p>
<p><span class="math display">\[
w \leftarrow w-\eta\left(\frac{1}{q} \frac{\partial F(W ; x)}{\partial
w}+\lambda w\right)
\]</span></p>
<p>It can be easily seen that when using Inverted Dropout, the learning
rate is scaled by a factor of \(q\). Since \(q\) has values in \(]
0,1]\) the ratio between \(\) and \(q\) can vary between:</p>
<p><span class="math display">\[
r(q)=\frac{\eta}{q} \in\left[\eta=\lim _{q \rightarrow 1}
r(q),+\infty=\lim _{q \rightarrow 0} r(q)\right]
\]</span></p>
<p>For this reason, from now on we’ll call \(q\) boosting factor because
it boosts the learning rate. Moreover, we’ll call \(r(q)\) the effective
learning rate. The effective learning rate, thus, is higher respect to
the learning rate chosen: for this reason normalizations that constrain
the parameter values can help to simplify the learning rate selection
process.</p>
<h3 id="summary">Summary</h3>
<ol type="1">
<li>Dropout exists in two versions: direct (not commonly implemented)
and inverted</li>
<li>Dropout on a single neuron can be modeled using a Bernoulli random
variable</li>
<li>Dropout on a set of neurons can be modeled using a Binomial random
variable</li>
<li>Even if the probability of dropping exactly \(n p\) neurons is low,
\(n p\) neurons are dropped on average on a layer of \(n\) neurons.</li>
<li>Inverted Dropout boost the learning rate</li>
<li>Inverted Dropout should be using together with other normalization
techniques that constrain the parameter values in order to simplify the
learning rate selection procedure</li>
<li>Dropout helps to prevent overfitting in deep neural networks.</li>
<li>Max Norm impose a constraint to the parameters size. Chosen a value
for the hyper-parameter \(c\) it impose the constraint \(|w| c .
\).</li>
</ol>

    </div>
    
</div>
</div>

                <div id="footer-wrapper">
                    <footer class="footer"><footer class="row pl-3 pr-3">
    <div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0">
        <div class="footer-left">
            <p class="mb-0"> © 2022 <a href="https://github.com/chen-gz">guangzong</a>. <span data-toggle="tooltip"
                    data-placement="top"
                    title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">All
                    rights reserved.</span></p>
        </div>
        <div class="footer-right">
            <p class="mb-0"> Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">hugo</a>
                with <a href="https://github.com/chen-gz/hugo_zong" target="_blank" rel="noopener">zong</a>
                theme.</p>
        </div>
    </div>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script> MathJax = {
            tex: {
                tags: 'ams',
            }
        };</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</footer>
</footer>
                </div>
            </div>
            <div id="panel-wrapper" class="d-flex flex-column"><div id="toc-wrapper" class="mb-5 pl-2">
    <span>Contents</span>
    <nav id="toc" data-toggle="toc">
        
        
        
    </nav>
</div>

<div id="access-lastmod" class="pb-4 pl-2">
    <div class="panel-heading">Recently Updated</div>
    <ul class="post-content pl-3 pb-1 ml-1 mt-2">
          
        <li><a href="https://zongpitt.com/papa-rudin/ch-8/1-measurability-on-cartesian-products/"></a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-7/3-differentiable-transformations/">Differentiable Transformations</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-7/1-derivatives-of-measures/">Derivatives of Measures</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-7/2-the-fundamental-theorem-of-calculus/">The Fundamental Theorem of Calculus</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-6/1-total-variation/">Total Variation</a></li>
        
    </ul>
</div>

<div id="access-tags" class="pl-2">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">
        
        <a class="post-tag" href="https://zongpitt.com/tags/algorithm/">Algorithm</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/android/">Android</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/c/">C</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/c-language/">c language</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/caddy/">Caddy</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/cadence/">Cadence</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/cdc/">CDC</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/classification/">classification</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/compress/">Compress</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/conf/">Conf</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/double-column/">Double Column</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/errno13/">Errno13</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/fish/">fish</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/gan/">GAN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/git/">Git</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/gtk/">Gtk</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/html/">Html</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/i3/">I3</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/idea/">Idea</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/java/">Java</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/latex/">Latex</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/lda/">LDA</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/linear-programming/">Linear Programming</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/linux/">Linux</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/lr/">LR</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/math/">Math</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mathematics/">Mathematics</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/matlab/">Matlab</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/matplot/">matplot</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/memory/">Memory</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mermaid/">Mermaid</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/misc/">Misc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/ml/">ML</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mosfet/">Mosfet</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/network/">Network</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/nn/">NN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/pandoc/">Pandoc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/pca/">PCA</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/probability/">Probability</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/productive/">Productive</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/program/">Program</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/programming/">Programming</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/python/">Python</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/qt/">Qt</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/random-variable/">Random Variable</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/rcnn/">RCNN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/rust/">Rust</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/security/">Security</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/software/">Software</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/std/">Std</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/svd/">Svd</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/tool/">Tool</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/usb/">USB</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vim/">Vim</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vpn/">Vpn</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vpnc/">Vpnc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/xterm/">xterm</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/zsh/">Zsh</a>
        
    </div>
</div>
</div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
    <div class="col-12 col-sm-11 post-content">
        <div id="search-hints"></div>
        <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3 col-12"></div>
    </div>
</div>

<script src="/js/fuse.min.js"></script>
<script src="/js/fastsearch.js"></script>


</div>
        <div id="mask"></div>
        <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"
            style="display: none">
            <i class="fas fa-angle-up"></i>
        </a>
    </div>
    
</body>

</html>
