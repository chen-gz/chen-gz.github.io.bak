---
title: Singular value decomposition 5
author: Guangzong Chen
date: 2021-06-17 17:18:00 -0400
categories: [Math]
tags: [Svd]
math: true
mermaid: true
---



The singular value decomposition for a matrix A writes A as a product (hanger)(stretcher)(aligner). 

It's an amazing and useful fact that every m x n matrix has a singular value decomposition. 

The following theorem goes two-thirds of the way to proving this fact: 

------

## Two-thirds Theorem

For an $m \times n$ matrix $\mathrm{A}: \mathbb{R}^{\mathrm{n}} \rightarrow \mathbb{R}^{\mathbb{m}}$ and any orthonormal basis $\left\{\vec{a}_{1}, \vec{a}_{2}, \ldots, \vec{a}_{n}\right\}$ of $\mathbb{R}^{n}$,
define $s_{i}=\left\|\hat{a} \vec{a}_{i}\right\|$
and
$\vec{h}_{i}=\left\{\begin{array}{cc}\overrightarrow{0} & \text { if } s_{i}=0 \\ \frac{1}{s_{i}} A \vec{a}_{i} & \text { if } s_{i} \neq 0\end{array}\right.$.
Then $A=\left(\begin{array}{llll}\vec{h}_{1} & \mid \vec{h}_{2} & \ldots & \mid \vec{h}_{n}\end{array}\right)\left(\begin{array}{cccc}s_{1} & 0 & 0 & 0 \\ 0 & s_{2} & 0 & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & 0 & s_{n}\end{array}\right)\left(\begin{array}{c}\vec{a}_{1} \\ \frac{\vec{a}_{2}}{\vdots} \\ \overrightarrow{a_{n}}\end{array}\right)$.

Proof: Using first the row way and then the column way to multiply a matrix times a point, you see that the right hand side of the equation sends $\vec{a}_{i}$ to $s_{i} \vec{h}_{i}=s_{i} \frac{1}{s_{i}} A \vec{a}_{i}=A \vec{a}_{i}$
Thus the two sides of the equation agree on the basis $\left\{\vec{a}_{1}, \vec{a}_{2}, \ldots, \vec{a}_{n}\right\}$ and so must be equal.

-----

The two-thirds theorem gets you two-thirds of the way to the SVD.
It says that given any orthonormal basis $\left\{\vec{a}_{1}, \vec{a}_{2}, \ldots, \vec{a}_{n}\right\}$ of $\mathbb{R}^{n}$ you can write
$$
\begin{aligned}
A=&\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \ldots \mid \vec{h}_{n}\right)\left(\begin{array}{cccc}
s_{1} & 0 & 0 & 0 \\
0 & s_{2} & 0 & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & 0 & s_{n}
\end{array}\right)\left(\frac{\vec{a}_{1}}{\vec{a}_{2}}\right) \\
&=\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \ldots \mid \vec{h}_{n}\right) \text { (stretcher) (aligner). }
\end{aligned}
$$
So you've got the stretcher and the aligner - - if $\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \ldots \mid \vec{h}_{n}\right)$ were a hanger matrix then this would be a Singular Value Decomposition for $A$.
For $\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \ldots \mid \vec{h}_{n}\right)$ to be a hanger matrix requires that the columns $\vec{h}_{i}=\frac{1}{s_{i}} A \vec{a}_{i}$ be pairwise perpendicular.

So one challenge to finding an SVD for $A$ is to find an orthonormal basis of $\mathbb{R}^{n}$, $\left\{a_{1}, \ldots, a_{n}\right\}$ so that for all $i \neq j, A a_{i} \cdot A a_{j}=0$

---

Theorem: If $A$ is an $\mathrm{m} \mathrm{x} \mathrm{n}$ matrix, then there is an orthonormal basis of $\mathbb{R}^{n}$, $\left\{a_{1}, \ldots, a_{n}\right\}$ so that for all $i \neq j, A a_{i} \cdot A a_{j}=0$

Proof 1: For $m \times 2$ matrices
You can get 2D perpframes using $v_{1}(t)=\left(\begin{array}{c}\cos (t) \\ \sin (t)\end{array}\right)$ and $v_{2}(t)=\left(\begin{array}{c}-\sin (t) \\ \cos (t)\end{array}\right)$ and specifying an angle $0 \leq t \leq \frac{\pi}{2}$
You'd like to show that for any $\mathrm{m} \times 2$ matrix there's an angle $\mathrm{t}$ so that $\left(A v_{1}(t)\right) \cdot\left(A v_{2}(t)\right)$ is zero.
How do you know there is always such a $t$ ?

Answer: Go with the example matrix $A=\left(\begin{array}{cc}1 & -1 \\ 2 & 1\end{array}\right)$ and look at the following plot that shows the perpframe $v_{1}(t)$ and $v_{2}(t)$ before and after the hit with $\mathrm{A}$.

![](https://raw.githubusercontent.com/chen-gz/picBed/master/tofrom.gif)

Notice that when $\mathrm{t}=0$, the angle between $\mathrm{A} \mathbf{v}_{1}[\mathrm{t}]$ and $\mathrm{A} \mathbf{v}_{2}[\mathrm{t}]$ is greater than $\frac{\mathrm{pi}}{2}$.
By the time $t=\frac{p i}{2}=1.57$, the angle is less than $\frac{\mathrm{Pi}}{2}$.
That's enough to guarantee that somewhere between $\mathrm{t}=0$ and $\mathrm{t}=\frac{\mathrm{Pi}}{2}$ there's an angle where $\mathrm{A} \mathrm{v}_{1}[\mathrm{t}]$ and $\mathrm{A} \boldsymbol{v}_{2}[\mathrm{t}]$ are perpendicular.
Most reasonable folks would accept the evidence given in the plots, but the doubters might want to look at the function:
$$
\mathrm{f}[\mathrm{t}]=\left(\mathrm{A} \cdot\left(\begin{array}{c}
\cos [t] \\
\sin [t]
\end{array}\right]\right) \cdot\left(\mathrm{A} \cdot\left(\begin{array}{c}
-\sin (t) \\
\cos (t)
\end{array}\right)\right)
$$
Then $f[0] \stackrel{(1)}{=} A\left(\begin{array}{l}1 \\ 0\end{array}\right) \cdot A\left(\begin{array}{l}0 \\ 1\end{array}\right) \stackrel{(2)}{=}-A\left(\begin{array}{c}-1 \\ 0\end{array}\right) \cdot A\left(\begin{array}{l}0 \\ 1\end{array}\right) \stackrel{(3)}{=}-f\left[\frac{\pi}{2}\right]$
(1) The definition of $\mathrm{f}$.
(2) Linearity of matrix hits.
(3) The definition of $\mathrm{f}$.
This equation says that $\mathrm{f}[0]$ and $\mathrm{f}\left[\frac{\pi}{2}\right]$ have opposite signs. This should convince even the doubters that there's a $t$ between 0 and $\frac{\pi}{2}$ that makes $\mathrm{f}[\mathrm{t}]$ zero.
The same argument holds for any $\mathrm{m}$ by 2 matrix.

----

Proof 2: General case
The proof above works well enough for $\mathrm{m} \times 2$ matrices, but this proof shows that for all $\mathrm{m} \times \mathrm{n}$ matrices there's is a perpendicular frame $\left\{\vec{a}_{1}, \vec{a}_{2}, \vec{a}_{3}, \ldots, \vec{a}_{n}\right\}$ of unit vectors so that $\hat{a} \widehat{a}_{i} \cdot A \vec{a}_{j}=0$ for $i<j$
Here's one way to get such a perpframe.
Start with a $\mathrm{m} \times \mathrm{n}$ matrix $A$.
Step $1:$ Let $\vec{a}_{1}$ be a unit vector maximizing $|A \vec{v}|^{2}$.
Step 2 : Let $s_{2}$ be the space perpendicular to span $\left\{\vec{a}_{1}\right\}$ Let $\vec{a}_{2}$ be a unit vector in $s_{2}$ maximizing $|A \vec{v}|^{2}$.

Step 3: Let $\mathrm{s}_{3}$ be the space perpendicular to span $\left\{\overrightarrow{\mathrm{a}}_{1}, \overrightarrow{\mathrm{a}}_{2}\right\}$ Let $\overrightarrow{\mathrm{a}}_{3}$ be a unit vector in $\mathrm{s}_{3}$ maximizing $\quad|\mathrm{A} \overrightarrow{\mathrm{v}}|^{2}$.
Step $\mathrm{n}$ : Let $\mathrm{s}_{\mathrm{n}}$ be the space perpendicular to span $\left\{\overrightarrow{\mathrm{a}}_{1}, \overrightarrow{\mathrm{a}}_{2}, \ldots, \overrightarrow{\mathrm{a}}_{n-1}\right\}$
Let $\vec{a}_{n}$ be a unit vector in $s_{n}$ maximizing $|A \vec{v}|^{2}$.

This gets you a perpendicular frame of unit vectors $\left\{\vec{a}_{1}, \vec{a}_{2}, \vec{a}_{3}, \ldots, \vec{a}_{n}\right\}$, but how do you know that $\mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}=0$ for $\mathrm{i}<\mathrm{j}$ ?
Look at $\vec{v}[t]=\cos [t] \vec{a}_{i}+\sin [t] \vec{a}_{j}$ and see that
- no matter what $\mathrm{t}$ you choose $\mathrm{v}[\mathrm{t}]$ is a unit vector in $\mathrm{s}_{\mathrm{i}}$

- $v[0]=\operatorname{Cos}[0] \vec{a}_{i}+\sin [0] \vec{a}_{j}=\vec{a}_{i}$
  Since $\overrightarrow{\mathrm{a}}_{\mathrm{i}}$ is a unit vector in $s_{\mathrm{i}}$ maximizing $|\mathrm{A} \overrightarrow{\mathrm{v}}|^{2}$ and $\vec{v}[\mathrm{t}]$ is in $s_{\mathrm{i}}$ for all $\mathrm{t}$, you know that $\mathrm{f}[\mathrm{t}]=|\mathrm{A} \vec{v}[\mathrm{t}]|^{2}$ has a maximum at $\mathrm{t}=0$
  This tells you $0=\mathrm{f}^{\prime}[0]$

Now compute:
$\mathrm{f}[\mathrm{t}]=|\mathrm{A} \vec{v}[\mathrm{t}]|^{2}$
$=\mathrm{A} \vec{v}[\mathrm{t}] \cdot \mathrm{A} \overrightarrow{\mathrm{v}}[\mathrm{t}]$
$=A\left(\operatorname{Cos}[t] \vec{a}_{i}+\operatorname{Sin}[t] \vec{a}_{j}\right) \cdot A\left(\operatorname{Cos}[t] \vec{a}_{i}+\sin [t] \vec{a}_{j}\right)$
$=\left(A \vec{a}_{i} \cdot A \vec{a}_{i}\right) \operatorname{Cos}[t]^{2}+2\left(A \vec{a}_{i} \cdot A \vec{a}_{j}\right) \sin [t] \operatorname{Cos}[t]+$
$\left(A \overrightarrow{\mathrm{a}}_{\mathrm{j}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right) \sin [\mathrm{t}]^{\varepsilon}$

When you remember $\left(\mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{i}}\right),\left(\mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right)$, and $\left(\mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right)$ are just numbers, you understand
that its nothing more than tedious to compute:
$\mathrm{f}^{\prime}[\mathrm{t}]=-2\left(\mathrm{~A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{i}}\right) \operatorname{Cos}[\mathrm{t}] \sin (\mathrm{t}]-2\left(\hat{\mathrm{A}} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right) \sin [\mathrm{t}] \sin [\mathrm{t}$
$+2\left(\mathrm{~A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \hat{\mathrm{A}} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right) \operatorname{Cos}[\mathrm{t}] \operatorname{Cos}[\mathrm{t}]+2\left(\mathrm{~A} \overrightarrow{\mathrm{a}}_{\mathrm{j}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}\right) \sin [\mathrm{t}] \operatorname{Cos}[\mathrm{t}]$
Plugging in $\mathrm{t}=0$ gives you $\mathrm{f}^{\prime}[0]=2 \mathrm{~A} \overrightarrow{\mathrm{a}}_{\mathrm{i}} \cdot \mathrm{A} \overrightarrow{\mathrm{a}}_{\mathrm{j}}$.
But you already know that $f^{\prime}[0]=0$, so after canceling the 2 you get $0=A \vec{a}_{i} \cdot A \vec{a}_{j}$ which is just what you wanted.

----

Proof 3: Based on the spectral theorem
This proof is slick IF YOU'VE ALREADY SEEN THE SPECTRAL THEOREM.
If you haven't seen the spectral theorem, then skip this proof.
Given $\mathrm{A}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{\mathrm{m}}$ and an orthonormal basis $\left\{\vec{a}_{1}, \vec{a}_{2}, \ldots, \vec{a}_{n}\right\}$ of $\mathbb{R}^{n}$,
$A \vec{a}_{i} \cdot A \vec{a}_{j}=0$ for $i \neq j$
iff $\vec{a}_{i} \cdot\left(A^{t} A \vec{a}_{j}\right)=0$ for $i \neq j$
iff $A^{t} A \vec{a}_{j}=\lambda \vec{a}_{j}$
iff $\left\{\vec{a}_{1}, \vec{a}_{2}, \ldots, \vec{a}_{n}\right\}$ are all eigenvectors of $A^{t} A$.
Conclusion: The desired basis is guaranteed by spectral theorem since $A^{t} A$ is symmetric.

-----

Theorem: Every matrix has a singular value decomposition.
The theorem above almost gives you the SVD for any matrix.
The only problem is that although the columns of the "hanger" matrix are pairwise perpendicular, they might not form a basis for $\mathbb{R}^{m}$.
For example, suppose for a $5 x 4$ matrix $A: \mathbb{R}^{4} \rightarrow \mathbb{R}^{5}$ the procedure outlined above gives you:
To complete the decomposition, let $\left\{\overrightarrow{\mathrm{h}}_{3}, \overrightarrow{\mathrm{h}}_{4}, \overrightarrow{\mathrm{h}}_{5}\right\}$ be an orthonormal basis for the three dimensional subspace of $\mathbb{R}^{5}$ perpendicular to $\operatorname{span}\left\{\vec{h}_{1}, \vec{h}_{2}\right\}$.

Then write
$$
\begin{gathered}
A=\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \overrightarrow{0} \mid \overrightarrow{0}\right)\left(\begin{array}{llll}
3 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right)\left(\begin{array}{l}
\frac{\vec{a}_{1}}{\vec{a}_{\hat{2}}} \\
\frac{\vec{a}_{3}}{\vec{a}_{4}}
\end{array}\right) \\
\stackrel{(1)}{=}\left(\vec{h}_{1}\left|\vec{h}_{2}\right| \vec{h}_{3}\left|\vec{h}_{4}\right| \vec{h}_{5}\right)\left(\begin{array}{llll}
3 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{array}\right)\left(\begin{array}{l}
\frac{\vec{a}_{1}}{\vec{a}_{2}} \\
\frac{\vec{a}_{3}}{\vec{a}_{4}}
\end{array}\right)
\end{gathered}
$$
(1) The two sides agree on the basis $\left\{\vec{a}_{1}, \vec{a}_{2}, \vec{a}_{3}, \vec{a}_{4}\right\}$.
This, finally, is a singular value decomposition for $A$.

----

Comments:
- The diagonal entries of the stretcher matrix are called the "singular values of $A " .$
- An extra row of zeros has been added to the stretcher matrix to produce the dimensions required for the multiplication. If $A$ is $\mathrm{m} \mathrm{x} \mathrm{n}$ with $m>n$, then rows will be deleted.

In either case, the dimensions of the stretcher matrix will always match the dimensions of
$A$
- The decomposition shows that the action of every matrix can be described as a rotation followed by a stretch followed by another rotation.
- The proofs above are meant to show that every matrix has an SVD. You can compute SVD's for $\mathrm{mx} 2$ matrices by hand, but you should use a machine to compute SVD's for bigger matrices.

----

Exercises
1. Above, you saw that if $\mathrm{A}$ is a $5 \mathrm{x} 4$ matrix $A: \mathbb{R}^{4} \rightarrow \mathbb{R}^{5}$ then the SVD for $\mathrm{A}$ has the form
$$
A=\left(\overrightarrow{\mathrm{h}}_{1}\left|\overrightarrow{\mathrm{h}}_{2}\right| \overrightarrow{\mathrm{h}}_{3}\left|\overrightarrow{\mathrm{h}}_{4}\right| \overrightarrow{\mathrm{h}}_{5} \text { ) }\left(\begin{array}{cccc}
s_{1} & 0 & 0 & 0 \\
0 & s_{2} & 0 & 0 \\
0 & 0 & s_{3} & 0 \\
0 & 0 & 0 & s_{4} \\
0 & 0 & 0 & 0
\end{array}\right)\left(\frac{\overrightarrow{\mathrm{a}}_{1}}{\overrightarrow{\mathrm{a}_{2}}}\right) .\right.
$$
Give the form for the SVD of a $4 \times 5$ matrix $A: \mathbb{R}^{5} \rightarrow \mathbb{R}^{4}$.
(Hint: The matrix has five singular values, but $s_{5}=0$ does not appear in the decomposition.)

----

2. Let $A=\left(\begin{array}{ll}1 & 2 \\ 0 & 1\end{array}\right) .$ Following the steps of Proof 1 above, define $\nu_{1}=\left(\begin{array}{c}\cos [t] \\ \sin [t]\end{array}\right]$ and $v_{2}=\left(\begin{array}{c}-\sin (t) \\ \cos (t)\end{array}\right)$.
a. Expand $\left(A v_{1}\right) \cdot\left(A v_{2}\right)$
b. After applying trig identities it turns out that $\left(A v_{1}\right) \cdot\left(A v_{2}\right)=2(\operatorname{Cos}[2 t]+\operatorname{Sin}[2 t]]$.
Use this fact to find a $t$ so that $\left(A v_{1}\right) \cdot\left(A v_{2}\right)=0$
c. Using part (b) as a start, give an SVD for $A$.
