---
title: Wasserstein GAN 2
author: Guangzong Chen
date: 2021-07-17 10:36:00 -0400
categories: [ML]
tags: [GAN]
math: true
mermaid: true
---

There are two paper to introduce Wasserstein GAN.
[Towards Principled Methods for Training Generative Adversarial Networks](https://arxiv.org/abs/1701.04862) and [Wasserstein GAN](https://arxiv.org/abs/1701.07875).

### Different Distances 

- The Total Variation (TV) distance
$$
\delta\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\sup _{A \in \Sigma}\left|\mathbb{P}_{r}(A)-\mathbb{P}_{g}(A)\right|
$$
- The Kullback-Leibler (KL) divergence

$$
K L\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right)=\int \log \left(\frac{P_{r}(x)}{P_{g}(x)}\right) P_{r}(x) d \mu(x)
$$

where both $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$ are assumed to be absolutely continuous, and therefore admit densities, with respect to a same measure $\mu$ defined on $\mathcal{X} \stackrel{2}{ }^{2}$ The KL divergence is famously assymetric and possibly infinite when there are points such that $P_{g}(x)=0$ and $P_{r}(x)>0$

- The Jensen-Shannon (JS) divergence

$$
J S\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=K L\left(\mathbb{P}_{r} \| \mathbb{P}_{m}\right)+K L\left(\mathbb{P}_{g} \| \mathbb{P}_{m}\right)
$$
where $\mathbb{P}_{m}$ is the mixture $\left(\mathbb{P}_{r}+\mathbb{P}_{g}\right) / 2 .$ This divergence is symmetrical and always defined because we can choose $\mu=\mathbb{P}_{m}$.

- The Earth-Mover (EM) distance or Wasserstein-1

$$
W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\inf _{\gamma \in \Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]
$$

where $\Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are respectively $\mathbb{P}_{r}$ and $\mathbb{P}_{g}$. Intuitively, $\gamma(x, y)$ indicates how much "mass" must be transported from $x$ to $y$ in order to transform the distributions $\mathbb{P}_{r}$ into the distribution $\mathbb{P}_{g} .$ The EM distance then is the "cost" of the optimal transport plan.

The GAN paper give an example for these distance. I just provide the result.  When $P_r$ and $P_g$ does not perfect align.  The distance show as follow.

- $$J S\left(\mathbb{P}_{0}, \mathbb{P}_{\theta}\right)= \begin{cases}\log 2 & \text { if } \theta \neq 0 \\ 0 & \text { if } \theta=0\end{cases} $$

- $$ W\left(\mathbb{P}_{0}, \mathbb{P}_{\theta}\right)=|\theta|$$

- $$K L\left(\mathbb{P}_{\theta} \| \mathbb{P}_{0}\right)=K L\left(\mathbb{P}_{0} \| \mathbb{P}_{\theta}\right)= \begin{cases}+\infty & \text { if } \theta \neq 0 \\ 0 & \text { if } \theta=0\end{cases}$$

- $$  \text { and } \delta\left(\mathbb{P}_{0}, \mathbb{P}_{\theta}\right)= \begin{cases}1 & \text { if } \theta \neq 0 \\ 0 & \text { if } \theta=0\end{cases}  $$

#### Assumption 1

Let $g: \mathcal{Z} \times \mathbb{R}^{d} \rightarrow \mathcal{X}$ be locally Lipschitz between finite dimensional vector spaces. We will denote $g_{\theta}(z)$ it's evaluation on coordinates $(z, \theta) .$ We say that $g$ satisfies assumption 1 for a certain probability distribution $p$ over $\mathcal{Z}$ if there are local Lipschitz constants $L(\theta, z)$ such that
$$
\mathbb{E}_{z \sim p}[L(\theta, z)]<+\infty
$$

#### Theorem 1.

Let $\mathbb{P}_{r}$ be a fixed distribution over $\mathcal{X} .$ Let $Z$ be a random variable (e.g Gaussian) over another space $\mathcal{Z} .$ Let $g: \mathcal{Z} \times \mathbb{R}^{d} \rightarrow \mathcal{X}$ be a function, that will be denoted $g_{\theta}(z)$ with $z$ the first coordinate and $\theta$ the second. Let $\mathbb{P}_{\theta}$ denote the distribution of $g_{\theta}(Z) .$ Then,
1. If $g$ is continuous in $\theta$, so is $W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)$.
2. If $g$ is locally Lipschitz and satisfies regularity assumption 1, then $W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)$ is continuous everywhere, and differentiable almost everywhere.
3. Statements 1-2 are false for the Jensen-Shannon divergence $J S\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)$ and all the KLs.

This theorem shows Earth-Mover distance is continue but $JS$, $KL$ are not.

#### Corollary 1.

Let $g_{\theta}$ be any feedforward neural network $p(z)$ a prior over $z$ such that $\mathbb{E}_{z \sim p(z)}[\|z\|]<\infty$ (e.g. Gaussian, uniform, etc.). Then assumption 1 is satisfied and therefore $W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)$ is continuous everywhere and differentiable almost everywhere.

#### Theorem 2.

Theorem 2. Let $\mathbb{P}$ be a distribution on a compact space $\mathcal{X}$ and $\left(\mathbb{P}_{n}\right)_{n \in \mathbb{N}}$ be a sequence of distributions on $\mathcal{X} .$ Then, considering all limits as $n \rightarrow \infty$,
1. The following statements are equivalent
- $\delta\left(\mathbb{P}_{n}, \mathbb{P}\right) \rightarrow 0$ with $\delta$ the total variation distance.
- $J S\left(\mathbb{P}_{n}, \mathbb{P}\right) \rightarrow 0$ with JS the Jensen-Shannon divergence.
2. The following statements are equivalent
- $W\left(\mathbb{P}_{n}, \mathbb{P}\right) \rightarrow 0$
- $\mathbb{P}_{n} \stackrel{\mathcal{D}}{\rightarrow} \mathbb{P}$ where $\stackrel{\mathcal{D}}{\rightarrow}$ represents convergence in distribution for random variables.
3. $K L\left(\mathbb{P}_{n} \| \mathbb{P}\right) \rightarrow 0$ or $K L\left(\mathbb{P} \| \mathbb{P}_{n}\right) \rightarrow 0$ imply the statements in (1).
4. The statements in (1) imply the statements in (2).

## Wasserstein GAN

Because use definition of Earth mover distance is highly intractable. 

Kantorovich-Rubinstein duality tell us that
$$
W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)=\sup _{\|f\|_{L} \leq 1} \mathbb{E}_{x \sim \mathbb{P}_{r}}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_{\theta}}[f(x)]
$$
where the supremum is over all the 1-Lipschitz functions $f: \mathcal{X} \rightarrow \mathbb{R}$. Note that if we replace $\|f\|_{L} \leq 1$ for $\|f\|_{L} \leq K$ (consider $K$ -Lipschitz for some constant $\left.K\right)$, then we end up with $K \cdot W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)$. 

In original paper proposed using clip to ensure the 1-Lipschitz condition. The issue cause by this setting is describe in the paper.  Another paper propose using gradient penalty to ensure 1-Lipschitz. It will give better result than weight clip. Adam will not work in weigh clip setting. RMSprop is used in this algorithm. WGAN-GP solve this issue.

#### Theorem 3 

Theorem 3. Let $\mathbb{P}_{r}$ be any distribution. Let $\mathbb{P}_{\theta}$ be the distribution of $g_{\theta}(Z)$ with $Z$ a random variable with density $p$ and $g_{\theta}$ a function satisfying assumption 1. Then, there is a solution $f: \mathcal{X} \rightarrow \mathbb{R}$ to the problem
$$
\max _{\|f\|_{L} \leq 1} \mathbb{E}_{x \sim \mathbb{P}_{r}}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_{\theta}}[f(x)]
$$
and we have
$$
\nabla_{\theta} W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)=-\mathbb{E}_{z \sim p(z)}\left[\nabla_{\theta} f\left(g_{\theta}(z)\right)\right]
$$
when both terms are well-defined.

## Algorithm 

![](https://raw.githubusercontent.com/chen-gz/picBed/master/20210718190440.png)

