---
title: Generative Adversarial Nets
author: Guangzong Chen
date: 2021-06-12 10:36:00 -0400
categories: [ML]
tags: [GAN]
math: true
mermaid: true
---

GAN is very popular network now. It can be used to generate fake read good fake dataset, change image style, generate carton images. Anyway, it is a really good network. As we all know it is useful, let's get into detail directly. Generative adversarial network usually contains two part, generative model $G$ and discriminative model $D .$ Figure 1 shows this GAN structure.

In GAN simultaneously train two models: a generative model $G$ that capture the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$. The training procedure for $G$ is to maximize the probability of $D$ making a mistake. The training procedure for $D$ is to minimize discrimiante error for fixed $G$. This is a kind of two-player minimax game

The unique solution exist for this system, with $G$ recovering the training data distribution adn $D$ equal to $\frac{1}{2}$ everywhere.

## Formulation

we will have following value function base on maximum likelihood estimation

$$
\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]
$$

where $p_{g}(\boldsymbol{x})$ is generator's distribution over data $\boldsymbol{x} . p_{\boldsymbol{z}}(\boldsymbol{z})$ is prior defined distribution base on noise variables $\boldsymbol{z} . G(\boldsymbol{z})$ is mapping which will map input noise variable $z$ to data space $G\left(\boldsymbol{z} ; \theta_{g}\right) . \theta_{g}$ is parameter of Generative model $G$. To explain this equation more intuitive, following explanation will not very accuracy. But it will help for understanding. The output of discriminator $D$ is 0 or $1 .$ It is binary classifier. Consider generate form of binary log likelihood function. We will maximize likelihood function to get best estimation.

$$
\max _{\theta} L(\theta)=\sum_{i=1}^{n} y_{i} \log \eta\left(x_{i} ; \theta\right)+\left(1-y_{i}\right) \log \left(1-\eta\left(x_{i} ; \theta\right)\right)
$$

where $y_{i}$ is ground trues, $\eta\left(x_{i} ; \theta\right)$ is estimator output based on parameter $\theta$. Let's look Eq.1, it is very similar to binary log likely hood function if we fix $G$ and multiply the number of sample. We will found it is same as out binary $\log$ likelihood function. What GAN did is max $D$ base on fix $G$, then $\min G$ base on fix $D$. Definitely we don't have very good $D$ or $G$ at the begin. So just repeat operation until it converge $(\max D$ based on fix $G, \min G$ based on fix $D)$.

## Algorithm
Algorithm is shows Algorithm $1 .$ The network is trained by gradient descent. As we mentioned before, GAN will train discriminator $D$ first then train generative model $G$.

![](https://raw.githubusercontent.com/chen-gz/picBed/master/20210613143410.png)

## Theoretical Results
### Global Optimality of $p_{g}=p_{\text {data }}$
Proposition 1. For $G$ fixed, the optimal discriminator $D$ is

$$
D_{G}^{*}(\boldsymbol{x})=\frac{p_{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_{g}(\boldsymbol{x})}
$$

$C(G)$ is defined as $\max _{D} V(G, D)$
Theorem 1. The global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_{g}=p_{\text {data }}$. At that point, $C(G)$ achieves the value $-\log 4$.

### Convergence of Algorithm 1
Proposition 2. If $G$ and $D$ have enough capacity, and at each step of Algorithm 1 , the discriminator is allowed to reach its optimum given $G$, and $p_{g}$ is updated so as to improve the criterion

$$
\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}}\left[\log D_{G}^{*}(\boldsymbol{x})\right]+\mathbb{E}_{\boldsymbol{x} \sim p_{g}}\left[\log \left(1-D_{G}^{*}(\boldsymbol{x})\right)\right]
$$

then $p_{g}$ converges to $p_{\text {data }}$

## Applications

1. style GAN - https://thispersondoesnotexist.com/
2. Image to image translation
