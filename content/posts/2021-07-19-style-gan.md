---
title: Style GAN
author: Guangzong Chen
date: 2021-07-19 10:36:00 -0400
categories: [ML]
tags: [GAN]
math: true
mermaid: true
---

Nvidia post style GAN. This  GAN structure can generate very high resolution image. [this person does not exist](https://thispersondoesnotexist.com/) shows a very good result to apply in human face generate.

There are two paper relate to style GAN. [A style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948) proposed the idea.  [Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958) improve the styleGAN. 



The most important idea in style GAN is control different feature in different stage instead of put all of them in the first stage. 

### Dataset

human face dataset, Flickr-Faces-HQ (FFHQ) is used in the paper.

### Architecture

Traditionally the latent code is provided to the generator through an input layer, i.e., the first layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a **learned constant** instead (Figure $1 \mathrm{~b}$, right).

![](https://raw.githubusercontent.com/chen-gz/picBed/master/20210719125139.png)

### mapping network

Mapping network $\mathbf{f}$ is compose by 8-layer MLP, it will map latent code $\mathbf{z}$ to in intermediate latent space $\mathbf{w}$. 



### Style control

The style information is come from intermediate latent space. Learned affine transformations then specialize $\mathbf{w}$ to styles $\mathbf{y}=\left(\mathbf{y}_{s}, \mathbf{y}_{b}\right)$ that control adaptive instance normalization (AdaIN) [27, 17, 21, 16] operations after each convolution layer of the synthesis network $g$. The AdaIN operation is
$$
\operatorname{AdaIN}\left(\mathbf{x}_{i}, \mathbf{y}\right)=\mathbf{y}_{s, i} \frac{\mathbf{x}_{i}-\mu\left(\mathbf{x}_{i}\right)}{\sigma\left(\mathbf{x}_{i}\right)}+\mathbf{y}_{b, i}
$$
where each feature map $\mathbf{x}_{i}$ is normalized separately, and then scaled and biased using the corresponding scalar components from style $\mathrm{y}$. Thus the dimensionality of $\mathbf{y}$ is twice the number of feature maps on that layer.

### Stochastic details

Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned per-feature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure $1 \mathrm{~b}$. The implications of adding the noise inputs are discussed in Sections $3.2$ and $3.3$.

![](https://raw.githubusercontent.com/chen-gz/picBed/master/20210719162447.png)

### progressive growing

progressive is proposed in [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://arxiv.org/abs/1710.10196)

The following picture shows how progressive growing works. 

![](https://raw.githubusercontent.com/chen-gz/picBed/master/20210719170927.png)

