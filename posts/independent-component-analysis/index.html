<!DOCTYPE html>
<html><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
        class="dark-mode-native-dark-original" />
    
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
     
    <link rel="stylesheet" href="https://zongpitt.com/sass/main.06935202011a97ba473ebeeb0cb6788daa9fb5a9fff471a4e85850293d1c5ac4.css" />
    <title>
          Guangzong Blog | Independent Component Analysis
    </title>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script>
    <script src=" https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script>

    <script
        src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script>
    <script type="text/javascript" src="/js/commons/back-to-top.js"></script>
    
    
    
    <script type="text/javascript" src="/js/commons/topbar-switch.js"></script>
    <script type="text/javascript" src="/js/commons/sidebar.js"></script>
    <script type="text/javascript" src="/js/utils/category-collapse.js"></script>
    <script type="text/javascript" src="/js/utils/checkbox.js"></script>
    <script type="text/javascript" src="/js/utils/clipboard.js"></script>
    <script type="text/javascript" src="/js/utils/img-extra.js"></script>
    <script type="text/javascript" src="/js/utils/pageviews.js"></script>
    
    <script type="text/javascript" src="/js/utils/timeago.js"></script>
    
    
</head>
<body data-spy="scroll" data-target="#toc" data-topbar-visible="true" class="d-flex"><div id="sidebar" class="d-flex flex-column align-items-end">
    <div class="profile-wrapper text-center">

        <div id="avatar">
            <a href="/" alt="avatar" class="mx-auto"> <img src=https://github.com/chen-gz/chen-gz/blob/main/mine_square.jpg?raw&#61;truehttps://raw.githubusercontent.com/chen-gz/chen-gz/blob/main/mine_square.jpg alt="avatar"
                    onerror="this.style.display='none'"> </a>
        </div>

        <div class="site-title mt-3"> <a href="/">Guangzong</a> </div>
        <div class="site-subtitle font-italic">Welcome to my blog</div>
    </div>

    <ul>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/">
                <span>
                    Home
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/archive/">
                <span>
                    Archive
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/tags/">
                <span>
                    Tags
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/about/">
                <span>
                    About
                </span>
            </a>
        </li>
        
        
        <li class="nav-item">
            

            <a class="nav-link" href="/categories/">
                <span>
                    Categories
                </span>
            </a>
        </li>
        
    </ul>

    <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center">
        <button class="mode-toggle btn" aria-label="Switch Mode"> 
            <i class="fas fa-adjust"></i> 
        </button>

        <span class="icon-border"></span>
        <a href="https://github.com/chen-gz" aria-label="github" target="_blank" rel="noopener">
            <i class="fab fa-github"></i>
        </a>

        <a href=" javascript:location.href = 'mailto:' + ['chen-gz','outlook.com'].join('@')" aria-label="email">
            <i class="fas fa-envelope"></i>
        </a>

        <a href="/feed.xml" aria-label="rss"> <i class="fas fa-rss"></i> </a>
    </div>

    
    
</div>
<div id="all-wrapper" class="d-flex flex-column"><div id="topbar-wrapper">
    <div id="topbar" class="d-flex">
        
        <span id="breadcrumb" class="d-flex">
              
               
            
            
            <div><a href='https://zongpitt.com/'>Home</a></div>
              
              
            
            <div><a href="https://zongpitt.com/posts">posts</a></div>
               
                 
                
        </span>
        <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>
        <div id="topbar-title">Guangzong</div>
        <i id="search-trigger" class="fas fa-search fa-fw"></i>
        <span id="search-wrapper" class="align-items-center">
            <i class="fas fa-search fa-fw"></i>
            <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off"
                placeholder="Search..." />
            <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
        </span>
    </div>
</div>
<div id="main-wrapper" class="d-flex mt-3">
            <div id="core-wrapper" class="d-flex flex-column">
                <div id="main" class="d-flex flex-column">
                    
<div class="post pl-1 pr-1 pl-md-2 pr-md-2">
    
    <div id="post-content">
        <h1 style="text-align: center;">Independent Component Analysis</h1>
        <br>
        
        <p>This blog mainly refers to <a
href="https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf">Independent
Component Analysis: Algorithms and Applications</a> which wrote by Aapo
Hyv√§rinen and Erkki Oja.</p>
<p>Some useful reference</p>
<ol type="1">
<li><a
href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf">Elements
of Information Theory</a></li>
<li><a
href="https://www.math.wustl.edu/~kuffner/AlastairYoung/BanksYoung1987.pdf">What
is projection pursuit</a> - Entropy Estimation</li>
<li><a href="">New approximations of differential entropy for
independent component analysis and projection pursuit</a> - Entropy
Estimation</li>
</ol>
<h3 id="definition">Definition</h3>
<h4 id="cocktail-party-problem">Cocktail party problem</h4>
<p>Imagine that you are in a room where two people are speaking
simultaneously. You have two microphones, which you hold in different
locations. The microphones give you two recorded time signals, which we
could denote by <span class="math inline">\(x_1(t)\)</span> and <span
class="math inline">\(x_2(t)\)</span>, with <span
class="math inline">\(x_1\)</span> and <span
class="math inline">\(x_2\)</span> the amplitudes, and <span
class="math inline">\(t\)</span> the time index. Each of these recorded
signals is a weighted sum of the speech signals emitted by the two
speakers, which we denote by <span class="math inline">\(s_1(t)\)</span>
and <span class="math inline">\(s_2(t)\)</span>. We could express this
as a linear equation: <span class="math display">\[
\begin{aligned}
&amp; x_1(t)=a_{11} s_1+a_{12} s_2 \\
&amp; x_2(t)=a_{21} s_1+a_{22} s_2
\end{aligned}
\]</span> where <span class="math inline">\(a_{11}, a_{12},
a_{21}\)</span>, and <span class="math inline">\(a_{22}\)</span> are
some parameters that depend on the distances of the microphones from the
speakers. It would be very useful if you could now estimate the two
original speech signals <span class="math inline">\(s_1(t)\)</span> and
<span class="math inline">\(s_2(t)\)</span>, using only the recorded
signals <span class="math inline">\(x_1(t)\)</span> and <span
class="math inline">\(x_2(t)\)</span>. This is called the cocktail-party
problem. For the time being, we omit any time delays or other extra
factors from our simplified mixing model.</p>
<h4 id="assumption">Assumption</h4>
<p><span class="math inline">\(s_1(t)\)</span> and <span
class="math inline">\(s_2(t)\)</span>, at each time instant <span
class="math inline">\(t\)</span>, are statistically independent. This is
not an unrealistic assumption in many cases, and it need not be exactly
true in practice.</p>
<h4 id="definition-and-notation-of-ica">Definition and notation of
ICA</h4>
<p>We assume mixture variables <span
class="math inline">\(\mathbf{x}\)</span> and the independent components
have zero mean.</p>
<p><span class="math inline">\(\mathbf{x}\)</span> = <span
class="math inline">\([x_1, x_2, \ldots, x_n]^T\)</span>, whose element
are the mixture <span class="math inline">\(x_1, x_2, \dots,
x_n\)</span>. <span class="math inline">\(\mathbf{s}\)</span> = <span
class="math inline">\([s_1, s_2, \ldots, s_n]^T\)</span>, whose element
are the input signals <span class="math inline">\(s_1, s_2, \dots,
s_n\)</span>.</p>
<p><span class="math inline">\(\mathbf{A}\)</span> is a matrix with
entries <span class="math inline">\(a_{ij}\)</span></p>
<p>Support ùêÄ is a square matrix and non-singular. So after we estimate
ùêÄ, we can compute its inverse ùêñ, and obtain the independent components
by:</p>
<p><span class="math display">\[
ùê¨ = ùêñùê±
\]</span></p>
<h4 id="ambiguities-of-ica">Ambiguities of ICA</h4>
<ol type="1">
<li>We cannot determine the variance (energies) of the independent
components.</li>
<li>We cannot determine the order the independent components.
(permutation)</li>
</ol>
<p>Because both ùêÄ and ùê¨ is unknown.</p>
<h3 id="what-is-independence">What is independence</h3>
<h4 id="definition-1">Definition</h4>
<p>Technically, independence can be defined by the probability
densities. Let us denote by <span class="math inline">\(p\left(y_1,
y_2\right)\)</span> the joint probability density function (pdf) of
<span class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span>. Let us further denote by <span
class="math inline">\(p_1\left(y_1\right)\)</span> the marginal pdf of
<span class="math inline">\(y_1\)</span>, i.e.¬†the pdf of <span
class="math inline">\(y_1\)</span> when it is considered alone: <span
class="math display">\[
p_1\left(y_1\right)=\int p\left(y_1, y_2\right) d y_2,
\]</span> and similarly for <span class="math inline">\(y_2\)</span>.
Then we define that <span class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span> are independent if and only if the
joint pdf is factorizable in the following way: <span
class="math display">\[
p\left(y_1, y_2\right)=p_1\left(y_1\right) p_2\left(y_2\right) .
\]</span> This definition extends naturally for any number <span
class="math inline">\(n\)</span> of random variables, in which case the
joint density must be a product of <span
class="math inline">\(n\)</span> terms.</p>
<h4 id="uncorrelated-variables-are-only-partly-independent">Uncorrelated
variables are only partly independent</h4>
<p>A weaker form of independence is uncorrelatedness. Two random
variables <span class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span> are said to be uncorrelated, if their
covariance is zero: <span class="math display">\[
E\left\{y_1 y_2\right\}-E\left\{y_1\right\} E\left\{y_2\right\}=0
\]</span> If the variables are independent, they are uncorrelated. On
the other hand, uncorrelatedness does not imply independence.</p>
<h4 id="gaussian-variables-are-forbidden">Gaussian variables are
forbidden</h4>
<p>The fundamental restriction in ICA is that the independent components
must be nongaussian for ICA to be possible. To see why gaussian
variables make ICA impossible, assume that the mixing matrix is
orthogonal and the <span class="math inline">\(s_i\)</span> are
gaussian. Then <span class="math inline">\(x_1\)</span> and <span
class="math inline">\(x_2\)</span> are gaussian, uncorrelated, and of
unit variance. Their joint density is given by <span
class="math display">\[
p\left(x_1, x_2\right)=\frac{1}{2 \pi} \exp
\left(-\frac{x_1^2+x_2^2}{2}\right)
\]</span> This distribution is illustrated in Fig. 7. The Figure shows
that the density is completely symmetric. Therefore, it does not contain
any information on the directions of the columns of the mixing matrix
<span class="math inline">\(\mathbf{A}\)</span>. This is why <span
class="math inline">\(\mathbf{A}\)</span> cannot be estimated.</p>
<p>More rigorously, one can prove that the distribution of any
orthogonal transformation of the gaussian <span
class="math inline">\(\left(x_1, x_2\right)\)</span> has exactly the
same distribution as <span class="math inline">\(\left(x_1,
x_2\right)\)</span>, and that <span class="math inline">\(x_1\)</span>
and <span class="math inline">\(x_2\)</span> are independent. Thus, in
the case of gaussian variables, we can only estimate the ICA model up to
an orthogonal transformation. In other words, the matrix A is not
identifiable for gaussian independent components. (Actually, if just one
of the independent components is gaussian, the ICA model can still be
estimated.)</p>
<h3 id="principle-of-ica-estimation">Principle of ICA estimation</h3>
<h4 id="nongaussian-is-independent">‚ÄúNongaussian is independent‚Äù</h4>
<p>The Central Limit Theorem, a classical result in probability theory,
tells that the distribution of a sum of independent random variables
tends toward a gaussian distribution, under certain conditions. Thus, a
sum of two independent random variables usually has a distribution that
is closer to gaussian than any of the two original random variables.</p>
<p>So our estimation is maximize nongaussian of <span
class="math inline">\(\mathbf{s}\)</span>.</p>
<h3 id="measure-of-nongaussianity">Measure of nongaussianity</h3>
<h4 id="kurtosis">kurtosis</h4>
<p>The classical measure of nongaussianity is kurtosis or the
fourth-order cumulant. The kurtosis of <span
class="math inline">\(y\)</span> is classically defined by <span
class="math display">\[
\operatorname{kurt}(y)=E\left\{y^4\right\}-3\left(E\left\{y^2\right\}\right)^2
\]</span> Actually, since we assumed that <span
class="math inline">\(y\)</span> is of unit variance, the right-hand
side simplifies to <span
class="math inline">\(E\left\{y^4\right\}-3\)</span>. This shows that
kurtosis is simply a normalized version of the fourth moment <span
class="math inline">\(E\left\{y^4\right\}\)</span>. For a gaussian <span
class="math inline">\(y\)</span>, the fourth moment equals <span
class="math inline">\(3\left(E\left\{y^2\right\}\right)^2\)</span>.
Thus, kurtosis is zero for a gaussian random variable. For most (but not
quite all) nongaussian random variables, kurtosis is nonzero.</p>
<h4 id="negentropy">Negentropy</h4>
<p>A second very important measure of nongaussianity is given by
negentropy. Negentropy is based on the informationtheoretic quantity of
(differential) entropy.</p>
<p>Entropy is the basic concept of information theory. The entropy of a
random variable can be interpreted as the degree of information that the
observation of the variable gives. The more ‚Äúrandom‚Äù, i.e.¬†unpredictable
and unstructured the variable is, the larger its entropy.</p>
<p>Entropy <span class="math inline">\(H\)</span> is defined for a
discrete random variable <span class="math inline">\(Y\)</span> as <span
class="math display">\[
H(Y)=-\sum_i P\left(Y=a_i\right) \log P\left(Y=a_i\right)
\]</span> where the <span class="math inline">\(a_i\)</span> are the
possible values of <span class="math inline">\(Y\)</span>.</p>
<p>This very well-known definition can be generalized for
continuous-valued random variables and vectors, in which case it is
often called differential entropy. The differential entropy <span
class="math inline">\(H\)</span> of a random vector <span
class="math inline">\(\mathbf{y}\)</span> with density <span
class="math inline">\(f(\mathbf{y})\)</span> is defined as: <span
class="math display">\[
H(\mathbf{y})=-\int f(\mathbf{y}) \log f(\mathbf{y}) \mathrm{d}
\mathbf{y} .
\]</span> A fundamental result of information theory is that a gaussian
variable has the largest entropy among all random variables of equal
variance. This means that entropy could be used as a measure of
nongaussianity. In fact, this shows that the gaussian distribution is
the ‚Äúmost random‚Äù or the least structured of all distributions. Entropy
is small for distributions that are clearly concentrated on certain
values, i.e., when the variable is clearly clustered, or has a pdf that
is very ‚Äúspiky‚Äù.</p>
<p>To obtain a measure of nongaussianity that is zero for a gaussian
variable and always nonnegative, one often uses a slightly modified
version of the definition of differential entropy, called negentropy.
Negentropy <span class="math inline">\(J\)</span> is defined as follows
<span class="math display">\[
J(\mathbf{y})=H\left(\mathbf{y}_{\text {gauss }}\right)-H(\mathbf{y})
\]</span> where <span class="math inline">\(\mathbf{y}_{\text {gauss
}}\)</span> is a Gaussian random variable of the same covariance matrix
as <span class="math inline">\(\mathbf{y}\)</span>. Due to the
above-mentioned properties, negentropy is always non-negative, and it is
zero if and only if <span class="math inline">\(\mathbf{y}\)</span> has
a Gaussian distribution. Negentropy has the additional interesting
property that it is invariant for invertible linear transformations
(Comon, 1994; Hyv√§rinen, <span class="math inline">\(1999
\mathrm{e}\)</span> ).</p>
<p>The advantage of using negentropy, or, equivalently, differential
entropy, as a measure of nongaussianity is that it is well justified by
statistical theory. In fact, negentropy is in some sense the optimal
estimator of nongaussianity, as far as statistical properties are
concerned. The problem in using negentropy is, however, that it is
computationally very difficult. <em>Estimating negentropy using the
definition would require an estimate (possibly nonparametric) of the
pdf.</em> Therefore, simpler approximations of negentropy are very
useful, as will be discussed next.</p>
<h5 id="approximations-of-negentropy">Approximations of negentropy</h5>
<ol type="1">
<li>Using higher-order moments, for example as follows (Jones and
Sibson, 1987):</li>
</ol>
<p><span class="math display">\[
J(y) \approx \frac{1}{12} E\left\{y^3\right\}^2+\frac{1}{48}
\operatorname{kurt}(y)^2
\]</span> The random variable <span class="math inline">\(y\)</span> is
assumed to be of zero mean and unit variance. However, the validity of
such approximations may be rather limited. In particular, these
approximations suffer from the nonrobustness encountered with
kurtosis.</p>
<ol start="2" type="1">
<li></li>
</ol>
<p><span class="math display">\[
J(y) \approx \sum_{i=1}^p
k_i\left[E\left\{G_i(y)\right\}-E\left\{G_i(v)\right\}\right]^2,
\label{neg_est}
\]</span> where <span class="math inline">\(k_i\)</span> are some
positive constants, and <span class="math inline">\(v\)</span> is a
Gaussian variable of zero mean and unit variance (i.e., standardized).
The variable <span class="math inline">\(y\)</span> is assumed to be of
zero mean and unit variance, and the functions <span
class="math inline">\(G_i\)</span> are some nonquadratic functions
(Hyv√§rinen, 1998b).</p>
<p>Note that even in cases where this approximation is not very
accurate, (12) can be used to construct a measure of nongaussianity that
is consistent in the sense that it is always non-negative, and equal to
zero if <span class="math inline">\(y\)</span> has a Gaussian
distribution. In the case where we use only one nonquadratic function
<span class="math inline">\(G\)</span>, the approximation becomes <span
class="math display">\[
J(y) \propto[E\{G(y)\}-E\{G(v)\}]^2
\]</span> for practically any non-quadratic function <span
class="math inline">\(G\)</span>. This is clearly a generalization of
the moment-based approximation in (23), if <span
class="math inline">\(y\)</span> is symmetric. Indeed, taking <span
class="math inline">\(G(y)=y^4\)</span>, one then obtains exactly (23),
i.e.¬†a kurtosis-based approximation. But the point here is that by
choosing <span class="math inline">\(G\)</span> wisely, one obtains
approximations of negentropy that are much better than the one given by
(23). In particular, choosing <span class="math inline">\(G\)</span>
that does not grow too fast, one obtains more robust estimators. The
following choices of <span class="math inline">\(G\)</span> have proved
very useful: <span class="math display">\[
G_1(u)=\frac{1}{a_1} \log \cosh a_1 u, \quad G_2(u)=-\exp \left(-u^2 /
2\right)
\label{eq_G}
\]</span> where <span class="math inline">\(1 \leq a_1 \leq 2\)</span>
is some suitable constant.</p>
<p>Thus we obtain approximations of negentropy that give a very good
compromise between the properties of the two classical nongaussianity
measures given by kurtosis and negentropy. They are conceptually simple,
fast to compute, yet have appealing statistical properties, especially
robustness.</p>
<h3 id="preprocessing-for-ica">PreProcessing for ICA</h3>
<ol type="1">
<li><p>Centering (zero mean)</p>
<p><span class="math display">\[
\tilde{\mathbf{x}} = ùê± - E\{ùê±\}
\]</span></p>
<p>Following will assume the mean of ùê± is zero;</p></li>
<li><p>Whitening (make the component of ùê± is uncorrelated)</p>
<ul>
<li><p>calculate covariance matrix <span class="math display">\[
  C = E\{ùê±ùê±_{T}\}
  \]</span></p></li>
<li><p>eigen-value decomposition</p>
<p><span class="math display">\[
  C = E\{ùê±ùê±_{T}\} = ùêÑùêÉùêÑ_{T}
  \]</span></p></li>
<li><p>Whitening <span class="math display">\[
  \tilde{\mathbf{x}}=\mathbf{E D}^{-1 / 2} \mathbf{E}^T \mathbf{x}
  \]</span></p></li>
<li><p>verify <span class="math display">\[
  E\left\{\tilde{\mathbf{x}} \tilde{\mathbf{x}}^T\right\}=\mathbf{I}
  \]</span></p></li>
</ul></li>
<li><p>further Processing Apply filter in signal x</p></li>
</ol>
<h3 id="the-fastica-algorithm">The FastICA Algorithm</h3>
<h4 id="fastica-for-one-unit">FastICA for one unit</h4>
<p>Here we use equation <span
class="math inline">\(\ref{neg_est}\)</span> two to maximize the
nongaussianity. The <span class="math inline">\(G\)</span> use equation
<span class="math inline">\(\ref{eq_G}\)</span>. The derivative of <span
class="math inline">\(\ref{eq_G}\)</span> is <span
class="math display">\[
\begin{array}{r}
g_1(u)=\tanh \left(a_1 u\right) \\
g_2(u)=u \exp \left(-u^2 / 2\right)
\end{array}
\]</span></p>
<p>The processure of FastICA</p>
<ol type="1">
<li><p>Choose an initial (e.g.¬†random) weight vector w.</p></li>
<li><p>Let <span class="math inline">\(\mathbf{w}^{+}=E\left\{\mathbf{x}
g\left(\mathbf{w}^T
\mathbf{x}\right)\right\}-E\left\{g^{\prime}\left(\mathbf{w}^T
\mathbf{x}\right)\right\} \mathbf{w}\)</span></p></li>
<li><p>Let <span class="math inline">\(\mathbf{w}=\mathbf{w}^{+}
/\left\|\mathbf{w}^{+}\right\|\)</span></p></li>
<li><p>If not converged, go back to 2 .</p></li>
</ol>
<p>Note that convergence means that the old and new values of <span
class="math inline">\(\mathbf{w}\)</span> point in the same direction,
i.e.¬†their dot-product is (almost) equal to 1 . It is not necessary that
the vector converges to a single point, since <span
class="math inline">\(\mathbf{w}\)</span> and <span
class="math inline">\(-\mathbf{w}\)</span> define the same direction.
This is again because the independent components can be defined only up
to a multiplicative sign. Note also that it is here assumed that the
data is prewhitened.</p>
<h4 id="fastica-for-serveral-unit">FastICA for serveral unit</h4>
<p>The one-unit algorithm of the preceding subsection estimaes just one
of the independent components, or one projection pursuit derection.</p>

    </div>
    
</div>
</div>

                <div id="footer-wrapper">
                    <footer class="footer"><footer class="row pl-3 pr-3">
    <div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0">
        <div class="footer-left">
            <p class="mb-0"> ¬© 2022 <a href="https://github.com/chen-gz">guangzong</a>. <span data-toggle="tooltip"
                    data-placement="top"
                    title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">All
                    rights reserved.</span></p>
        </div>
        <div class="footer-right">
            <p class="mb-0"> Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">hugo</a>
                with <a href="https://github.com/chen-gz/hugo_zong" target="_blank" rel="noopener">zong</a>
                theme.</p>
        </div>
    </div>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script> MathJax = {
            tex: {
                tags: 'ams',
            }
        };</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</footer>
</footer>
                </div>
            </div>
            <div id="panel-wrapper" class="d-flex flex-column"><div id="toc-wrapper" class="mb-5 pl-2">
    <span>Contents</span>
    <nav id="toc" data-toggle="toc">
        
        
        
    </nav>
</div>

<div id="access-lastmod" class="pb-4 pl-2">
    <div class="panel-heading">Recently Updated</div>
    <ul class="post-content pl-3 pb-1 ml-1 mt-2">
          
        <li><a href="https://zongpitt.com/papa-rudin/ch-8/2-product-measures/">2 Product Measures</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-8/3-the-fubini-theorem/">3 The Fubini Theorem</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-8/1-measurability-on-cartesian-products/">1 Measurability on Cartesian Products</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-7/3-differentiable-transformations/">Differentiable Transformations</a></li>
        
        <li><a href="https://zongpitt.com/papa-rudin/ch-7/1-derivatives-of-measures/">Derivatives of Measures</a></li>
        
    </ul>
</div>

<div id="access-tags" class="pl-2">
    <div class="panel-heading">Trending Tags</div>
    <div class="d-flex flex-wrap mt-3 mb-1 mr-3">
        
        <a class="post-tag" href="https://zongpitt.com/tags/algorithm/">Algorithm</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/android/">Android</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/c/">C</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/c-language/">c language</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/caddy/">Caddy</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/cadence/">Cadence</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/cdc/">CDC</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/classification/">classification</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/compress/">Compress</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/conf/">Conf</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/double-column/">Double Column</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/errno13/">Errno13</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/fish/">fish</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/gan/">GAN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/git/">Git</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/gtk/">Gtk</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/html/">Html</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/i3/">I3</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/idea/">Idea</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/java/">Java</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/latex/">Latex</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/lda/">LDA</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/linear-programming/">Linear Programming</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/linux/">Linux</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/lr/">LR</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/math/">Math</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mathematics/">Mathematics</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/matlab/">Matlab</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/matplot/">matplot</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/memory/">Memory</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mermaid/">Mermaid</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/misc/">Misc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/ml/">ML</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/mosfet/">Mosfet</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/network/">Network</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/nn/">NN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/pandoc/">Pandoc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/pca/">PCA</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/probability/">Probability</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/productive/">Productive</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/program/">Program</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/programming/">Programming</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/python/">Python</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/qt/">Qt</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/random-variable/">Random Variable</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/rcnn/">RCNN</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/rust/">Rust</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/security/">Security</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/software/">Software</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/std/">Std</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/svd/">Svd</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/tool/">Tool</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/usb/">USB</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vim/">Vim</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vpn/">Vpn</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/vpnc/">Vpnc</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/xterm/">xterm</a>
        
        <a class="post-tag" href="https://zongpitt.com/tags/zsh/">Zsh</a>
        
    </div>
</div>
</div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
    <div class="col-12 col-sm-11 post-content">
        <div id="search-hints"></div>
        <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3 col-12"></div>
    </div>
</div>

<script src="/js/fuse.min.js"></script>
<script src="/js/fastsearch.js"></script>


</div>
        <div id="mask"></div>
        <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"
            style="display: none">
            <i class="fas fa-angle-up"></i>
        </a>
    </div>
    
</body>

</html>
