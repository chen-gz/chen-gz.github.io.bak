---
author: zong
date: 2021-05-18
title: t-SNE
categories: [ML]
math: true
---

## Introduction

The content is from *Visualizing Data using t-SNE* which proposed by Lauren and Hinton.
This method is mainly focus on visualizing data. 
It used a none lienar method to map high dimension data to low dimension data.
Then it will be easy for human to tell the relationship between data and verify our algorithm.
This post will only introduce he general idea of t-SNE.
The trick and implement detail may not explain in this post.

## Basic idea

The t-SNE is a variation of Stochastic Neighbor Embedding(SNE).
The basic idea is using propability to decribe the dataset. 
When dataset map from high dimensino to low dimension, the probability of the dataset will keep same.
How to use probability to describe the data set will expalin later.
After create probability description for dataset, the aim is keep the description same when map high dimension dataset to low dimension dataset. 
Keep the description same can be done by minimize the cost function, the cost function used in SNE is Kullback-Leibler divergence.
That's all!

Next let's go to detain one by one.

## propobility (similarity) description

Let's consider 3 points, $x$, $y$, $z$. Suppose $x$ and $y$ are close to each other and far away from $z$. 
From intuition, we will say $x$ and $y$ are very similar, $x$ and $z$ is differnt. 
This intuition is based Euclid distance.
For SNE, it difine probabity to descripe this intuition instead using distance.

Let's look most simple version of similarity.
For given a two point $i$ and $j$, we can tell similarity between two point by using Gaussian distribution.

$$
p(i|j) = p(j|i) = e^{-\frac{\|x\_i-y\_j\|^2}{\sigma^2}} 
$$

where $\sigma$ is variance of Gaussian distribution. 

But there is a problem here, $\sum\_{j} p(j|i) \neq 1$ this is not accept for a probability model.
So if we are using probabilty model to describe the dataset, this problem should be fixed.
Base on this, we updata our definition. For this time, we only define $p(j|i)$.

$$
p(j|i) = \frac{e^{-\frac{\|x_i-x_j\|^2}{\sigma^2}}}{\sum_{k \neq i} e^{-\frac{\|x_k - x_i\|^2}{\sigma^2}}}
$$

Now satisfy $\sum_j p(j|i) = 1$.

Up to now, I explian how to define probility of dataset.

The t-SNE use different definition, it will explain later. 

## Distance description

The idae of map from high dimension to low dimension is keep probability description same.
For example, in high dimension we have probabilty description for point $i$ and $j$, $p(j|i)$ and in low dimension we have probabilty description for point $i$ and $j$, $q(j|i)$. 
What we want is they are close to each other. 
So the problem become minimize the diverge between $p(j|i)$ and $q(j|i)$.
In SNE, kullback-leibler divergence used to description the diverge between $p(j|i)$ and $q(j|i)$.

Now, we have the cost function

$$
C = \sum\_{i} KL(P\_i||Q\_i) = \sum\_i \sum\_j p_{j|i} \log \frac{p\_{j|i}}{q\_{j|i}}
$$

minimize this cost function. Everything is done here.

## Problems of SNE

The mothod description above does not work very well sometime. That's why it has been improve. 

* the probability of data does not symmetric. This means $p(i|j) \neq p(j|i)$. 
This does not fit intuition and it does not work well some time. 
And it also will make some data much more important to other.
* Crowding Problem. Can be solve by using t-distribution.

## Procedure of t-SNE (simplified version)







