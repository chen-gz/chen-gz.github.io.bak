<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Programming on Guangzong Blog</title>
    <link>https://zongpitt.com/tags/linear-programming/</link>
    <description>Recent content in Linear Programming on Guangzong Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 08 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://zongpitt.com/tags/linear-programming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>凸规划的一些想法</title>
      <link>https://zongpitt.com/posts/2020-11-08-%E5%85%B3%E4%BA%8E%E5%87%B8%E8%A7%84%E5%88%92%E7%9A%84%E6%83%B3%E6%B3%95/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-11-08-%E5%85%B3%E4%BA%8E%E5%87%B8%E8%A7%84%E5%88%92%E7%9A%84%E6%83%B3%E6%B3%95/</guid>
      <description>凸规划是指在凸函数上的寻找最小值或者最大值。
一元函数的最值和极值 凸规划的基本想法来源于使用函数导数求函数极值。在一元函数中我们通过通过寻找导数值为0的点来寻找极大值和极小值。 \[ \frac{df(x)}{dx} = 0 \] 导数值为0的点可以确定为极值，但无法确定是极大值还是极小值。 因此通过二阶导数正负来确定是哪一种。 \[ \text{在极大值处有 } \frac{d^2f(x)}{dx^2} &amp;lt; 0 \\ \text{在极小值处有 }\frac{d^2f(x)}{dx^2} &amp;gt; 0 \\ \text{驻点 非极值点} \frac{d^2f(x)}{dx^2} = 0 \] 在凸函数上，极值将退化为最值。
关于一元函数的极值可以理解为：函数f(x)在x方向上的变化量为0的点为极值点（可能是驻点）。
多元函数的最值和极值 多元函数的极值和最值可以理解为一元函数的推广：多元函数的极值点在所有方向的上的变化量都为0.。在函数连续可微的情况下，这个条件可以退化成为：在基向量方向上，函数的变化量为0；
例如，二元函数 \(f(x,y)\) 有一点\((x_0, y_0)\) 在方向\((0,1)\)和方向\((1,0)\)上的变化量为都0，可以推出函数\(f(x,y)\)在点\((x_0, y_0)\)上的所有方向变化量都为0。
表达式为 \[ \frac{df(\mathbf{x})}{d\mathbf{x^T}} = 0 \]
\[ \left[\begin{array}{c} \frac{\partial f}{\partial x_{1}} \\ \frac{\partial f}{\partial x_{2}} \\ \vdots \\ \frac{\partial f}{\partial x_{n}} \end{array}\right]=0 \]
这就是所谓的一阶必要条件(first order necessary condition)。
和一元函数类似，如果函数\(f(x,y)\) 有一点\((x_0,y_0)\)的hessian矩阵是positive defined（正定的），该点为极小值点；反之，为极大值点。此为二阶必要条件。如果既不是正定也不是负定，那该点为驻点。
draft 凸规划得定理大多数可以有一元函数得全导数扩展。</description>
    </item>
    
    <item>
      <title>constrain optimization</title>
      <link>https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions-constain/</link>
      <pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions-constain/</guid>
      <description>TANGENT PLANE Definition. A point \(x^{*}\) satisfying the constraint \(\mathbf{h}\left(\mathbf{x}^{*}\right)=\mathbf{0}\) is said to be a regular point of the constraint if the gradient vectors \(\nabla h_{1}\left(\mathbf{x}^{*}\right), \nabla h_{2}\left(\mathbf{x}^{*}\right), \ldots, \nabla h_{m}\left(\mathbf{x}^{*}\right)\) are linearly independent.
Theorem. At a regular point \(\mathbf{x}^{*}\) of the surface \(S\) defined by \(\mathbf{h}(x)=0\) the tangent plane is equal to \[ M=\left\{y: \nabla h\left(x^{*}\right) y=0\right\} \]
FIRST-ORDER NECESSARY CONDITIONS (EQUALITY CONSTRAINTS) Lemma. Let \(\mathbf{x}^{*}\) be a regular point of the constraints \(\mathbf{h}(\boldsymbol{x})=\boldsymbol{0}\) and a local extreme point ( \(a\) minimum or maximum) of \(f\) subject to these constraints.</description>
    </item>
    
    <item>
      <title>Conjugate Direction Method</title>
      <link>https://zongpitt.com/posts/2020-11-02-conjugate-direction-method/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-11-02-conjugate-direction-method/</guid>
      <description>CONJUGATE DIRECTIONS Definition. Given a symmetric matrix \(\mathbf{Q}\), two vectors \(\mathbf{d}_{1}\) and \(\mathbf{d}_{2}\) are said to be \(\mathbf{Q}\) -orthogonal, or conjugate with respect to \(\mathbf{Q},\) if \(\mathbf{d}_{1}^{T} \mathbf{Q} \mathbf{d}_{2}=0 .\)
Proposition. If \(\mathbf{Q}\) is positive definite and the set of nonzero vectors \(\mathrm{d}_{0}, \mathrm{d}_{1}, \mathrm{d}_{2}, \ldots, \mathrm{d}_{k}\) are \(\mathbf{Q}\)-orthogonal, then these vectors are linearly independent.
Conjugate Direction Theorem Let \(\left\{\mathbf{d}_{i}\right\}_{i=0}^{n-1}\) be a set of nonzero \(\mathbf{Q}\) -orthogonal vectors. For any \(\mathbf{x}_{0} \in E^{n}\) the sequence \(\left\{\mathbf{x}_{\mathbf{k}}\right\}\) generated according to \[ \begin{equation} \mathbf{x}_{k+1}=\mathbf{x}_{k}+\alpha_{k} \mathbf{d}_{k}, k \geqslant 0 \end{equation} \] with \[ \begin{equation} \alpha_{k}=-\frac{\mathbf{g}_{k}^{T} \mathbf{d}_{k}}{\mathbf{d}_{k}^{T} \mathbf{Q} \mathbf{d}_{\mathbf{k}}} \end{equation} \] and \[ \mathrm{g}_{k}=\mathbf{Q} \mathbf{x}_{k}-\mathbf{b} \] converges to the unique solution, \(\mathbf{x}^{*},\) of \(\mathbf{Q} \mathbf{x}=\mathbf{b}\) after \(n\) steps, that is, \(\mathbf{x}_{n}=\mathbf{x}^{*}\)</description>
    </item>
    
    <item>
      <title>Convex And Concave Functions</title>
      <link>https://zongpitt.com/posts/2020-10-24-convex-and-concave-function/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-10-24-convex-and-concave-function/</guid>
      <description>Convex definition Definition. A function \(f\) defined on a convex set \(\Omega\) is said to be convex if, for every \(\mathbf{x}_{1}, \mathbf{x}_{2} \in\) \(\Omega\) and every \(\alpha, 0 \leqslant \alpha \leqslant 1,\) there holds
\[ \begin{equation} f\left(\alpha \mathbf{x}_{1}+(1-\alpha) \mathbf{x}_{2}\right) \leqslant \alpha f\left(\mathbf{x}_{1}\right)+(1-\alpha) f\left(\mathbf{x}_{2}\right) \end{equation} \]
If, for every \(\alpha, 0&amp;lt;\alpha&amp;lt;1,\) and \(\mathbf{x}_{1} \neq \mathbf{x}_{2},\) there holds
\[ \begin{equation} f\left(\alpha \mathbf{x}_{1}+(1-\alpha) \mathbf{x}_{2}\right)&amp;lt;\alpha f\left(\mathbf{x}_{1}\right)+(1-\alpha) f\left(\mathbf{x}_{2}\right) \end{equation} \]
then \(f\) is said to be strictly convex.</description>
    </item>
    
    <item>
      <title>First Order necessary condition</title>
      <link>https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-10-24-first-order-necessary-conditions/</guid>
      <description>Proposition 1. (First-order necessary conditions). Let \(\Omega\) be a subset of \(E^{n}\) and let \(f \in C^{1}\) be a function on \(\Omega\). If \(\mathbf{x}^{*}\) is a relative minimum point of \(f\) over \(\Omega,\) then for any \(\mathbf{d} \in E^{n}\) that is a feasible direction at \(\mathbf{x}^{*}\), we have \(\nabla f\left(\mathbf{x}^{*}\right) \mathbf{d} \geqslant 0 .\)
Corollary. (Unconstrained case). Let \(\Omega\) be a subset of \(E^{n},\) and let \(f \in C^{1}\) be function on \(\Omega\).</description>
    </item>
    
    <item>
      <title>Second Order necessary condition</title>
      <link>https://zongpitt.com/posts/2020-10-24-second-order-necessary-conditions/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-10-24-second-order-necessary-conditions/</guid>
      <description>Proposition 1. (Second-order necessary conditions). Let \(\Omega\) be a subset of \(E^{n}\) and let \(f \in C^{2}\) be a function on \(\Omega\). If \(\mathbf{x}^{*}\) is a relative minimum point of \(f\) over \(\Omega,\) then for any \(\mathbf{d} \in E^{n}\) that is a feasible direction at \(\mathbf{x}^{*}\) we have
\(\nabla f\left(\mathbf{x}^{*}\right) \mathbf{d} \geqslant 0\) if \(\nabla f\left(\mathbf{x}^{*}\right) \mathbf{d}=0,\) then \(\mathbf{d}^{T} \nabla^{2} f\left(\mathbf{x}^{*}\right) \mathbf{d} \geqslant 0\) Proposition 2. (Second-order necessary conditions-unconstrained case). Let \(\mathbf{x}^{*}\) be an interior point of the set \(\Omega,\) and suppose \(\mathbf{x}^{*}\) is a relative minimum point over \(\Omega\) of the function \(f \in C^{2} .</description>
    </item>
    
    <item>
      <title>A linear Programming Standard Form Problem</title>
      <link>https://zongpitt.com/posts/2020-08-29-a-linear-programming-standard-form-problem/</link>
      <pubDate>Sat, 29 Aug 2020 17:43:00 -0400</pubDate>
      
      <guid>https://zongpitt.com/posts/2020-08-29-a-linear-programming-standard-form-problem/</guid>
      <description>Problem from “Linear and Nonlinear Programing” problem 2.9
Linear programming Standard Form The standard from of linear programming is
\[ \begin{equation} \begin{array}{cl} \operatorname{minimize} &amp;amp; \mathbf{c}_{1}^{T} \mathbf{x}\\ \text { subject to } &amp;amp; \mathbf{Ax} = \mathbf{b} \\ &amp;amp; \mathbf{x} \geq 0 \end{array} \end{equation} \]
Description of Problem A class of piecewise linear functions can be represented as \(f(x)=\) Maximum \((\mathbf{c}_{1}^{T} \mathbf{x}+ d_{1}, \mathbf{c}_{2}^{T} \mathbf{x}+d_{2}, \ldots, \mathbf{c}_{p}^{T} \mathbf{x}+d_{p}).\) For such a function \(f\), consider the problem</description>
    </item>
    
  </channel>
</rss>
